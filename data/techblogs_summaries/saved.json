{"https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/": [{"text": "NVIDIA AI Workbench, now in beta, aims to streamline how enterprise developers create, share, and scale AI and machine learning projects. It allows developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. The beta release includes new features such as simplified setup and installation on Windows, Ubuntu, and macOS, expanded support for container runtimes and Git servers, and new base images for project creation. The article walks through a coding copilot reference example using AI Workbench to fine-tune a generative AI model on a GPU system. Key concepts include AI Workbench Projects, fine-tuning methods such as Quantized Low Rank Adaptation (QLoRA), and a walkthrough of a Mistral 7B fine-tuning project. AI Workbench helps simplify the process of developing and deploying AI models by providing an intuitive user experience, streamlined configuration, and automation for handling Git and container-based developer environments.", "text_components": ["Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta\nNVIDIA AI Workbench is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning (ML) projects. Announced at SIGGRAPH 2023, NVIDIA AI Workbench enables developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. To learn more, see Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench.\nThis post explains how NVIDIA AI Workbench helps streamline the AI workflow and details new features of the beta release. It also walks through a coding copilot reference example, which enables you to use AI Workbench to create, test, and customize a pretrained generative AI model on your platform of choice.", "What is NVIDIA AI Workbench?\nWith AI Workbench, developers and data scientists have the flexibility to start an AI or ML project locally on a PC or workstation and then migrate it anywhere. Projects can be pushed out to a data center, public cloud, or NVIDIA DGX Cloud, or moved to a local RTX PC or workstation for inference and lightweight customization, depending on project requirements.\nAI Workbench helps developers simplify and shorten setup, development, and migration for AI workflows by providing the ability to work on their choice of heterogeneous compute resources. Benefits include:\nFree and quick install on the system of choice with an intuitive UX or CLI for project creation and management.\nStreamlined configuration for compute resources and runtimes, providing reproducibility and flexibility to work on different GPU resources.\nSimplified version control and management for containers and Git repositories and integrations with GitHub, GitLab, and the NVIDIA NGC catalog.\nAutomation and streamlining to handle Git and container-based developer environments, enabling users to work on their choice of system, laptop, workstation, server, or the cloud.\nReproducibility across users and systems with transparent handling for idiosyncrasies like credentials, secrets, and file system changes without the overhead.\nScalable creation and distribution of complex workflows and applications for generative AI, GPU-enabled ML, and data science.", "What\u2019s new in the beta release\nThe AI Workbench beta release includes the following exciting new features, with updates to the user interface and expanded support for container runtimes and Git servers.\nSimplified setup and installation on Windows 11, Ubuntu, 22.04, and macOS 11 or higher.\nInstall AI Workbench quickly in two ways: click-through install using the desktop app on local systems or command-line install on remote systems.\nWork from anywhere with support for the three major operating systems for a uniform experience. AI Workbench runs on Windows distributions that support WSL2, Ubuntu 22.04, and macOS version 11 and higher.\nSimplified version control and streamlined development with containerized environments.\nAccess simple and comprehensive Git-compliant version control with both the Desktop App and CLI. Push, pull, and fetch features are now included.\nCreate a containerized JupyterLab environment with isolation and reproducibility without having to handle details.\nChoose from two container runtime options: Docker or Podman.\nExpanded feature parity between the user interface and the CLI.\nSee commit history and summaries directly in the Desktop App.\nView improved container state and application status notifications in the Desktop App.\nExpanded default base images.\nAccess three new base images for project creation, in addition to the Python Basic and PyTorch Basic images already in the NGC catalog. New base images for CUDA 11.0, CUDA 12.0, and CUDA 12.2 provide the foundation for further customization.\nThree new example projects for reference.\nMistral: Fine-tune a Mistral 7B large language model (LLM) on a custom code instructions dataset using QLoRA PEFT.\nRAG: Converse with your data using a local, user-friendly developer workflow for retrieval-augmented generation (RAG).\nNeMotron-3: Fine-tune a Nemotron-3 8B LLM on a custom QA dataset using NVIDIA NeMo.", "Create your own coding copilot\nThis section walks through an example of how AI Workbench can significantly simplify the process of using and fine-tuning a generative AI model on a GPU system of the user\u2019s choice.", "Key concepts\nA few key concepts used in this example are outlined below.", "AI Workbench Project\nAn AI Workbench Project is a Git repository that contains a set of configuration files that can be read by AI Workbench to automate the creation and management of a containerized development environment. A project references everything needed for a configured, containerized development environment and includes:\nCode, data, and models\nSimple configuration files that drive AI Workbench automation for container customization and package installation\nA project specification metadata file to wrap the repository in a way that\u2019s compatible with AI Workbench\nVisit NVIDIA on GitHub to reference NVIDIA projects that provide starting points for adapting your own data and use cases. Additionally, AI Workbench early access members can contribute and use third-party community project examples.\nThe Mistral 7B fine-tuning reference project showcased in this post highlights how to leverage the power of AI Workbench to build a basic coding copilot on a system of your choice.", "Fine-tuning\nWhile Mistral 7B is a strong baseline for multiple downstream tasks, it can lack domain-specific knowledge based on proprietary or otherwise sensitive information. Fine-tuning is used to improve the model\u2019s responses in these cases.\nThere are two versions of fine-tuning. The first, full fine-tuning, uses the new data to update all of the model weights. This can improve domain-specific results but often requires more time and larger, more expensive GPUs. The second, parameter efficient fine-tuning (PEFT), is a family of techniques that update a subset of the model weights. PEFT is often preferable to full fine-tuning because it produces comparable results in far less time and with smaller, less expensive GPUs.\nThis example focuses primarily on the Quantized Low Rank Adaptation (QLoRA) method of PEFT. Low Rank Adaptation (LoRA) is a method of PEFT that uses smaller weight matrices in the retraining as approximations instead of updating the full weight matrix. This rank decomposition optimization technique enables greater memory efficiency and can reduce the GPU size required for successful fine-tuning.\nQLoRA is a further optimization that reduces the precision of model weights to provide even greater advances in memory and space efficiency. The most common quantization used for this LoRA fine-tuning workflow is 4-bit quantization, which provides a decent balance between model performance and fine-tuning feasibility.", "Walkthrough of Mistral 7B fine-tuning project in NVIDIA AI Workbench\nThis walkthrough includes high-level code and details. For more information, see the full Mistral 7B fine-tuning reference project on GitHub. The project fine-tunes the Mistral 7B base model on the TokenBender code instructions dataset, consisting of 122K Alpaca-style code instructions and code solutions.\nScreenshot of the Mistral 7B fine-tuning project in the NVIDIA AI Workbench user interface.\nFigure 1. Building the Mistral 7B fine-tuning project in NVIDIA AI Workbench\nFirst, download the data and split it into 80% training, 10% validation, and 10% testing datasets. One entry of an instruction in the dataset is shown below as an example:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction: Output the maximum element in an array. ### Input: [1, 5, 10, 8, 15] ### Output: 15\n```\nNext, download the Mistral 7B model weights to this project:\n```\nmodel_id = \"mistralai/Mistral-7B-v0.1\"\nbb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bb_config)\n```\nNotice that the 4-bit quantization configuration is specified for the base model.\nNext, evaluate the performance of the base model on a specific sample programming question. This establishes a baseline for comparison between the base model and the final, fine-tuned model.\n```\nbase_prompt = \"\"\"Write a function to output the prime factorization of 2023 in python, C, and C++\"\"\"\n\nbase_tokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n    add_bos_token=True,\n)\n\nmodel_input = base_tokenizer(base_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(base_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))\n\n***** Output ***** \n\n## Prime Factorization of 2023\n\nThe prime factorization of 2023 is 13 x 157.\n\n## Prime Factorization of 2023 in Python\n\nThe prime factorization of 2023 in python is given below.\n\ndef prime_factorization(n):\n    factors = []\n    for i in range(2, n + 1):\n        if n % i == 0:\n            factors.append(i)\n    return factors\n\nprint(prime_factorization(2023))\n\n...\n```\nNotice that the base model doesn\u2019t perform well out of the box. First, the base model seems to think the prime factorization of 2,023 is 13 x 157. This amounts to 2041. The actual answer is 7 x 17 x 17.\nSecond, the Python function the model outputs is incorrect as well. Running the suggested code gives an answer of [7, 17, 119, 289, 2,023] when in fact 119, 289, and 2,023 are not prime factors.\nFine-tuning is necessary to improve model performance. Begin with preprocessing the dataset by reformatting the dataset entries to better fit the instruction prompt [INST] for fine-tuning. Then tokenize each of these prompts.\nNext, specify the configuration for QLoRA fine-tuning and perform the fine-tuning. By default, the fine-tuning takes 1,000 iterations, with checkpointing and evaluation every 50 steps. These hyperparameters can be adjusted depending on hardware resource constraints. On an NVIDIA A100 80 GB GPU system, this configuration can take about 6.5 hours.\n```\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\n\n# Training configs\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_val_ds,\n    args=transformers.TrainingArguments(\n        output_dir=\"./mistral-code-instruct\",\n        warmup_steps=5,\n        per_device_train_batch_size=2,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=1000,\n        learning_rate=2.5e-5,\n        logging_steps=50,\n        bf16=True,\n        optim=\"paged_adamw_8bit\",\n        logging_dir=\"./logs\",\n        save_strategy=\"steps\",\n        save_steps=50,\n        evaluation_strategy=\"steps\", \n        eval_steps=50,\n        report_to=\"none\",\n        do_eval=True,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# Train! \ntrainer.train()\n```\nUsing the final fine-tuning checkpoint, define the updated Mistral 7B model:\n```\nft_model = PeftModel.from_pretrained(base_model, \"mistral-code-instruct/checkpoint-1000\")\n```\nTo evaluate the fine-tuned model\u2019s performance, ask a coding question similar to the initial one and request the generation of a code snippet:\n```\neval_prompt = f\"\"\"\nFor a given integer n, print out all its prime factors one on each line. \nn = 30\n\"\"\"\n\ninput_ids = tokenizer(eval_prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\noutputs = ft_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True, top_p=0.9,temperature=0.5)\n\n***** Output ***** \n\nGenerated response:\n2\n3\n5 \n\n#include <stdio.h>\n\nint main() {\n    int n = 30;\n    int i;\n    for (i = 2; i <= n; i++) {\n        while (n % i == 0) {\n            printf(\"%d\\n\", i);\n            n /= i;\n        }\n    }\n    return 0;\n}\n\n...\n```\nThe generated code snippet response from the fine-tuned model looks much better. Use a sandbox environment to try the code for yourself.\nThat\u2019s all there is to fine-tuning the Mistral 7B LLM. This project provides a reference workflow for your development needs. You can always choose to customize the project to better suit your enterprise data or use case. Switch out the dataset with one of your own, or fine-tune the model to another use case, such as text summarization or question-answering.", "Get started with AI Workbench\nNVIDIA AI Workbench helps you create, share, and scale enterprise AI and ML workflows between different GPU-enabled environments. Sign up for beta access to NVIDIA AI Workbench. To learn more about AI Workbench, check out these resources:\nWatch a video demo of AI Workbench that walks through a custom image generation example project with Stable Diffusion XL.\nRead Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench to get a sneak peek of more example projects.\nReference the NVIDIA AI Workbench User Guide to get your AI and ML projects up and running with NVIDIA AI Workbench."], "document_title": "Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta", "document_url": "https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/", "document_date": "2024-01-30T20:02:55", "document_date_modified": "2024-01-31T01:06:02", "document_full_text": "Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta\nNVIDIA AI Workbench is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning (ML) projects. Announced at SIGGRAPH 2023, NVIDIA AI Workbench enables developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. To learn more, see Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench.\nThis post explains how NVIDIA AI Workbench helps streamline the AI workflow and details new features of the beta release. It also walks through a coding copilot reference example, which enables you to use AI Workbench to create, test, and customize a pretrained generative AI model on your platform of choice.\nWhat is NVIDIA AI Workbench?\nWith AI Workbench, developers and data scientists have the flexibility to start an AI or ML project locally on a PC or workstation and then migrate it anywhere. Projects can be pushed out to a data center, public cloud, or NVIDIA DGX Cloud, or moved to a local RTX PC or workstation for inference and lightweight customization, depending on project requirements.\nAI Workbench helps developers simplify and shorten setup, development, and migration for AI workflows by providing the ability to work on their choice of heterogeneous compute resources. Benefits include:\nFree and quick install on the system of choice with an intuitive UX or CLI for project creation and management.\nStreamlined configuration for compute resources and runtimes, providing reproducibility and flexibility to work on different GPU resources.\nSimplified version control and management for containers and Git repositories and integrations with GitHub, GitLab, and the NVIDIA NGC catalog.\nAutomation and streamlining to handle Git and container-based developer environments, enabling users to work on their choice of system, laptop, workstation, server, or the cloud.\nReproducibility across users and systems with transparent handling for idiosyncrasies like credentials, secrets, and file system changes without the overhead.\nScalable creation and distribution of complex workflows and applications for generative AI, GPU-enabled ML, and data science.\nWhat\u2019s new in the beta release\nThe AI Workbench beta release includes the following exciting new features, with updates to the user interface and expanded support for container runtimes and Git servers.\nSimplified setup and installation on Windows 11, Ubuntu, 22.04, and macOS 11 or higher.\nInstall AI Workbench quickly in two ways: click-through install using the desktop app on local systems or command-line install on remote systems.\nWork from anywhere with support for the three major operating systems for a uniform experience. AI Workbench runs on Windows distributions that support WSL2, Ubuntu 22.04, and macOS version 11 and higher.\nSimplified version control and streamlined development with containerized environments.\nAccess simple and comprehensive Git-compliant version control with both the Desktop App and CLI. Push, pull, and fetch features are now included.\nCreate a containerized JupyterLab environment with isolation and reproducibility without having to handle details.\nChoose from two container runtime options: Docker or Podman.\nExpanded feature parity between the user interface and the CLI.\nSee commit history and summaries directly in the Desktop App.\nView improved container state and application status notifications in the Desktop App.\nExpanded default base images.\nAccess three new base images for project creation, in addition to the Python Basic and PyTorch Basic images already in the NGC catalog. New base images for CUDA 11.0, CUDA 12.0, and CUDA 12.2 provide the foundation for further customization.\nThree new example projects for reference.\nMistral: Fine-tune a Mistral 7B large language model (LLM) on a custom code instructions dataset using QLoRA PEFT.\nRAG: Converse with your data using a local, user-friendly developer workflow for retrieval-augmented generation (RAG).\nNeMotron-3: Fine-tune a Nemotron-3 8B LLM on a custom QA dataset using NVIDIA NeMo.\nCreate your own coding copilot\nThis section walks through an example of how AI Workbench can significantly simplify the process of using and fine-tuning a generative AI model on a GPU system of the user\u2019s choice.\nKey concepts\nA few key concepts used in this example are outlined below.\nAI Workbench Project\nAn AI Workbench Project is a Git repository that contains a set of configuration files that can be read by AI Workbench to automate the creation and management of a containerized development environment. A project references everything needed for a configured, containerized development environment and includes:\nCode, data, and models\nSimple configuration files that drive AI Workbench automation for container customization and package installation\nA project specification metadata file to wrap the repository in a way that\u2019s compatible with AI Workbench\nVisit NVIDIA on GitHub to reference NVIDIA projects that provide starting points for adapting your own data and use cases. Additionally, AI Workbench early access members can contribute and use third-party community project examples.\nThe Mistral 7B fine-tuning reference project showcased in this post highlights how to leverage the power of AI Workbench to build a basic coding copilot on a system of your choice.\nFine-tuning\nWhile Mistral 7B is a strong baseline for multiple downstream tasks, it can lack domain-specific knowledge based on proprietary or otherwise sensitive information. Fine-tuning is used to improve the model\u2019s responses in these cases.\nThere are two versions of fine-tuning. The first, full fine-tuning, uses the new data to update all of the model weights. This can improve domain-specific results but often requires more time and larger, more expensive GPUs. The second, parameter efficient fine-tuning (PEFT), is a family of techniques that update a subset of the model weights. PEFT is often preferable to full fine-tuning because it produces comparable results in far less time and with smaller, less expensive GPUs.\nThis example focuses primarily on the Quantized Low Rank Adaptation (QLoRA) method of PEFT. Low Rank Adaptation (LoRA) is a method of PEFT that uses smaller weight matrices in the retraining as approximations instead of updating the full weight matrix. This rank decomposition optimization technique enables greater memory efficiency and can reduce the GPU size required for successful fine-tuning.\nQLoRA is a further optimization that reduces the precision of model weights to provide even greater advances in memory and space efficiency. The most common quantization used for this LoRA fine-tuning workflow is 4-bit quantization, which provides a decent balance between model performance and fine-tuning feasibility.\nWalkthrough of Mistral 7B fine-tuning project in NVIDIA AI Workbench\nThis walkthrough includes high-level code and details. For more information, see the full Mistral 7B fine-tuning reference project on GitHub. The project fine-tunes the Mistral 7B base model on the TokenBender code instructions dataset, consisting of 122K Alpaca-style code instructions and code solutions.\nScreenshot of the Mistral 7B fine-tuning project in the NVIDIA AI Workbench user interface.\nFigure 1. Building the Mistral 7B fine-tuning project in NVIDIA AI Workbench\nFirst, download the data and split it into 80% training, 10% validation, and 10% testing datasets. One entry of an instruction in the dataset is shown below as an example:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction: Output the maximum element in an array. ### Input: [1, 5, 10, 8, 15] ### Output: 15\n```\nNext, download the Mistral 7B model weights to this project:\n```\nmodel_id = \"mistralai/Mistral-7B-v0.1\"\nbb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bb_config)\n```\nNotice that the 4-bit quantization configuration is specified for the base model.\nNext, evaluate the performance of the base model on a specific sample programming question. This establishes a baseline for comparison between the base model and the final, fine-tuned model.\n```\nbase_prompt = \"\"\"Write a function to output the prime factorization of 2023 in python, C, and C++\"\"\"\n\nbase_tokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n    add_bos_token=True,\n)\n\nmodel_input = base_tokenizer(base_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(base_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))\n\n***** Output ***** \n\n## Prime Factorization of 2023\n\nThe prime factorization of 2023 is 13 x 157.\n\n## Prime Factorization of 2023 in Python\n\nThe prime factorization of 2023 in python is given below.\n\ndef prime_factorization(n):\n    factors = []\n    for i in range(2, n + 1):\n        if n % i == 0:\n            factors.append(i)\n    return factors\n\nprint(prime_factorization(2023))\n\n...\n```\nNotice that the base model doesn\u2019t perform well out of the box. First, the base model seems to think the prime factorization of 2,023 is 13 x 157. This amounts to 2041. The actual answer is 7 x 17 x 17.\nSecond, the Python function the model outputs is incorrect as well. Running the suggested code gives an answer of [7, 17, 119, 289, 2,023] when in fact 119, 289, and 2,023 are not prime factors.\nFine-tuning is necessary to improve model performance. Begin with preprocessing the dataset by reformatting the dataset entries to better fit the instruction prompt [INST] for fine-tuning. Then tokenize each of these prompts.\nNext, specify the configuration for QLoRA fine-tuning and perform the fine-tuning. By default, the fine-tuning takes 1,000 iterations, with checkpointing and evaluation every 50 steps. These hyperparameters can be adjusted depending on hardware resource constraints. On an NVIDIA A100 80 GB GPU system, this configuration can take about 6.5 hours.\n```\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\n\n# Training configs\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_val_ds,\n    args=transformers.TrainingArguments(\n        output_dir=\"./mistral-code-instruct\",\n        warmup_steps=5,\n        per_device_train_batch_size=2,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=1000,\n        learning_rate=2.5e-5,\n        logging_steps=50,\n        bf16=True,\n        optim=\"paged_adamw_8bit\",\n        logging_dir=\"./logs\",\n        save_strategy=\"steps\",\n        save_steps=50,\n        evaluation_strategy=\"steps\", \n        eval_steps=50,\n        report_to=\"none\",\n        do_eval=True,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# Train! \ntrainer.train()\n```\nUsing the final fine-tuning checkpoint, define the updated Mistral 7B model:\n```\nft_model = PeftModel.from_pretrained(base_model, \"mistral-code-instruct/checkpoint-1000\")\n```\nTo evaluate the fine-tuned model\u2019s performance, ask a coding question similar to the initial one and request the generation of a code snippet:\n```\neval_prompt = f\"\"\"\nFor a given integer n, print out all its prime factors one on each line. \nn = 30\n\"\"\"\n\ninput_ids = tokenizer(eval_prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\noutputs = ft_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True, top_p=0.9,temperature=0.5)\n\n***** Output ***** \n\nGenerated response:\n2\n3\n5 \n\n#include <stdio.h>\n\nint main() {\n    int n = 30;\n    int i;\n    for (i = 2; i <= n; i++) {\n        while (n % i == 0) {\n            printf(\"%d\\n\", i);\n            n /= i;\n        }\n    }\n    return 0;\n}\n\n...\n```\nThe generated code snippet response from the fine-tuned model looks much better. Use a sandbox environment to try the code for yourself.\nThat\u2019s all there is to fine-tuning the Mistral 7B LLM. This project provides a reference workflow for your development needs. You can always choose to customize the project to better suit your enterprise data or use case. Switch out the dataset with one of your own, or fine-tune the model to another use case, such as text summarization or question-answering.\nGet started with AI Workbench\nNVIDIA AI Workbench helps you create, share, and scale enterprise AI and ML workflows between different GPU-enabled environments. Sign up for beta access to NVIDIA AI Workbench. To learn more about AI Workbench, check out these resources:\nWatch a video demo of AI Workbench that walks through a custom image generation example project with Stable Diffusion XL.\nRead Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench to get a sneak peek of more example projects.\nReference the NVIDIA AI Workbench User Guide to get your AI and ML projects up and running with NVIDIA AI Workbench."}], "https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/": [{"text": "Accelerated networking is crucial for modern data centers to optimize networking workloads, especially as AI and other complex workloads grow. By offloading demanding tasks to specialized hardware like CPUs, GPUs, DPUs, or SuperNICs, data centers can enhance performance, scalability, and efficiency. Techniques such as lossless networking, RDMA, adaptive routing, and in-network computing can help organizations unlock the full potential of modern applications, including AI. Implementing SuperNICs and DPUs can offload workloads from the host processor to accelerate communications and cope with the increasing need to move data. Network acceleration also enables high-speed, low-latency data transfers between servers, efficient workload distribution, and faster model training. Network abstraction, network optimization, end-to-end stack optimization, and in-network computing are essential for optimizing network performance and accommodating the unique traffic patterns of modern workloads. To build efficient, high-performance networks with acceleration, organizations can refer to whitepapers and ebooks like NVIDIA Spectrum-X Network Platform Architecture and Networking for the Era of AI.", "text_components": ["Modernizing the Data Center with Accelerated Networking\nAccelerated networking combines CPUs, GPUs, DPUs (data processing units), or SuperNICs into an accelerated computing fabric specifically designed to optimize networking workloads. It uses specialized hardware to offload demanding tasks to enhance server capabilities. As AI and other new workloads continue to grow in complexity and scale, the need for accelerated networking becomes paramount.\nData centers are the new unit of computing, and modern workloads are starting to challenge network infrastructure as networking services place further strains on the CPU. The network infrastructure, with an agile, automated, and programmable framework with accelerators and offloads, is key to unlocking the full potential of AI technologies and driving innovation.\nThis post explores the benefits and implementation tactics of accelerated networking technologies in data centers, highlighting their role in enhancing performance, scalability, and efficiency.", "Accelerating your network\nNetwork acceleration requires optimizing every aspect of the network, including processors, network interface cards (NICs), switches, cables, optics, and networking acceleration software. Leveraging lossless networking, remote direct memory access (RDMA), adaptive routing, congestion control, performance isolation, and in-network computing will help organizations unlock the full potential of modern applications, including AI.\nMaximum efficiency across shared networks can be obtained by properly controlling \u200cdata injection rates. When dealing with large data flows, Ethernet switches that implement adaptive routing algorithms can dynamically load-balance the data across the network, prevent congestion, and reduce latency. Switch multipathing and packet spraying techniques can further enhance network efficiency, ensuring timely data arrival and minimizing bottlenecks. This prevents data collisions between the switch and NICs or DPUs, while traffic flow isolation techniques ensure timely delivery by preventing one flow from negatively impacting others.\nAnother optimization technique is to deploy SuperNICs and DPUs. A SuperNIC is a type of network accelerator for AI cloud data centers that delivers robust and seamless connectivity between GPU servers. A DPU is a rapidly emerging class of processor that enables enhanced, accelerated networking. With the help of SuperNICs and DPUs, workloads can be offloaded from the host processor to accelerate communications, enabling data centers to cope with the ever-increasing need to move data.\nTo implement accelerated networking, consider the following techniques.", "Accelerated services\nWorkloads have undergone a significant paradigm shift, transitioning to decentralization, splitting workloads through containers and micro-segmentation. This has caused a dramatic increase in in-network bandwidth between servers (east-west traffic).\nAI workloads are a distributed computing problem, requiring the utilization of multiple interconnected servers or nodes. This places a tremendous strain on the network and CPU. Workload decentralization requires re-examining network infrastructure to add accelerators to relieve the CPU and GPUs from processing networking, storage, and security services. This frees the CPU to focus on application workloads. Acceleration ensures high-speed, low-latency data transfers between these nodes, and enables efficient workload distribution and faster model training.", "Network abstraction\nThe move to highly virtualized data centers and cloud models is straining legacy networks. Traditional data center networks were not designed to support the dynamic nature of today\u2019s virtualized workloads. Network abstraction, including network overlays, can run multiple separate, discrete virtualized network layers on top of the physical network. These are crucial in providing flexibility, scale, and acceleration. However, if not implemented properly, they can impede network flows.", "Network optimization\nA vast amount of collected and processed data has moved workloads into a data-centric era. The availability of large datasets combined with technological advances such as machine learning and generative AI increase the need for more data to feed learning algorithms. A ramification of this data explosion is the need to move, process, retrieve, and store large datasets.\nLossless networking can guarantee accurate data transmission without any loss or corruption and is vital for moving, processing, retrieving, and storing these large datasets. RDMA technology enhances networking performance by enabling direct data transfers between memory locations without involving CPUs. The combination of lossless networking and RDMA can optimize data transfer efficiency and reduce CPU and GPU idle time, enabling the efficient movement of data to feed modern applications.", "End-to-end stack optimization\nModern workloads have unique network traffic patterns. Traditional workloads generate traffic patterns with many flows, small packets, and low variance. Traffic for modern applications involves large packets, fewer flows, and high variance, including elephant flows and frequent changes in traffic patterns.\nAdaptive routing algorithms are used to dynamically load-balance data across the network, preventing congestion and high latency for these new traffic patterns. Congestion control mechanisms, such as explicit congestion notification (ECN), also ensure efficient data flow and minimize performance degradation. To account for this, networks must be architected with an optimized end-to-end stack to accelerate new traffic patterns.", "In-network computing\nThe large datasets of modern workloads require ultra-fast processing of highly parallelized algorithms and therefore are more complex. As computing requirements grow, in-network computing offers hardware-based acceleration of collective communication operations, effectively offloading collective operations from the CPU to the network. This feature significantly improves the performance of distributed AI model training, reduces communication overhead, and accelerates model convergence to eliminate the need to send data multiple times between endpoints and accelerates network performance.\nNetwork acceleration reduces CPU utilization, leaving more capacity for CPUs to process application workloads. It also reduces jitter to improve data streams, and offers higher overall throughput, which enables more data to be processed faster.", "Summary\nTechniques for network acceleration continue to evolve and are becoming more specialized. The newest evolution will address AI workloads, which require consistent, predictable performance and compute and power efficiencies capable of running multi-tenant environments.\nTo learn more about building the most efficient, high-performance networks with acceleration, see the two whitepapers, NVIDIA Spectrum-X Network Platform Architecture and Networking for the Era of AI: The Network Defines the Data Center, and the ebook, Modernize Your Data Center with Accelerated Networking."], "document_title": "Modernizing the Data Center with Accelerated Networking", "document_url": "https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/", "document_date": "2024-01-30T20:00:00", "document_date_modified": "2024-01-31T01:05:31", "document_full_text": "Modernizing the Data Center with Accelerated Networking\nAccelerated networking combines CPUs, GPUs, DPUs (data processing units), or SuperNICs into an accelerated computing fabric specifically designed to optimize networking workloads. It uses specialized hardware to offload demanding tasks to enhance server capabilities. As AI and other new workloads continue to grow in complexity and scale, the need for accelerated networking becomes paramount.\nData centers are the new unit of computing, and modern workloads are starting to challenge network infrastructure as networking services place further strains on the CPU. The network infrastructure, with an agile, automated, and programmable framework with accelerators and offloads, is key to unlocking the full potential of AI technologies and driving innovation.\nThis post explores the benefits and implementation tactics of accelerated networking technologies in data centers, highlighting their role in enhancing performance, scalability, and efficiency.\nAccelerating your network\nNetwork acceleration requires optimizing every aspect of the network, including processors, network interface cards (NICs), switches, cables, optics, and networking acceleration software. Leveraging lossless networking, remote direct memory access (RDMA), adaptive routing, congestion control, performance isolation, and in-network computing will help organizations unlock the full potential of modern applications, including AI.\nMaximum efficiency across shared networks can be obtained by properly controlling \u200cdata injection rates. When dealing with large data flows, Ethernet switches that implement adaptive routing algorithms can dynamically load-balance the data across the network, prevent congestion, and reduce latency. Switch multipathing and packet spraying techniques can further enhance network efficiency, ensuring timely data arrival and minimizing bottlenecks. This prevents data collisions between the switch and NICs or DPUs, while traffic flow isolation techniques ensure timely delivery by preventing one flow from negatively impacting others.\nAnother optimization technique is to deploy SuperNICs and DPUs. A SuperNIC is a type of network accelerator for AI cloud data centers that delivers robust and seamless connectivity between GPU servers. A DPU is a rapidly emerging class of processor that enables enhanced, accelerated networking. With the help of SuperNICs and DPUs, workloads can be offloaded from the host processor to accelerate communications, enabling data centers to cope with the ever-increasing need to move data.\nTo implement accelerated networking, consider the following techniques.\nAccelerated services\nWorkloads have undergone a significant paradigm shift, transitioning to decentralization, splitting workloads through containers and micro-segmentation. This has caused a dramatic increase in in-network bandwidth between servers (east-west traffic).\nAI workloads are a distributed computing problem, requiring the utilization of multiple interconnected servers or nodes. This places a tremendous strain on the network and CPU. Workload decentralization requires re-examining network infrastructure to add accelerators to relieve the CPU and GPUs from processing networking, storage, and security services. This frees the CPU to focus on application workloads. Acceleration ensures high-speed, low-latency data transfers between these nodes, and enables efficient workload distribution and faster model training.\nNetwork abstraction\nThe move to highly virtualized data centers and cloud models is straining legacy networks. Traditional data center networks were not designed to support the dynamic nature of today\u2019s virtualized workloads. Network abstraction, including network overlays, can run multiple separate, discrete virtualized network layers on top of the physical network. These are crucial in providing flexibility, scale, and acceleration. However, if not implemented properly, they can impede network flows.\nNetwork optimization\nA vast amount of collected and processed data has moved workloads into a data-centric era. The availability of large datasets combined with technological advances such as machine learning and generative AI increase the need for more data to feed learning algorithms. A ramification of this data explosion is the need to move, process, retrieve, and store large datasets.\nLossless networking can guarantee accurate data transmission without any loss or corruption and is vital for moving, processing, retrieving, and storing these large datasets. RDMA technology enhances networking performance by enabling direct data transfers between memory locations without involving CPUs. The combination of lossless networking and RDMA can optimize data transfer efficiency and reduce CPU and GPU idle time, enabling the efficient movement of data to feed modern applications.\nEnd-to-end stack optimization\nModern workloads have unique network traffic patterns. Traditional workloads generate traffic patterns with many flows, small packets, and low variance. Traffic for modern applications involves large packets, fewer flows, and high variance, including elephant flows and frequent changes in traffic patterns.\nAdaptive routing algorithms are used to dynamically load-balance data across the network, preventing congestion and high latency for these new traffic patterns. Congestion control mechanisms, such as explicit congestion notification (ECN), also ensure efficient data flow and minimize performance degradation. To account for this, networks must be architected with an optimized end-to-end stack to accelerate new traffic patterns.\nIn-network computing\nThe large datasets of modern workloads require ultra-fast processing of highly parallelized algorithms and therefore are more complex. As computing requirements grow, in-network computing offers hardware-based acceleration of collective communication operations, effectively offloading collective operations from the CPU to the network. This feature significantly improves the performance of distributed AI model training, reduces communication overhead, and accelerates model convergence to eliminate the need to send data multiple times between endpoints and accelerates network performance.\nNetwork acceleration reduces CPU utilization, leaving more capacity for CPUs to process application workloads. It also reduces jitter to improve data streams, and offers higher overall throughput, which enables more data to be processed faster.\nSummary\nTechniques for network acceleration continue to evolve and are becoming more specialized. The newest evolution will address AI workloads, which require consistent, predictable performance and compute and power efficiencies capable of running multi-tenant environments.\nTo learn more about building the most efficient, high-performance networks with acceleration, see the two whitepapers, NVIDIA Spectrum-X Network Platform Architecture and Networking for the Era of AI: The Network Defines the Data Center, and the ebook, Modernize Your Data Center with Accelerated Networking."}], "https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/": [{"text": "The article discusses the integration of convolution operations and self-attention mechanisms in computer vision tasks to enhance performance and efficiency. The Convolutional Self-Attention (CSA) module is proposed as a way to model both local and global feature relationships using only convolutions, resulting in improved hardware utilization and reduced deployment latency. Experimental results demonstrate competitive accuracy compared to contemporary transformer networks, with superior efficiency on optimized GPUs. The CSA module is compared against benchmark models like Swin Transformer, ConvNext, and Convolutional Vision Transformer, showing commendable accuracy and the fastest latency in comparisons. The strategic design of CSA allows for efficient computations that balance accuracy and latency well, making it compatible with TensorRT restricted mode for deployment in AV applications. The article concludes by highlighting the advantages of CSA in achieving all-to-all relational encoding with reduced computational load, making it a suitable model design for AV production and other computer vision tasks.", "text_components": ["Emulating the Attention Mechanism in Transformer Models with a Fully Convolutional Network\nThe past decade has seen a remarkable surge in the adoption of deep learning techniques for computer vision (CV) tasks. Convolutional neural networks (CNNs) have been the cornerstone of this revolution, exhibiting exceptional performance and enabling significant advancements in visual perception.\nBy employing localized filters and hierarchical architectures, CNNs have proven adept at capturing spatial hierarchies, detecting patterns, and extracting informative features from images, as explained in Deep Residual Learning for Image Recognition. Convolutional layers exhibit translation equivariance, enabling them to generalize to translations and spatial transformations. However, despite their success, CNNs exhibit limitations in capturing long-range dependencies and global contextual understanding, which become increasingly crucial in complex scenes or tasks requiring fine-grained understanding.\nIn contrast, transformers have emerged as a compelling alternative architecture in computer vision, driven by their success in natural language processing (NLP), as explained in Attention Is All You Need. By eschewing local convolutions, transformers offer a self-attention mechanism that supports global relationships among visual features. The attention mechanism enables transformers to capture long-range interactions between image elements, facilitating a more holistic understanding of the visual scene that leads to better accuracy. Figure 1 shows an example of self-attention for vision applications. For more details, see An Image is Worth 16\u00d716 Words: Transformers for Image Recognition at Scale and Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.\nSelf-attention, nonetheless, encounters challenges in effectively capturing local contextual information within images, accentuating the significance of broader global receptive fields. Additionally, the computational complexity associated with self-attention, characterized by quadratic interactions between visual feature elements, poses significant challenges for handling large images in computer vision.\nAt the frontier of innovation, the automotive industry is increasingly recognizing the need for the widespread incorporation of transformer-like networks. However, the integration of these networks presents unique challenges. Specifically, within the NVIDIA TensorRT framework for certain operating systems on NVIDIA DRIVE products, limited specialized functionality is supported as compared to standard use cases.\nThese specialized APIs still include highly optimized convolution operations, among others, reflecting the industry\u2019s long-standing commitment to refining convolutional networks. We aim to harness these optimized convolution operations strategically to drive more efficient and effective implementation of transformer networks. Our goal is to empower the automotive industry to meet the dynamic demands of modern applications while working harmoniously within the confines of existing software frameworks and hardware platforms.\nIllustration of a transformer-based model.\nFigure 1. An illustration of self-attention in a typical transformer block for vision applications\nWhile recognizing the value of \u200cself-attention, it is imperative to place greater emphasis on the impact of convolutions, especially in CV tasks for. This is true for the following reasons:\nThe inherent characteristic differences between images and texts, as previously mentioned, highlight the challenges of applying self-attention directly to CV tasks, calling for hybrid approaches or alternative architectures that combine the strengths of self-attention and convolutional layers.\nIn autonomous vehicle (AV) applications, high-resolution images are often used in real-time applications. The optimization of self-attention computation on hardware platforms has fallen behind the rapid emergence of new transformers by AV industry and chip makers, failing to meet user demands. Current implementations of transformer-based models inadequately leverage the computational capabilities of GPUs.\nIn many cases of production in autonomous driving, inference performed in the restricted mode of deep learning runtime libraries may not yet have full support of state-of-the-art transformer networks. For example, current operations in transformers are not fully covered in TensorRT restricted mode.\nThis post presents our recent work on emulating the attention mechanism in transformer models using a fully convolutional network. Our method combines the strengths of conventional convolution kernels optimized for current GPU hardware platforms with self-attention modules, resulting in superior performance compared to contemporary transformer-like models. Our work addresses the increasing user demand for transformer usability in various industries with computer vision problems. Our method not only provides the fastest latency performance with comparable accuracy when running on TensorRT, but is also fully compatible in TensorRT restricted mode.", "Fusing convolutions and self-attention\nRecent research shows a growing interest in merging the strengths of CNNs and transformers. By combining convolution operations for local feature information with self-attention modules for global feature relations, researchers aim to enhance the capabilities of both architectures.\nSwin Transformer is one notable example. This recent vision-transformer \u200cintroduces the concept of shifted windows to enable transformers to effectively learn local features. By incorporating local self-attention within smaller regions, Swin captures local relationships and dependencies, thereby improving performance in tasks requiring fine-grained information. However, a challenge arises with the quadratic increase in computational complexity of self-attention as input sizes grow, which can quickly impose latency burdens.\nTo address this issue, researchers have explored merging convolution operations and self-attentions. Convolution-based approaches mimic transformer training configurations or selectively use convolutions and self-attentions in various parts of networks.\nFor example, Convolutional Vision Transformer (CvT) intuitively incorporated convolutional features into self-attention modules. Conv-Next, on the other hand, resembles vision transformers with conventional CNNs. Nonetheless, the approach fails to explicitly address the limited receptive fields commonly encountered in traditional convolutional network models. Unlike self-attention, convolutional operations possess a fixed receptive field size and a shared set of parameters. This characteristic enables convolutions to process input data in a locally focused and parameter-efficient manner.", "Convolutional Self-Attention\nWe present Convolutional Self-Attention (CSA), which completely replaces conventional attention mechanisms with convolution operations for vision tasks, enabling the modeling of both local and global feature relations. By relying solely on convolutions, our overall model achieves remarkable efficiency on highly optimized GPUs and deep learning accelerators. Experimental results convincingly demonstrate its competitive accuracy in comparison to contemporary transformer networks, while displaying improved hardware utilization and significantly reduced deployment latency.\nThe overall proposed model consists of repetitive uses of down-sampling convolution layers and our proposed CSA blocks along its feed-forwarding flow, as depicted in Figure 2. Each CSA block emulates a transformer block employing convolution operations.\nIllustration of the overall proposed model and our proposed CSA blocks along its feed-forwarding flow.\nFigure 2. Our network inference flow with convolutional self-attention blocks\nFigure 3 shows the structure and flow of the CSA module. The CSA blocks can differ in implementation but are designed to emulate the relational encoding process of self-attention. To achieve relational encoding, we rotate the tensor along the channel-axis, converting channel features into spatial format (height and width).\nThis rotated feature tensor is elementwise multiplied with the original tensor before rotation, followed by convolutions. This replicates the first inner product of self-attention, but with a difference in concept, as our method allows for one-to-many relational embedding through elementwise multiplication and convolution. The resulting relational feature tensor is then normalized, activated, and multiplied with another visual feature from the input tensor, value (V).\nIllustration of the structure and flow of the CSA module.\nFigure 3. An illustration of a convolutional self-attention module\nOur method achieves a global receptive field by strategically rearranging feature tensors and utilizing local convolution kernel windows. This explicit relational encoding enables each feature pixel to be projected to all others, resulting in comprehensive inter-pixel interactions. This is because the structural rearrangements of tensors in our approach enable convolution windows to capture global relationships among visual features, leveraging the strengths of convolutional operations for one-to-many visual feature relational reasoning.\nIn comparison, CSA modules encode relations among all feature pixels through inner-product operations, which can impose significant computational burdens on hardware. By achieving one-to-all relational encoding, our approach reduces the computational load while preserving the ability to capture long-range dependencies and structural information across the entire feature map.\nTo manage the quadratic computational increments resulting from increasing input sizes, our design can incorporate spatial reduction layers to reduce the tensor size, as illustrated in Figure 4. This not only helps decrease computational overhead but also enables the network to focus on regional relationships among visual features, which carries more semantics, rather than pixel-level relationships.\nIllustration of our design can incorporate spatial reduction layers to reduce the tensor size.\nFigure 4. Use of spatial reduction for cases with large input size", "Performance in accuracy and latency\nThe CSA module is compared against relevant contemporary CV classification models with the ImageNet-1K dataset in terms of accuracy against the validation data and latency measured with TensorRT-8.6.11.4. We target for AV application of CSA with restricted mode in mind, so the models are compared on the NVIDIA DRIVE Orin platform. NVIDIA DRIVE Orin is a high-performance and energy-efficient system-on-a-chip (SoC) and is part of the NVIDIA DRIVE platform for use in autonomous vehicles.", "Benchmark entries\nThe Swin Transformer network is an innovative deep learning architecture that combines self-attention mechanisms, originally popularized by models like the vision transformer (ViT), with a hierarchical and parallelizable design.\nThe ConvNext model is developed through a progressive transformation of a standard ResNet to resemble a vision Transformer, competing favorably with Swin Transformers in specific tasks while maintaining the simplicity and efficiency of conventional convolutional networks.\nThe Convolutional Vision Transformer (CvT) enhances performance and efficiency by incorporating convolutions into ViT. CvT performs very competitively on ImageNet-1k with fewer parameters and lower GMACs.\nThe rationale for the benchmark entries lies in their relevance to CSA and contemporary significance. In our experiments, we specifically compared CSA with the benchmarks of Swin-tiny, ConvNext-tiny, and CvT-13 that share similar model sizes, which is succinctly shown in Table 1.\nA table showing model size and safety compliance of all model entries.\nTable 1. Model size and safety compliance of all model entries\nWe are presenting our results using two kinds of precision modes, FP16 and mixed precision (that is, with FP32, FP16, and INT8 all enabled) in TensorRT. This approach allows us to provide a balanced assessment of our models\u2019 performance. The model quantization for all methods was achieved using Post-Training Quantization (PTQ) and 500 images were used during the calibration process.\nWe measure accuracy using the Top-1 accuracy on the ImageNet dataset and report inference latency in milliseconds with various batch sizes of 1, 4, 8, and 16. This ensures that our comparison is conducted in a fair and unbiased manner. One can further optimize the TensorRT inference of CSA to strike a balance between accuracy and latency, with the aim of exploiting latency within acceptable accuracy constraints.\nIn the pursuit of optimizing precision modes to enhance latency at the potential expense of accuracy, quantization strategies emerge as a compelling solution. TensorRT offers a diverse array of quantization methods, including percentile, mean squared error (MSE), and entropy quantization, all of which demonstrate effectiveness in mitigating precision loss. In our study, which centers on mixed-precision inferences across a range of benchmark methods, we selected the entropy quantization methodology. This approach, grounded in information theory, allocates codes to minimize the average code word length, resulting in minimal accuracy degradation, with the noteworthy exception of the CvT benchmark.\nWhile all benchmarks, including CSA, maintain an indistinguishable level of accuracy even after precision reduction, ConvNext outperforms the other benchmarks to a slight extent despite the reduced precisions. Conversely, CSA exhibits the smallest drop in accuracy.\nTensorRT accuracy benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\nFigure 5. TensorRT accuracy comparison for ImageNet-1K dataset\nTensorRT latency benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\nFigure 6. TensorRT-8.6.11.4 latency comparison for 224 x 224 inputs with the batch sizes of 1, 4, 8, and 16\nConvNext stands out for its accuracy in both FP16 and mixed-precision modes but comes with the trade-off of comparably slow latency. In this context, CSA emerges as a highly competitive option, offering commendable accuracy, while achieving the fastest latency.\nCompared to ConvNext-tiny, CSA delivers a remarkable 49% improvement in latency for the case of batch size of one while maintaining its strong accuracy performance. This underscores CSA\u2019s impressive capabilities and positions it as a strong choice within this context.\nMemory bandwidth benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\nFigure 7. Average memory bandwidth per frame for batch sizes of 1, 4, 8, and 16\nCSA outperforms the benchmarks for efficient memory traffic during inference, resulting in the least bandwidth of average memory per frame. It should be noted that CSA\u2019s memory traffic is persistent even as the batch size grows, while the other methods in the benchmark are growing gradually.\nThanks to the strategic design of CSA, our model can leverage efficient convolution kernel implementation in TensorRT, resulting in highly efficient computations that strike a harmonious balance between accuracy and latency, while being fully compatible with TensorRT restricted mode. Other methods, either with higher accuracy or lower latency, are not compatible. In practice, this makes it currently difficult to deploy those models in production where the TensorRT restricted model is required.", "Conclusion\nUnlike other convolutional models that try to ingest the attention module from a transformer model, Convolutional Self-Attention (CSA) explicitly finds relationships among features one-to-many with only convolutions in conjunction with simple tensor shape manipulations. The differences between our method and relevant methods listed below:\nBy strategically rearranging feature tensors, explicit relational encoding ensures that each feature pixel is projected to all others, achieving a global receptive field while utilizing local convolution kernel windows.\nIn contrast to conventional self-attention modules that encode relations among all input features with increase computational cost with respect to the input size, our method succinctly achieves all-to-all relational encoding with convolution operations in a hierarchical manner at each stage with reduced input size, which lower the computational load on hardware.\nThese advantages enable faster inference speed for models of comparable size and match or exceed the performance of other methods.\nMore importantly, CSA operates without bells and hassles in TensorRT restricted model, making it suitable for AV production for safety-critical applications. We expect CSA to serve as a reference model design for our customers who are using the NVIDIA DRIVE platform and beyond. For more information, visit the NVIDIA Developer AV Forum and TensorRT Forum."], "document_title": "Emulating the Attention Mechanism in Transformer Models with a Fully Convolutional Network", "document_url": "https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/", "document_date": "2024-01-29T17:00:00", "document_date_modified": "2024-01-30T18:10:43", "document_full_text": "Emulating the Attention Mechanism in Transformer Models with a Fully Convolutional Network\nThe past decade has seen a remarkable surge in the adoption of deep learning techniques for computer vision (CV) tasks. Convolutional neural networks (CNNs) have been the cornerstone of this revolution, exhibiting exceptional performance and enabling significant advancements in visual perception.\nBy employing localized filters and hierarchical architectures, CNNs have proven adept at capturing spatial hierarchies, detecting patterns, and extracting informative features from images, as explained in Deep Residual Learning for Image Recognition. Convolutional layers exhibit translation equivariance, enabling them to generalize to translations and spatial transformations. However, despite their success, CNNs exhibit limitations in capturing long-range dependencies and global contextual understanding, which become increasingly crucial in complex scenes or tasks requiring fine-grained understanding.\nIn contrast, transformers have emerged as a compelling alternative architecture in computer vision, driven by their success in natural language processing (NLP), as explained in Attention Is All You Need. By eschewing local convolutions, transformers offer a self-attention mechanism that supports global relationships among visual features. The attention mechanism enables transformers to capture long-range interactions between image elements, facilitating a more holistic understanding of the visual scene that leads to better accuracy. Figure 1 shows an example of self-attention for vision applications. For more details, see An Image is Worth 16\u00d716 Words: Transformers for Image Recognition at Scale and Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.\nSelf-attention, nonetheless, encounters challenges in effectively capturing local contextual information within images, accentuating the significance of broader global receptive fields. Additionally, the computational complexity associated with self-attention, characterized by quadratic interactions between visual feature elements, poses significant challenges for handling large images in computer vision.\nAt the frontier of innovation, the automotive industry is increasingly recognizing the need for the widespread incorporation of transformer-like networks. However, the integration of these networks presents unique challenges. Specifically, within the NVIDIA TensorRT framework for certain operating systems on NVIDIA DRIVE products, limited specialized functionality is supported as compared to standard use cases.\nThese specialized APIs still include highly optimized convolution operations, among others, reflecting the industry\u2019s long-standing commitment to refining convolutional networks. We aim to harness these optimized convolution operations strategically to drive more efficient and effective implementation of transformer networks. Our goal is to empower the automotive industry to meet the dynamic demands of modern applications while working harmoniously within the confines of existing software frameworks and hardware platforms.\nIllustration of a transformer-based model.\nFigure 1. An illustration of self-attention in a typical transformer block for vision applications\nWhile recognizing the value of \u200cself-attention, it is imperative to place greater emphasis on the impact of convolutions, especially in CV tasks for. This is true for the following reasons:\nThe inherent characteristic differences between images and texts, as previously mentioned, highlight the challenges of applying self-attention directly to CV tasks, calling for hybrid approaches or alternative architectures that combine the strengths of self-attention and convolutional layers.\nIn autonomous vehicle (AV) applications, high-resolution images are often used in real-time applications. The optimization of self-attention computation on hardware platforms has fallen behind the rapid emergence of new transformers by AV industry and chip makers, failing to meet user demands. Current implementations of transformer-based models inadequately leverage the computational capabilities of GPUs.\nIn many cases of production in autonomous driving, inference performed in the restricted mode of deep learning runtime libraries may not yet have full support of state-of-the-art transformer networks. For example, current operations in transformers are not fully covered in TensorRT restricted mode.\nThis post presents our recent work on emulating the attention mechanism in transformer models using a fully convolutional network. Our method combines the strengths of conventional convolution kernels optimized for current GPU hardware platforms with self-attention modules, resulting in superior performance compared to contemporary transformer-like models. Our work addresses the increasing user demand for transformer usability in various industries with computer vision problems. Our method not only provides the fastest latency performance with comparable accuracy when running on TensorRT, but is also fully compatible in TensorRT restricted mode.\nFusing convolutions and self-attention\nRecent research shows a growing interest in merging the strengths of CNNs and transformers. By combining convolution operations for local feature information with self-attention modules for global feature relations, researchers aim to enhance the capabilities of both architectures.\nSwin Transformer is one notable example. This recent vision-transformer \u200cintroduces the concept of shifted windows to enable transformers to effectively learn local features. By incorporating local self-attention within smaller regions, Swin captures local relationships and dependencies, thereby improving performance in tasks requiring fine-grained information. However, a challenge arises with the quadratic increase in computational complexity of self-attention as input sizes grow, which can quickly impose latency burdens.\nTo address this issue, researchers have explored merging convolution operations and self-attentions. Convolution-based approaches mimic transformer training configurations or selectively use convolutions and self-attentions in various parts of networks.\nFor example, Convolutional Vision Transformer (CvT) intuitively incorporated convolutional features into self-attention modules. Conv-Next, on the other hand, resembles vision transformers with conventional CNNs. Nonetheless, the approach fails to explicitly address the limited receptive fields commonly encountered in traditional convolutional network models. Unlike self-attention, convolutional operations possess a fixed receptive field size and a shared set of parameters. This characteristic enables convolutions to process input data in a locally focused and parameter-efficient manner.\nConvolutional Self-Attention\nWe present Convolutional Self-Attention (CSA), which completely replaces conventional attention mechanisms with convolution operations for vision tasks, enabling the modeling of both local and global feature relations. By relying solely on convolutions, our overall model achieves remarkable efficiency on highly optimized GPUs and deep learning accelerators. Experimental results convincingly demonstrate its competitive accuracy in comparison to contemporary transformer networks, while displaying improved hardware utilization and significantly reduced deployment latency.\nThe overall proposed model consists of repetitive uses of down-sampling convolution layers and our proposed CSA blocks along its feed-forwarding flow, as depicted in Figure 2. Each CSA block emulates a transformer block employing convolution operations.\nIllustration of the overall proposed model and our proposed CSA blocks along its feed-forwarding flow.\nFigure 2. Our network inference flow with convolutional self-attention blocks\nFigure 3 shows the structure and flow of the CSA module. The CSA blocks can differ in implementation but are designed to emulate the relational encoding process of self-attention. To achieve relational encoding, we rotate the tensor along the channel-axis, converting channel features into spatial format (height and width).\nThis rotated feature tensor is elementwise multiplied with the original tensor before rotation, followed by convolutions. This replicates the first inner product of self-attention, but with a difference in concept, as our method allows for one-to-many relational embedding through elementwise multiplication and convolution. The resulting relational feature tensor is then normalized, activated, and multiplied with another visual feature from the input tensor, value (V).\nIllustration of the structure and flow of the CSA module.\nFigure 3. An illustration of a convolutional self-attention module\nOur method achieves a global receptive field by strategically rearranging feature tensors and utilizing local convolution kernel windows. This explicit relational encoding enables each feature pixel to be projected to all others, resulting in comprehensive inter-pixel interactions. This is because the structural rearrangements of tensors in our approach enable convolution windows to capture global relationships among visual features, leveraging the strengths of convolutional operations for one-to-many visual feature relational reasoning.\nIn comparison, CSA modules encode relations among all feature pixels through inner-product operations, which can impose significant computational burdens on hardware. By achieving one-to-all relational encoding, our approach reduces the computational load while preserving the ability to capture long-range dependencies and structural information across the entire feature map.\nTo manage the quadratic computational increments resulting from increasing input sizes, our design can incorporate spatial reduction layers to reduce the tensor size, as illustrated in Figure 4. This not only helps decrease computational overhead but also enables the network to focus on regional relationships among visual features, which carries more semantics, rather than pixel-level relationships.\nIllustration of our design can incorporate spatial reduction layers to reduce the tensor size.\nFigure 4. Use of spatial reduction for cases with large input size\nPerformance in accuracy and latency\nThe CSA module is compared against relevant contemporary CV classification models with the ImageNet-1K dataset in terms of accuracy against the validation data and latency measured with TensorRT-8.6.11.4. We target for AV application of CSA with restricted mode in mind, so the models are compared on the NVIDIA DRIVE Orin platform. NVIDIA DRIVE Orin is a high-performance and energy-efficient system-on-a-chip (SoC) and is part of the NVIDIA DRIVE platform for use in autonomous vehicles.\nBenchmark entries\nThe Swin Transformer network is an innovative deep learning architecture that combines self-attention mechanisms, originally popularized by models like the vision transformer (ViT), with a hierarchical and parallelizable design.\nThe ConvNext model is developed through a progressive transformation of a standard ResNet to resemble a vision Transformer, competing favorably with Swin Transformers in specific tasks while maintaining the simplicity and efficiency of conventional convolutional networks.\nThe Convolutional Vision Transformer (CvT) enhances performance and efficiency by incorporating convolutions into ViT. CvT performs very competitively on ImageNet-1k with fewer parameters and lower GMACs.\nThe rationale for the benchmark entries lies in their relevance to CSA and contemporary significance. In our experiments, we specifically compared CSA with the benchmarks of Swin-tiny, ConvNext-tiny, and CvT-13 that share similar model sizes, which is succinctly shown in Table 1.\nA table showing model size and safety compliance of all model entries.\nTable 1. Model size and safety compliance of all model entries\nWe are presenting our results using two kinds of precision modes, FP16 and mixed precision (that is, with FP32, FP16, and INT8 all enabled) in TensorRT. This approach allows us to provide a balanced assessment of our models\u2019 performance. The model quantization for all methods was achieved using Post-Training Quantization (PTQ) and 500 images were used during the calibration process.\nWe measure accuracy using the Top-1 accuracy on the ImageNet dataset and report inference latency in milliseconds with various batch sizes of 1, 4, 8, and 16. This ensures that our comparison is conducted in a fair and unbiased manner. One can further optimize the TensorRT inference of CSA to strike a balance between accuracy and latency, with the aim of exploiting latency within acceptable accuracy constraints.\nIn the pursuit of optimizing precision modes to enhance latency at the potential expense of accuracy, quantization strategies emerge as a compelling solution. TensorRT offers a diverse array of quantization methods, including percentile, mean squared error (MSE), and entropy quantization, all of which demonstrate effectiveness in mitigating precision loss. In our study, which centers on mixed-precision inferences across a range of benchmark methods, we selected the entropy quantization methodology. This approach, grounded in information theory, allocates codes to minimize the average code word length, resulting in minimal accuracy degradation, with the noteworthy exception of the CvT benchmark.\nWhile all benchmarks, including CSA, maintain an indistinguishable level of accuracy even after precision reduction, ConvNext outperforms the other benchmarks to a slight extent despite the reduced precisions. Conversely, CSA exhibits the smallest drop in accuracy.\nTensorRT accuracy benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\nFigure 5. TensorRT accuracy comparison for ImageNet-1K dataset\nTensorRT latency benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\nFigure 6. TensorRT-8.6.11.4 latency comparison for 224 x 224 inputs with the batch sizes of 1, 4, 8, and 16\nConvNext stands out for its accuracy in both FP16 and mixed-precision modes but comes with the trade-off of comparably slow latency. In this context, CSA emerges as a highly competitive option, offering commendable accuracy, while achieving the fastest latency.\nCompared to ConvNext-tiny, CSA delivers a remarkable 49% improvement in latency for the case of batch size of one while maintaining its strong accuracy performance. This underscores CSA\u2019s impressive capabilities and positions it as a strong choice within this context.\nMemory bandwidth benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\nFigure 7. Average memory bandwidth per frame for batch sizes of 1, 4, 8, and 16\nCSA outperforms the benchmarks for efficient memory traffic during inference, resulting in the least bandwidth of average memory per frame. It should be noted that CSA\u2019s memory traffic is persistent even as the batch size grows, while the other methods in the benchmark are growing gradually.\nThanks to the strategic design of CSA, our model can leverage efficient convolution kernel implementation in TensorRT, resulting in highly efficient computations that strike a harmonious balance between accuracy and latency, while being fully compatible with TensorRT restricted mode. Other methods, either with higher accuracy or lower latency, are not compatible. In practice, this makes it currently difficult to deploy those models in production where the TensorRT restricted model is required.\nConclusion\nUnlike other convolutional models that try to ingest the attention module from a transformer model, Convolutional Self-Attention (CSA) explicitly finds relationships among features one-to-many with only convolutions in conjunction with simple tensor shape manipulations. The differences between our method and relevant methods listed below:\nBy strategically rearranging feature tensors, explicit relational encoding ensures that each feature pixel is projected to all others, achieving a global receptive field while utilizing local convolution kernel windows.\nIn contrast to conventional self-attention modules that encode relations among all input features with increase computational cost with respect to the input size, our method succinctly achieves all-to-all relational encoding with convolution operations in a hierarchical manner at each stage with reduced input size, which lower the computational load on hardware.\nThese advantages enable faster inference speed for models of comparable size and match or exceed the performance of other methods.\nMore importantly, CSA operates without bells and hassles in TensorRT restricted model, making it suitable for AV production for safety-critical applications. We expect CSA to serve as a reference model design for our customers who are using the NVIDIA DRIVE platform and beyond. For more information, visit the NVIDIA Developer AV Forum and TensorRT Forum."}], "https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/": [{"text": "NVIDIA has announced Metropolis microservices for Jetson, offering a suite of customizable building blocks for developing vision AI applications at the edge. These APIs and microservices aim to streamline the development and deployment process, reducing time from years to months. The platform includes over 15 microservices for video storage, AI perception, analytics, and more. Reference applications, such as AI-enabled network video recorder and zero-shot detection using generative AI, demonstrate the capabilities of these microservices. Developers have the flexibility to choose and integrate various services based on their product maturity. Partners like AAEON, Advantech, and others are incorporating these microservices into their offerings. Overall, NVIDIA Metropolis microservices for Jetson provide a powerful and efficient way to develop edge AI applications.", "text_components": ["Announcing NVIDIA Metropolis Microservices for Jetson for Rapid Edge AI Development\nBuilding vision AI applications for the edge often comes with notoriously long and costly development cycles. At the same time, quickly developing edge AI applications that are cloud-native, flexible, and secure has never been more important. Now, a powerful yet simple API-driven edge AI development workflow is available with the new NVIDIA Metropolis microservices.\nNVIDIA Metropolis microservices is a suite of customizable, cloud-native building blocks for developing vision AI applications and solutions. This release introduces an expanded set of APIs and microservices on the NVIDIA Jetson platform to further accelerate the development and deployment of vision AI applications at the edge.\nThese new Jetson microservices empower developers to modernize their AI application stack, streamline processes, and safeguard applications for the future. You can easily incorporate the latest in generative AI advancements through APIs and microservices such as video storage and management, prebuilt AI perception pipelines, tracking algorithms, system monitoring, IoT services for secure edge-to-cloud connectivity, and more.\nDownload NVIDIA Metropolis microservices for Jetson.\nA graphic that compares using a do-it-yourself approach, where developers have to create all the pieces from scratch, and using prebuilt NVIDIA Metropolis microservices, which reduce development time from years to months.\nFigure 1. Develop edge AI applications faster with NVIDIA Metropolis microservices", "Release highlights\nProduction-ready edge AI applications require numerous components, including AI models, optimized processing and inference pipelines, glue logic, security measures, cloud connectivity, and more. NVIDIA Metropolis microservices for Jetson streamline the application development process by offering pre-built microservices for the most ubiquitous components using a cloud-native, modular, and extensible architecture.\nThe flexibility of the platform enhances development efficiency, with standard APIs seamlessly integrating with other applications and services. The platform also provides essential services such as IoT, security, and monitoring, offering ready-to-use core components for production applications.\nWith access to more than 15 microservices across Application, Platform services, and Cloud services, developers are freed to concentrate on building intellectual property and achieving differentiation in the market.\nNVIDIA partners are incorporating NVIDIA Metropolis microservices into their offerings, including AAEON, Aetina, Advantech, Allxon, CRG, CVEDIA, Namla, Rebotnix, RidgeRun, Seeed Studio, and Silicon Highway. More are added daily.", "Reference workflows and applications\nTwo reference applications are included to help get you started with NVIDIA Metropolis microservices for Jetson: the AI-enabled network video recorder (AI-NVR) and a generative AI application with zero-shot detection. These workflows show how the microservices and APIs come together to build complete applications from video ingestion, AI inference, analytics, and monitoring to securely connecting to the cloud.", "AI-enabled network video recorder\nThe AI-NVR reference workflow brings nearly all the microservices together in one comprehensive app. It includes:\nVideo ingestion and storage using the Video Storage Toolkit (VST) microservice\nPeople detection and tracking with the AI Perception service with NVIDIA DeepStream\nLine crossing and Region of Interest (ROI) insights and alerts using the Analytics service\nAn Android reference mobile application is provided to demonstrate the use of APIs to build client applications. To learn more, check out the NVIDIA On-Demand playlist, AI-NVR Using Metropolis Microservices for Jetson.\nGraphic showing the complete cloud-native architecture of AI-NVR application showing VST, AI perception service, analytics service, and all the other platform services.\nFigure 2. The AI-enabled network video recorder (AI-NVR) application architecture", "Zero-shot detection using generative AI\nMetropolis microservices for Jetson enables developers to prototype and productize generative AI applications for the edge. The generative AI reference application enables zero-shot detection of live streaming data. Models can detect any objects specified with a prompt.\nPrompts can be made remotely over REST APIs to the AI Perception service to dynamically change classes to detect. Generative AI enables a new breed of AI-powered applications for the edge. To learn more about generative AI with Metropolis microservices, see Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson.\nGraphic showing the cloud-native architecture of the generative AI reference application for zero-shot object detection using a visual prompting agent.\nFigure 3. Generative AI reference application for zero-shot object detection", "Powerful microservices and APIs\nMetropolis microservices for Jetson is a collection of feature-rich microservices and APIs, including application services, platform and Board Support Package (BSP) services, and cloud services. The modular and extensible microservices make it easy to build modern cloud-native applications for the edge.\nAs a developer, you have the flexibility to choose one, several, or all of the services depending on the maturity of your product.\nGraphic showing the complete software stack from the reference AI workflow, application microservice, platform, and BSP services to cloud services.\nFigure 4. Metropolis microservices for Jetson software stack", "Application services\nVideo Storage Toolkit: Service for video ingestion and storage\nAI Perception service using NVIDIA DeepStream: For AI inference, object tracking and metadata creation\nAI Perception service for zero-shot detection: For generative AI inference with the NanoOWL model and visual prompting\nAnalytics service: Object counting analytics such as line crossing, Region of Interest, and Field of View", "Platform services\nRedis: Global message bus for inter-process communication\nAPI Gateway: Maps incoming API requests to the subsequent services\nMonitoring: Monitor and visualize edge device status such as utilization and app KPIs\nIoT Gateway: Secure bidirectional communication between edge and cloud", "Cloud services\nIoT Cloud: Create secure connection from cloud to edge, including authentication and authorization", "Summary\nNVIDIA Metropolis microservices fast-tracks vision AI development for the edge, providing ready-to-use applications, over 15 microservices for platform services, pixel perception, video storage, analytics, and more. Download NVIDIA Metropolis microservices for Jetson.\nTo learn about the technical details of Metropolis microservices for Jetson, read the NVIDIA Metropolis Microservices for Jetson whitepaper.\nFor a tutorial on building applications using Metropolis APIs, see Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs. To learn even more, register to join us for the two-part webinar, Accelerate Edge AI Development With Metropolis APIs and Microservices for Jetson (Part 1) and How to Build With Metropolis Microservices for Jetson (Part 2)."], "document_title": "Announcing NVIDIA Metropolis Microservices for Jetson for Rapid Edge AI Development", "document_url": "https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/", "document_date": "2024-01-25T18:30:00", "document_date_modified": "2024-01-25T18:41:47", "document_full_text": "Announcing NVIDIA Metropolis Microservices for Jetson for Rapid Edge AI Development\nBuilding vision AI applications for the edge often comes with notoriously long and costly development cycles. At the same time, quickly developing edge AI applications that are cloud-native, flexible, and secure has never been more important. Now, a powerful yet simple API-driven edge AI development workflow is available with the new NVIDIA Metropolis microservices.\nNVIDIA Metropolis microservices is a suite of customizable, cloud-native building blocks for developing vision AI applications and solutions. This release introduces an expanded set of APIs and microservices on the NVIDIA Jetson platform to further accelerate the development and deployment of vision AI applications at the edge.\nThese new Jetson microservices empower developers to modernize their AI application stack, streamline processes, and safeguard applications for the future. You can easily incorporate the latest in generative AI advancements through APIs and microservices such as video storage and management, prebuilt AI perception pipelines, tracking algorithms, system monitoring, IoT services for secure edge-to-cloud connectivity, and more.\nDownload NVIDIA Metropolis microservices for Jetson.\nA graphic that compares using a do-it-yourself approach, where developers have to create all the pieces from scratch, and using prebuilt NVIDIA Metropolis microservices, which reduce development time from years to months.\nFigure 1. Develop edge AI applications faster with NVIDIA Metropolis microservices\nRelease highlights\nProduction-ready edge AI applications require numerous components, including AI models, optimized processing and inference pipelines, glue logic, security measures, cloud connectivity, and more. NVIDIA Metropolis microservices for Jetson streamline the application development process by offering pre-built microservices for the most ubiquitous components using a cloud-native, modular, and extensible architecture.\nThe flexibility of the platform enhances development efficiency, with standard APIs seamlessly integrating with other applications and services. The platform also provides essential services such as IoT, security, and monitoring, offering ready-to-use core components for production applications.\nWith access to more than 15 microservices across Application, Platform services, and Cloud services, developers are freed to concentrate on building intellectual property and achieving differentiation in the market.\nNVIDIA partners are incorporating NVIDIA Metropolis microservices into their offerings, including AAEON, Aetina, Advantech, Allxon, CRG, CVEDIA, Namla, Rebotnix, RidgeRun, Seeed Studio, and Silicon Highway. More are added daily.\nReference workflows and applications\nTwo reference applications are included to help get you started with NVIDIA Metropolis microservices for Jetson: the AI-enabled network video recorder (AI-NVR) and a generative AI application with zero-shot detection. These workflows show how the microservices and APIs come together to build complete applications from video ingestion, AI inference, analytics, and monitoring to securely connecting to the cloud.\nAI-enabled network video recorder\nThe AI-NVR reference workflow brings nearly all the microservices together in one comprehensive app. It includes:\nVideo ingestion and storage using the Video Storage Toolkit (VST) microservice\nPeople detection and tracking with the AI Perception service with NVIDIA DeepStream\nLine crossing and Region of Interest (ROI) insights and alerts using the Analytics service\nAn Android reference mobile application is provided to demonstrate the use of APIs to build client applications. To learn more, check out the NVIDIA On-Demand playlist, AI-NVR Using Metropolis Microservices for Jetson.\nGraphic showing the complete cloud-native architecture of AI-NVR application showing VST, AI perception service, analytics service, and all the other platform services.\nFigure 2. The AI-enabled network video recorder (AI-NVR) application architecture\nZero-shot detection using generative AI\nMetropolis microservices for Jetson enables developers to prototype and productize generative AI applications for the edge. The generative AI reference application enables zero-shot detection of live streaming data. Models can detect any objects specified with a prompt.\nPrompts can be made remotely over REST APIs to the AI Perception service to dynamically change classes to detect. Generative AI enables a new breed of AI-powered applications for the edge. To learn more about generative AI with Metropolis microservices, see Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson.\nGraphic showing the cloud-native architecture of the generative AI reference application for zero-shot object detection using a visual prompting agent.\nFigure 3. Generative AI reference application for zero-shot object detection\nPowerful microservices and APIs\nMetropolis microservices for Jetson is a collection of feature-rich microservices and APIs, including application services, platform and Board Support Package (BSP) services, and cloud services. The modular and extensible microservices make it easy to build modern cloud-native applications for the edge.\nAs a developer, you have the flexibility to choose one, several, or all of the services depending on the maturity of your product.\nGraphic showing the complete software stack from the reference AI workflow, application microservice, platform, and BSP services to cloud services.\nFigure 4. Metropolis microservices for Jetson software stack\nApplication services\nVideo Storage Toolkit: Service for video ingestion and storage\nAI Perception service using NVIDIA DeepStream: For AI inference, object tracking and metadata creation\nAI Perception service for zero-shot detection: For generative AI inference with the NanoOWL model and visual prompting\nAnalytics service: Object counting analytics such as line crossing, Region of Interest, and Field of View\nPlatform services\nRedis: Global message bus for inter-process communication\nAPI Gateway: Maps incoming API requests to the subsequent services\nMonitoring: Monitor and visualize edge device status such as utilization and app KPIs\nIoT Gateway: Secure bidirectional communication between edge and cloud\nCloud services\nIoT Cloud: Create secure connection from cloud to edge, including authentication and authorization\nSummary\nNVIDIA Metropolis microservices fast-tracks vision AI development for the edge, providing ready-to-use applications, over 15 microservices for platform services, pixel perception, video storage, analytics, and more. Download NVIDIA Metropolis microservices for Jetson.\nTo learn about the technical details of Metropolis microservices for Jetson, read the NVIDIA Metropolis Microservices for Jetson whitepaper.\nFor a tutorial on building applications using Metropolis APIs, see Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs. To learn even more, register to join us for the two-part webinar, Accelerate Edge AI Development With Metropolis APIs and Microservices for Jetson (Part 1) and How to Build With Metropolis Microservices for Jetson (Part 2)."}], "https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/": [{"text": "The article discusses the challenges of deploying AI models in production and introduces NVIDIA AI Enterprise as a solution to accelerate the process. It highlights the complexity of the AI software stack and the importance of maintaining security in the face of increasing vulnerabilities. NVIDIA AI Enterprise offers three supported branches for different needs, with regular security updates and API stability. The platform also provides transparency through security advisories and exploitability information. Furthermore, software optimization over time leads to performance gains without hardware upgrades, reducing energy consumption and costs. Enterprise support is included with every subscription, providing organizations with access to NVIDIA experts for assistance and troubleshooting. Overall, NVIDIA AI Enterprise aims to simplify the process of deploying and maintaining production AI models, allowing organizations to focus on leveraging AI for valuable insights. Interested users can request a free 90-day evaluation license to experience the platform.", "text_components": ["Advancing Production AI with NVIDIA AI Enterprise\nWhile harnessing the potential of AI is a priority for many of today\u2019s enterprises, developing and deploying an AI model involves time and effort. Often, challenges must be overcome to move a model into production, especially for mission-critical business operations. According to IDC research, only 18% of enterprises surveyed could put an AI model into production in under a month.\nThis post explores the challenges that slow down AI deployments and introduces the benefits of using a consistent, secure, and reliable platform to accelerate the journey of taking AI into production.", "The ever-growing complexity of the AI software stack\nOpen-source software (OSS) plays a critical role in advancing AI adoption. According to The State of the Octoverse 2023, there were 65K public generative AI-related GitHub projects in 2023 with 248% year-over-year growth. While the open-source community has helped fuel the AI era, the diverse range of OSS used in building AI applications makes maintaining a reliable, enterprise-grade AI software stack a complex and resource-intensive endeavor that is similar to maintaining an open-source OS.\nFor example, NVIDIA Triton Inference Server, used to standardize and scale AI deployments, relies on countless software dependencies. In Figure 1, green dots represent CUDA libraries, white dots represent OSS packages, and the lines in between represent dependencies. Any single change, such as a regular software update or security patch, can introduce an API change and result in an application failure or downtime.\nA graphic representation of NVIDIA Triton Inference Server software dependencies. Green dots represent CUDA libraries, white dots represent OSS packages, and the lines in between represent dependencies.\nFigure 1. Software dependencies of NVIDIA Triton Inference Server", "Continuous security monitoring\nThe inevitable increase in security vulnerabilities makes maintaining the AI software stack even more challenging. A recent open-source security and risk analysis report by Synopsys, indicates a 236% surge in high-risk attack patterns in OSS vulnerabilities for big data, AI, Business Intelligent, and machine learning over a 5-year period.\nNew vulnerabilities are constantly being discovered. For example, Figure 2 shows a comparison of security scanning results for the NVIDIA Triton container. In just over 3 weeks, one critical vulnerability was identified. In addition, the number of high vulnerabilities grew from four to 11. Continuous monitoring and rapid response times to fix vulnerabilities are critical for maintaining business continuity.\nTwo screenshots showing that the vulnerabilities of NVIDIA Triton increased in 3 weeks.\nFigure 2. Security scan results comparison for NVIDIA Triton", "NVIDIA AI Enterprise for production AI\nTo help address these challenges, NVIDIA introduced NVIDIA AI Enterprise, an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade AI. Built on open source and curated, optimized, and supported by NVIDIA, the NVIDIA AI Enterprise software platform enables developers to focus on building and deploying new AI services.\nNVIDIA AI Enterprise includes three supported branches: production branches, feature branches, and long-term support branches. Customers have access to all three branches and can use any mix of the three.\nProduction branches ensure API stability and regular security updates; ideal for deploying AI in production when stability is required. Released every 6 months with a 9-month lifecycle.\nFeature branches include the top-of-tree software updates; ideal for AI developers who want the faster-moving, latest development environment. Released monthly.\nLong-term support branches are ideal for highly regulated industries. Released every 2.5 years with a lifecycle of up to 3 years.", "API stability and security\nThroughout the 9-month lifecycle of each NVIDIA AI Enterprise production branch, NVIDIA continuously monitors critical and high common vulnerabilities and exposures (CVEs) and releases monthly security patches (Figure 3). By doing so, the AI frameworks, libraries, models, and tools included in NVIDIA AI Enterprise can be updated for security fixes while eliminating the risk of breaking an API.\nGraphic of NVIDIA AI Enterprise production branch lifecycle timeline.\nFigure 3. NVIDIA AI Enterprise production branch lifecycle timeline\nFigure 4 compares the version of Triton available through the production branch release of NVIDIA AI Enterprise to the open-source version of Triton. The commercial version available with the production branch of NVIDIA AI Enterprise has zero critical and high vulnerabilities, while the open-source version has nine high vulnerabilities.\nTwo screenshots of vulnerability scanning results of two PyTorch images. One from NGC, and one from NVIDIA AI Enterprise.\nFigure 4. Triton security scan results comparison", "Security through transparency\nIn addition to product branches with monthly CVE patches and bug fixes, NVIDIA AI Enterprise customers can also receive security advisories and exploitability information from NVIDIA, including Vulnerability Exploitability eXchange (VEX) and Software Bill of Materials (SBOM), vulnerability context, and remediation guidance.\nA VEX document is a relatively recent addition to the field of cybersecurity. Unlike a CVE entry, which provides general information about a vulnerability, a VEX document programmatically provides important context-specific details. It indicates whether a vulnerability is relevant (or exploitable) to particular components within the AI stack. It is also used to communicate false positives flagged by vulnerability scanning tools. VEX documents at NVIDIA are delivered in the CyclonDX format, which provides a standardized machine-readable way to share the information with customers.", "Software optimization over time for better performance and lower TCO\nAs NVIDIA continues to evolve AI software and optimize performance over time, advances in NVIDIA AI software deliver up to 54% performance gains without a hardware upgrade. Figure 5 shows NVIDIA MLPerf Inference v3.0 compared to v2.1 submission results with NVIDIA H100 GPUs. This not only improves efficiency and performance, but also reduces energy consumption, footprint, and investment in the data center or cloud.\nChart of NVIDIA MLPerf Inference v3.0 compared to v2.1 submission results on NVIDIA H100.\nFigure 5. NVIDIA inference software delivers up to 54% performance gains without a hardware upgrade", "Enterprise support\nEnterprise support is included with every NVIDIA AI Enterprise subscription, enabling organizations to benefit from the transparency of open source with the assurance of full software stack support provided by NVIDIA. Business-standard support includes:\nUnlimited technical support cases accepted through the customer portal and phone 24 hours a day, 7 days a week\nEscalation support during local business hours\nTimely resolution from NVIDIA experts and engineers\nUp to 3 years of long-term support\nWhether you need to connect with AI experts, access knowledge base resources, or troubleshoot performance issues, NVIDIA is here to help you and provide the support you need to keep your AI stable and secure.", "Get started with NVIDIA AI Enterprise\nNVIDIA AI Enterprise reduces the costs and burden of maintaining and securing the complex software platform for production AI, freeing organizations to focus on building AI and harnessing its game-changing insights.\nTo experience the enterprise platform, request a free 90-day evaluation license that grants access to all software branches and enterprise support.\nAlready an NVIDIA AI Enterprise user? Access the latest version of the production branch."], "document_title": "Advancing Production AI with NVIDIA AI Enterprise", "document_url": "https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/", "document_date": "2024-01-25T18:00:00", "document_date_modified": "2024-01-29T22:41:19", "document_full_text": "Advancing Production AI with NVIDIA AI Enterprise\nWhile harnessing the potential of AI is a priority for many of today\u2019s enterprises, developing and deploying an AI model involves time and effort. Often, challenges must be overcome to move a model into production, especially for mission-critical business operations. According to IDC research, only 18% of enterprises surveyed could put an AI model into production in under a month.\nThis post explores the challenges that slow down AI deployments and introduces the benefits of using a consistent, secure, and reliable platform to accelerate the journey of taking AI into production.\nThe ever-growing complexity of the AI software stack\nOpen-source software (OSS) plays a critical role in advancing AI adoption. According to The State of the Octoverse 2023, there were 65K public generative AI-related GitHub projects in 2023 with 248% year-over-year growth. While the open-source community has helped fuel the AI era, the diverse range of OSS used in building AI applications makes maintaining a reliable, enterprise-grade AI software stack a complex and resource-intensive endeavor that is similar to maintaining an open-source OS.\nFor example, NVIDIA Triton Inference Server, used to standardize and scale AI deployments, relies on countless software dependencies. In Figure 1, green dots represent CUDA libraries, white dots represent OSS packages, and the lines in between represent dependencies. Any single change, such as a regular software update or security patch, can introduce an API change and result in an application failure or downtime.\nA graphic representation of NVIDIA Triton Inference Server software dependencies. Green dots represent CUDA libraries, white dots represent OSS packages, and the lines in between represent dependencies.\nFigure 1. Software dependencies of NVIDIA Triton Inference Server\nContinuous security monitoring\nThe inevitable increase in security vulnerabilities makes maintaining the AI software stack even more challenging. A recent open-source security and risk analysis report by Synopsys, indicates a 236% surge in high-risk attack patterns in OSS vulnerabilities for big data, AI, Business Intelligent, and machine learning over a 5-year period.\nNew vulnerabilities are constantly being discovered. For example, Figure 2 shows a comparison of security scanning results for the NVIDIA Triton container. In just over 3 weeks, one critical vulnerability was identified. In addition, the number of high vulnerabilities grew from four to 11. Continuous monitoring and rapid response times to fix vulnerabilities are critical for maintaining business continuity.\nTwo screenshots showing that the vulnerabilities of NVIDIA Triton increased in 3 weeks.\nFigure 2. Security scan results comparison for NVIDIA Triton\nNVIDIA AI Enterprise for production AI\nTo help address these challenges, NVIDIA introduced NVIDIA AI Enterprise, an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade AI. Built on open source and curated, optimized, and supported by NVIDIA, the NVIDIA AI Enterprise software platform enables developers to focus on building and deploying new AI services.\nNVIDIA AI Enterprise includes three supported branches: production branches, feature branches, and long-term support branches. Customers have access to all three branches and can use any mix of the three.\nProduction branches ensure API stability and regular security updates; ideal for deploying AI in production when stability is required. Released every 6 months with a 9-month lifecycle.\nFeature branches include the top-of-tree software updates; ideal for AI developers who want the faster-moving, latest development environment. Released monthly.\nLong-term support branches are ideal for highly regulated industries. Released every 2.5 years with a lifecycle of up to 3 years.\nAPI stability and security\nThroughout the 9-month lifecycle of each NVIDIA AI Enterprise production branch, NVIDIA continuously monitors critical and high common vulnerabilities and exposures (CVEs) and releases monthly security patches (Figure 3). By doing so, the AI frameworks, libraries, models, and tools included in NVIDIA AI Enterprise can be updated for security fixes while eliminating the risk of breaking an API.\nGraphic of NVIDIA AI Enterprise production branch lifecycle timeline.\nFigure 3. NVIDIA AI Enterprise production branch lifecycle timeline\nFigure 4 compares the version of Triton available through the production branch release of NVIDIA AI Enterprise to the open-source version of Triton. The commercial version available with the production branch of NVIDIA AI Enterprise has zero critical and high vulnerabilities, while the open-source version has nine high vulnerabilities.\nTwo screenshots of vulnerability scanning results of two PyTorch images. One from NGC, and one from NVIDIA AI Enterprise.\nFigure 4. Triton security scan results comparison\nSecurity through transparency\nIn addition to product branches with monthly CVE patches and bug fixes, NVIDIA AI Enterprise customers can also receive security advisories and exploitability information from NVIDIA, including Vulnerability Exploitability eXchange (VEX) and Software Bill of Materials (SBOM), vulnerability context, and remediation guidance.\nA VEX document is a relatively recent addition to the field of cybersecurity. Unlike a CVE entry, which provides general information about a vulnerability, a VEX document programmatically provides important context-specific details. It indicates whether a vulnerability is relevant (or exploitable) to particular components within the AI stack. It is also used to communicate false positives flagged by vulnerability scanning tools. VEX documents at NVIDIA are delivered in the CyclonDX format, which provides a standardized machine-readable way to share the information with customers.\nSoftware optimization over time for better performance and lower TCO\nAs NVIDIA continues to evolve AI software and optimize performance over time, advances in NVIDIA AI software deliver up to 54% performance gains without a hardware upgrade. Figure 5 shows NVIDIA MLPerf Inference v3.0 compared to v2.1 submission results with NVIDIA H100 GPUs. This not only improves efficiency and performance, but also reduces energy consumption, footprint, and investment in the data center or cloud.\nChart of NVIDIA MLPerf Inference v3.0 compared to v2.1 submission results on NVIDIA H100.\nFigure 5. NVIDIA inference software delivers up to 54% performance gains without a hardware upgrade\nEnterprise support\nEnterprise support is included with every NVIDIA AI Enterprise subscription, enabling organizations to benefit from the transparency of open source with the assurance of full software stack support provided by NVIDIA. Business-standard support includes:\nUnlimited technical support cases accepted through the customer portal and phone 24 hours a day, 7 days a week\nEscalation support during local business hours\nTimely resolution from NVIDIA experts and engineers\nUp to 3 years of long-term support\nWhether you need to connect with AI experts, access knowledge base resources, or troubleshoot performance issues, NVIDIA is here to help you and provide the support you need to keep your AI stable and secure.\nGet started with NVIDIA AI Enterprise\nNVIDIA AI Enterprise reduces the costs and burden of maintaining and securing the complex software platform for production AI, freeing organizations to focus on building AI and harnessing its game-changing insights.\nTo experience the enterprise platform, request a free 90-day evaluation license that grants access to all software branches and enterprise support.\nAlready an NVIDIA AI Enterprise user? Access the latest version of the production branch."}], "https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/": [{"text": "Enterprises are increasingly adopting AI into their workflows and must address challenges such as optimization, scalability, and security. The NVIDIA NGC catalog provides GPU-optimized software containers for various stages of AI development, ensuring compatibility and performance across different NVIDIA GPUs. These containers offer benefits such as encapsulation of dependencies, reproducibility, and time and resource savings. Security and trust are central to the containers, which are regularly updated and scanned for vulnerabilities. The NGC catalog also offers Helm charts for container deployment on Kubernetes clusters. NVIDIA AI Enterprise is an enterprise-grade software platform that supports the development and deployment of production-grade AI applications, with multiple supported branches and features. It provides stability, security, manageability, and support throughout the AI development journey. Enterprises can leverage these tools to seamlessly transition from development to production and ensure uninterrupted AI excellence. Visit the NGC catalog to explore pretrained models, containers, Helm charts, and Jupyter Notebooks, and apply for a free 90-day trial of NVIDIA AI Enterprise to get started.", "text_components": ["Build Enterprise-Grade AI with NVIDIA AI Software\nFollowing the introduction of ChatGPT, enterprises around the globe are realizing the benefits and capabilities of AI, and are racing to adopt it into their workflows.\nAs this adoption accelerates, it becomes imperative for enterprises not only to keep pace with the rapid advancements in AI, but also address related challenges such as optimization, scalability, and security.\nThe enterprise AI development journey typically begins with the data ETL (extract, transform, load) phase, during which data is prepared for training. This is followed by training the AI models. Once the models are trained, the next steps involve deployment and running inference. Enterprises need to use optimized and secure software for each of these stages to build production-ready AI applications.", "Accelerate AI development with the NGC catalog\nNVIDIA NGC catalog enables enterprises to accelerate their AI development by providing a suite of GPU-optimized software and SDKs in the form of containers.\nSome of the popular containers in the catalog include RAPIDS for the data ETL phase, TensorFlow and PyTorch for the model development phase, and NVIDIA TensorRT and NVIDIA Triton Inference Server for the model deployment phase.\nFigure 1 shows the different containers available for various use cases, such as NLP, object detection, recommendation, and more. Explore the NGC catalog to find the right software for your use case.\nScreenshot of the available containers for various use cases on the NGC catalog.\nFigure 1. The NGC catalog offers containers for a wide variety of use cases", "Benefits of NGC containers\nAll the containers in the catalog are optimized to run on the latest NVIDIA accelerated computing stack and are updated regularly to provide enhanced performance on the same hardware. The containers are tested to ensure compatibility and performance across various NVIDIA GPUs.\nEnterprises can deploy these containers anywhere, including on-premises and both multi-cloud and hybrid-cloud environments. The containers can be deployed with various container orchestration systems, including the widely used open-source platform Kubernetes. Additionally, the catalog offers a range of Helm charts to facilitate the deployment and management of containers on Kubernetes clusters.\nSecurity and trust are \u200calso an integral part of the containers and models available in the catalog. All \u200ccontainers are scanned for CVEs and are assigned a security rating so that enterprises can be confident about the software they are downloading. Figure 2 shows the different security ratings for the containers.\nScreenshot of the different security ratings available for the containers in the NGC catalog.\nFigure 2. Different security ratings of the containers in the NGC catalog\nContainers in general also provide several other benefits, such as:\n\u200b\u200b Encapsulation of dependencies: Containers encapsulate all the dependencies, such as libraries and other software components, to ensure that the application can run reliably in any environment without the need for additional configuration.\nReproducibility: Containers contain everything needed to run an application, so they will behave the same way regardless of where they are deployed. This reduces the chances of issues related to environment-specific configurations.\nTime and resource savings: By simplifying the deployment process and ensuring consistency across environments, containers save considerable time and resources that would otherwise be spent on manual setup and troubleshooting.\nUsing the catalog software, enterprise developers can start building POCs and testing their solutions. When enterprises are ready to move into production, they need to ensure that the requirements for integrating AI into their business applications are met.\nThese requirements include enterprise-grade security, software stack stability, enterprise support, and manageability. Having a clear path to assist in transitioning from development to production is part of the success factor of time to value.", "Seamless transition from development to production\nThe diverse range of software components and associated interdependencies make maintaining a secure, reliable AI software stack a massive undertaking, especially when AI needs to be deployed and integrated into enterprise applications. NGC catalog enables enterprise developers to seamlessly transition by offering both community-based software for development and enterprise-grade software branches for production.\nFor example, the catalog offers multiple containers for NVIDIA Triton Inference Server (Figure 3). NVIDIA Triton Inference Server Feature Branch is the latest version, released in a monthly cadence, to provide developers access to the latest features and performance optimizations.\nImage showing the search results for NVIDIA Triton Inference Server query.\nFigure 3. Different NGC containers are available for NVIDIA Triton Inference Server\nNVIDIA Triton Inference Server Production Branch, available exclusively with NVIDIA AI Enterprise, is the production-grade version purposely built to provide stability and a secure environment for building mission-critical AI applications. The production branch releases every 6 months with a 9-month lifetime to ensure API stability. Within the 9-month lifecycle, NVIDIA continuously tracks critical vulnerabilities and releases CVE patches and bug fixes monthly without breaking the software stack.\nWith different optimized AI software options, enterprise developers can leverage the catalog to choose the proper software packages that support a given AI pipeline while maintaining security.", "Uninterrupted AI excellence with NVIDIA AI Enterprise\nSecurity, reliability, and manageability are critical for enterprise-grade AI. To address these challenges, NVIDIA has introduced NVIDIA AI Enterprise, an enterprise-grade software platform that accelerates the data science pipeline and streamlines the development and deployment of production-grade AI applications.\nBuilt on open source and curated, optimized, and supported by NVIDIA, NVIDIA AI Enterprise offers multiple supported branches (production, feature, and long-term support). It also includes enterprise-grade security, stability, manageability, and support throughout your AI journey with NGC.", "Summary\nThe NGC catalog and NVIDIA AI Enterprise offer enterprises the necessary tools to keep pace with the rapid advancements in AI while addressing related challenges such as optimization, scalability, and security.\nVisit the NGC catalog to browse hundreds of pretrained models, containers, Helm charts, and Jupyter Notebooks. To get started with NVIDIA AI Enterprise, apply for a free 90-day trial."], "document_title": "Build Enterprise-Grade AI with NVIDIA AI Software", "document_url": "https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/", "document_date": "2024-01-24T20:30:00", "document_date_modified": "2024-01-25T18:41:13", "document_full_text": "Build Enterprise-Grade AI with NVIDIA AI Software\nFollowing the introduction of ChatGPT, enterprises around the globe are realizing the benefits and capabilities of AI, and are racing to adopt it into their workflows.\nAs this adoption accelerates, it becomes imperative for enterprises not only to keep pace with the rapid advancements in AI, but also address related challenges such as optimization, scalability, and security.\nThe enterprise AI development journey typically begins with the data ETL (extract, transform, load) phase, during which data is prepared for training. This is followed by training the AI models. Once the models are trained, the next steps involve deployment and running inference. Enterprises need to use optimized and secure software for each of these stages to build production-ready AI applications.\nAccelerate AI development with the NGC catalog\nNVIDIA NGC catalog enables enterprises to accelerate their AI development by providing a suite of GPU-optimized software and SDKs in the form of containers.\nSome of the popular containers in the catalog include RAPIDS for the data ETL phase, TensorFlow and PyTorch for the model development phase, and NVIDIA TensorRT and NVIDIA Triton Inference Server for the model deployment phase.\nFigure 1 shows the different containers available for various use cases, such as NLP, object detection, recommendation, and more. Explore the NGC catalog to find the right software for your use case.\nScreenshot of the available containers for various use cases on the NGC catalog.\nFigure 1. The NGC catalog offers containers for a wide variety of use cases\nBenefits of NGC containers\nAll the containers in the catalog are optimized to run on the latest NVIDIA accelerated computing stack and are updated regularly to provide enhanced performance on the same hardware. The containers are tested to ensure compatibility and performance across various NVIDIA GPUs.\nEnterprises can deploy these containers anywhere, including on-premises and both multi-cloud and hybrid-cloud environments. The containers can be deployed with various container orchestration systems, including the widely used open-source platform Kubernetes. Additionally, the catalog offers a range of Helm charts to facilitate the deployment and management of containers on Kubernetes clusters.\nSecurity and trust are \u200calso an integral part of the containers and models available in the catalog. All \u200ccontainers are scanned for CVEs and are assigned a security rating so that enterprises can be confident about the software they are downloading. Figure 2 shows the different security ratings for the containers.\nScreenshot of the different security ratings available for the containers in the NGC catalog.\nFigure 2. Different security ratings of the containers in the NGC catalog\nContainers in general also provide several other benefits, such as:\n\u200b\u200b Encapsulation of dependencies: Containers encapsulate all the dependencies, such as libraries and other software components, to ensure that the application can run reliably in any environment without the need for additional configuration.\nReproducibility: Containers contain everything needed to run an application, so they will behave the same way regardless of where they are deployed. This reduces the chances of issues related to environment-specific configurations.\nTime and resource savings: By simplifying the deployment process and ensuring consistency across environments, containers save considerable time and resources that would otherwise be spent on manual setup and troubleshooting.\nUsing the catalog software, enterprise developers can start building POCs and testing their solutions. When enterprises are ready to move into production, they need to ensure that the requirements for integrating AI into their business applications are met.\nThese requirements include enterprise-grade security, software stack stability, enterprise support, and manageability. Having a clear path to assist in transitioning from development to production is part of the success factor of time to value.\nSeamless transition from development to production\nThe diverse range of software components and associated interdependencies make maintaining a secure, reliable AI software stack a massive undertaking, especially when AI needs to be deployed and integrated into enterprise applications. NGC catalog enables enterprise developers to seamlessly transition by offering both community-based software for development and enterprise-grade software branches for production.\nFor example, the catalog offers multiple containers for NVIDIA Triton Inference Server (Figure 3). NVIDIA Triton Inference Server Feature Branch is the latest version, released in a monthly cadence, to provide developers access to the latest features and performance optimizations.\nImage showing the search results for NVIDIA Triton Inference Server query.\nFigure 3. Different NGC containers are available for NVIDIA Triton Inference Server\nNVIDIA Triton Inference Server Production Branch, available exclusively with NVIDIA AI Enterprise, is the production-grade version purposely built to provide stability and a secure environment for building mission-critical AI applications. The production branch releases every 6 months with a 9-month lifetime to ensure API stability. Within the 9-month lifecycle, NVIDIA continuously tracks critical vulnerabilities and releases CVE patches and bug fixes monthly without breaking the software stack.\nWith different optimized AI software options, enterprise developers can leverage the catalog to choose the proper software packages that support a given AI pipeline while maintaining security.\nUninterrupted AI excellence with NVIDIA AI Enterprise\nSecurity, reliability, and manageability are critical for enterprise-grade AI. To address these challenges, NVIDIA has introduced NVIDIA AI Enterprise, an enterprise-grade software platform that accelerates the data science pipeline and streamlines the development and deployment of production-grade AI applications.\nBuilt on open source and curated, optimized, and supported by NVIDIA, NVIDIA AI Enterprise offers multiple supported branches (production, feature, and long-term support). It also includes enterprise-grade security, stability, manageability, and support throughout your AI journey with NGC.\nSummary\nThe NGC catalog and NVIDIA AI Enterprise offer enterprises the necessary tools to keep pace with the rapid advancements in AI while addressing related challenges such as optimization, scalability, and security.\nVisit the NGC catalog to browse hundreds of pretrained models, containers, Helm charts, and Jupyter Notebooks. To get started with NVIDIA AI Enterprise, apply for a free 90-day trial."}], "https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/": [{"text": "NVIDIA DOCA 2.5, a software framework for data center infrastructure developers, is crucial for efficient AI cloud deployments. It is the first LTS version for AI cloud deployments with stability and robustness. DOCA 2.5, alongside NVIDIA switches, BlueField DPUs, and SuperNICs, supports demanding AI workloads. The NVIDIA Spectrum-X platform, with BlueField-3 SuperNICs, enhances AI performance and security. DOCA PCC library enables customized congestion control in multi-tenant AI cloud environments. DOCA Flow aids in developing network policies and managing resources programmatically. DOCA services like host-based networking, Firefly, and Storage SNAPv4 provide advanced features for AI infrastructure. NVIDIA's full-stack architecture ensures operational effectiveness across platforms. The latest DOCA SDK enables advanced GPU-accelerated AI workloads. NVIDIA's networking solutions are essential for organizations embedding AI into their infrastructure. The NVIDIA DOCA framework is crucial for powering AI clouds efficiently.", "text_components": ["Delivering Efficient, High-Performance AI Clouds with NVIDIA DOCA 2.5\nAs a comprehensive software framework for data center infrastructure developers, NVIDIA DOCA has been adopted by leading AI, cloud, enterprise, and ISV innovators. The release of DOCA 2.5 marks its third anniversary. And, due to the stability and robustness of the code base combined with several networking and platform upgrades, DOCA 2.5 is the first NVIDIA BlueField-3 long-term support (LTS) version for AI cloud deployments.\nAlongside NVIDIA switches, BlueField DPUs, and SuperNICs, DOCA 2.5 is an essential element of a co-designed platform created to support the most demanding AI workloads. Forming part of the NVIDIA full stack architecture, networking components from NVIDIA deliver optimum application performance and security and data center efficiency. When deployed alongside the NVIDIA computing platform and software tools, they offer additional benefits and synergies.\nHere are some of the newest network offerings from NVIDIA and how DOCA 2.5 is an integral part of AI infrastructure.", "Backbone of AI infrastructure\nIt\u2019s now widely understood that a high-performance network is the backbone of efficient AI infrastructure. To achieve optimal AI performance, significant consideration must be given to its capabilities, implementation, and deployment for both generative AI and foundational models.\nDue to their distinct properties and significant computational demands, modern AI workloads require specialized network infrastructure to operate at peak efficiency. Leading the way in AI and accelerated computing, we created the NVIDIA Spectrum-X Ethernet networking platform to meet this requirement and improve the effectiveness and performance of AI clouds.\nThe Spectrum-4 Ethernet switch and BlueField-3 SuperNIC from NVIDIA form the basis of the Spectrum-X platform and the foundation of our accelerated computing fabric for artificial intelligence. The BlueField-3 SuperNIC offers numerous technology benefits for a wide range of industries. When deployed in our flagship AI systems, BlueField-3 SuperNICs not only enhance performance but also provide deterministic and isolated performance for tenant jobs.\nPictures of an NVIDIA Spectrum-4 Switch and NVIDIA BlueField-3 DPU.\nFigure 1. NVIDIA Spectrum-X and BlueField-3 hardware", "NVIDIA synergy\nThe Spectrum-X platform combines co-designed, best-in-class hardware to deliver unparalleled performance synergies and an unmatched customer experience. Integral to the design, BlueField-3 SuperNICs take Ethernet networking to new heights for AI systems running on a cluster of GPU-based servers.\nIn contrast, conventional network interface cards lack the required features for AI workloads. BlueField SuperNICs ensure that the required processes for effectively executed cloud-based AI workloads are delivered with efficiency and speed.\nWhen combined with an NVIDIA GPU, this marriage of technologies (available for most enterprise-class servers) creates an optimized solution for AI cloud computing, delivering matchless levels of efficiency, performance, and flexibility.\nValidated across the full stack of NVIDIA hardware and software, Spectrum-X and NVIDIA GPUs create a truly peerless Ethernet solution for AI clouds. With such broad levels of integration available, the opportunity for fine-tuning provides custom-like levels of modification for truly unique solutions, dedicated to the delivery of precision workloads.\nAs a component of the full stack, DOCA is a critical piece of the AI puzzle and ties together compute, networking, storage, and security.\nDiagram includes SONic, Cumulus, NetQ, DOCA Services, NVIDIA Air, SAI/SPSDK, DOCA, and Magnum IO.\nFigure 2. NVIDIA hardware and software stack", "New features for AI clouds and data center infrastructure\nDOCA helps to enable the most advanced, GPU-accelerated, AI workloads today. For systems that include a GPU and NVIDIA BlueField-3 DPUs or BlueField-3 SuperNICs, there are further advantages for developers.\nBlueField-3 DPU\nBlueField-3 SuperNIC\nMission\n> Cloud infrastructure processor\n> Offload, accelerate, and isolate data center infrastructure\n> Optimized for N-S in GPU-class systems\n> Accelerated networking for AI computing\n> Best-in-class RoCE networking\n> Optimized for E-W in GPU-class systems\nShared Capabilities\n> VPC network acceleration\n> Network encryption acceleration\n> Programmable network pipeline\n> Precision timing\n> Platform security\nUnique Capabilities\n> Powerful computing\n> Secure, zero-trust management\n> Data storage acceleration\n> Elastic infrastructure provisioning\n> 1-2 DPUs per system\n> Powerful networking\n> AI networking feature set\n> Full-stack NVIDIA AI optimization\n> Power-efficient, low-profile design\n> Up to 8 SuperNICs per system\nTable 1. NVIDIA BlueField-3 DPU and SuperNIC comparison\nSpecifically, DOCA capitalizes on the numerous NVIDIA-led development, integration, and testing programs that enable and optimize the entire range of AI application frameworks. The convergence of NVIDIA technologies fuels data center innovation and rapid AI application deployment.\nReleased in December 2023, DOCA 2.5 offers several enhancements that boost performance within the data center. There\u2019s a continuing increase in both the number of virtual functions and the volume of \u2018east-west\u2019 network traffic. In response, the use of DOCA and BlueField-3 SuperNICs is imperative to optimize the network and establish its function as the backbone of a modern AI infrastructure.\nDiagram shows an application layer (including networking, security, and storage), DOCA services (including Orchestration, Telemetry, and Firefly), libraries (including Crypto, App Shield, and Rivermax), and drivers (including UCX, UCC, and RDMA).\nFigure 4. DOCA 2.5 architecture", "DOCA-PCC now available\nWithin multi-tenant AI cloud environments where multiple AI jobs run simultaneously, there is a potential for network congestion to arise.\nThe DOCA PCC library, now GA, provides a high-level programming interface that enables partners to implement customized congestion control (CC) algorithms. This library uses the NVIDIA BlueField-3 SuperNIC acceleration for CC management and provides an API that abstracts hardware complexity to simplify programming. Partners can focus on the functionality of your CC algorithm and implement it quickly with BlueField hardware acceleration.\nDOCA PCC also gives you the flexibility to develop an optimal solution to handle congestion in your clusters. Customized congestion control is critical for AI workflows, enabling performance isolation, improving fairness, and preventing packet drop on lossy networks.\nNVIDIA Spectrum-X is a breakthrough Ethernet networking solution for building multi-tenant, hyperscale AI clouds. It uses DOCA PCC to implement congestion control.", "DOCA Flow: New and enhanced features for cloud deployments\nDOCA Flow is an essential programming tool used to develop DOCA services. DOCA 2.5 adds additional support for the development of NVIDIA OVS-DOCA, an innovative and performant virtual switch that is native to NVIDIA NICs and DPUs, and NVIDIA DOCA HBN services.\nWith NVIDIA DOCA Flow, you can define and control the flow of network traffic, implement network policies, and manage network resources programmatically. It offers network virtualization, telemetry, load balancing, security enforcement, and traffic monitoring.\nThese capabilities are beneficial for processing high-packet workloads with low latency, conserving CPU resources, and reducing power usage. Fundamentally, DOCA Flow is a key enabler for multiple use cases in cloud networking. Used for the development of custom software-defined networking (SDN), this is a key building block for CSPs designing the networks of tomorrow.", "DOCA services\nThe following are some examples of DOCA services that have been upgraded in the DOCA 2.5 release:\nHost-base networking\nDOCA Firefly\nStorage SNAPv4", "Host-based networking\nUpgraded in DOCA 2.5, host-based networking (HBN) is a DOCA service that enables network architects to design networks based purely on L3 protocols, enabling routing to run on the servers of the network. In the case of BlueField, the HBN solution packages a set of network functions inside a container that is packaged as a service pod running on the DPU.\nDOCA HBN gives network architects the ability to create controller-less virtual private clouds (VPCs). This is ideal for CSPs, telcos, and enterprise customers deploying bare-metal as a service (BMaaS) infrastructures.\nCompared to conventional networking solutions, using DOCA HBN presents you with a number of benefits. In addition to improving the scalability and efficiency of deployment, DOCA HBN offers enhanced security options, a simplified underlay network fabric, and reduced OPEX. If used in conjunction with a third-party switch manufacturer, DOCA HBN shifts several top-of-rack (ToR) switch functions to the BlueField-3 DPU or SuperNIC, leading to a reduction in third-party license costs.\nFor more information about the new HBN functions, including support of RoCE, Routing, and ACL enhancements, see the DOCA 2.5 release notes.", "DOCA Firefly\nThis feature provides Precision Time Protocol (PTP)\u2013based time synchronization services that use the hardware acceleration of NVIDIA DPUs and SuperNICs.\nIndustry-specific PTP use cases include the following:\nTelco: Network-based time synchronization essential for 5G mobile deployment\nMedia and entertainment:\nQoS for video, audio, and metadata transmission\nMeeting stringent broadcast quality requirements\nData center: Time distribution\nFinancial services industry:\nHigh-frequency trading (HFT)\nMiFID II compliance (required)\nNew to DOCA 2.5, DOCA Firefly now includes industry-specific profiles to improve the user experience and simplify deployment. Profiles currently include Media and Telco, which are configured to include industry-specific functions and performance parameters.", "Storage SNAPv4\nThe DOCA SNAPv4 service on BlueField-3 adds inline AES-XTS, the default cryptographic algorithm for protecting the confidentiality of data-at-rest on storage devices. SNAP now accelerates AES-XTS encryption in hardware, which optimizes and improves the encryption process while benefiting from a reduced CPU overhead.\nThe SNAPv4 service for virtio-blk now offers Recovery/Hot-Upgrade/LM without force-in-order. This new feature improves support for Recovery, Hot-Upgrade, and Live-Migration functions and means that it\u2019s no longer necessary to operate using force-in-order traffic. This equates to a more practical tool for customers in real-world settings whereby typical customers, such as CSPs, can now offer improved uptime and uninterrupted performance for end users undertaking vital storage tasks.", "More updates\nFor more information about the following list of updates and features, see the DOCA 2.5 release notes:\nDevice Attestation\nDPA User Application Signing and Authentication [Beta]\nDPU Firmware TPM [Beta]\nDPU Upgrade tool\nNew qualification, certification, and management features", "Conclusion\nModern AI workloads require sophisticated network solutions to operate effectively at peak efficiency. Today, organizations across the globe are facing a similar significant challenge when trying to embed AI into their existing operational and technical infrastructure.\nTo meet this requirement, NVIDIA, as the leader in AI and accelerated computing, has created an optimized networking platform to drive the performance of AI cloud computing. Central to the effectiveness of this platform are the synergies gained from complementary technologies employed by the various NVIDIA-branded hardware and software solutions.\nIn their full-stack architecture, NVIDIA implemented several design considerations to ensure increased operational effectiveness between the various platforms. When combined with NVIDIA GPUs, Spectrum-X, a solution comprised of NVIDIA Ethernet Switches and BlueField SuperNICs, creates a truly peerless Ethernet platform for AI clouds. With the latest release of NVIDIA DOCA SDK, NVIDIA has made additional strides to further enable the most advanced, GPU-accelerated, AI workloads today.\nTo begin your development journey with all the benefits DOCA has to offer, download NVIDIA DOCA today. For more information, see the following resources:\nDemystifying NVIDIA DOCA\nUnderstanding When to Use DOCA Drivers and DOCA Libraries\nIntroduction to DOCA for DPUs free course\nGetting Started with DOCA Flow self-paced course\nDelivering an AI-Ready Infrastructure Today for Powering the AI Factories of Tomorrow GTC session"], "document_title": "Delivering Efficient, High-Performance AI Clouds with NVIDIA DOCA 2.5", "document_url": "https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/", "document_date": "2024-01-24T19:31:39", "document_date_modified": "2024-01-25T18:17:24", "document_full_text": "Delivering Efficient, High-Performance AI Clouds with NVIDIA DOCA 2.5\nAs a comprehensive software framework for data center infrastructure developers, NVIDIA DOCA has been adopted by leading AI, cloud, enterprise, and ISV innovators. The release of DOCA 2.5 marks its third anniversary. And, due to the stability and robustness of the code base combined with several networking and platform upgrades, DOCA 2.5 is the first NVIDIA BlueField-3 long-term support (LTS) version for AI cloud deployments.\nAlongside NVIDIA switches, BlueField DPUs, and SuperNICs, DOCA 2.5 is an essential element of a co-designed platform created to support the most demanding AI workloads. Forming part of the NVIDIA full stack architecture, networking components from NVIDIA deliver optimum application performance and security and data center efficiency. When deployed alongside the NVIDIA computing platform and software tools, they offer additional benefits and synergies.\nHere are some of the newest network offerings from NVIDIA and how DOCA 2.5 is an integral part of AI infrastructure.\nBackbone of AI infrastructure\nIt\u2019s now widely understood that a high-performance network is the backbone of efficient AI infrastructure. To achieve optimal AI performance, significant consideration must be given to its capabilities, implementation, and deployment for both generative AI and foundational models.\nDue to their distinct properties and significant computational demands, modern AI workloads require specialized network infrastructure to operate at peak efficiency. Leading the way in AI and accelerated computing, we created the NVIDIA Spectrum-X Ethernet networking platform to meet this requirement and improve the effectiveness and performance of AI clouds.\nThe Spectrum-4 Ethernet switch and BlueField-3 SuperNIC from NVIDIA form the basis of the Spectrum-X platform and the foundation of our accelerated computing fabric for artificial intelligence. The BlueField-3 SuperNIC offers numerous technology benefits for a wide range of industries. When deployed in our flagship AI systems, BlueField-3 SuperNICs not only enhance performance but also provide deterministic and isolated performance for tenant jobs.\nPictures of an NVIDIA Spectrum-4 Switch and NVIDIA BlueField-3 DPU.\nFigure 1. NVIDIA Spectrum-X and BlueField-3 hardware\nNVIDIA synergy\nThe Spectrum-X platform combines co-designed, best-in-class hardware to deliver unparalleled performance synergies and an unmatched customer experience. Integral to the design, BlueField-3 SuperNICs take Ethernet networking to new heights for AI systems running on a cluster of GPU-based servers.\nIn contrast, conventional network interface cards lack the required features for AI workloads. BlueField SuperNICs ensure that the required processes for effectively executed cloud-based AI workloads are delivered with efficiency and speed.\nWhen combined with an NVIDIA GPU, this marriage of technologies (available for most enterprise-class servers) creates an optimized solution for AI cloud computing, delivering matchless levels of efficiency, performance, and flexibility.\nValidated across the full stack of NVIDIA hardware and software, Spectrum-X and NVIDIA GPUs create a truly peerless Ethernet solution for AI clouds. With such broad levels of integration available, the opportunity for fine-tuning provides custom-like levels of modification for truly unique solutions, dedicated to the delivery of precision workloads.\nAs a component of the full stack, DOCA is a critical piece of the AI puzzle and ties together compute, networking, storage, and security.\nDiagram includes SONic, Cumulus, NetQ, DOCA Services, NVIDIA Air, SAI/SPSDK, DOCA, and Magnum IO.\nFigure 2. NVIDIA hardware and software stack\nNew features for AI clouds and data center infrastructure\nDOCA helps to enable the most advanced, GPU-accelerated, AI workloads today. For systems that include a GPU and NVIDIA BlueField-3 DPUs or BlueField-3 SuperNICs, there are further advantages for developers.\nBlueField-3 DPU\nBlueField-3 SuperNIC\nMission\n> Cloud infrastructure processor\n> Offload, accelerate, and isolate data center infrastructure\n> Optimized for N-S in GPU-class systems\n> Accelerated networking for AI computing\n> Best-in-class RoCE networking\n> Optimized for E-W in GPU-class systems\nShared Capabilities\n> VPC network acceleration\n> Network encryption acceleration\n> Programmable network pipeline\n> Precision timing\n> Platform security\nUnique Capabilities\n> Powerful computing\n> Secure, zero-trust management\n> Data storage acceleration\n> Elastic infrastructure provisioning\n> 1-2 DPUs per system\n> Powerful networking\n> AI networking feature set\n> Full-stack NVIDIA AI optimization\n> Power-efficient, low-profile design\n> Up to 8 SuperNICs per system\nTable 1. NVIDIA BlueField-3 DPU and SuperNIC comparison\nSpecifically, DOCA capitalizes on the numerous NVIDIA-led development, integration, and testing programs that enable and optimize the entire range of AI application frameworks. The convergence of NVIDIA technologies fuels data center innovation and rapid AI application deployment.\nReleased in December 2023, DOCA 2.5 offers several enhancements that boost performance within the data center. There\u2019s a continuing increase in both the number of virtual functions and the volume of \u2018east-west\u2019 network traffic. In response, the use of DOCA and BlueField-3 SuperNICs is imperative to optimize the network and establish its function as the backbone of a modern AI infrastructure.\nDiagram shows an application layer (including networking, security, and storage), DOCA services (including Orchestration, Telemetry, and Firefly), libraries (including Crypto, App Shield, and Rivermax), and drivers (including UCX, UCC, and RDMA).\nFigure 4. DOCA 2.5 architecture\nDOCA-PCC now available\nWithin multi-tenant AI cloud environments where multiple AI jobs run simultaneously, there is a potential for network congestion to arise.\nThe DOCA PCC library, now GA, provides a high-level programming interface that enables partners to implement customized congestion control (CC) algorithms. This library uses the NVIDIA BlueField-3 SuperNIC acceleration for CC management and provides an API that abstracts hardware complexity to simplify programming. Partners can focus on the functionality of your CC algorithm and implement it quickly with BlueField hardware acceleration.\nDOCA PCC also gives you the flexibility to develop an optimal solution to handle congestion in your clusters. Customized congestion control is critical for AI workflows, enabling performance isolation, improving fairness, and preventing packet drop on lossy networks.\nNVIDIA Spectrum-X is a breakthrough Ethernet networking solution for building multi-tenant, hyperscale AI clouds. It uses DOCA PCC to implement congestion control.\nDOCA Flow: New and enhanced features for cloud deployments\nDOCA Flow is an essential programming tool used to develop DOCA services. DOCA 2.5 adds additional support for the development of NVIDIA OVS-DOCA, an innovative and performant virtual switch that is native to NVIDIA NICs and DPUs, and NVIDIA DOCA HBN services.\nWith NVIDIA DOCA Flow, you can define and control the flow of network traffic, implement network policies, and manage network resources programmatically. It offers network virtualization, telemetry, load balancing, security enforcement, and traffic monitoring.\nThese capabilities are beneficial for processing high-packet workloads with low latency, conserving CPU resources, and reducing power usage. Fundamentally, DOCA Flow is a key enabler for multiple use cases in cloud networking. Used for the development of custom software-defined networking (SDN), this is a key building block for CSPs designing the networks of tomorrow.\nDOCA services\nThe following are some examples of DOCA services that have been upgraded in the DOCA 2.5 release:\nHost-base networking\nDOCA Firefly\nStorage SNAPv4\nHost-based networking\nUpgraded in DOCA 2.5, host-based networking (HBN) is a DOCA service that enables network architects to design networks based purely on L3 protocols, enabling routing to run on the servers of the network. In the case of BlueField, the HBN solution packages a set of network functions inside a container that is packaged as a service pod running on the DPU.\nDOCA HBN gives network architects the ability to create controller-less virtual private clouds (VPCs). This is ideal for CSPs, telcos, and enterprise customers deploying bare-metal as a service (BMaaS) infrastructures.\nCompared to conventional networking solutions, using DOCA HBN presents you with a number of benefits. In addition to improving the scalability and efficiency of deployment, DOCA HBN offers enhanced security options, a simplified underlay network fabric, and reduced OPEX. If used in conjunction with a third-party switch manufacturer, DOCA HBN shifts several top-of-rack (ToR) switch functions to the BlueField-3 DPU or SuperNIC, leading to a reduction in third-party license costs.\nFor more information about the new HBN functions, including support of RoCE, Routing, and ACL enhancements, see the DOCA 2.5 release notes.\nDOCA Firefly\nThis feature provides Precision Time Protocol (PTP)\u2013based time synchronization services that use the hardware acceleration of NVIDIA DPUs and SuperNICs.\nIndustry-specific PTP use cases include the following:\nTelco: Network-based time synchronization essential for 5G mobile deployment\nMedia and entertainment:\nQoS for video, audio, and metadata transmission\nMeeting stringent broadcast quality requirements\nData center: Time distribution\nFinancial services industry:\nHigh-frequency trading (HFT)\nMiFID II compliance (required)\nNew to DOCA 2.5, DOCA Firefly now includes industry-specific profiles to improve the user experience and simplify deployment. Profiles currently include Media and Telco, which are configured to include industry-specific functions and performance parameters.\nStorage SNAPv4\nThe DOCA SNAPv4 service on BlueField-3 adds inline AES-XTS, the default cryptographic algorithm for protecting the confidentiality of data-at-rest on storage devices. SNAP now accelerates AES-XTS encryption in hardware, which optimizes and improves the encryption process while benefiting from a reduced CPU overhead.\nThe SNAPv4 service for virtio-blk now offers Recovery/Hot-Upgrade/LM without force-in-order. This new feature improves support for Recovery, Hot-Upgrade, and Live-Migration functions and means that it\u2019s no longer necessary to operate using force-in-order traffic. This equates to a more practical tool for customers in real-world settings whereby typical customers, such as CSPs, can now offer improved uptime and uninterrupted performance for end users undertaking vital storage tasks.\nMore updates\nFor more information about the following list of updates and features, see the DOCA 2.5 release notes:\nDevice Attestation\nDPA User Application Signing and Authentication [Beta]\nDPU Firmware TPM [Beta]\nDPU Upgrade tool\nNew qualification, certification, and management features\nConclusion\nModern AI workloads require sophisticated network solutions to operate effectively at peak efficiency. Today, organizations across the globe are facing a similar significant challenge when trying to embed AI into their existing operational and technical infrastructure.\nTo meet this requirement, NVIDIA, as the leader in AI and accelerated computing, has created an optimized networking platform to drive the performance of AI cloud computing. Central to the effectiveness of this platform are the synergies gained from complementary technologies employed by the various NVIDIA-branded hardware and software solutions.\nIn their full-stack architecture, NVIDIA implemented several design considerations to ensure increased operational effectiveness between the various platforms. When combined with NVIDIA GPUs, Spectrum-X, a solution comprised of NVIDIA Ethernet Switches and BlueField SuperNICs, creates a truly peerless Ethernet platform for AI clouds. With the latest release of NVIDIA DOCA SDK, NVIDIA has made additional strides to further enable the most advanced, GPU-accelerated, AI workloads today.\nTo begin your development journey with all the benefits DOCA has to offer, download NVIDIA DOCA today. For more information, see the following resources:\nDemystifying NVIDIA DOCA\nUnderstanding When to Use DOCA Drivers and DOCA Libraries\nIntroduction to DOCA for DPUs free course\nGetting Started with DOCA Flow self-paced course\nDelivering an AI-Ready Infrastructure Today for Powering the AI Factories of Tomorrow GTC session"}], "https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/": [{"text": "The article discusses the importance of safety in industrial automation and how artificial intelligence (AI) can be used to enhance safety measures in factories. Different types of safety, such as system safety, worker safety, and functional safety, are explained. AI-powered safety platforms can predict and prevent accidents by monitoring machines and robots. The use of AI in safety-related systems is being standardized in upcoming international activities. NVIDIA IGX Orin is highlighted as a platform that combines hardware and software to implement both reactive and proactive safety measures. An example of proactive and reactive safety in action is provided, showing how AI can detect and respond to worker movements near industrial machines. The use of NVIDIA DeepStream and Protex.AI for computer vision and safety zone monitoring is also discussed. The article encourages readers to learn more about AI-powered safety solutions and sign up for related sessions at the GTC conference.", "text_components": ["Using the Power of AI to Make Factories Safer\nAs industrial automation increases, safety becomes a greater challenge and top priority for enterprises.\nSafety encompasses multiple aspects:\nSystem safety: A rational pursuit of acceptable mishap risk within a systems perspective.\nSafety of the intended functionality: The absence of unreasonable risk due to hazards resulting from functional insufficiencies of the intended functionality or by reasonably foreseeable misuse by persons.\nFunctional safety: Part of the overall safety that depends on the correct functioning of the safety-related systems and external risk reduction facilities.\nWorker safety: Providing a safe working environment for employees by incorporating safe equipment and safe procedures at the workplace.\nAnd other, more general types of safety.\nThe same technological solution that\u2019s driving automation can be used to also address safety: artificial intelligence.\nAI-powered stationary outside-in safety platforms, which monitor activity across many distributed machines or robots, can predictively and proactively orchestrate consistent safety policies. Machines and robots equipped with inside-out reactive safety can detect any specific interaction within their workspace and take appropriate measures.\nDiagram shows orchestrating distributed mobile machines with stationary safety platforms.\nFigure 1. Inside-out and outside-in safety\nIEC 61508 is an international standard published by the International Electrotechnical Commission (IEC). It consists of methods to apply, design, deploy, and maintain safety-related systems.\nUsing IEC 61508 general architecture, AI can either be used to implement the safety function or to assist functional safety to provide additional risk reduction measures.\nWorkflow diagram shows an EUC control system assisting reactive functional safety with proactive safety functions.\nFigure 2. Reactive and proactive safety\nThe use of AI in safety-related systems is being analyzed in upcoming international standardization activities such as ISO/IEC TR 5469 and ISO/IEC AWI TS 22440 being developed by ISO/IEC joint working group JWG 4.\nThe examples in Table 1 describe some typical safety use cases that can benefit from AI.\nUse case\nSurveilled machines\nDetecting position and movement of humans toward a machine safety zone.\nconveyor belt, high-speed door, collaborative robot, autonomous mobile robot, forklift\nDetecting machines moving in the direction of a forbidden area or in the direction of other machines; machines moving at excessive speed or excessive strength.\nautonomous mobile robot, forklift\nDetecting defects or material that could cause safety hazards based on norms such as ANSI/RIA R15.08, and so on.\nautonomous mobile robot, train, airplane\nDetecting and classifying worker or object poses and movement.\nFactory, warehouses\nTable 1. Use cases and machines that can benefit from AI proactive safety functions The following examples of responses to those proactive safety functions are ordered by increasing level of intervention:\nUpdating statistics to later optimize worker-machine or machine-machine interactions, change the factory floor plan, or improve worker training.\nInforming a safety supervisor.\nGenerating visible and audible alerts to workers, through machine local alerts or personal devices.\nSlowing down or stopping a machine (acting as a speed or force limiter).\nChanging the position or trajectory of the machine.\nPreventing the start of the equipment mission.\nActivating hazard protection devices.\nNVIDIA IGX Orin combines industrial-grade hardware with enterprise-level software and support to implement both inside-out reactive and outside-in proactive safety in the same platform:\nAI safety framework\nFramework to provide proactive safety through AI.\nSafety Extension Package (SEP)\nSafety services for fault avoidance, detection, and control leveraging hardware safety extensions\nEdge Safety Link, a safety communication protocol.\nHardware safety extensions:\nHundreds of diagnostic mechanisms implemented in NVIDIA Orin SoC elements (CPU, GPU, accelerators, interfaces, ECC, and so on), In-System-Test, clock, temperature and voltage monitors, and so on.\nFunctional Safety Island (FSI): An independent, redundant set of processors with onboard memory that is used as a monitoring and safety processor working beside other SoC engines.\nSafety MCU: A third-party, safety-certified microcontroller, collaborating with FSI and Orin SoC, and controlling board-level voltage and temperature monitors.\nSafety documentation\nSet of documents (safety application note, safety manual, and FMEDA) to support customer safety cases.", "Safety in action example\nHere\u2019s an example of proactive and reactive safety in action, implemented in collaboration with FORT Robotics and Protex.AI.\nWorkflow diagram shows warn zone, stop zone, and communication channels to safety protocol elements.\nFigure 3. Proactive and reactive safety in action\nIn the example, an HD IP camera connected with Ethernet to NVIDIA IGX running Protex AI stack detects a worker moving toward an industrial mobile machine.\nPhoto of FORT robotics warn and stop zones labeled.\nFigure 4. Overall setting with camera and safety zones\nIf a worker crosses the warning safety zone, IGX commands switch on a visible alarm (proactive safety).\nIf the worker continues or crosses the stop safety zone, the FORT safety stack on IGX commands (WiFi) the FORT endpoint (embedded in the machine) to activate the emergency stop (reactive safety).\nStill of the camera feed in the safety stack.\nFigure 5. Setting safety zones in Protex.AI user interface\nThe Protex edge computer vision app uses NVIDIA DeepStream 6.2 and the Protex model has been trained with NVIDIA TAO Toolkit and quantized with NVIDIA TensorRT.\nScreenshot of Protex.AI user interface showing how easily the safety zone is defined and the camera-person-zone-event chain built up.\nFigure 6. Proactive safety stack and the camera-person-zone-event chain", "Get started\nFor more information, see NVIDIA IGX Orin and be sure to sign up for the Functional Safety for Industry 4.0: Keeping Supply Chains Safe, Secure, and Efficient using AI at the Edge GTC session."], "document_title": "Using the Power of AI to Make Factories Safer", "document_url": "https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/", "document_date": "2024-01-24T17:00:00", "document_date_modified": "2024-01-25T18:17:26", "document_full_text": "Using the Power of AI to Make Factories Safer\nAs industrial automation increases, safety becomes a greater challenge and top priority for enterprises.\nSafety encompasses multiple aspects:\nSystem safety: A rational pursuit of acceptable mishap risk within a systems perspective.\nSafety of the intended functionality: The absence of unreasonable risk due to hazards resulting from functional insufficiencies of the intended functionality or by reasonably foreseeable misuse by persons.\nFunctional safety: Part of the overall safety that depends on the correct functioning of the safety-related systems and external risk reduction facilities.\nWorker safety: Providing a safe working environment for employees by incorporating safe equipment and safe procedures at the workplace.\nAnd other, more general types of safety.\nThe same technological solution that\u2019s driving automation can be used to also address safety: artificial intelligence.\nAI-powered stationary outside-in safety platforms, which monitor activity across many distributed machines or robots, can predictively and proactively orchestrate consistent safety policies. Machines and robots equipped with inside-out reactive safety can detect any specific interaction within their workspace and take appropriate measures.\nDiagram shows orchestrating distributed mobile machines with stationary safety platforms.\nFigure 1. Inside-out and outside-in safety\nIEC 61508 is an international standard published by the International Electrotechnical Commission (IEC). It consists of methods to apply, design, deploy, and maintain safety-related systems.\nUsing IEC 61508 general architecture, AI can either be used to implement the safety function or to assist functional safety to provide additional risk reduction measures.\nWorkflow diagram shows an EUC control system assisting reactive functional safety with proactive safety functions.\nFigure 2. Reactive and proactive safety\nThe use of AI in safety-related systems is being analyzed in upcoming international standardization activities such as ISO/IEC TR 5469 and ISO/IEC AWI TS 22440 being developed by ISO/IEC joint working group JWG 4.\nThe examples in Table 1 describe some typical safety use cases that can benefit from AI.\nUse case\nSurveilled machines\nDetecting position and movement of humans toward a machine safety zone.\nconveyor belt, high-speed door, collaborative robot, autonomous mobile robot, forklift\nDetecting machines moving in the direction of a forbidden area or in the direction of other machines; machines moving at excessive speed or excessive strength.\nautonomous mobile robot, forklift\nDetecting defects or material that could cause safety hazards based on norms such as ANSI/RIA R15.08, and so on.\nautonomous mobile robot, train, airplane\nDetecting and classifying worker or object poses and movement.\nFactory, warehouses\nTable 1. Use cases and machines that can benefit from AI proactive safety functions The following examples of responses to those proactive safety functions are ordered by increasing level of intervention:\nUpdating statistics to later optimize worker-machine or machine-machine interactions, change the factory floor plan, or improve worker training.\nInforming a safety supervisor.\nGenerating visible and audible alerts to workers, through machine local alerts or personal devices.\nSlowing down or stopping a machine (acting as a speed or force limiter).\nChanging the position or trajectory of the machine.\nPreventing the start of the equipment mission.\nActivating hazard protection devices.\nNVIDIA IGX Orin combines industrial-grade hardware with enterprise-level software and support to implement both inside-out reactive and outside-in proactive safety in the same platform:\nAI safety framework\nFramework to provide proactive safety through AI.\nSafety Extension Package (SEP)\nSafety services for fault avoidance, detection, and control leveraging hardware safety extensions\nEdge Safety Link, a safety communication protocol.\nHardware safety extensions:\nHundreds of diagnostic mechanisms implemented in NVIDIA Orin SoC elements (CPU, GPU, accelerators, interfaces, ECC, and so on), In-System-Test, clock, temperature and voltage monitors, and so on.\nFunctional Safety Island (FSI): An independent, redundant set of processors with onboard memory that is used as a monitoring and safety processor working beside other SoC engines.\nSafety MCU: A third-party, safety-certified microcontroller, collaborating with FSI and Orin SoC, and controlling board-level voltage and temperature monitors.\nSafety documentation\nSet of documents (safety application note, safety manual, and FMEDA) to support customer safety cases.\nSafety in action example\nHere\u2019s an example of proactive and reactive safety in action, implemented in collaboration with FORT Robotics and Protex.AI.\nWorkflow diagram shows warn zone, stop zone, and communication channels to safety protocol elements.\nFigure 3. Proactive and reactive safety in action\nIn the example, an HD IP camera connected with Ethernet to NVIDIA IGX running Protex AI stack detects a worker moving toward an industrial mobile machine.\nPhoto of FORT robotics warn and stop zones labeled.\nFigure 4. Overall setting with camera and safety zones\nIf a worker crosses the warning safety zone, IGX commands switch on a visible alarm (proactive safety).\nIf the worker continues or crosses the stop safety zone, the FORT safety stack on IGX commands (WiFi) the FORT endpoint (embedded in the machine) to activate the emergency stop (reactive safety).\nStill of the camera feed in the safety stack.\nFigure 5. Setting safety zones in Protex.AI user interface\nThe Protex edge computer vision app uses NVIDIA DeepStream 6.2 and the Protex model has been trained with NVIDIA TAO Toolkit and quantized with NVIDIA TensorRT.\nScreenshot of Protex.AI user interface showing how easily the safety zone is defined and the camera-person-zone-event chain built up.\nFigure 6. Proactive safety stack and the camera-person-zone-event chain\nGet started\nFor more information, see NVIDIA IGX Orin and be sure to sign up for the Functional Safety for Industry 4.0: Keeping Supply Chains Safe, Secure, and Efficient using AI at the Edge GTC session."}], "https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/": [{"text": "The article discusses how InfiniBand, despite sounding complex, is actually easier to deploy and manage compared to Ethernet when it comes to data center networking for AI infrastructure. The InfiniBand Cluster Operation and Maintenance Guide helps simplify the setup and operation of a full-stack InfiniBand network, with a focus on using NVIDIA Unified Fabric Manager (UFM) for initial provisioning and ongoing maintenance. The guide covers steps for cluster build, operation, congestion analysis, and maintenance checks, with recommendations for minutely/ongoing, weekly, and quarterly/annual maintenance tasks. It also provides guidance for issue resolution, including common scenarios and steps to resolve them. The article emphasizes the ease of adopting InfiniBand and leveraging UFM for network monitoring and management, highlighting that it is simpler than traditional networking certification textbooks. Overall, the article promotes choosing NVIDIA Quantum InfiniBand for AI infrastructure due to its performance and simplicity.", "text_components": ["Simplifying Network Operations for AI with NVIDIA Quantum InfiniBand\nA common technological misconception is that performance and complexity are directly linked. That is, the highest-performance implementation is also the most challenging to implement and manage. When considering data center networking, however, this is not the case.\nInfiniBand is a protocol that sounds daunting and exotic in comparison to Ethernet, but because it is built from the ground up for the highest performance, it is actually simpler to deploy and maintain. When you are considering connectivity for AI infrastructure, the InfiniBand Cluster Operation and Maintenance Guide helps make setting up and operating a full-stack InfiniBand network as simple as possible.\nThis comprehensive guide covers the essential steps needed to streamline Day 0, Day 1, and Day 2 network operations. In particular, the guide details how to use NVIDIA Unified Fabric Manager (UFM) to assist in initial provisioning as well as ongoing maintenance plans.\nUFM is a powerful toolset with wide-ranging telemetry and analytics capabilities. However, getting started with UFM for the basics of cluster monitoring and management does not require any advanced prerequisites or specialized knowledge.\nImage of UFM Enterprise page with bar graphs\nFigure 1. UFM Fabric dashboard", "Cluster build and operation\nThe guide walks you through bring-up:\nVerifying UFM operational status\nGenerating the fabric health report and topology validation\nVerifying cluster performance\nThe guide also provides an introduction to congestion analysis with UFM Telemetry. UFM telemetry and monitoring capabilities are powerful. Third-party plugins to tools such as Grafana, Fluentd, Slurm, and Zabbix enable you to capture vital networking metrics and use them with your platform of choice.\nWhen the administrator knows that the cluster is in a healthy initial state, the guide suggests a cluster maintenance regime, with a list of checks for periodic maintenance.\nMinutely/Ongoing Maintenance:\nCheck for scenarios on the troubleshooting list and follow the instructions for resolution.\nWeekly Maintenance:\nMonitor trends in link monitoring key indicators (available in the UFM user interface).\nRun cluster topology validation checks and fabric health validation tests.\nVerify performance KPIs with ClusterKit (included as part of the HPC-X software package).\nReview temperature differentials captured within UFM to ensure that your cooling system is working properly.\nQuarterly/Annual Maintenance:\nExamine the most recent firmware and software release notes and validated configurations and upgrade if possible.\nConduct an annual review of NVIDIA network health by contacting NVIDIA Networking support or your designated NVIDIA point of contact\nMany of these checks may be automated and are configurable through the API. The guide provides links to the appropriate UFM API documentation to make this setup easy and seamless.", "Issue resolution\nOf course, no system is perfect. Even a well-oiled machine like an InfiniBand cluster encounters unexpected issues every so often.\nHowever, as an admin, the cluster maintenance guide is your one-stop shop. It includes a chapter describing the most frequently encountered scenarios, and how to solve them. This section includes the scenario and how to detect it (with corresponding UFM Alert Event IDs), and then a set of steps to take to reach a resolution. It covers simple and common errors such as bad ports, flapping links, and cable connection issues, as well as more complex challenges such as performance degradation or low bandwidth.\nImage of UFM Enterprise page with events and alarms listed.\nFigure 2. UFM Events & Alarms dashboard", "Summary\nWhen building a network, performance is a key consideration, but performance and ease of use don\u2019t have to be thought of as a tradeoff.\nInfiniBand is easy to adopt, deploy, and operationalize for AI. Leveraging the power of UFM, the cluster operations and maintenance guide contains everything a network administrator needs to know. It\u2019s a lot simpler than cracking open your networking certification textbooks as the cluster guide is less than 40 pages.\nConsider choosing the simplicity of NVIDIA Quantum InfiniBand for your AI infrastructure."], "document_title": "Simplifying Network Operations for AI with NVIDIA Quantum InfiniBand", "document_url": "https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/", "document_date": "2024-01-23T18:00:00", "document_date_modified": "2024-01-25T18:17:27", "document_full_text": "Simplifying Network Operations for AI with NVIDIA Quantum InfiniBand\nA common technological misconception is that performance and complexity are directly linked. That is, the highest-performance implementation is also the most challenging to implement and manage. When considering data center networking, however, this is not the case.\nInfiniBand is a protocol that sounds daunting and exotic in comparison to Ethernet, but because it is built from the ground up for the highest performance, it is actually simpler to deploy and maintain. When you are considering connectivity for AI infrastructure, the InfiniBand Cluster Operation and Maintenance Guide helps make setting up and operating a full-stack InfiniBand network as simple as possible.\nThis comprehensive guide covers the essential steps needed to streamline Day 0, Day 1, and Day 2 network operations. In particular, the guide details how to use NVIDIA Unified Fabric Manager (UFM) to assist in initial provisioning as well as ongoing maintenance plans.\nUFM is a powerful toolset with wide-ranging telemetry and analytics capabilities. However, getting started with UFM for the basics of cluster monitoring and management does not require any advanced prerequisites or specialized knowledge.\nImage of UFM Enterprise page with bar graphs\nFigure 1. UFM Fabric dashboard\nCluster build and operation\nThe guide walks you through bring-up:\nVerifying UFM operational status\nGenerating the fabric health report and topology validation\nVerifying cluster performance\nThe guide also provides an introduction to congestion analysis with UFM Telemetry. UFM telemetry and monitoring capabilities are powerful. Third-party plugins to tools such as Grafana, Fluentd, Slurm, and Zabbix enable you to capture vital networking metrics and use them with your platform of choice.\nWhen the administrator knows that the cluster is in a healthy initial state, the guide suggests a cluster maintenance regime, with a list of checks for periodic maintenance.\nMinutely/Ongoing Maintenance:\nCheck for scenarios on the troubleshooting list and follow the instructions for resolution.\nWeekly Maintenance:\nMonitor trends in link monitoring key indicators (available in the UFM user interface).\nRun cluster topology validation checks and fabric health validation tests.\nVerify performance KPIs with ClusterKit (included as part of the HPC-X software package).\nReview temperature differentials captured within UFM to ensure that your cooling system is working properly.\nQuarterly/Annual Maintenance:\nExamine the most recent firmware and software release notes and validated configurations and upgrade if possible.\nConduct an annual review of NVIDIA network health by contacting NVIDIA Networking support or your designated NVIDIA point of contact\nMany of these checks may be automated and are configurable through the API. The guide provides links to the appropriate UFM API documentation to make this setup easy and seamless.\nIssue resolution\nOf course, no system is perfect. Even a well-oiled machine like an InfiniBand cluster encounters unexpected issues every so often.\nHowever, as an admin, the cluster maintenance guide is your one-stop shop. It includes a chapter describing the most frequently encountered scenarios, and how to solve them. This section includes the scenario and how to detect it (with corresponding UFM Alert Event IDs), and then a set of steps to take to reach a resolution. It covers simple and common errors such as bad ports, flapping links, and cable connection issues, as well as more complex challenges such as performance degradation or low bandwidth.\nImage of UFM Enterprise page with events and alarms listed.\nFigure 2. UFM Events & Alarms dashboard\nSummary\nWhen building a network, performance is a key consideration, but performance and ease of use don\u2019t have to be thought of as a tradeoff.\nInfiniBand is easy to adopt, deploy, and operationalize for AI. Leveraging the power of UFM, the cluster operations and maintenance guide contains everything a network administrator needs to know. It\u2019s a lot simpler than cracking open your networking certification textbooks as the cluster guide is less than 40 pages.\nConsider choosing the simplicity of NVIDIA Quantum InfiniBand for your AI infrastructure."}], "https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/": [{"text": "NVIDIA Metropolis microservices offer cloud-native APIs to develop vision AI applications at the edge, now including support for NVIDIA Jetson. These APIs enable video streaming, AI-based analytics, and insights generation. The modular architecture provides reusable microservices such as Video Storage Toolkit, AI perception services, and Analytics, accessed through Ingress platform services to expose APIs. WebRTC protocol enables real-time video streaming, with entities like user agents, signaling servers, ICE servers, and TURN servers involved in the process. Clients can build web or native applications to enable WebRTC streaming by following specific steps. The Analytics microservice supports modules for detecting people or objects in a camera's field of view, crossing tripwires, or within defined regions of interest. Clients can configure tripwires, define alert rules, and retrieve counts and alerts using APIs. Secure, remote access to device APIs is facilitated through the IoT cloud, ensuring authorization and authentication for device interaction. Overall, NVIDIA Metropolis APIs and microservices enable the development of powerful vision AI applications at the edge.", "text_components": ["Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs\nNVIDIA Metropolis microservices provide powerful, customizable, cloud-native APIs and microservices to develop vision AI applications and solutions. The framework now includes NVIDIA Jetson, enabling developers to quickly build and productize performant and mature vision AI applications at the edge.\nAPIs enhance flexibility, interoperability, and efficiency in software development by enabling seamless communication and integration between different applications and services. Two of the common functionalities for building video analytics applications are video streaming and AI-based generation of insights and analytics.\nThis post features the API workflow for building vision AI applications and integrating them into any client application. We explain three key steps for building your applications:\nStream video from the edge to any device through WebRTC using APIs.\nGenerate insights and alerts for people/object movement using tripwire functionality, accessed through APIs.\nUse the reference cloud for secure, remote device API access.", "Modular architecture\nNVIDIA Metropolis microservices for Jetson provide a modular architecture with a large collection of software, including customizable, reusable microservices for building vision AI applications. The suite also offers platform services for infrastructural capabilities and a reference cloud. Various microservices include Video Storage Toolkit (VST), an AI perception service based on NVIDIA DeepStream, and an Analytics service. Each provides APIs to configure and access the functionality of the microservices.\nThese APIs are presented externally to the system using the Ingress platform service, based on a standard pattern used in cloud-native architectures to expose APIs within a system using a single gateway. Client applications exercise microservice functionality by invoking the respective APIs through the Ingress service. Further, NVIDIA Metropolis microservices provide an IoT cloud module that enables clients to be authenticated and authorized when accessing these APIs remotely.\nDiagram showing the cloud-native workflow for a complete application using NVIDIA Metropolis microservice for NVIDIA Jetson.\nFigure 1. Cloud-native NVIDIA Metropolis microservices for Jetson", "Video streaming through WebRTC\nViewing video streams from cameras connected to a system using a client app such as a mobile app or browser is a common requirement for video analytics systems. This functionality is supported through a standardized call flow based on the VST APIs. The VST microservice supports remote streaming using the WebRTC (Web Real-Time Communication) protocol, which is designed for reliable, peer-to-peer delivery of video and other data over the Internet.\nThis section presents an overview of the salient concepts underlying the WebRTC protocol, and using VST APIs to enable WebRTC-based streaming. WebRTC is a powerful open-source project that enables real-time communication directly between two peers, such as a web browser and a Jetson device running VST.", "Entities for WebRTC streaming\nA typical WebRTC session involves a few different entities, which are listed below:\nUser agent: Represents the mobile, browser, or web application initiating the communication using the VST APIs.\nSignaling server: A web server implemented within the VST involved in the establishment of communication channels for the WebRTC session.\nICE (Interactive Connectivity Establishment) server: A logical module implemented within the VST-WebRTC stack to determine the best connection path between peers. This is necessary for traversing firewalls and NATs (Network Address Translators).\nSTUN (Session Traversal Utilities for NAT) server: A type of ICE server that helps in discovering public IP addresses and Ports. It\u2019s necessary when peers are using private (NAT-based) IP addresses. This is a third-party entity hosted on a public cloud network.\nTURN (Traversal Using Relays around NAT) server: Acts as a relay if direct peer-to-peer communication fails and is only needed when peers are on different networks. This is supported through third-party services such as Twilio\u200c.\nGraphic showing the different entities involved with peer-to-peer WebRTC streaming, including signaling server, WebRTC, STUN, TURN.\nFigure 2. Entities for WebRTC streaming", "WebRTC session stages\nWebRTC sessions use control paths and data paths to enable session creation and streaming.\nA control path enables setting up and managing sessions between peers whose stages include initialization, signaling, ICE candidate exchange, and connection establishment. VST enables user agents to perform these operations remotely through its APIs.\nA data path enables real-time media data transfer along with adaptation and quality control, and finally closing the connection.", "Enabling WebRTC streaming through VST APIs\nFigure 3 shows the call flow between clients and VST capturing the control and data paths for enabling WebRTC sessions.\nDiagram showing the call flow between clients and VST, capturing the control and data paths for enabling WebRTC sessions.\nFigure 3. WebRTC call flow with VST The call flow starts with a client discovering various video streams using the ```api/v1/sensor/list``` API.\nThe control and data paths are implemented based on the following call flow:\nClient calls ```GET api/v1/live/iceServers or api/v1/replay/iceServers``` to get the list of ICE servers from VST.\nClient creates a local offer and sends the offer to VST using ```POST api/v1/live/stream/start or api/v1/replay/stream/start```.\nVST creates an answer for the client and returns it as a response.\nClient completes the ICE exchange using ```GET``` and ```POST``` requests for ```api/v1/live/iceCandidate``` or ```api/v1/replay/iceCandidate``` with the ```peerid``` as query parameter.\nVideo data starts flowing when the peer connection is complete.\nWhen streaming is underway, the client can control streaming using the following stream APIs:\nPause the video pipeline: ```api/v1/replay/stream/pause```\nResume the video pipeline: ```api/v1/replay/stream/resume```\nFind a specific time in the video: ```api/v1/replay/stream/seek```", "Build client applications\nThese concepts can be applied to add video streaming capability to browser-based web applications by invoking VST APIs over HTTP in JavaScript, while leveraging WebRTC support in JavaScript supported by most browsers. Similar concepts can be applied to building native WebRTC client applications as well.\nTo set up WebRTC streaming with JavaScript, follow these steps:", "Initialize a peer connection\nCreate a new ```RTCPeerConnection``` object with appropriate configuration settings.", "Handle the track addition\nSet up an event listener for the ```ontrack``` event.\nWhen a new track is added, update the remote video element to display the incoming video stream.", "Generate an offer\nUse the ```createOffer``` method to generate an offer for the peer connection.\nSet the local description of the peer connection to the generated offer.", "Send the offer to VST\nObtain the local description (offer) using ```peerConnection.localDescription```.\nSend the offer to the VST using the appropriate start API; for example, ```api/v1/live/stream/start```.", "Receive an answer from VST\nWhen an answer SDP is received from VST as the start API response, set it as the remote description using ```peerConnection.setRemoteDescription```.", "Handle ICE candidates\nExchange ICE candidates using ```GET``` and ```POST``` requests of the ```api/v1/live/iceCandidate``` API.\nAdd the received ICE candidates to the peer connection using ```peerConnection.addIceCandidate```.", "Generate spatial insights and alerts for object movement\nThe Analytics microservice supports three people or objects analytics modules:\nField of View (FOV): Count people or objects in the camera\u2019s field of view.\nTripwire: Detect people or objects crossing across user-defined tripwire line segments.\nRegion of Interest (ROI): Count people or objects in the defined region of interest.\nIn combination, these modules provide a powerful set of tools to understand the movement of people or objects across physical spaces, with use cases ranging across retail warehouses, security, and safety. Client applications use APIs to identify sensor lists, create tripwires, and retrieve counts and alerts for each of these functionalities.\nThis section walks through an end-to-end example of these operations for tripwires. A similar methodology can be used to enable FOV and ROI. For each case, invoke the HTTP APIs in the programming language or HTTP client of your choice.", "Retrieve the sensor list\nAs a first step, retrieve the name of the sensor for which the tripwire is to be configured.\nInvoke the VST API for listing all the sensors. Identify the sensor of interest from the returned list. The ```name``` attribute of the sensor object is to be used as the sensor ID in subsequent steps to configure and retrieve tripwire counts and alerts. Replace <device-ip> with the IP address of your device.\n```http://<device-ip>:30080/vst/api/v1/sensor/list```", "Create a tripwire configuration\nIn this step, configure a tripwire to specify which lines you wish to count the people crossing.\nWhen configuring a tripwire, specify the following properties:\nSensor ID: Identification of the sensor for which the tripwire is to be configured.\nTripwire ID: Identification of the tripwire. A sensor may have multiple definitions. Each tripwire needs to have a unique identifier.\nWire: A sequence of points representing the line segments comprising the tripwire.\nDirection: A vector (two points) depicting the directionality of crossing (entry/exit).\nNote that the coordinates for points are in the camera coordinates (image plane). The top left corner is (0,0).\nClient applications like the reference mobile app provided with NVIDIA Metropolis microservices offer a visual aid to select a point without needing to determine the (x, y) locations manually. Figure 4 shows an example of a tripwire created and rendered through the mobile app. The user selects the tripwire anchor points using the touch interface in the app to draw the tripwire (green lines), along with directionality (red arrow).\nVisual rendering in the reference mobile app, provided with Metropolis microservices. Users can draw tripwires, specify directions, and view the real-time analytics.\nFigure 4. Tripwire visual rendering in the reference mobile app provided with NVIDIA Metropolis microservices\nTo configure a tripwire with ```Id = main_door``` for a sensor with ```Id = Amcrest_3```, invoke the following HTTP API call in the programming language of your choice:\n```http://<device-ip>:30080/emdx/api/config/tripwire?sensorId=Amcrest_3```\n{\n\"deleteIfPresent\": false,\n\"tripwires\": [\n{\n\"direction\": {\n\"entry\": {\n\"name\": \"Inside the room\"\n},\n\"exit\": {\n\"name\": \"Outside of the room\"\n},\n\"p1\": { \"x\": 753, \"y\": 744},\n\"p2\": { \"x\": 448, \"y\": 856}\n},\n\"id\": \"main_door\",\n\"name\": \"Main door\",\n\"wire\": [\n{ \"x\": 321, \"y\": 664 },\n{ \"x\": 544, \"y\": 648 },\n{ \"x\": 656, \"y\": 953 },\n{ \"x\": 323, \"y\": 1067}\n]\n}\n],\n\"sensorId\": \"Amcrest_3\"\n}", "Configure tripwire alert rule (optional)\nIt is optional to configure an alert rule for a given tripwire. Alert rules are specific conditions which when met result in an alert event being generated.\nTo configure an alert rule that will raise an alert whenever one person crosses the tripwire (main door) in the direction of entry, invoke the following API request:\n```http://<device-ip>:30080/emdx/api/config/rule/alerts/tripwire```\n{\n\"sensorId\": \"Amcrest_3\",\n\"rules\": [\n{\n\"rule_id\": \"cd2218f6-e4d2-4ad4-9b15-3396e4336064\",\n\"id\": \"main_door\",\n\"type\": \"tripwire\",\n\"rule_type\": \"increment\",\n\"time_interval\": 1,\n\"count_threshold\": 1,\n\"direction\": \"entry\"\n}\n]\n}", "Retrieve tripwire counts and alerts\nThis step explains how to retrieve the counts of the people crossing the tripwire defined previously. Optionally, you can also retrieve the alerts generated based on configured alert rules for that tripwire.\nThe counts can be queried for a particular tripwire ( ```sensorId```, ```tripwireId``` ), for a time range ( ```fromTimestamp```, ```toTimestamp``` ) and aggregated to a specified time window ( ```fixedInterval``` ). Optionally, you can retrieve alerts and counts by setting the ```alerts``` query parameter to ```true```:\n```http://<device-ip>:30080/emdx/api/metrics/tripwire/histogram?sensorId=Amcrest_3&tripwireId=main_door&fromTimestamp=2020-10-30T20:00:00.000Z&toTimestamp=2020-10-30T20:01:00.000Z&fixedInterval=1000&alerts=true```\n{\n\"alerts\": [\n{\n\"count\": 1,\n\"description\": \"1 people entered tripwire\",\n\"duration\": 1.000,\n\"startTimestamp\": \"2020-10-30T20:00:59.000Z\",\n\"endTimestamp\": \"2020-10-30T20:01:00.000Z\",\n\"id\": \"unique-alert-id\",\n\"rule_type\": \"increment\",\n\"rule_id\": \"cd2218f6-e4d2-4ad4-9b15-3396e4336064\",\n\"sensorId\": \"Amcrest_3\",\n\"type\": \"tripwire\",\n\"direction\": \"entry\",\n\"directionName\": \"Inside the room\",\n\"attributes\": [..],\n}\n],\n\"counts\": [\n{\n\"agg_window\": \"1 sec\",\n\"histogram\": [\n{\n\"end\": \"2020-10-30T20:00:01.000Z\",\n\"start\": \"2020-10-30T20:00:00.000Z\",\n\"sum_count\": 1\n}\n],\n\"attributes\": [...],\n\"sensorId\": \"Amcrest_3\",\n\"type\": \"exit\"\n},\n{\n\"agg_window\": \"1 sec\",\n\"histogram\": [\n{\n\"end\": \"2020-10-30T20:00:01.000Z\",\n\"start\": \"2020-10-30T20:00:00.000Z\",\n\"sum_count\": 0\n},\n\u2026..\n],\n\"attributes\": [.. ],\n\"sensorId\": \"Amcrest_3\",\n\"type\": \"entry\"\n}\n]\n}\nThe histograms are returned for each direction separately. The entire time range is divided into time windows of ```fixedInterval```. Crossings for each time window ```start,end``` are reported as ```sum_count```.", "Retrieve tripwire alerts\nTo retrieve all the alerts for a given sensor, invoke the following API:\n```http://<device-ip>:30080/emdx/api/alerts?sensorId=Amcrest_3&fromTimestamp=2020-10-30T20:00:00.000Z&toTimestamp=2020-10-31T01:00:00.000Z```", "Secure, remote, cloud-based API access\nAPIs enable clients to access device configuration and functionality remotely using the HTTP protocol. While in the development stage, API invocation is recommended by directing HTTP requests to the device IP address. However, in production scenarios, the IP addresses of devices would typically be unknown to clients.\nIn addition, Jetson devices might be located behind firewalls, rendering them unreachable, or they may use NAT-based IP addresses that may not be valid externally. The IoT cloud facilitates product-grade remote API invocation by providing a mechanism to forward requests from network-separated clients to devices in a secure manner.\nThis section describes the mechanism by which clients can obtain security tokens and use them to make HTTP through the cloud, to be forwarded to the appropriate device.\nWhile the focus of this section is to showcase how clients can invoke device APIs through the cloud,\u200c note that cloud architecture provides a secure \u2018device claim\u2019 mechanism for authorized access to specific devices through the cloud. All user device access through the cloud goes through authentication and authorization, and users can only access devices that they have previously claimed.\nThis functionality is designed with high customizability, enabling seamless integration with Original Design Manufacturers (ODM) and Original Equipment Manufacturers (OEM) operators\u2019 existing security frameworks and cloud backend infrastructures.", "Workflow for device API invocation through the IoT Cloud\nThe reference IoT cloud implementation uses Amazon Cognito as the identity provider (IdP), but users are welcome to use any third-party identity provider. To access the device APIs through cloud endpoints, use the authentication and authorization call flow outlined below.", "Authenticate with the Amazon Cognito\nSign in to the login URI page using a web console to be authenticated with Amazon Cognito. With successful authentication, Amazon Cognito returns a unique authorization code. Using authorization code, makes a request to Amazon Cognito to issue a time-bound ID token. Present this ID token while invoking IoT cloud security APIs.\nThe call flow diagram initiated by a user to an IDP service such as Amazon Cognito to authenticate the user.\nFigure 5. Call flow to authorize with IDP", "Generate a JWT token and invoke device APIs\nTo access the IoT device APIs, first request an authorization token from the IoT cloud security. Upon a valid request, the IoT cloud security issues an ephemeral signed authorization JWT token. Use this token to then invoke device APIs through the IoT cloud transport, which validates it and forwards the request to the device.\nNote that an unauthorized HTTP error code is returned if the user does not have rights to perform the operation based on the device claim.\nCall flow diagram showing the user initiating the authorization by requesting an authorization token from IoT cloud and using it to invoke the device APIs.\nFigure 6. Call flow to authorize a user with IoT cloud", "Summary\nBuild powerful, market-ready vision AI applications at the edge with NVIDIA Metropolis APIs and microservices. APIs provide a standardized, secure, and distributed means of exercising the capabilities of various NVIDIA Metropolis microservices. The reference mobile application included as part of this release showcases a mature end-user application built using these APIs with a user-friendly interface capturing configuration, video streaming, analytics, alerts, cloud integration, and device claim. This application includes source code, with a walkthrough of the various modules in the mobile app section of the release documentation.\nDownload NVIDIA Metropolis microservices for Jetson. And register to join us for the two-part webinar, Accelerate Edge AI Development With Metropolis APIs and Microservices for Jetson (Part 1) and How to Build With Metropolis Microservices for Jetson (Part 2)."], "document_title": "Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs", "document_url": "https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/", "document_date": "2024-01-23T17:00:00", "document_date_modified": "2024-01-25T18:17:27", "document_full_text": "Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs\nNVIDIA Metropolis microservices provide powerful, customizable, cloud-native APIs and microservices to develop vision AI applications and solutions. The framework now includes NVIDIA Jetson, enabling developers to quickly build and productize performant and mature vision AI applications at the edge.\nAPIs enhance flexibility, interoperability, and efficiency in software development by enabling seamless communication and integration between different applications and services. Two of the common functionalities for building video analytics applications are video streaming and AI-based generation of insights and analytics.\nThis post features the API workflow for building vision AI applications and integrating them into any client application. We explain three key steps for building your applications:\nStream video from the edge to any device through WebRTC using APIs.\nGenerate insights and alerts for people/object movement using tripwire functionality, accessed through APIs.\nUse the reference cloud for secure, remote device API access.\nModular architecture\nNVIDIA Metropolis microservices for Jetson provide a modular architecture with a large collection of software, including customizable, reusable microservices for building vision AI applications. The suite also offers platform services for infrastructural capabilities and a reference cloud. Various microservices include Video Storage Toolkit (VST), an AI perception service based on NVIDIA DeepStream, and an Analytics service. Each provides APIs to configure and access the functionality of the microservices.\nThese APIs are presented externally to the system using the Ingress platform service, based on a standard pattern used in cloud-native architectures to expose APIs within a system using a single gateway. Client applications exercise microservice functionality by invoking the respective APIs through the Ingress service. Further, NVIDIA Metropolis microservices provide an IoT cloud module that enables clients to be authenticated and authorized when accessing these APIs remotely.\nDiagram showing the cloud-native workflow for a complete application using NVIDIA Metropolis microservice for NVIDIA Jetson.\nFigure 1. Cloud-native NVIDIA Metropolis microservices for Jetson\nVideo streaming through WebRTC\nViewing video streams from cameras connected to a system using a client app such as a mobile app or browser is a common requirement for video analytics systems. This functionality is supported through a standardized call flow based on the VST APIs. The VST microservice supports remote streaming using the WebRTC (Web Real-Time Communication) protocol, which is designed for reliable, peer-to-peer delivery of video and other data over the Internet.\nThis section presents an overview of the salient concepts underlying the WebRTC protocol, and using VST APIs to enable WebRTC-based streaming. WebRTC is a powerful open-source project that enables real-time communication directly between two peers, such as a web browser and a Jetson device running VST.\nEntities for WebRTC streaming\nA typical WebRTC session involves a few different entities, which are listed below:\nUser agent: Represents the mobile, browser, or web application initiating the communication using the VST APIs.\nSignaling server: A web server implemented within the VST involved in the establishment of communication channels for the WebRTC session.\nICE (Interactive Connectivity Establishment) server: A logical module implemented within the VST-WebRTC stack to determine the best connection path between peers. This is necessary for traversing firewalls and NATs (Network Address Translators).\nSTUN (Session Traversal Utilities for NAT) server: A type of ICE server that helps in discovering public IP addresses and Ports. It\u2019s necessary when peers are using private (NAT-based) IP addresses. This is a third-party entity hosted on a public cloud network.\nTURN (Traversal Using Relays around NAT) server: Acts as a relay if direct peer-to-peer communication fails and is only needed when peers are on different networks. This is supported through third-party services such as Twilio\u200c.\nGraphic showing the different entities involved with peer-to-peer WebRTC streaming, including signaling server, WebRTC, STUN, TURN.\nFigure 2. Entities for WebRTC streaming\nWebRTC session stages\nWebRTC sessions use control paths and data paths to enable session creation and streaming.\nA control path enables setting up and managing sessions between peers whose stages include initialization, signaling, ICE candidate exchange, and connection establishment. VST enables user agents to perform these operations remotely through its APIs.\nA data path enables real-time media data transfer along with adaptation and quality control, and finally closing the connection.\nEnabling WebRTC streaming through VST APIs\nFigure 3 shows the call flow between clients and VST capturing the control and data paths for enabling WebRTC sessions.\nDiagram showing the call flow between clients and VST, capturing the control and data paths for enabling WebRTC sessions.\nFigure 3. WebRTC call flow with VST The call flow starts with a client discovering various video streams using the ```api/v1/sensor/list``` API.\nThe control and data paths are implemented based on the following call flow:\nClient calls ```GET api/v1/live/iceServers or api/v1/replay/iceServers``` to get the list of ICE servers from VST.\nClient creates a local offer and sends the offer to VST using ```POST api/v1/live/stream/start or api/v1/replay/stream/start```.\nVST creates an answer for the client and returns it as a response.\nClient completes the ICE exchange using ```GET``` and ```POST``` requests for ```api/v1/live/iceCandidate``` or ```api/v1/replay/iceCandidate``` with the ```peerid``` as query parameter.\nVideo data starts flowing when the peer connection is complete.\nWhen streaming is underway, the client can control streaming using the following stream APIs:\nPause the video pipeline: ```api/v1/replay/stream/pause```\nResume the video pipeline: ```api/v1/replay/stream/resume```\nFind a specific time in the video: ```api/v1/replay/stream/seek```\nBuild client applications\nThese concepts can be applied to add video streaming capability to browser-based web applications by invoking VST APIs over HTTP in JavaScript, while leveraging WebRTC support in JavaScript supported by most browsers. Similar concepts can be applied to building native WebRTC client applications as well.\nTo set up WebRTC streaming with JavaScript, follow these steps:\nInitialize a peer connection\nCreate a new ```RTCPeerConnection``` object with appropriate configuration settings.\nHandle the track addition\nSet up an event listener for the ```ontrack``` event.\nWhen a new track is added, update the remote video element to display the incoming video stream.\nGenerate an offer\nUse the ```createOffer``` method to generate an offer for the peer connection.\nSet the local description of the peer connection to the generated offer.\nSend the offer to VST\nObtain the local description (offer) using ```peerConnection.localDescription```.\nSend the offer to the VST using the appropriate start API; for example, ```api/v1/live/stream/start```.\nReceive an answer from VST\nWhen an answer SDP is received from VST as the start API response, set it as the remote description using ```peerConnection.setRemoteDescription```.\nHandle ICE candidates\nExchange ICE candidates using ```GET``` and ```POST``` requests of the ```api/v1/live/iceCandidate``` API.\nAdd the received ICE candidates to the peer connection using ```peerConnection.addIceCandidate```.\nGenerate spatial insights and alerts for object movement\nThe Analytics microservice supports three people or objects analytics modules:\nField of View (FOV): Count people or objects in the camera\u2019s field of view.\nTripwire: Detect people or objects crossing across user-defined tripwire line segments.\nRegion of Interest (ROI): Count people or objects in the defined region of interest.\nIn combination, these modules provide a powerful set of tools to understand the movement of people or objects across physical spaces, with use cases ranging across retail warehouses, security, and safety. Client applications use APIs to identify sensor lists, create tripwires, and retrieve counts and alerts for each of these functionalities.\nThis section walks through an end-to-end example of these operations for tripwires. A similar methodology can be used to enable FOV and ROI. For each case, invoke the HTTP APIs in the programming language or HTTP client of your choice.\nRetrieve the sensor list\nAs a first step, retrieve the name of the sensor for which the tripwire is to be configured.\nInvoke the VST API for listing all the sensors. Identify the sensor of interest from the returned list. The ```name``` attribute of the sensor object is to be used as the sensor ID in subsequent steps to configure and retrieve tripwire counts and alerts. Replace <device-ip> with the IP address of your device.\n```http://<device-ip>:30080/vst/api/v1/sensor/list```\nCreate a tripwire configuration\nIn this step, configure a tripwire to specify which lines you wish to count the people crossing.\nWhen configuring a tripwire, specify the following properties:\nSensor ID: Identification of the sensor for which the tripwire is to be configured.\nTripwire ID: Identification of the tripwire. A sensor may have multiple definitions. Each tripwire needs to have a unique identifier.\nWire: A sequence of points representing the line segments comprising the tripwire.\nDirection: A vector (two points) depicting the directionality of crossing (entry/exit).\nNote that the coordinates for points are in the camera coordinates (image plane). The top left corner is (0,0).\nClient applications like the reference mobile app provided with NVIDIA Metropolis microservices offer a visual aid to select a point without needing to determine the (x, y) locations manually. Figure 4 shows an example of a tripwire created and rendered through the mobile app. The user selects the tripwire anchor points using the touch interface in the app to draw the tripwire (green lines), along with directionality (red arrow).\nVisual rendering in the reference mobile app, provided with Metropolis microservices. Users can draw tripwires, specify directions, and view the real-time analytics.\nFigure 4. Tripwire visual rendering in the reference mobile app provided with NVIDIA Metropolis microservices\nTo configure a tripwire with ```Id = main_door``` for a sensor with ```Id = Amcrest_3```, invoke the following HTTP API call in the programming language of your choice:\n```http://<device-ip>:30080/emdx/api/config/tripwire?sensorId=Amcrest_3```\n{\n\"deleteIfPresent\": false,\n\"tripwires\": [\n{\n\"direction\": {\n\"entry\": {\n\"name\": \"Inside the room\"\n},\n\"exit\": {\n\"name\": \"Outside of the room\"\n},\n\"p1\": { \"x\": 753, \"y\": 744},\n\"p2\": { \"x\": 448, \"y\": 856}\n},\n\"id\": \"main_door\",\n\"name\": \"Main door\",\n\"wire\": [\n{ \"x\": 321, \"y\": 664 },\n{ \"x\": 544, \"y\": 648 },\n{ \"x\": 656, \"y\": 953 },\n{ \"x\": 323, \"y\": 1067}\n]\n}\n],\n\"sensorId\": \"Amcrest_3\"\n}\nConfigure tripwire alert rule (optional)\nIt is optional to configure an alert rule for a given tripwire. Alert rules are specific conditions which when met result in an alert event being generated.\nTo configure an alert rule that will raise an alert whenever one person crosses the tripwire (main door) in the direction of entry, invoke the following API request:\n```http://<device-ip>:30080/emdx/api/config/rule/alerts/tripwire```\n{\n\"sensorId\": \"Amcrest_3\",\n\"rules\": [\n{\n\"rule_id\": \"cd2218f6-e4d2-4ad4-9b15-3396e4336064\",\n\"id\": \"main_door\",\n\"type\": \"tripwire\",\n\"rule_type\": \"increment\",\n\"time_interval\": 1,\n\"count_threshold\": 1,\n\"direction\": \"entry\"\n}\n]\n}\nRetrieve tripwire counts and alerts\nThis step explains how to retrieve the counts of the people crossing the tripwire defined previously. Optionally, you can also retrieve the alerts generated based on configured alert rules for that tripwire.\nThe counts can be queried for a particular tripwire ( ```sensorId```, ```tripwireId``` ), for a time range ( ```fromTimestamp```, ```toTimestamp``` ) and aggregated to a specified time window ( ```fixedInterval``` ). Optionally, you can retrieve alerts and counts by setting the ```alerts``` query parameter to ```true```:\n```http://<device-ip>:30080/emdx/api/metrics/tripwire/histogram?sensorId=Amcrest_3&tripwireId=main_door&fromTimestamp=2020-10-30T20:00:00.000Z&toTimestamp=2020-10-30T20:01:00.000Z&fixedInterval=1000&alerts=true```\n{\n\"alerts\": [\n{\n\"count\": 1,\n\"description\": \"1 people entered tripwire\",\n\"duration\": 1.000,\n\"startTimestamp\": \"2020-10-30T20:00:59.000Z\",\n\"endTimestamp\": \"2020-10-30T20:01:00.000Z\",\n\"id\": \"unique-alert-id\",\n\"rule_type\": \"increment\",\n\"rule_id\": \"cd2218f6-e4d2-4ad4-9b15-3396e4336064\",\n\"sensorId\": \"Amcrest_3\",\n\"type\": \"tripwire\",\n\"direction\": \"entry\",\n\"directionName\": \"Inside the room\",\n\"attributes\": [..],\n}\n],\n\"counts\": [\n{\n\"agg_window\": \"1 sec\",\n\"histogram\": [\n{\n\"end\": \"2020-10-30T20:00:01.000Z\",\n\"start\": \"2020-10-30T20:00:00.000Z\",\n\"sum_count\": 1\n}\n],\n\"attributes\": [...],\n\"sensorId\": \"Amcrest_3\",\n\"type\": \"exit\"\n},\n{\n\"agg_window\": \"1 sec\",\n\"histogram\": [\n{\n\"end\": \"2020-10-30T20:00:01.000Z\",\n\"start\": \"2020-10-30T20:00:00.000Z\",\n\"sum_count\": 0\n},\n\u2026..\n],\n\"attributes\": [.. ],\n\"sensorId\": \"Amcrest_3\",\n\"type\": \"entry\"\n}\n]\n}\nThe histograms are returned for each direction separately. The entire time range is divided into time windows of ```fixedInterval```. Crossings for each time window ```start,end``` are reported as ```sum_count```.\nRetrieve tripwire alerts\nTo retrieve all the alerts for a given sensor, invoke the following API:\n```http://<device-ip>:30080/emdx/api/alerts?sensorId=Amcrest_3&fromTimestamp=2020-10-30T20:00:00.000Z&toTimestamp=2020-10-31T01:00:00.000Z```\nSecure, remote, cloud-based API access\nAPIs enable clients to access device configuration and functionality remotely using the HTTP protocol. While in the development stage, API invocation is recommended by directing HTTP requests to the device IP address. However, in production scenarios, the IP addresses of devices would typically be unknown to clients.\nIn addition, Jetson devices might be located behind firewalls, rendering them unreachable, or they may use NAT-based IP addresses that may not be valid externally. The IoT cloud facilitates product-grade remote API invocation by providing a mechanism to forward requests from network-separated clients to devices in a secure manner.\nThis section describes the mechanism by which clients can obtain security tokens and use them to make HTTP through the cloud, to be forwarded to the appropriate device.\nWhile the focus of this section is to showcase how clients can invoke device APIs through the cloud,\u200c note that cloud architecture provides a secure \u2018device claim\u2019 mechanism for authorized access to specific devices through the cloud. All user device access through the cloud goes through authentication and authorization, and users can only access devices that they have previously claimed.\nThis functionality is designed with high customizability, enabling seamless integration with Original Design Manufacturers (ODM) and Original Equipment Manufacturers (OEM) operators\u2019 existing security frameworks and cloud backend infrastructures.\nWorkflow for device API invocation through the IoT Cloud\nThe reference IoT cloud implementation uses Amazon Cognito as the identity provider (IdP), but users are welcome to use any third-party identity provider. To access the device APIs through cloud endpoints, use the authentication and authorization call flow outlined below.\nAuthenticate with the Amazon Cognito\nSign in to the login URI page using a web console to be authenticated with Amazon Cognito. With successful authentication, Amazon Cognito returns a unique authorization code. Using authorization code, makes a request to Amazon Cognito to issue a time-bound ID token. Present this ID token while invoking IoT cloud security APIs.\nThe call flow diagram initiated by a user to an IDP service such as Amazon Cognito to authenticate the user.\nFigure 5. Call flow to authorize with IDP\nGenerate a JWT token and invoke device APIs\nTo access the IoT device APIs, first request an authorization token from the IoT cloud security. Upon a valid request, the IoT cloud security issues an ephemeral signed authorization JWT token. Use this token to then invoke device APIs through the IoT cloud transport, which validates it and forwards the request to the device.\nNote that an unauthorized HTTP error code is returned if the user does not have rights to perform the operation based on the device claim.\nCall flow diagram showing the user initiating the authorization by requesting an authorization token from IoT cloud and using it to invoke the device APIs.\nFigure 6. Call flow to authorize a user with IoT cloud\nSummary\nBuild powerful, market-ready vision AI applications at the edge with NVIDIA Metropolis APIs and microservices. APIs provide a standardized, secure, and distributed means of exercising the capabilities of various NVIDIA Metropolis microservices. The reference mobile application included as part of this release showcases a mature end-user application built using these APIs with a user-friendly interface capturing configuration, video streaming, analytics, alerts, cloud integration, and device claim. This application includes source code, with a walkthrough of the various modules in the mobile app section of the release documentation.\nDownload NVIDIA Metropolis microservices for Jetson. And register to join us for the two-part webinar, Accelerate Edge AI Development With Metropolis APIs and Microservices for Jetson (Part 1) and How to Build With Metropolis Microservices for Jetson (Part 2)."}], "https://developer.nvidia.com/blog/model-monday-query-graphs-with-optimized-deplot-model/": [{"text": "The article discusses the release of the NVIDIA-optimized DePlot model, which helps in visual language reasoning for charts and plots. It breaks down the problem into steps involving plot-to-text translation and textual reasoning using a large language model. This approach has shown significant improvements in chart comprehension and reasoning capabilities. Users can experience DePlot directly from their browser or use the API for testing the model. The article also mentions the availability of enterprise-grade AI runtime software like NVIDIA AI Enterprise for deploying generative AI models in business operations. It emphasizes the importance of security, reliability, and enterprise support in AI model deployments. The article encourages users to try out the DePlot model through the UI or API, optimize it with NVIDIA TensorRT-LLM, and sign up for an NVIDIA AI Enterprise trial for support in taking applications to production.", "text_components": ["Model Monday: Query Graphs with Optimized DePlot Model\nNVIDIA Foundation Models and Endpoints provides access to a curated set of community and NVIDIA-built generative AI models to experience, customize, and deploy in enterprise applications.\nOn Mondays throughout the year, we\u2019ll be releasing new models. This week, we released the NVIDIA-optimized DePlot model, which you can experience directly from your browser.\nIf you haven\u2019t already, try the leading models like Nemotron-3, Mixtral 8X7B, Llama 2, and Stable Diffusion in the NVIDIA AI playground.", "DePlot\nA leap in visual language reasoning, DePlot by Google Research enables comprehension of charts and plots when coupled with a large language model (LLM). As opposed to prior multimodal LLMs that are trained end-to-end for plot de-rendering and numerical reasoning, this approach breaks down the problem into the following steps:\nPlot-to-text translation using a pretrained image-to-text model\nTextual reasoning using an LLM\nSpecifically, DePlot refers to the image-to-text Transformer model in the first step, used for modality conversion from a plot to the text format. The linearized tables generated by DePlot can be directly ingested as part of a prompt into an LLM in the second step to facilitate reasoning.\nPrevious state-of-the-art (SOTA) models required at least tens of thousands of human-written examples to accomplish such plot or chart comprehension while still being limited in their reasoning capabilities on complex queries.\nUsing this plug-and-play approach, the DePlot+LLM pipeline achieves over 29.4% improvement over the previous SOTA on the ChartQA benchmark with just one-shot prompting!\nScreenshot of the DePlot playground dashboard shows a bar chart with configuration settings.\nFigure 1. DePlot converts a plot into a structured table\nFigure 1 shows how DePlot converts a plot into a structured table that can be used as context for LLMs to answer reasoning-based questions. Try DePlot now.", "Using the model in a browser\nYou can now experience DePlot directly from your browser using a simple user interface on the DePlot playground on the NGC catalog. The following video shows the results generated from the models running on a fully accelerated stack.\nVideo 1. NVIDIA AI Foundation model interface The video shows the NVIDIA AI Foundation model interface used to extract information from a graph using DePlot running on a fully accelerated stack.", "Using the model with the API\nIf you would rather use the API to test out the model, we\u2019ve got you covered. After you sign in to the NGC catalog, you have access to NVIDIA cloud credits that enable you to truly experience the models at scale by connecting your application to the API endpoint.\nThe following Python example uses ```base64``` and requests modules to encode the plot image and issue requests to the API endpoint. Before proceeding, ensure that you have an environment capable of executing Python code, such as a Jupyter notebook.", "Obtain the NGC catalog API key\nOn the API tab, select Generate Key. If you haven\u2019t registered yet, you are prompted to sign up or sign in.\nSet the API key in your code:\n```\n# Will be used to issue requests to the endpoint \nAPI_KEY = \u201cnvapi-xxxx\u201c\n```", "Encode your chart or graph in base64 format\nTo provide an image input as part of your request, you must encode it in ```base64``` format.\n```\n# Fetch an example chart from ChartQA dataset \n!wget -cO -  https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png > chartQA-example.png \n\n# Encode the image into base64 format  \nimport base64 \n\nwith open(os.path.join(os.getcwd(), \"chartQA-example.png\"), \"rb\") as image_file: \n    encoded_string = base64.b64encode(image_file.read())\n```\nAs an option, you can visualize the chart:\n```\nfrom IPython import display \ndisplay.Image(base64.b64decode(encoded_string))\n```\nBar chart show the breakdown of support for government economic assistance across various European countries. This example chart is used to feed the model and generate values as a table.\nFigure 2. Barchart example for table value generation", "Send an inference request\n```\nimport requests   \n\ninvoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/3bc390c7-eeec-40f7-a64d-0c6a719985f7\" \nfetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\" \n\n# Ensure that you have configured API_KEY \nheaders = { \n    \"Authorization\": \"Bearer {}\".format(API_KEY), \n    \"Accept\": \"application/json\", \n} \n\n# To re-use connections \nsession = requests.Session()  \n\n# The payload consists of a base64 encoded image accompanied by header text. \npayload = { \n  \"messages\": [ \n    { \n        \"content\": \"Generate underlying data table of the figure below:<img src=\\\"data:image/png;base64,{}\\\" />\".format(encoded_string.decode('UTF-8')), \n        \"role\": \"user\" \n    } \n  ], \n  \"temperature\": 0.1, \n  \"top_p\": 0.7, \n  \"max_tokens\": 1024, \n  \"stream\": False \n} \n\nresponse = session.post(invoke_url, headers=headers, json=payload)   \n\nwhile response.status_code == 202: \n    request_id = response.headers.get(\"NVCF-REQID\") \n    fetch_url = fetch_url_format + request_id \n    response = session.get(fetch_url, headers=headers) \n\nresponse.raise_for_status() \nresponse_body = response.json() \nprint(response_body)\n```\nThe output looks like the following:\n```\n{'id': '4423f30d-2d88-495d-83cc-710da97889e3', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Entity | Individuals responsibility | Government's responsibility <0x0A> MEDIAN | 39.0 | 55.0 <0x0A> Germany | nan | 35.0 <0x0A> UK | 45.0 | 49.0 <0x0A> Sweden | 37.0 | 53.0 <0x0A> Denmark | 42.0 | 54.0 <0x0A> France | 38.0 | 55.0 <0x0A> Netherla nns | 40.0 | 58.0 <0x0A> Spain | 29.0 | 63.0 <0x0A> Italy | 22.0 | 74.0\"}, 'finish_reason': 'stop'}], 'usage': {'completion_tokens': 149, 'prompt_tokens': 0, 'total_tokens': 149}} \n```\nThe response body includes the output of DePlot along with additional metadata. The output is generated left-to-right autoregressively as a textual sequence in Markdown format, with separators ```-```, ```|```, and ```<0x0A>``` (newline).", "Visualize the output table\n```\nresponse_table = response_body['choices'][0]['message']['content'] \n\n# Replace the <0x0A> with \\n for better readability \nprint(response_table.replace(\"<0x0A>\", \"\\n\")) \n```\nThe output looks like the following:\n```\nEntity | Individuals responsibility | Government's responsibility  \nMEDIAN | 39.0 | 55.0  \nGermany | nan | 35.0  \nUK | 45.0 | 49.0  \nSweden | 37.0 | 53.0  \nDenmark | 42.0 | 54.0  \nFrance | 38.0 | 55.0  \nNetherla nns | 40.0 | 58.0  \nSpain | 29.0 | 63.0  \nItaly | 22.0 | 74.0\n```\nIn this case, the response was largely accurate. This output can be used as part of the input context to an LLM for a downstream task like question answering (QA).", "Enterprise-grade AI runtime for model deployments\nSecurity, reliability, and enterprise support are critical when AI models are ready to deploy for business operations.\nNVIDIA AI Enterprise, an end-to-end AI runtime software platform, is designed to accelerate the data science pipeline and streamline the development and deployment of production-grade generative AI applications.\nNVIDIA AI Enterprise provides the security, support, stability, and manageability to improve the productivity of AI teams, reduce the total cost of AI infrastructure, and ensure a smooth transition from POC to production.", "Get started\nTry the DePlot model through the UI or the API. If this model is the right fit for your application, optimize the model with NVIDIA TensorRT-LLM.\nIf you\u2019re building an enterprise application, sign up for an NVIDIA AI Enterprise trial to get support for taking your application to production."], "document_title": "Model Monday: Query Graphs with Optimized DePlot Model", "document_url": "https://developer.nvidia.com/blog/model-monday-query-graphs-with-optimized-deplot-model/", "document_date": "2024-01-23T00:34:34", "document_date_modified": "2024-01-25T18:25:28", "document_full_text": "Model Monday: Query Graphs with Optimized DePlot Model\nNVIDIA Foundation Models and Endpoints provides access to a curated set of community and NVIDIA-built generative AI models to experience, customize, and deploy in enterprise applications.\nOn Mondays throughout the year, we\u2019ll be releasing new models. This week, we released the NVIDIA-optimized DePlot model, which you can experience directly from your browser.\nIf you haven\u2019t already, try the leading models like Nemotron-3, Mixtral 8X7B, Llama 2, and Stable Diffusion in the NVIDIA AI playground.\nDePlot\nA leap in visual language reasoning, DePlot by Google Research enables comprehension of charts and plots when coupled with a large language model (LLM). As opposed to prior multimodal LLMs that are trained end-to-end for plot de-rendering and numerical reasoning, this approach breaks down the problem into the following steps:\nPlot-to-text translation using a pretrained image-to-text model\nTextual reasoning using an LLM\nSpecifically, DePlot refers to the image-to-text Transformer model in the first step, used for modality conversion from a plot to the text format. The linearized tables generated by DePlot can be directly ingested as part of a prompt into an LLM in the second step to facilitate reasoning.\nPrevious state-of-the-art (SOTA) models required at least tens of thousands of human-written examples to accomplish such plot or chart comprehension while still being limited in their reasoning capabilities on complex queries.\nUsing this plug-and-play approach, the DePlot+LLM pipeline achieves over 29.4% improvement over the previous SOTA on the ChartQA benchmark with just one-shot prompting!\nScreenshot of the DePlot playground dashboard shows a bar chart with configuration settings.\nFigure 1. DePlot converts a plot into a structured table\nFigure 1 shows how DePlot converts a plot into a structured table that can be used as context for LLMs to answer reasoning-based questions. Try DePlot now.\nUsing the model in a browser\nYou can now experience DePlot directly from your browser using a simple user interface on the DePlot playground on the NGC catalog. The following video shows the results generated from the models running on a fully accelerated stack.\nVideo 1. NVIDIA AI Foundation model interface The video shows the NVIDIA AI Foundation model interface used to extract information from a graph using DePlot running on a fully accelerated stack.\nUsing the model with the API\nIf you would rather use the API to test out the model, we\u2019ve got you covered. After you sign in to the NGC catalog, you have access to NVIDIA cloud credits that enable you to truly experience the models at scale by connecting your application to the API endpoint.\nThe following Python example uses ```base64``` and requests modules to encode the plot image and issue requests to the API endpoint. Before proceeding, ensure that you have an environment capable of executing Python code, such as a Jupyter notebook.\nObtain the NGC catalog API key\nOn the API tab, select Generate Key. If you haven\u2019t registered yet, you are prompted to sign up or sign in.\nSet the API key in your code:\n```\n# Will be used to issue requests to the endpoint \nAPI_KEY = \u201cnvapi-xxxx\u201c\n```\nEncode your chart or graph in base64 format\nTo provide an image input as part of your request, you must encode it in ```base64``` format.\n```\n# Fetch an example chart from ChartQA dataset \n!wget -cO -  https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png > chartQA-example.png \n\n# Encode the image into base64 format  \nimport base64 \n\nwith open(os.path.join(os.getcwd(), \"chartQA-example.png\"), \"rb\") as image_file: \n    encoded_string = base64.b64encode(image_file.read())\n```\nAs an option, you can visualize the chart:\n```\nfrom IPython import display \ndisplay.Image(base64.b64decode(encoded_string))\n```\nBar chart show the breakdown of support for government economic assistance across various European countries. This example chart is used to feed the model and generate values as a table.\nFigure 2. Barchart example for table value generation\nSend an inference request\n```\nimport requests   \n\ninvoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/3bc390c7-eeec-40f7-a64d-0c6a719985f7\" \nfetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\" \n\n# Ensure that you have configured API_KEY \nheaders = { \n    \"Authorization\": \"Bearer {}\".format(API_KEY), \n    \"Accept\": \"application/json\", \n} \n\n# To re-use connections \nsession = requests.Session()  \n\n# The payload consists of a base64 encoded image accompanied by header text. \npayload = { \n  \"messages\": [ \n    { \n        \"content\": \"Generate underlying data table of the figure below:<img src=\\\"data:image/png;base64,{}\\\" />\".format(encoded_string.decode('UTF-8')), \n        \"role\": \"user\" \n    } \n  ], \n  \"temperature\": 0.1, \n  \"top_p\": 0.7, \n  \"max_tokens\": 1024, \n  \"stream\": False \n} \n\nresponse = session.post(invoke_url, headers=headers, json=payload)   \n\nwhile response.status_code == 202: \n    request_id = response.headers.get(\"NVCF-REQID\") \n    fetch_url = fetch_url_format + request_id \n    response = session.get(fetch_url, headers=headers) \n\nresponse.raise_for_status() \nresponse_body = response.json() \nprint(response_body)\n```\nThe output looks like the following:\n```\n{'id': '4423f30d-2d88-495d-83cc-710da97889e3', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Entity | Individuals responsibility | Government's responsibility <0x0A> MEDIAN | 39.0 | 55.0 <0x0A> Germany | nan | 35.0 <0x0A> UK | 45.0 | 49.0 <0x0A> Sweden | 37.0 | 53.0 <0x0A> Denmark | 42.0 | 54.0 <0x0A> France | 38.0 | 55.0 <0x0A> Netherla nns | 40.0 | 58.0 <0x0A> Spain | 29.0 | 63.0 <0x0A> Italy | 22.0 | 74.0\"}, 'finish_reason': 'stop'}], 'usage': {'completion_tokens': 149, 'prompt_tokens': 0, 'total_tokens': 149}} \n```\nThe response body includes the output of DePlot along with additional metadata. The output is generated left-to-right autoregressively as a textual sequence in Markdown format, with separators ```-```, ```|```, and ```<0x0A>``` (newline).\nVisualize the output table\n```\nresponse_table = response_body['choices'][0]['message']['content'] \n\n# Replace the <0x0A> with \\n for better readability \nprint(response_table.replace(\"<0x0A>\", \"\\n\")) \n```\nThe output looks like the following:\n```\nEntity | Individuals responsibility | Government's responsibility  \nMEDIAN | 39.0 | 55.0  \nGermany | nan | 35.0  \nUK | 45.0 | 49.0  \nSweden | 37.0 | 53.0  \nDenmark | 42.0 | 54.0  \nFrance | 38.0 | 55.0  \nNetherla nns | 40.0 | 58.0  \nSpain | 29.0 | 63.0  \nItaly | 22.0 | 74.0\n```\nIn this case, the response was largely accurate. This output can be used as part of the input context to an LLM for a downstream task like question answering (QA).\nEnterprise-grade AI runtime for model deployments\nSecurity, reliability, and enterprise support are critical when AI models are ready to deploy for business operations.\nNVIDIA AI Enterprise, an end-to-end AI runtime software platform, is designed to accelerate the data science pipeline and streamline the development and deployment of production-grade generative AI applications.\nNVIDIA AI Enterprise provides the security, support, stability, and manageability to improve the productivity of AI teams, reduce the total cost of AI infrastructure, and ensure a smooth transition from POC to production.\nGet started\nTry the DePlot model through the UI or the API. If this model is the right fit for your application, optimize the model with NVIDIA TensorRT-LLM.\nIf you\u2019re building an enterprise application, sign up for an NVIDIA AI Enterprise trial to get support for taking your application to production."}], "https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/": [{"text": "The article discusses the integration of generative AI models with NVIDIA Metropolis Microservices for Jetson, allowing for the rapid development of intelligent video analytic applications. It explains how to develop and deploy generative AI-powered applications on the NVIDIA Jetson edge AI platform using a reference example. Generative AI models, such as CLIP, Owl, Llama, GPT, and Stable Diffusion, offer broad understanding across domains and can be used for tasks like zero or few-shot learning. The article details the steps to customize and deploy a generative AI application with Metropolis Microservices, including adding RTSP I/O, setting up a REST endpoint for prompt updates, generating overlays, interacting with Video Storage Toolkit (VST), and outputting metadata to Redis. The deployment process involves containerizing the application, setting up platform services like Ingress and Redis, and launching the application with docker-compose. Overall, the article provides a comprehensive guide on leveraging generative AI and Metropolis Microservices to build production-quality vision AI applications on Jetson.", "text_components": ["Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson\nNVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches.\nThis post explains how to develop and deploy generative AI \u2013powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that can be used as a general recipe for nearly any model.\nThe reference example uses a stand-alone zero-shot detection NanoOwl application and integrates it with Metropolis Microservices for Jetson, so that you can quickly prototype and deploy it in production.", "Transform your applications with generative AI\nGenerative AI is a new class of machine learning that enables models to understand the world in a more open way than previous methods.\nAt the heart of most generative AI is a transformer-based model that has been trained on internet-scale data. These models have a much broader understanding across domains, enabling them to be used as a backbone for a variety of tasks. This flexibility enables models like CLIP, Owl, Llama, GPT, and Stable Diffusion to comprehend natural language inputs. They are capable of zero or few-shot learning.\nGIF shows the NanoOwl model detecting a person, face, hands, and shoes on request.\nFigure 1. Zero-shot detection using NanoOwl\nFor more information about generative AI models for Jetson, see the NVIDIA Jetson Generative AI Lab and Bringing Generative AI to Life with NVIDIA Jetson.", "Metropolis Microservices for Jetson\nMetropolis Microservices can be used to rapidly build production-ready AI applications on Jetson. Metropolis Microservices are a set of modular and easily deployable Docker containers for camera management, system monitoring, IoT device integration, networking, storage, and more. These can be brought together to create powerful applications. Figure 2 shows the available microservices.\nThe diagram shows reference AI workflows, app microservices, an AI stack, and platform software.\nFigure 2. Metropolis Microservices for Jetson stack\nFor more information, see the Metropolis Microservices for Jetson whitepaper.", "Integrating generative AI apps with Metropolis Microservices\nMetropolis Microservices and generative AI can be combined to take advantage of models that require little to no training. Figure 3 shows a diagram of the NanoOwl reference example that can be used as a general recipe to build generative AI\u2013powered applications with Metropolis Microservices on Jetson.\nThe system diagram shows the user interfaces, live camera streams, and microservices on Jetson, including the generative AI application, Video Storage Toolkit, ingress, Redis, and monitoring.\nFigure 3. Generative AI reference application using Metropolis Microservices for Jetson", "Application customization with Metropolis Microservices\nThere are many open-source generative AI models available on GitHub and some have been optimized to run specifically on Jetson. You can find several of these models in the Jetson Generative AI Lab.\nMost of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output.\nIn the Python reference example, we used NanoOwl as the generative AI model. However, the general recipe of the reference example can be applied to nearly any generative AI model.\nThe diagram shows how a generative AI application can take in an RTSP stream and output detection information to RTSP and Redis.\nFigure 4. Overview of a generative AI application using Metropolis Microservices\nTo run any generative AI model with Metropolis Microservices, you must first align the input and output from other microservices (Figure 4).\nFor streaming video, the input and output uses the RTSP protocol. RTSP is streamed from Video Storage Toolkit (VST), a video ingestion and management microservice. The output is streamed over RTSP with the overlaid inference output. The output metadata is sent to a Redis stream where other applications can read the data. For more information, see the Video Storage Toolkit with Metropolis Microservices demo videos.\nSecond, as a generative AI application requires some external interface such as prompts, you need the application to take REST API requests.\nLastly, the application must be containerized to integrate seamlessly with other microservices. Figure 5 shows an example of NanoOwl object detection and metadata output on Redis.\nGIF shows the generative AI application detecting various objects like boxes, pallets, and people.\nFigure 5. Generative AI application running", "Prepare the generative AI application\nThis reference example uses NanoOwl. However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe. For more information about the full implementation, see the reference example on the /NVIDIA-AI-IOT/mmj_genai GitHub project.\nTo prepare a generative AI model for integration with Metropolis Microservices, take the following steps:\nCall the ```predict``` function for model inference\nAdd RTSP I/O using the ```jetson-utils``` library.\nAdd a REST endpoint for prompt updates with Flask.\nUse ```mmj_utils``` to generate overlays.\nUse ```mmj_utils``` to interact with VST to get streams.\nUse ```mmj_utils``` to output metadata to Redis.", "Call the predict function for model inference\nNanoOwl wraps the generative AI model in an ```OwlPredictor``` class. When this class is instantiated, it loads the model into memory. To make an inference on an image and text input, call the ```predict``` function to get the output.\nIn this case, the output is a list of bounding boxes and labels for the detected objects.\n```\nimport PIL.Image\nimport time\nimport torch\nfrom nanoowl.owl_predictor import OwlPredictor\n\nimage = PIL.Image.open(\"my_image.png\")\nprompt = [\"an owl\", \"a person\"]\n\n#Load model\n predictor = OwlPredictor(\n       \"google/owlvit-base-patch32\",\n        image_encoder_engine=\"../data/owlvit_image_encoder_patch32.engine\"\n    )\n#Embed Text\ntext_encodings = predictor.encode_text(text)\n\n#Inference\noutput = predictor.predict(\nimage=image, \n        \ttext=prompt, \n        \ttext_encodings=text_encodings,\n       \t threshold=0.1,\n        \tpad_square=False)\n```\nMost generative AI models have similar Python interfaces. There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the ```OwlPredictor``` class.", "Add RTSP I/O using the jetson-utils library\nYou can add RTSP video stream input using the jetson-utils library. This library provides ```videoSource``` and ```videoOutput``` classes that can be used to capture frames from an RTSP stream and output frames on a new RTSP stream.\n```\nfrom jetson_utils import videoSource, videoOutput\n\nstream_input = \"rtsp://0.0.0.0:8554/input\"\nstream_output = \"rtsp://0.0.0.0:8555/output\"\n\n#Create stream I/O\nv_input = videoSource(stream_input)\nv_output = videoOutput(stream_output)\n\nwhile(True):\n\timage = v_input.Capture() #get image from stream\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\n\tnew_image = postprocess(output)\n\n\tv_output.Render(new_image) #write image to stream \n```\nThis code example captures frames from an RTSP stream, which can then be passed to a model inference function. A new image is created from the model outputs and rendered to an output RTSP stream.", "Add a REST endpoint for prompt updates with Flask\nMany generative AI models accept some kind of prompt or text input. To enable a user or another service to update the prompt dynamically, add a REST endpoint using Flask that accepts prompt updates and passes them to the model.\nTo make the Flask server integrate more easily with your model, create a wrapper class that can be called to launch a Flask server in its own thread. For more information, see the /NVIDIA-AI-IOT/mmj_genai GitHub project.\n```\nfrom flask_server import FlaskServer\n\n#Launch flask server and connect queue to receive prompt updates \nflask_queue = Queue() #hold prompts from flask input \nflask = FlaskServer(flask_queue)\nflask.start_flask()\n\nwhile(True):\n\t...\n\n\tif not flask_queue.empty(): #get prompt update\n            prompt = flask_queue.get()\n\t\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t...\n```\nConnect your main script and the Flask endpoint through a queue that holds any incoming prompt updates. When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes.", "Use mmj_utils to generate overlays\nFor computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object.\nImage shows a factory floor with people, pallets, and equipment delineated by labeled bounding boxes.\nFigure 6. Detection overlay generated with mmj_utils\nTo do this, use the utility class called ```DetectionGenerationCUDA``` from the mmj_utils library. This library depends on ```jetson_utils```, which provides CUDA-accelerated functions used to generate the overlay.\n```\nfrom mmj_utils.overlay_gen import DetectionOverlayCUDA\n\noverlay_gen = DetectionOverlayCUDA(draw_bbox=True, draw_text=True, text_size=45) #make overlay object\n\nwhile(True):\n\t...\n\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t\n\t#Generate overlay and output\n       text_labels = [objects[x] for x in output.labels]\n       bboxes = output.boxes.tolist()\n       image = overlay_gen(image, text_labels, bboxes)#generate overlay\n       v_output.Render(image)\n```\nYou can instantiate the ```DetectionGenerationCUDA``` object with several keyword arguments to adjust the text size, bounding box size, and colors to suit your needs. For more information about overlay generation with ```mmj_utils```, see /NVIDIA-AI-IOT/mmj_utils GitHub repo.\nTo generate the overlay, call the object and pass the input image, list of labels, and bounding boxes generated by the model. It then draws the labels and bounding boxes on the input image and returns the modified image with the overlay. This modified image can then be rendered out on the RTSP stream.", "Use mmj_utils to interact with VST to get streams\nVST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the VST REST API.\nInstead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST.\n```\nfrom mmj_utils.vst import VST\n\nvst = VST(\"http://0.0.0.0:81\")\nvst_rtsp_streams = vst.get_rtsp_streams()\nstream_input = vst_rtsp_streams[0]\n\nv_input = videoSource(stream_input)\n...\n```\nThis connects to VST and grabs the first valid RTSP link. More complex logic could be added here to connect to a specific source or change the inputs dynamically.", "Use mmj_utils to output metadata to Redis\nGenerative AI models generate\u200c metadata that can be used downstream by other services for generating analytics and insights.\nIn this case, NanoOwl outputs bounding boxes on detected objects. You can output this information in Metropolis Schema on a Redis stream, which can be captured by an analytic service. In the ```mmj_utils``` library, there is a class to help produce detection metadata on Redis.\n```\nfrom mmj_utils.schema_gen import SchemaGenerator\n\n\nschema_gen = SchemaGenerator(sensor_id=1, sensor_type=\"camera\", sensor_loc=[10,20,30])\nschema_gen.connect_redis(aredis_host=0.0.0.0, redis_port=6379, redis_stream=\"owl\")\n\nwhile True:\n\t...\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t\n\t#Output metadata\n       text_labels = [objects[x] for x in output.labels]\nschema_gen(text_labels, bboxes)\n```\nYou can instantiate a ```SchemaGenerator``` object with information about the input camera stream and connect to Redis. The object can then be called by passing in text labels and bounding boxes produced by the model. The detection information gets converted to Metropolis Schema and output to Redis to be used by other microservices.\nThe diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream.\nFigure 7. Generative AI application", "Application deployment\nTo deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through ```docker compose```.\nWith the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices.\nContainerize the generative AI application.\nSet up the necessary platform services.\nLaunch the application with ```docker compose```.\nView outputs in real time.", "Containerize the generative AI application\nThe first step for deployment is to containerize the generative AI application using Docker.\nAn easy way to do this is to use the jetson-containers project. This project provides an easy way to build Docker containers for Jetson to support machine learning applications including generative AI models. Use jetson-containers to make a container with the necessary dependencies and then customize the container further to include the application code and any other packages needed to run your generative AI model.\nFor more information about how to build the container for the NanoOwl example, see the /src/readme file in the GitHub project.", "Set up the necessary platform services\nNext, set up the necessary platform services provided by Metropolis Microservices. These platform services provide many features needed to deploy an application with Metropolis Microservices.\nThis reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with ```systemctl```.\nFor more information about how to install and launch the necessary platform services, see the Metropolis Microservices for Jetson Quickstart Guide.", "Launch the application with docker compose\nWith the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using ```docker compose```.\nTo do this, create a ```docker-compose.yaml``` file that defines the containers to be launched along with any necessary launch options. After you define the ```docker compose``` file, you can start or stop your application using the ```docker compose up``` and ```docker compose down``` commands.\nFor more information about docker deployment, see the /deploy/readme file in the GitHub project.", "View outputs in real time\nAfter the application is deployed, you can add an RTSP stream through VST and interact with the generative AI model through the REST API to send prompt updates and view the detections change in real time by watching the RTSP output. You can also see the metadata output on Redis.\nVideo 1. Transform Edge AI Applications with Generative AI Using Metropolis Microservices for Jetson", "Conclusion\nThis post explained how to take a generative AI model and integrate it with Metropolis Microservices for Jetson. With generative AI and Metropolis Microservices, you can rapidly build intelligent video analytic applications that are both flexible and accurate.\nFor more information about the provided services, see the Metropolis Microservices for Jetson product page. To view the full reference application and more detailed steps about how to build and deploy it for yourself, see the /NVIDIA-AI-IOT/mmj_genai GitHub project."], "document_title": "Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson", "document_url": "https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/", "document_date": "2024-01-23T17:00:00", "document_date_modified": "2024-01-25T18:17:29", "document_full_text": "Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson\nNVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches.\nThis post explains how to develop and deploy generative AI \u2013powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that can be used as a general recipe for nearly any model.\nThe reference example uses a stand-alone zero-shot detection NanoOwl application and integrates it with Metropolis Microservices for Jetson, so that you can quickly prototype and deploy it in production.\nTransform your applications with generative AI\nGenerative AI is a new class of machine learning that enables models to understand the world in a more open way than previous methods.\nAt the heart of most generative AI is a transformer-based model that has been trained on internet-scale data. These models have a much broader understanding across domains, enabling them to be used as a backbone for a variety of tasks. This flexibility enables models like CLIP, Owl, Llama, GPT, and Stable Diffusion to comprehend natural language inputs. They are capable of zero or few-shot learning.\nGIF shows the NanoOwl model detecting a person, face, hands, and shoes on request.\nFigure 1. Zero-shot detection using NanoOwl\nFor more information about generative AI models for Jetson, see the NVIDIA Jetson Generative AI Lab and Bringing Generative AI to Life with NVIDIA Jetson.\nMetropolis Microservices for Jetson\nMetropolis Microservices can be used to rapidly build production-ready AI applications on Jetson. Metropolis Microservices are a set of modular and easily deployable Docker containers for camera management, system monitoring, IoT device integration, networking, storage, and more. These can be brought together to create powerful applications. Figure 2 shows the available microservices.\nThe diagram shows reference AI workflows, app microservices, an AI stack, and platform software.\nFigure 2. Metropolis Microservices for Jetson stack\nFor more information, see the Metropolis Microservices for Jetson whitepaper.\nIntegrating generative AI apps with Metropolis Microservices\nMetropolis Microservices and generative AI can be combined to take advantage of models that require little to no training. Figure 3 shows a diagram of the NanoOwl reference example that can be used as a general recipe to build generative AI\u2013powered applications with Metropolis Microservices on Jetson.\nThe system diagram shows the user interfaces, live camera streams, and microservices on Jetson, including the generative AI application, Video Storage Toolkit, ingress, Redis, and monitoring.\nFigure 3. Generative AI reference application using Metropolis Microservices for Jetson\nApplication customization with Metropolis Microservices\nThere are many open-source generative AI models available on GitHub and some have been optimized to run specifically on Jetson. You can find several of these models in the Jetson Generative AI Lab.\nMost of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output.\nIn the Python reference example, we used NanoOwl as the generative AI model. However, the general recipe of the reference example can be applied to nearly any generative AI model.\nThe diagram shows how a generative AI application can take in an RTSP stream and output detection information to RTSP and Redis.\nFigure 4. Overview of a generative AI application using Metropolis Microservices\nTo run any generative AI model with Metropolis Microservices, you must first align the input and output from other microservices (Figure 4).\nFor streaming video, the input and output uses the RTSP protocol. RTSP is streamed from Video Storage Toolkit (VST), a video ingestion and management microservice. The output is streamed over RTSP with the overlaid inference output. The output metadata is sent to a Redis stream where other applications can read the data. For more information, see the Video Storage Toolkit with Metropolis Microservices demo videos.\nSecond, as a generative AI application requires some external interface such as prompts, you need the application to take REST API requests.\nLastly, the application must be containerized to integrate seamlessly with other microservices. Figure 5 shows an example of NanoOwl object detection and metadata output on Redis.\nGIF shows the generative AI application detecting various objects like boxes, pallets, and people.\nFigure 5. Generative AI application running\nPrepare the generative AI application\nThis reference example uses NanoOwl. However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe. For more information about the full implementation, see the reference example on the /NVIDIA-AI-IOT/mmj_genai GitHub project.\nTo prepare a generative AI model for integration with Metropolis Microservices, take the following steps:\nCall the ```predict``` function for model inference\nAdd RTSP I/O using the ```jetson-utils``` library.\nAdd a REST endpoint for prompt updates with Flask.\nUse ```mmj_utils``` to generate overlays.\nUse ```mmj_utils``` to interact with VST to get streams.\nUse ```mmj_utils``` to output metadata to Redis.\nCall the predict function for model inference\nNanoOwl wraps the generative AI model in an ```OwlPredictor``` class. When this class is instantiated, it loads the model into memory. To make an inference on an image and text input, call the ```predict``` function to get the output.\nIn this case, the output is a list of bounding boxes and labels for the detected objects.\n```\nimport PIL.Image\nimport time\nimport torch\nfrom nanoowl.owl_predictor import OwlPredictor\n\nimage = PIL.Image.open(\"my_image.png\")\nprompt = [\"an owl\", \"a person\"]\n\n#Load model\n predictor = OwlPredictor(\n       \"google/owlvit-base-patch32\",\n        image_encoder_engine=\"../data/owlvit_image_encoder_patch32.engine\"\n    )\n#Embed Text\ntext_encodings = predictor.encode_text(text)\n\n#Inference\noutput = predictor.predict(\nimage=image, \n        \ttext=prompt, \n        \ttext_encodings=text_encodings,\n       \t threshold=0.1,\n        \tpad_square=False)\n```\nMost generative AI models have similar Python interfaces. There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the ```OwlPredictor``` class.\nAdd RTSP I/O using the jetson-utils library\nYou can add RTSP video stream input using the jetson-utils library. This library provides ```videoSource``` and ```videoOutput``` classes that can be used to capture frames from an RTSP stream and output frames on a new RTSP stream.\n```\nfrom jetson_utils import videoSource, videoOutput\n\nstream_input = \"rtsp://0.0.0.0:8554/input\"\nstream_output = \"rtsp://0.0.0.0:8555/output\"\n\n#Create stream I/O\nv_input = videoSource(stream_input)\nv_output = videoOutput(stream_output)\n\nwhile(True):\n\timage = v_input.Capture() #get image from stream\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\n\tnew_image = postprocess(output)\n\n\tv_output.Render(new_image) #write image to stream \n```\nThis code example captures frames from an RTSP stream, which can then be passed to a model inference function. A new image is created from the model outputs and rendered to an output RTSP stream.\nAdd a REST endpoint for prompt updates with Flask\nMany generative AI models accept some kind of prompt or text input. To enable a user or another service to update the prompt dynamically, add a REST endpoint using Flask that accepts prompt updates and passes them to the model.\nTo make the Flask server integrate more easily with your model, create a wrapper class that can be called to launch a Flask server in its own thread. For more information, see the /NVIDIA-AI-IOT/mmj_genai GitHub project.\n```\nfrom flask_server import FlaskServer\n\n#Launch flask server and connect queue to receive prompt updates \nflask_queue = Queue() #hold prompts from flask input \nflask = FlaskServer(flask_queue)\nflask.start_flask()\n\nwhile(True):\n\t...\n\n\tif not flask_queue.empty(): #get prompt update\n            prompt = flask_queue.get()\n\t\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t...\n```\nConnect your main script and the Flask endpoint through a queue that holds any incoming prompt updates. When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes.\nUse mmj_utils to generate overlays\nFor computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object.\nImage shows a factory floor with people, pallets, and equipment delineated by labeled bounding boxes.\nFigure 6. Detection overlay generated with mmj_utils\nTo do this, use the utility class called ```DetectionGenerationCUDA``` from the mmj_utils library. This library depends on ```jetson_utils```, which provides CUDA-accelerated functions used to generate the overlay.\n```\nfrom mmj_utils.overlay_gen import DetectionOverlayCUDA\n\noverlay_gen = DetectionOverlayCUDA(draw_bbox=True, draw_text=True, text_size=45) #make overlay object\n\nwhile(True):\n\t...\n\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t\n\t#Generate overlay and output\n       text_labels = [objects[x] for x in output.labels]\n       bboxes = output.boxes.tolist()\n       image = overlay_gen(image, text_labels, bboxes)#generate overlay\n       v_output.Render(image)\n```\nYou can instantiate the ```DetectionGenerationCUDA``` object with several keyword arguments to adjust the text size, bounding box size, and colors to suit your needs. For more information about overlay generation with ```mmj_utils```, see /NVIDIA-AI-IOT/mmj_utils GitHub repo.\nTo generate the overlay, call the object and pass the input image, list of labels, and bounding boxes generated by the model. It then draws the labels and bounding boxes on the input image and returns the modified image with the overlay. This modified image can then be rendered out on the RTSP stream.\nUse mmj_utils to interact with VST to get streams\nVST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the VST REST API.\nInstead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST.\n```\nfrom mmj_utils.vst import VST\n\nvst = VST(\"http://0.0.0.0:81\")\nvst_rtsp_streams = vst.get_rtsp_streams()\nstream_input = vst_rtsp_streams[0]\n\nv_input = videoSource(stream_input)\n...\n```\nThis connects to VST and grabs the first valid RTSP link. More complex logic could be added here to connect to a specific source or change the inputs dynamically.\nUse mmj_utils to output metadata to Redis\nGenerative AI models generate\u200c metadata that can be used downstream by other services for generating analytics and insights.\nIn this case, NanoOwl outputs bounding boxes on detected objects. You can output this information in Metropolis Schema on a Redis stream, which can be captured by an analytic service. In the ```mmj_utils``` library, there is a class to help produce detection metadata on Redis.\n```\nfrom mmj_utils.schema_gen import SchemaGenerator\n\n\nschema_gen = SchemaGenerator(sensor_id=1, sensor_type=\"camera\", sensor_loc=[10,20,30])\nschema_gen.connect_redis(aredis_host=0.0.0.0, redis_port=6379, redis_stream=\"owl\")\n\nwhile True:\n\t...\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t\n\t#Output metadata\n       text_labels = [objects[x] for x in output.labels]\nschema_gen(text_labels, bboxes)\n```\nYou can instantiate a ```SchemaGenerator``` object with information about the input camera stream and connect to Redis. The object can then be called by passing in text labels and bounding boxes produced by the model. The detection information gets converted to Metropolis Schema and output to Redis to be used by other microservices.\nThe diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream.\nFigure 7. Generative AI application\nApplication deployment\nTo deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through ```docker compose```.\nWith the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices.\nContainerize the generative AI application.\nSet up the necessary platform services.\nLaunch the application with ```docker compose```.\nView outputs in real time.\nContainerize the generative AI application\nThe first step for deployment is to containerize the generative AI application using Docker.\nAn easy way to do this is to use the jetson-containers project. This project provides an easy way to build Docker containers for Jetson to support machine learning applications including generative AI models. Use jetson-containers to make a container with the necessary dependencies and then customize the container further to include the application code and any other packages needed to run your generative AI model.\nFor more information about how to build the container for the NanoOwl example, see the /src/readme file in the GitHub project.\nSet up the necessary platform services\nNext, set up the necessary platform services provided by Metropolis Microservices. These platform services provide many features needed to deploy an application with Metropolis Microservices.\nThis reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with ```systemctl```.\nFor more information about how to install and launch the necessary platform services, see the Metropolis Microservices for Jetson Quickstart Guide.\nLaunch the application with docker compose\nWith the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using ```docker compose```.\nTo do this, create a ```docker-compose.yaml``` file that defines the containers to be launched along with any necessary launch options. After you define the ```docker compose``` file, you can start or stop your application using the ```docker compose up``` and ```docker compose down``` commands.\nFor more information about docker deployment, see the /deploy/readme file in the GitHub project.\nView outputs in real time\nAfter the application is deployed, you can add an RTSP stream through VST and interact with the generative AI model through the REST API to send prompt updates and view the detections change in real time by watching the RTSP output. You can also see the metadata output on Redis.\nVideo 1. Transform Edge AI Applications with Generative AI Using Metropolis Microservices for Jetson\nConclusion\nThis post explained how to take a generative AI model and integrate it with Metropolis Microservices for Jetson. With generative AI and Metropolis Microservices, you can rapidly build intelligent video analytic applications that are both flexible and accurate.\nFor more information about the provided services, see the Metropolis Microservices for Jetson product page. To view the full reference application and more detailed steps about how to build and deploy it for yourself, see the /NVIDIA-AI-IOT/mmj_genai GitHub project."}], "https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/": [{"text": "The article discusses the importance of benchmarking camera performance on workstations when designing digital twin simulations for robots. It provides a detailed guide on setting up and running a camera benchmark on NVIDIA Isaac Sim, a simulation software. The demo requires an NVIDIA RTX GPU and the most current version of NVIDIA Isaac Sim installed on a Linux-based machine. The script provided in the article automatically launches the simulator, loads necessary cameras, and displays the output in the terminal. Users can modify the default configuration by creating a new JSON file called config.json to specify camera location, resolution, and simulation type. The script also measures the average ROS 2 frequency output and camera fps. Overall, the article aims to help users monitor camera performance and ROS topic output on their workstations using NVIDIA Isaac Sim. It encourages readers to join NVIDIA Isaac office hours on YouTube for further assistance.", "text_components": ["Benchmarking Camera Performance on Your Workstation with NVIDIA Isaac Sim\nRobots are typically equipped with cameras. When designing a digital twin simulation, it\u2019s important to replicate its performance in a simulated environment accurately.\nHowever, to make sure the simulation runs smoothly, it\u2019s crucial to check the performance of the workstation that is running the simulation. In this blog post, we explore the steps to setting up and running a camera benchmark on your workstation on NVIDIA Isaac Sim.\nBefore starting, we explore how Isaac Sim is designed to add cameras and how to export the outputs for ROS 2.\nNVIDIA Isaac Sim can simulate multiple types of sensors, starting from range sensors, like LIDAR, ultrasonic, generic range sensors, contact sensors, and IMU force and proximity sensors.\nThe camera sensor is the most advanced simulation that you can find in Isaac Sim and you can add this sensor quickly using the user interface.\nA screenshot example of adding a camera to Isaac Sim\nFigure 1. Example of adding a Camera on NVIDIA Isaac Sim, selecting configuration and output\nAlso, you can use a Python script to load your camera or multiple cameras from a Python object, like in the following example.\n```\ncamera = Camera(\n    prim_path=\"/World/camera\",\n    position=np.array([0.0, 0.0, 25.0]),\n    frequency=20,\n    resolution=(256, 256),\n    orientation=rot_utils.euler_angles_to_quats(np.array([0, 90, 0]), degrees=True),\n)\n```\nFor more details about the NVIDIA Isaac Sim camera, review our documentation.", "Set up your demo\nYou\u2019ll need a workstation with an NVIDIA RTX GPU and the most current version of NVIDIA Isaac Sim 2023.1.0 installed to use this benchmark. This demo is only compatible with Linux-based machines running Ubuntu 20.04 or Ubuntu 22.04.\nThis script doesn\u2019t require the user interface to run Isaac Sim. It can be done with a terminal. The script will launch the simulator, load all the necessary cameras, and display the output in the terminal.", "Installation and run\nClone the demo isaac_camera_benchmark:\ngit clone https://github.com/nvidia_iot/isaac_camera_benchmark.git\ncd isaac_camera_benchmark This repository contains all the scripts and files to run the demo on your screen, but before running the script, remember to download NVIDIA Isaac Sim.\nFor guidance, follow the instructions on NVIDIA Omniverse.\nA screenshot of an NVIDIA Omniverse library page.\nFigure 2. NVIDIA Omniverse, library page, and NVIDIA Isaac Sim download button\nFrom this page, select NVIDIA Isaac Sim 2023.1.0 and download. Then go to your terminal and run the script:\n./run_camera_benchmark.sh This script will automatically start the latest NVIDIA Isaac Sim and initialize a simulation.\nIn this simulation, three cameras move in a clockwise direction around three different locations within a small virtual warehouse. At the same time, the script displays the current frame rate and the average ROS 2 frequency output on the terminal.\nA screenshot of an example camera output from Isaac Sim.\nFigure 3. An example of camera outputs on a terminal\nThe FPS output may vary depending on your workstation configuration and may differ from your hardware setup.\nTo store the ROS2 output coming from this benchmark, run:\n./ros2record.sh\nA screenshot of the ros2record terminal script.\nFigure 4. A script recording of the output from all configured cameras\nAll ros2bag files are available in the folder ```isaac_camera_benchmark/rosbag```.\nThe next chapter explains the script\u2019s inner workings and how to change its configuration to test multiple cameras or use a different resolution.", "Change the default configuration\nThere are three main files, and you can design your configuration by writing a configuration file.\nThe first script is ```run_camera_benchmark.sh```, which loads the latest version of Isaac Sim, passes a script of all the configurations, and runs this demo.\nThe main script of this repository is ```camera_benchmark.py``` script, which runs Isaac Sim with only the ROS2_bridge extension, and by default, it loads the ```warehouse_with_forklifts.usd``` environment that contains a small virtual warehouse with two forklifts. The simulation will run in ray-tracing lighting.\nWhen the environment is loaded, add every camera configured in a file called config.json or load three cameras at 640\u00d7480 resolution.\nThis script automatically adds a camera object in the environment and also builds a graph that reads the output coming from Isaac Sim, fixes a resolution, and publishes on ROS 2 output.\nA screenshot of the generated graph from script.\nFigure 5. A generated graph from ```camera_benchmar.py```\nAt the same time, the ```camera_benchmar.py``` loads a ROS 2 node called benchmark_camera_node that is connected from a ROS 2 camera output and measures the topic frequency average.\nThis script also reads from Isaac Sim the camera fps and publishes the output on the terminal.\nTo change the default configuration, you can simply create a new JSON file called config.json. This file must contain the configuration of the cameras. There are different options that you can set:\nTranslate: Where the camera should be located in the environment, this variable must be a vector with three coordinates like [0.0 0.0 0.0].\nResolution: Camera output resolution is an integer vector with a camera resolution such as 640 \u00d7 480. Suggested resolutions include:\n640 \u00d7 480\n1024 \u00d7 768\n1920 \u00d7 1080 (FHD)\n2560 \u00d7 1440 (2K)\n3840 \u00d7 2160 (4K)\nThe following is an example of a new ```config.json```.\n```\n{\n   \"camera\": [\n       {\"translate\": [0.0, 0.0, 3.0], \"resolution\": [640,480]}\n       {\"translate\": [-1.0, 0.0, 6.0], \"resolution\": [1024,768]}\n   ]\n}\n```\nThis output generates a new benchmark like the following image.\nA screenshot showing the camera output from Isaac Sim.\nFigure 6. Example output cameras and on terminal FPs ros topic and Isaac Sim fps You can add a configuration to the config.json file to specify the type of simulation you want to start on Isaac Sim.\nrenderer: Pick the type of render. The default is RayTracedLighting\nheadless: To run Isaac Sim without a user interface, you can change this boolean variable with True\nSee the following for another example of the configuration file.\n```\n{\n   \"simulation\": {\"renderer\": \"RayTracedLighting\", \"headless\": true},\n   \"camera\": [\n       {\"translate\": [0.0, 0.0, 3.0], \"resolution\": [640,480]},\n       {\"translate\": [-1.0, 0.0, 6.0], \"resolution\": [1024,768]}\n   ]\n}\n```", "Conclusion\nThis script enables you to monitor the camera output and the performance of a ROS topic on your workstation. It also provides an example of how to create a new Isaac Sim script with ROS 2.\nBe sure to join our NVIDIA Isaac office hours every Wednesday on YouTube."], "document_title": "Benchmarking Camera Performance on Your Workstation with NVIDIA Isaac Sim", "document_url": "https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/", "document_date": "2024-01-22T15:00:00", "document_date_modified": "2024-01-25T18:17:30", "document_full_text": "Benchmarking Camera Performance on Your Workstation with NVIDIA Isaac Sim\nRobots are typically equipped with cameras. When designing a digital twin simulation, it\u2019s important to replicate its performance in a simulated environment accurately.\nHowever, to make sure the simulation runs smoothly, it\u2019s crucial to check the performance of the workstation that is running the simulation. In this blog post, we explore the steps to setting up and running a camera benchmark on your workstation on NVIDIA Isaac Sim.\nBefore starting, we explore how Isaac Sim is designed to add cameras and how to export the outputs for ROS 2.\nNVIDIA Isaac Sim can simulate multiple types of sensors, starting from range sensors, like LIDAR, ultrasonic, generic range sensors, contact sensors, and IMU force and proximity sensors.\nThe camera sensor is the most advanced simulation that you can find in Isaac Sim and you can add this sensor quickly using the user interface.\nA screenshot example of adding a camera to Isaac Sim\nFigure 1. Example of adding a Camera on NVIDIA Isaac Sim, selecting configuration and output\nAlso, you can use a Python script to load your camera or multiple cameras from a Python object, like in the following example.\n```\ncamera = Camera(\n    prim_path=\"/World/camera\",\n    position=np.array([0.0, 0.0, 25.0]),\n    frequency=20,\n    resolution=(256, 256),\n    orientation=rot_utils.euler_angles_to_quats(np.array([0, 90, 0]), degrees=True),\n)\n```\nFor more details about the NVIDIA Isaac Sim camera, review our documentation.\nSet up your demo\nYou\u2019ll need a workstation with an NVIDIA RTX GPU and the most current version of NVIDIA Isaac Sim 2023.1.0 installed to use this benchmark. This demo is only compatible with Linux-based machines running Ubuntu 20.04 or Ubuntu 22.04.\nThis script doesn\u2019t require the user interface to run Isaac Sim. It can be done with a terminal. The script will launch the simulator, load all the necessary cameras, and display the output in the terminal.\nInstallation and run\nClone the demo isaac_camera_benchmark:\ngit clone https://github.com/nvidia_iot/isaac_camera_benchmark.git\ncd isaac_camera_benchmark This repository contains all the scripts and files to run the demo on your screen, but before running the script, remember to download NVIDIA Isaac Sim.\nFor guidance, follow the instructions on NVIDIA Omniverse.\nA screenshot of an NVIDIA Omniverse library page.\nFigure 2. NVIDIA Omniverse, library page, and NVIDIA Isaac Sim download button\nFrom this page, select NVIDIA Isaac Sim 2023.1.0 and download. Then go to your terminal and run the script:\n./run_camera_benchmark.sh This script will automatically start the latest NVIDIA Isaac Sim and initialize a simulation.\nIn this simulation, three cameras move in a clockwise direction around three different locations within a small virtual warehouse. At the same time, the script displays the current frame rate and the average ROS 2 frequency output on the terminal.\nA screenshot of an example camera output from Isaac Sim.\nFigure 3. An example of camera outputs on a terminal\nThe FPS output may vary depending on your workstation configuration and may differ from your hardware setup.\nTo store the ROS2 output coming from this benchmark, run:\n./ros2record.sh\nA screenshot of the ros2record terminal script.\nFigure 4. A script recording of the output from all configured cameras\nAll ros2bag files are available in the folder ```isaac_camera_benchmark/rosbag```.\nThe next chapter explains the script\u2019s inner workings and how to change its configuration to test multiple cameras or use a different resolution.\nChange the default configuration\nThere are three main files, and you can design your configuration by writing a configuration file.\nThe first script is ```run_camera_benchmark.sh```, which loads the latest version of Isaac Sim, passes a script of all the configurations, and runs this demo.\nThe main script of this repository is ```camera_benchmark.py``` script, which runs Isaac Sim with only the ROS2_bridge extension, and by default, it loads the ```warehouse_with_forklifts.usd``` environment that contains a small virtual warehouse with two forklifts. The simulation will run in ray-tracing lighting.\nWhen the environment is loaded, add every camera configured in a file called config.json or load three cameras at 640\u00d7480 resolution.\nThis script automatically adds a camera object in the environment and also builds a graph that reads the output coming from Isaac Sim, fixes a resolution, and publishes on ROS 2 output.\nA screenshot of the generated graph from script.\nFigure 5. A generated graph from ```camera_benchmar.py```\nAt the same time, the ```camera_benchmar.py``` loads a ROS 2 node called benchmark_camera_node that is connected from a ROS 2 camera output and measures the topic frequency average.\nThis script also reads from Isaac Sim the camera fps and publishes the output on the terminal.\nTo change the default configuration, you can simply create a new JSON file called config.json. This file must contain the configuration of the cameras. There are different options that you can set:\nTranslate: Where the camera should be located in the environment, this variable must be a vector with three coordinates like [0.0 0.0 0.0].\nResolution: Camera output resolution is an integer vector with a camera resolution such as 640 \u00d7 480. Suggested resolutions include:\n640 \u00d7 480\n1024 \u00d7 768\n1920 \u00d7 1080 (FHD)\n2560 \u00d7 1440 (2K)\n3840 \u00d7 2160 (4K)\nThe following is an example of a new ```config.json```.\n```\n{\n   \"camera\": [\n       {\"translate\": [0.0, 0.0, 3.0], \"resolution\": [640,480]}\n       {\"translate\": [-1.0, 0.0, 6.0], \"resolution\": [1024,768]}\n   ]\n}\n```\nThis output generates a new benchmark like the following image.\nA screenshot showing the camera output from Isaac Sim.\nFigure 6. Example output cameras and on terminal FPs ros topic and Isaac Sim fps You can add a configuration to the config.json file to specify the type of simulation you want to start on Isaac Sim.\nrenderer: Pick the type of render. The default is RayTracedLighting\nheadless: To run Isaac Sim without a user interface, you can change this boolean variable with True\nSee the following for another example of the configuration file.\n```\n{\n   \"simulation\": {\"renderer\": \"RayTracedLighting\", \"headless\": true},\n   \"camera\": [\n       {\"translate\": [0.0, 0.0, 3.0], \"resolution\": [640,480]},\n       {\"translate\": [-1.0, 0.0, 6.0], \"resolution\": [1024,768]}\n   ]\n}\n```\nConclusion\nThis script enables you to monitor the camera output and the performance of a ROS topic on your workstation. It also provides an example of how to create a new Isaac Sim script with ROS 2.\nBe sure to join our NVIDIA Isaac office hours every Wednesday on YouTube."}], "https://developer.nvidia.com/blog/simulating-railroads-with-openusd/": [{"text": "Railroad simulation is a crucial tool for engineers and developers to optimize railway systems, ensuring efficiency and safety. Deutsche Bahn (DB) is using railOmniverse, a C++ extension in NVIDIA Omniverse, to simulate their complex railroad systems accurately. The extension, developed by Trend Verlag in collaboration with DB, provides tools for creating and managing track systems, motor models, brakes, and wheel friction. By integrating railOmniverse into their digital twins, DB can save time in creating realistic railway simulations. The TrackJoint feature in railOmniverse connects the wheel frames of a train to the track system, maintaining stability and realism. The Trax Library, part of railOmniverse, includes tools for track system creation, motor modeling, friction and brakes, bogies, sensors, and signals. Trend plans to release a public version of railOmniverse in early 2024. The collaboration between DB and Trend using OpenUSD has facilitated the development of realistic railway simulations, contributing to the advancement of rail infrastructure.", "text_components": ["Simulating Railroads with OpenUSD\nRailroad simulation is important in modern transportation and logistics, providing a virtual testing ground for the intricate interplay of tracks, switches, and rolling stock. It serves as a crucial tool for engineers and developers to fine-tune and optimize railway systems, ensuring efficiency, safety, and cost-effectiveness.\nPhysically realistic simulations enable comprehensive scenario analysis, predictive maintenance, and the exploration of innovative solutions, ultimately contributing to the advancement and sustainability of rail infrastructure.\nHowever, simulating railroads effectively and accurately is no easy feat. A wide range of interconnected and dynamic components must be modeled precisely. Real-world physics, safety protocols, and the diversity of operational scenarios must also be considered.", "Realistic rail simulations with railOmniverse extension\nWithin the sector initiative Digitale Schiene Deutschland, Deutsche Bahn (DB) aims to use digitalization to improve the capacity, quality, and efficiency of their rail network. One measure is the creation of a physically accurate emulation of the country-wide rail system in Germany.\nrailOmniverse is a C++ extension developed in NVIDIA Omniverse by Trend Verlag (Trend) with Deutsche Bahn to facilitate the simulation of their highly complex railroad systems. Based on the Trend Trax Track Library, the extension provides tools for creating and managing track systems, motor models, brakes, and wheel friction in a physical railroad simulation. railOmniverse uses NVIDIA PhysX to support the realistic simulation of physical interactions, forces, and dynamics between the various components of a railroad system.\nOmniverse is a development computing platform that enables developers to build interoperable 3D workflows and tools based on Universal Scene Description (OpenUSD). OpenUSD is an extensible open-source framework for describing, composing, and collaborating within 3D worlds.\nIntegrating the railOmniverse extension into their digital twins has enabled DB to take advantage of the Trend TrackJoint and other features in their Trax Track Library. This saves countless hours in creating realistic railway simulations.\n\u201cWe believe [railOmniverse] is the best approach so far, as it allows us smooth train movement that can still derail in extreme conditions,\u201d said Jose Minguez, senior software developer at DB Systel GmbH during his GTC 2023 session, Building a Digital Twin of the German Rail Network to Deliver Next-Generation Railway Systems. \u201cIt also gives us the freedom to tweak some parameters to influence speed changes based on variables like different weather conditions or different load weights in the train.\u201d\nVideo 1. Physical simulation in NVIDIA Omniverse, using the railOmniverse extension", "Fine-tuning physical simulations for trains with TrackJoint and NVIDIA PhysX\nNumerous experiments were conducted by Trend, and later by DB, exploring collisions of various kinds, additional forces, positional corrections, and so on. The goal was to confine train movement to the track. However, these approaches consistently compromised the realism of the simulation and often led to stability issues.\nThe introduction of the TrackJoint feature provided a robust solution. TrackJoint is a component that connects the wheel frames of a train to the track system. It restricts the movements of the wheel frames relative to the track while maintaining stability and realism in the simulation.\nAfter numerous experiments, the Trax Library was designed to omit simulating the intricate physical details of each spinning wheel, but instead focus on maintaining a per-bogie granularity. Bogies are physical bodies that run along the track.\nEven the train\u2019s drive can be implemented as a constraint, seamlessly integrating a motor model that encompasses acceleration, braking, and friction into the simulation. This model accommodates virtually all types of propulsion engines, along with their gearboxes and wheel configurations, simulated by a traction force characteristic curve tailored to the specifics of a particular rolling stock.\nIntegration into NVIDIA PhysX was achieved using the custom joint interface within Omniverse. The implementation of the TrackJoint mirrors the approach used for built-in joints, such as rotational or spherical. This involves defining a series of constraints for relative translational and rotational velocities, which then are provided to the PhysX constraint solver for direct use.\nWhen dealing with a wheel frame and its connections to the main body of a rolling stock in Omniverse, users can leverage the utilities provided by the platform. For instance, rotational joints can be employed for bogie hinges, and distance joints for couplings between wagons.\nThe wheel frame itself is represented by an NVIDIA PhysX body equipped with a TrackJoint, similar to any other PhysX joint. This implies that the new elements introduced by railOmniverse can seamlessly integrate with all the standard simulation tools used in Omniverse and PhysX.\nPhysX, with its focus on relative velocities in the solver, enables tracks that are not restricted to the static environment, but also attached to a physical body. This opens up possibilities including turntables or trains running on other rolling stock (such as adapter wagons or a train ferry ship).", "Collaboration and custom schemas with OpenUSD\nThe OpenUSD interchange proved to be an effective medium for collaboration between DB and Trend. To operate a train on a track requires tracks and switches first. DB and Trend agreed on custom USD primitive definitions for these entities, which enabled creating from DB-provided sample data using the functionality of the Trax Library.\nLeveraging the schema mechanism, the Trend development team defined and registered new primitives for tracks, switches, the TrackJoint, and various aspects of the motor model. This facilitated simultaneous development on both ends. For instance, DB provided track geometry data using Omniverse Bezier BasisCurves, while Trend expanded the system with a specialized spline curve, parametrized by arc length, enabling its use in simulations with the TrackJoint.", "Exploring the Trax Library\nLibraries such as the Trend Trax Track Library are crucial to saving time and effort when creating realistic simulations. Using a library that pre-solves the fundamental problems is essential. With the right library, you aren\u2019t starting your simulation project tomorrow; you started years ago when the Trax Library began.\nA technical diagram of Trend Verlag\u2019s USD implementation and Omniverse connection, including track data, the Trax Library, custom USD prims, PhysX, and Isaac Sim.\nFigure 1. The Trend Verlag USD implementation and Omniverse connection, featuring custom USD primitives for various aspects of the Trax Library\nThe library includes numerous features such as the TrackJoint that have been tested in real-world use cases and are ready to use in your own railway simulations. These include the following:\nTrack System Creation Tools: Tools for defining the geometry of the tracks, specifying curves and twists, and managing switches and transitions between different track segments.\nMotor Model: The model enables simulating the behavior of train motors. It establishes a relationship between velocity and the fraction of maximal motor force that can be applied at a specific velocity. This ensures realistic acceleration and deceleration of the train.\nFriction and Brakes: Models for wheel-related friction and brakes. This enables simulating the effects of friction on the train\u2019s movement and apply braking forces to slow down or stop the train.\nBogies and RailRunners: Interfaces for representing bogies. The RailRunner interface allows you to connect multiple bogies together to form a train. It also includes models for hinged bogies and rolling stock configurations.\nSensors and Signals: Mechanisms for detecting events along the track. Sensors can be used to detect the passage of wheelsets or other specific events, while signals convey information to be sent to a train as it moves along a track range.\nCurve Theory: The library incorporates mathematical concepts from Curve Theory, which describes curves in terms of curvature and torsion. This supports accurate representation of curves and velocities while moving along them.\nTrend plans to release a public version of railOmniverse in early 2024. For in-depth information, we maintain a detailed text called the Trax Book, outlining the basic principles used by the Trax Library. Updates on the development of railOmniverse can be found in Chapter 12 of the Trax Book.\nWe are eager to learn about your use cases. If you have questions or suggestions, email horstmann.marc@trendverlag.de. To sign up for the railOmniverse newsletter, include \u2018railNewsletter\u2019 in the subject line.\nJoin NVIDIA for OpenUSD Day at GTC 2024 for a full day of expert-led sessions and panels. Register for GTC 2024 and attend in person or virtually to learn the latest in AI with in-depth sessions, workshops, and training on building OpenUSD-based extensions, apps, and services on Omniverse.\nGet started with NVIDIA Omniverse by downloading the standard license free, access OpenUSD resources, and learn how Omniverse Enterprise can connect your team. Stay up to date on Instagram, Medium and Twitter. For more, join the Omniverse community on the forums, Discord server, Twitch and YouTube channels."], "document_title": "Simulating Railroads with OpenUSD", "document_url": "https://developer.nvidia.com/blog/simulating-railroads-with-openusd/", "document_date": "2024-01-17T21:00:00", "document_date_modified": "2024-01-25T18:17:32", "document_full_text": "Simulating Railroads with OpenUSD\nRailroad simulation is important in modern transportation and logistics, providing a virtual testing ground for the intricate interplay of tracks, switches, and rolling stock. It serves as a crucial tool for engineers and developers to fine-tune and optimize railway systems, ensuring efficiency, safety, and cost-effectiveness.\nPhysically realistic simulations enable comprehensive scenario analysis, predictive maintenance, and the exploration of innovative solutions, ultimately contributing to the advancement and sustainability of rail infrastructure.\nHowever, simulating railroads effectively and accurately is no easy feat. A wide range of interconnected and dynamic components must be modeled precisely. Real-world physics, safety protocols, and the diversity of operational scenarios must also be considered.\nRealistic rail simulations with railOmniverse extension\nWithin the sector initiative Digitale Schiene Deutschland, Deutsche Bahn (DB) aims to use digitalization to improve the capacity, quality, and efficiency of their rail network. One measure is the creation of a physically accurate emulation of the country-wide rail system in Germany.\nrailOmniverse is a C++ extension developed in NVIDIA Omniverse by Trend Verlag (Trend) with Deutsche Bahn to facilitate the simulation of their highly complex railroad systems. Based on the Trend Trax Track Library, the extension provides tools for creating and managing track systems, motor models, brakes, and wheel friction in a physical railroad simulation. railOmniverse uses NVIDIA PhysX to support the realistic simulation of physical interactions, forces, and dynamics between the various components of a railroad system.\nOmniverse is a development computing platform that enables developers to build interoperable 3D workflows and tools based on Universal Scene Description (OpenUSD). OpenUSD is an extensible open-source framework for describing, composing, and collaborating within 3D worlds.\nIntegrating the railOmniverse extension into their digital twins has enabled DB to take advantage of the Trend TrackJoint and other features in their Trax Track Library. This saves countless hours in creating realistic railway simulations.\n\u201cWe believe [railOmniverse] is the best approach so far, as it allows us smooth train movement that can still derail in extreme conditions,\u201d said Jose Minguez, senior software developer at DB Systel GmbH during his GTC 2023 session, Building a Digital Twin of the German Rail Network to Deliver Next-Generation Railway Systems. \u201cIt also gives us the freedom to tweak some parameters to influence speed changes based on variables like different weather conditions or different load weights in the train.\u201d\nVideo 1. Physical simulation in NVIDIA Omniverse, using the railOmniverse extension\nFine-tuning physical simulations for trains with TrackJoint and NVIDIA PhysX\nNumerous experiments were conducted by Trend, and later by DB, exploring collisions of various kinds, additional forces, positional corrections, and so on. The goal was to confine train movement to the track. However, these approaches consistently compromised the realism of the simulation and often led to stability issues.\nThe introduction of the TrackJoint feature provided a robust solution. TrackJoint is a component that connects the wheel frames of a train to the track system. It restricts the movements of the wheel frames relative to the track while maintaining stability and realism in the simulation.\nAfter numerous experiments, the Trax Library was designed to omit simulating the intricate physical details of each spinning wheel, but instead focus on maintaining a per-bogie granularity. Bogies are physical bodies that run along the track.\nEven the train\u2019s drive can be implemented as a constraint, seamlessly integrating a motor model that encompasses acceleration, braking, and friction into the simulation. This model accommodates virtually all types of propulsion engines, along with their gearboxes and wheel configurations, simulated by a traction force characteristic curve tailored to the specifics of a particular rolling stock.\nIntegration into NVIDIA PhysX was achieved using the custom joint interface within Omniverse. The implementation of the TrackJoint mirrors the approach used for built-in joints, such as rotational or spherical. This involves defining a series of constraints for relative translational and rotational velocities, which then are provided to the PhysX constraint solver for direct use.\nWhen dealing with a wheel frame and its connections to the main body of a rolling stock in Omniverse, users can leverage the utilities provided by the platform. For instance, rotational joints can be employed for bogie hinges, and distance joints for couplings between wagons.\nThe wheel frame itself is represented by an NVIDIA PhysX body equipped with a TrackJoint, similar to any other PhysX joint. This implies that the new elements introduced by railOmniverse can seamlessly integrate with all the standard simulation tools used in Omniverse and PhysX.\nPhysX, with its focus on relative velocities in the solver, enables tracks that are not restricted to the static environment, but also attached to a physical body. This opens up possibilities including turntables or trains running on other rolling stock (such as adapter wagons or a train ferry ship).\nCollaboration and custom schemas with OpenUSD\nThe OpenUSD interchange proved to be an effective medium for collaboration between DB and Trend. To operate a train on a track requires tracks and switches first. DB and Trend agreed on custom USD primitive definitions for these entities, which enabled creating from DB-provided sample data using the functionality of the Trax Library.\nLeveraging the schema mechanism, the Trend development team defined and registered new primitives for tracks, switches, the TrackJoint, and various aspects of the motor model. This facilitated simultaneous development on both ends. For instance, DB provided track geometry data using Omniverse Bezier BasisCurves, while Trend expanded the system with a specialized spline curve, parametrized by arc length, enabling its use in simulations with the TrackJoint.\nExploring the Trax Library\nLibraries such as the Trend Trax Track Library are crucial to saving time and effort when creating realistic simulations. Using a library that pre-solves the fundamental problems is essential. With the right library, you aren\u2019t starting your simulation project tomorrow; you started years ago when the Trax Library began.\nA technical diagram of Trend Verlag\u2019s USD implementation and Omniverse connection, including track data, the Trax Library, custom USD prims, PhysX, and Isaac Sim.\nFigure 1. The Trend Verlag USD implementation and Omniverse connection, featuring custom USD primitives for various aspects of the Trax Library\nThe library includes numerous features such as the TrackJoint that have been tested in real-world use cases and are ready to use in your own railway simulations. These include the following:\nTrack System Creation Tools: Tools for defining the geometry of the tracks, specifying curves and twists, and managing switches and transitions between different track segments.\nMotor Model: The model enables simulating the behavior of train motors. It establishes a relationship between velocity and the fraction of maximal motor force that can be applied at a specific velocity. This ensures realistic acceleration and deceleration of the train.\nFriction and Brakes: Models for wheel-related friction and brakes. This enables simulating the effects of friction on the train\u2019s movement and apply braking forces to slow down or stop the train.\nBogies and RailRunners: Interfaces for representing bogies. The RailRunner interface allows you to connect multiple bogies together to form a train. It also includes models for hinged bogies and rolling stock configurations.\nSensors and Signals: Mechanisms for detecting events along the track. Sensors can be used to detect the passage of wheelsets or other specific events, while signals convey information to be sent to a train as it moves along a track range.\nCurve Theory: The library incorporates mathematical concepts from Curve Theory, which describes curves in terms of curvature and torsion. This supports accurate representation of curves and velocities while moving along them.\nTrend plans to release a public version of railOmniverse in early 2024. For in-depth information, we maintain a detailed text called the Trax Book, outlining the basic principles used by the Trax Library. Updates on the development of railOmniverse can be found in Chapter 12 of the Trax Book.\nWe are eager to learn about your use cases. If you have questions or suggestions, email horstmann.marc@trendverlag.de. To sign up for the railOmniverse newsletter, include \u2018railNewsletter\u2019 in the subject line.\nJoin NVIDIA for OpenUSD Day at GTC 2024 for a full day of expert-led sessions and panels. Register for GTC 2024 and attend in person or virtually to learn the latest in AI with in-depth sessions, workshops, and training on building OpenUSD-based extensions, apps, and services on Omniverse.\nGet started with NVIDIA Omniverse by downloading the standard license free, access OpenUSD resources, and learn how Omniverse Enterprise can connect your team. Stay up to date on Instagram, Medium and Twitter. For more, join the Omniverse community on the forums, Discord server, Twitch and YouTube channels."}], "https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/": [{"text": "NVIDIA NeMo has released new pretrained models for Dutch and Persian speech recognition, utilizing the FastConformer architecture and trained with CTC and transducer objectives for accuracy. The Persian model achieved a 13.16% WER and 3.85% CER, with CER being a more realistic metric due to compound word notation. The Dutch model achieved a 9.2% and 12.1% WER on MCV and MLS data, producing transcripts with punctuation and capitalization. These models are available for download with a permissive CC-4.0 BY license for commercial use on NGC and HuggingFace platforms. ASR technology is crucial for conversational AI applications, enabling voice communication with AI systems and enhancing content accessibility through conversational analytics and audio captioning.", "text_components": ["New Support for Dutch and Persian Released by NVIDIA NeMo ASR\nBreaking barriers in speech recognition, NVIDIA NeMo proudly presents pretrained models tailored for Dutch and Persian\u2014languages often overlooked in the AI landscape.\nThese models leverage the recently introduced FastConformer architecture and were trained simultaneously with CTC and transducer objectives to maximize each model\u2019s accuracy.\nAutomatic speech recognition (ASR) is a fundamental technology for conversational AI applications, as it enables users to communicate with AI systems and other devices using voice. It\u2019s also widely adopted in conversational analytics and audio captioning, resulting in broader content accessibility.", "Persian speech recognition model\nThe Persian model was trained on Mozilla\u2019s Common Voice (MCV) 15.0 Persian data. Notably, two techniques helped maximize the model\u2019s performance: initialization from a pretrained English checkpoint and a custom train-test split that allowed the use of an extra 300 hours of MCV-validated recordings.\nThis model achieves a 13.16% word error rate (WER) and 3.85% character error rate (CER) in evaluation. While WER is a standard metric for ASR, it does not necessarily reflect ASR performance in the Persian language well due to flexibility in compound word notation. This means a compound word may not be separated by a whitespace. In these cases, CER may be a more realistic indication of an ASR system\u2019s accuracy.", "Dutch speech recognition model\nThe Dutch model is trained on 40 hours of MCV data, 547 hours of Multilingual LibriSpeech (MLS), and 34 hours of VoxPopuli data.\nThis model achieves a 9.2% and 12.1% word error rate on MCV and MLS in evaluation, which is among the top of the available open-source Dutch models. This model can also produce transcripts with punctuation and capitalization.", "Try the models\nThese models are permissively licensed with a CC-4.0 BY license that enables commercial use. They are available to download at both NGC and HuggingFace:\nNGC: Complete list of ASR models offered by NVIDIA NeMo\nDutch\nPersian\nHuggingFace: Complete list of ASR models offered by NVIDIA NeMo\nDutch\nPersian"], "document_title": "New Support for Dutch and Persian Released by NVIDIA NeMo ASR", "document_url": "https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/", "document_date": "2024-01-16T18:29:16", "document_date_modified": "2024-01-25T18:17:32", "document_full_text": "New Support for Dutch and Persian Released by NVIDIA NeMo ASR\nBreaking barriers in speech recognition, NVIDIA NeMo proudly presents pretrained models tailored for Dutch and Persian\u2014languages often overlooked in the AI landscape.\nThese models leverage the recently introduced FastConformer architecture and were trained simultaneously with CTC and transducer objectives to maximize each model\u2019s accuracy.\nAutomatic speech recognition (ASR) is a fundamental technology for conversational AI applications, as it enables users to communicate with AI systems and other devices using voice. It\u2019s also widely adopted in conversational analytics and audio captioning, resulting in broader content accessibility.\nPersian speech recognition model\nThe Persian model was trained on Mozilla\u2019s Common Voice (MCV) 15.0 Persian data. Notably, two techniques helped maximize the model\u2019s performance: initialization from a pretrained English checkpoint and a custom train-test split that allowed the use of an extra 300 hours of MCV-validated recordings.\nThis model achieves a 13.16% word error rate (WER) and 3.85% character error rate (CER) in evaluation. While WER is a standard metric for ASR, it does not necessarily reflect ASR performance in the Persian language well due to flexibility in compound word notation. This means a compound word may not be separated by a whitespace. In these cases, CER may be a more realistic indication of an ASR system\u2019s accuracy.\nDutch speech recognition model\nThe Dutch model is trained on 40 hours of MCV data, 547 hours of Multilingual LibriSpeech (MLS), and 34 hours of VoxPopuli data.\nThis model achieves a 9.2% and 12.1% word error rate on MCV and MLS in evaluation, which is among the top of the available open-source Dutch models. This model can also produce transcripts with punctuation and capitalization.\nTry the models\nThese models are permissively licensed with a CC-4.0 BY license that enables commercial use. They are available to download at both NGC and HuggingFace:\nNGC: Complete list of ASR models offered by NVIDIA NeMo\nDutch\nPersian\nHuggingFace: Complete list of ASR models offered by NVIDIA NeMo\nDutch\nPersian"}], "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/": [{"text": "The article discusses the importance of optimizing deep learning models for inference to improve performance and efficiency. Techniques like inference computation graph simplification, quantization, and lowering precision are used to achieve this. The article showcases benchmarking results of scene text detection and recognition models using ONNX Runtime and NVIDIA TensorRT with NVIDIA Triton Inference Server. The scene text detection component uses the CRAFT model for locating text fields in images, while the scene text recognition component utilizes the PARseq algorithm for accurate text recognition. Conversion of models to ONNX and TensorRT formats is detailed, along with benchmarking results showing speed-ups compared to TorchScript. The orchestrator module coordinates between detection and recognition models, with benchmarking results showing improved throughput using ONNX-FP32 and TensorRT-FP32. The article emphasizes the importance of optimizing models for inference and highlights the benefits of using NVIDIA AI Enterprise for building efficient scene-text-OCR systems with low latency and high performance.", "text_components": ["Robust Scene Text Detection and Recognition: Inference Optimization\nIn this post, we delve deeper into the inference optimization process to improve the performance and efficiency of our machine learning models during the inference stage. We discuss the techniques employed, such as inference computation graph simplification, quantization, and lowering precision. We also showcase the benchmarking results of our scene text detection and recognition models, comparing the performance of the ONNX Runtime and NVIDIA TensorRT using NVIDIA Triton Inference Server.\nFinally, we summarize the importance of optimizing deep learning models for inference and the benefits of using an end-to-end NVIDIA software solution, NVIDIA AI Enterprise, for building efficient and robust scene-text-OCR systems.\nThe first post in this series, Robust Scene Text Detection and Recognition: Introduction, discussed the importance of robust scene text detection and recognition (STDR) in various industries and the challenges. The second post, Robust Scene Text Detection and Recognition: Implementation, discussed the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning.", "Inference optimization\nInference optimization is done to improve the performance and efficiency of machine learning models during the inference stage. It helps in reducing the time, computational resources, and cost required for making predictions, and can also improve accuracy in some cases.\nWe have used techniques like inference computation graph simplification, quantization, and lowering precision for inference optimization. These models were originally trained using the PyTorch library, exported in torchScript format, converted to the ONNX format, and then transformed into an NVIDIA TensorRT engine.\nTo carry out the ONNX to TensorRT conversion, we used the NGC container image for TensorRT, version 22.07. Following the conversion process, we deployed the model for inference using the NVIDIA Triton Inference Server, version 22.07. System performance was benchmarked on an NVIDIA A5000 laptop GPU with 16 GB of GPU memory.\nThe diagram shows model optimization using ONNX and TensorRT Optimizer and then model inference using NVIDIA TensorRT runtime in a deployment environment.\nFigure 1. Inference optimization flow\nWe discuss the details of the optimization of each building block of scene text detection and recognition (STDR) later in this post.", "Scene text detection\nScene text detection is a crucial component of our scene-text-OCR system. This component takes an image of a scene as input and outputs the locations of text fields within the image. In this article, we are using the pretrained CRAFT model for general scene text detection tasks. This model, which is trained on a diverse set of images, is capable of handling dynamic input images and accurately locating text fields. The average width of the images used as input in our deployments is around 720 points. Here, we have benchmarked two image input sizes: (3,720,720) and (3,1200,1200).\nOur benchmark shows around 2.3x speed-up with TensorRT compared to TorchScript for inference.\nA graph comparing the performance of three modes of text detection model inference (on an NVIDIA A5000 mobile GPU) using Triton Server: PyTorchScript, ONNX with CUDA and TensorRT, tested on two image sizes (3,700,700 and 3,1200,1200).\nFigure 2. Triton Inference Server benchmark comparison of scene text detection CRAFT model on image sizes\nThe deployed CRAFT model is a TensorRT engine with FP32 precision. The following code examples are a quick guide for conversion.\nCreate a conda environment:\n```\n$ conda create \u2013n <your_env_name> python=3.8\n$ conda activate <your_env_name>\n```\nClone the CRAFT repo and install requirement.txt:\n```\n$ git clone https://github.com/clovaai/CRAFT-pytorch.git\n$ cd CRAFT-pytorch\n$ pip install \u2013r requirement.txt\n```\nLoad the model and convert it to an .onnx format that takes dynamic shapes:\n```\ninput_tensor_detec = torch.randn((1, 3, 768, 768), requires_grad=False)\ninput_tensor_detec=input_tensor_detec.to(device=\"cuda\u201d)\n\n# Load net\nnet = CRAFT()\nnet.load_state_dict(copyStateDict(torch.load(model_path)))\nnet = net.cuda()\nnet.eval()\n\n# Convert the model into ONNX\ntorch.onnx.export(net, input_tensor_detec, output_dir,\n              verbose=False, opset_version=11,\n              do_constant_folding= True,\n            export_params=True,\n              input_names=[\"input\"],\n              output_names=[\"output\", \"output1\"], dynamic_axes={\"input\": {0: \"batch\", 2: \"height\", 3: \"width\"}})\n```\nSimplify the ONNX graph. Use ONNX Simplifier to simplify the ONNX model. It infers the whole computation graph and then replaces the redundant operators with their constant outputs (also known as constant folding). The following code example shows the operation folding report for a graph simplification of a CRAFT model:\n```\n$ onnxsim <path to non_simplified onnx model> <path to simplified onnx model>\n```\nThe screenshot shows simplification in the Cast (from 3 to 0), Concat (from 10 to 7), and Constant (from 21 to 0) values.\nFigure 3. onnxsim report for CRAFT model\nFor this post, use NVIDIA TensorRT pre-configured Docker containers to convert the ONNX model to a TensorRT serialized plan file. The following code example works with the tensorrt:22.07-py3 NGC container:\n```\n~$ docker run -it --gpus all -v <path to onnx model>:/models \\\nnvcr.io/nvidia/tensorrt:22.07-py3\nroot@576df0ec3a49:/workspace#$ trtexec --onnx=/models/craft.onnx \\\n--explicitBatch --workspace=5000 --minShapes=input:1x3x256x256 \\\n--optShapes=input:1x3x700x700 --maxShapes=input:1x3x1200x1200 \\\n--buildOnly \u2013saveEngine=/models/craft.engine\n```\nThe following code example shows the config.pbtxt file for the scene text detection model:\n```\nname: \"craft\"\ndefault_model_filename: \"detec_trt.engine\"\nplatform: \"tensorrt_plan\"\nmax_batch_size : 1\ninput [\n  {\n\tname: \"input\"\n\tdata_type: TYPE_FP32\n\tdims: [ 3, -1, -1 ]\n  }\n]\noutput [\n  {\n\tname: \"output\"\n\tdata_type: TYPE_FP32\n\tdims: [ -1, -1, 2 ]\n  },\n  {\n\tname: \"output1\"\n\tdata_type: TYPE_FP32\n\tdims: [ 32, -1, -1 ]\n  }\n]\n```", "Scene text recognition\nScene text recognition is an integral module of the STDR pipeline. We used the PARseq algorithm, a state-of-the-art technique for efficient and customizable text recognition to achieve accurate results.\nTo maximize the performance of our pipeline, we converted the PARseq TorchScript model to ONNX and then further converted it to a TensorRT engine, ensuring low latency in text recognition, as each image may contain multiple text fields.\nWe found that using an input size of 3x32x128 for the model proved to be the optimal balance between inference time and accuracy. Figure 4 shows the benchmarking results for the PARseq model. We benchmarked around 3x acceleration compared to TorchScript inference.\nA graph comparing the performance of three modes of text recognition model inference (on NVIDIA A5000 mobile GPU) using Triton Server: PyTorchScript, ONNX with CUDA and TensorRT, tested on a fixed image size of (3,32,128).\nFigure 4. Triton Inference Server benchmark comparison of scene text recognition PARseq model\nThe pretrained models published by the authors work well with most of the cases. You can also fine-tune the model if you want to get more accurate output on a custom dataset. The following code examples show the important steps for conversion.\nInstall PARSeq:\n```\n$ git clone https://github.com/baudm/parseq.git\n$ pip install -r requirements.txt\n$ pip install -e .\n```\nYou can use your own fine-tuned model or pretrained model from the model repository and convert it into .onnx format. Use an ONNX version later than 1.12.0.\n```\nfrom strhub.models.utils import load_from_checkpoint\n\n# To ONNX\ndevice = \"cuda\"\nckpt_path = \"...\"\nonnx_path = \"...\"\nimg = ...\n\nparseq = load_from_checkpoint(ckpt_path)\nparseq.refine_iters = 0\nparseq.decode_ar = False\nparseq = parseq.to(device).eval()\n\nparseq.to_onnx(onnx_path, img, do_constant_folding=True, opset_version=14)  # opset v14 or newer is required\n\n# check\nonnx_model = onnx.load(onnx_path)\nonnx.checker.check_model(onnx_model, full_check=True) ==> pass\n```\nTo convert to TensorRT format, simplify the ONNX model using onnx-simplifier:\n```\n$ onnxsim <path to non_simplified onnx model> <path to simplified onnx model>\n```\nA screenshot of model simplification changed values. Fifteen values were lowered as a result of simplification.\nFigure 5. onnxsim report for the PARSeq model\nAfter converting the model to a simplified ONNX format, use the trtexec tool for the conversion. This conversion is done inside the TensorRT container version 22.07.\n```\n~$ docker run -it --gpus all -v <path to onnx model repository>:/models nvcr.io/nvidia/tensorrt:22.07-py3\nroot@576df0ec3a49:/workspace# trtexec --onnx=/models/parseq_simple.onnx --fp16 \\\n--workspace=1024 --saveEngine=/models/parseq_fp16.trt --minShapes=input:1x3x32x128 \\\n--optShapes=input:4x3x32x128 --maxShapes=input:16x3x32x128\n```\nThe following code example shows the ```config.pbtxt``` file for the scene text recognition model:\n```\nname: \"parseq\"\nmax_batch_size: 16\nplatform: \"tensorrt_plan\"\ndefault_model_filename: \"parseq_exp_fp32.trt\"\n \ninput {\n\tname: \"input\"\n\tdata_type: TYPE_FP32\n\tdims: [3, 32, 128]\n}\n \noutput {\n\tname: \"output\"\n\tdata_type: TYPE_FP32\n\tdims: [26, 95]\n}\n \ninstance_group [\n\t{\n  \tcount: 1\n  \tkind: KIND_GPU\n\t}\n]\n```", "Orchestrator\nThe orchestrator module is a Python backend that maintains flow and performs pre-processing for the STDR pipeline. To do the pipeline benchmarking, we used four different images with different image sizes to create custom inputs for ```perf_analyzer```.\nWe created two versions of the pipeline, one pipeline using the ONNX Runtime CPU/ GPU backend and another using TensorRT plans, so that the pipeline can work in both GPU and non-GPU environments. We benchmarked the ```onnx_backend``` pipeline and ```tensorrt_plan``` pipeline on an NVIDIA RTX A5000 laptop GPU (16 GB) using NVIDIA Triton Inference Server.\nThe input sample for the benchmark has four different images with sizes (3x472x338), (3x3280x2625), (3x512x413), and (3x1600x1200).\nBar chart shows 1x throughput for ONNX-FP32 and 1.5x throughput for TensorRT-FP32.\nFigure 6. Triton Inference Server benchmark comparison of scene text recognition and detection on ONNX runtime and TensorRT plan\nThe orchestrator is a Python backend module that coordinates between the scene text detection and scene text recognition models. The configuration file for the orchestrator is as follows:\n```\nname: \"pipeline\"\nbackend: \"python\"\nmax_batch_size: 1\ninput [\n  {\n\tname: \"input\"\n\tdata_type: TYPE_UINT8\n\tdims: [ -1, -1, 3 ]\n  }\n]\noutput [\n  {\n\tname: \"output\"\n\tdata_type: TYPE_STRING\n\tdims: [ -1 ]\n  }\n]\n \ninstance_group [\n\t{\n  \tcount: 1\n  \tkind: KIND_GPU\n\t}\n]\n```", "Summary\nIn summary, the deployment of scene text detection and recognition systems requires careful consideration of real-world scenarios, and optimizing deep learning models for inference is crucial.\nTo ensure production-ready optimization and performance, NVIDIA offers an end-to-end software solution, NVIDIA AI Enterprise, that consists of best-in-class AI software and tools including TensorRT and Triton Inference Server for easy access to build enterprise AI applications. The solution is instrumental in achieving low latency and high-performance inference across various devices.\nBy using these technologies, you can build efficient and robust scene-text-OCR systems for a range of applications."], "document_title": "Robust Scene Text Detection and Recognition: Inference Optimization", "document_url": "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/", "document_date": "2024-01-16T17:02:00", "document_date_modified": "2024-01-25T18:17:33", "document_full_text": "Robust Scene Text Detection and Recognition: Inference Optimization\nIn this post, we delve deeper into the inference optimization process to improve the performance and efficiency of our machine learning models during the inference stage. We discuss the techniques employed, such as inference computation graph simplification, quantization, and lowering precision. We also showcase the benchmarking results of our scene text detection and recognition models, comparing the performance of the ONNX Runtime and NVIDIA TensorRT using NVIDIA Triton Inference Server.\nFinally, we summarize the importance of optimizing deep learning models for inference and the benefits of using an end-to-end NVIDIA software solution, NVIDIA AI Enterprise, for building efficient and robust scene-text-OCR systems.\nThe first post in this series, Robust Scene Text Detection and Recognition: Introduction, discussed the importance of robust scene text detection and recognition (STDR) in various industries and the challenges. The second post, Robust Scene Text Detection and Recognition: Implementation, discussed the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning.\nInference optimization\nInference optimization is done to improve the performance and efficiency of machine learning models during the inference stage. It helps in reducing the time, computational resources, and cost required for making predictions, and can also improve accuracy in some cases.\nWe have used techniques like inference computation graph simplification, quantization, and lowering precision for inference optimization. These models were originally trained using the PyTorch library, exported in torchScript format, converted to the ONNX format, and then transformed into an NVIDIA TensorRT engine.\nTo carry out the ONNX to TensorRT conversion, we used the NGC container image for TensorRT, version 22.07. Following the conversion process, we deployed the model for inference using the NVIDIA Triton Inference Server, version 22.07. System performance was benchmarked on an NVIDIA A5000 laptop GPU with 16 GB of GPU memory.\nThe diagram shows model optimization using ONNX and TensorRT Optimizer and then model inference using NVIDIA TensorRT runtime in a deployment environment.\nFigure 1. Inference optimization flow\nWe discuss the details of the optimization of each building block of scene text detection and recognition (STDR) later in this post.\nScene text detection\nScene text detection is a crucial component of our scene-text-OCR system. This component takes an image of a scene as input and outputs the locations of text fields within the image. In this article, we are using the pretrained CRAFT model for general scene text detection tasks. This model, which is trained on a diverse set of images, is capable of handling dynamic input images and accurately locating text fields. The average width of the images used as input in our deployments is around 720 points. Here, we have benchmarked two image input sizes: (3,720,720) and (3,1200,1200).\nOur benchmark shows around 2.3x speed-up with TensorRT compared to TorchScript for inference.\nA graph comparing the performance of three modes of text detection model inference (on an NVIDIA A5000 mobile GPU) using Triton Server: PyTorchScript, ONNX with CUDA and TensorRT, tested on two image sizes (3,700,700 and 3,1200,1200).\nFigure 2. Triton Inference Server benchmark comparison of scene text detection CRAFT model on image sizes\nThe deployed CRAFT model is a TensorRT engine with FP32 precision. The following code examples are a quick guide for conversion.\nCreate a conda environment:\n```\n$ conda create \u2013n <your_env_name> python=3.8\n$ conda activate <your_env_name>\n```\nClone the CRAFT repo and install requirement.txt:\n```\n$ git clone https://github.com/clovaai/CRAFT-pytorch.git\n$ cd CRAFT-pytorch\n$ pip install \u2013r requirement.txt\n```\nLoad the model and convert it to an .onnx format that takes dynamic shapes:\n```\ninput_tensor_detec = torch.randn((1, 3, 768, 768), requires_grad=False)\ninput_tensor_detec=input_tensor_detec.to(device=\"cuda\u201d)\n\n# Load net\nnet = CRAFT()\nnet.load_state_dict(copyStateDict(torch.load(model_path)))\nnet = net.cuda()\nnet.eval()\n\n# Convert the model into ONNX\ntorch.onnx.export(net, input_tensor_detec, output_dir,\n              verbose=False, opset_version=11,\n              do_constant_folding= True,\n            export_params=True,\n              input_names=[\"input\"],\n              output_names=[\"output\", \"output1\"], dynamic_axes={\"input\": {0: \"batch\", 2: \"height\", 3: \"width\"}})\n```\nSimplify the ONNX graph. Use ONNX Simplifier to simplify the ONNX model. It infers the whole computation graph and then replaces the redundant operators with their constant outputs (also known as constant folding). The following code example shows the operation folding report for a graph simplification of a CRAFT model:\n```\n$ onnxsim <path to non_simplified onnx model> <path to simplified onnx model>\n```\nThe screenshot shows simplification in the Cast (from 3 to 0), Concat (from 10 to 7), and Constant (from 21 to 0) values.\nFigure 3. onnxsim report for CRAFT model\nFor this post, use NVIDIA TensorRT pre-configured Docker containers to convert the ONNX model to a TensorRT serialized plan file. The following code example works with the tensorrt:22.07-py3 NGC container:\n```\n~$ docker run -it --gpus all -v <path to onnx model>:/models \\\nnvcr.io/nvidia/tensorrt:22.07-py3\nroot@576df0ec3a49:/workspace#$ trtexec --onnx=/models/craft.onnx \\\n--explicitBatch --workspace=5000 --minShapes=input:1x3x256x256 \\\n--optShapes=input:1x3x700x700 --maxShapes=input:1x3x1200x1200 \\\n--buildOnly \u2013saveEngine=/models/craft.engine\n```\nThe following code example shows the config.pbtxt file for the scene text detection model:\n```\nname: \"craft\"\ndefault_model_filename: \"detec_trt.engine\"\nplatform: \"tensorrt_plan\"\nmax_batch_size : 1\ninput [\n  {\n\tname: \"input\"\n\tdata_type: TYPE_FP32\n\tdims: [ 3, -1, -1 ]\n  }\n]\noutput [\n  {\n\tname: \"output\"\n\tdata_type: TYPE_FP32\n\tdims: [ -1, -1, 2 ]\n  },\n  {\n\tname: \"output1\"\n\tdata_type: TYPE_FP32\n\tdims: [ 32, -1, -1 ]\n  }\n]\n```\nScene text recognition\nScene text recognition is an integral module of the STDR pipeline. We used the PARseq algorithm, a state-of-the-art technique for efficient and customizable text recognition to achieve accurate results.\nTo maximize the performance of our pipeline, we converted the PARseq TorchScript model to ONNX and then further converted it to a TensorRT engine, ensuring low latency in text recognition, as each image may contain multiple text fields.\nWe found that using an input size of 3x32x128 for the model proved to be the optimal balance between inference time and accuracy. Figure 4 shows the benchmarking results for the PARseq model. We benchmarked around 3x acceleration compared to TorchScript inference.\nA graph comparing the performance of three modes of text recognition model inference (on NVIDIA A5000 mobile GPU) using Triton Server: PyTorchScript, ONNX with CUDA and TensorRT, tested on a fixed image size of (3,32,128).\nFigure 4. Triton Inference Server benchmark comparison of scene text recognition PARseq model\nThe pretrained models published by the authors work well with most of the cases. You can also fine-tune the model if you want to get more accurate output on a custom dataset. The following code examples show the important steps for conversion.\nInstall PARSeq:\n```\n$ git clone https://github.com/baudm/parseq.git\n$ pip install -r requirements.txt\n$ pip install -e .\n```\nYou can use your own fine-tuned model or pretrained model from the model repository and convert it into .onnx format. Use an ONNX version later than 1.12.0.\n```\nfrom strhub.models.utils import load_from_checkpoint\n\n# To ONNX\ndevice = \"cuda\"\nckpt_path = \"...\"\nonnx_path = \"...\"\nimg = ...\n\nparseq = load_from_checkpoint(ckpt_path)\nparseq.refine_iters = 0\nparseq.decode_ar = False\nparseq = parseq.to(device).eval()\n\nparseq.to_onnx(onnx_path, img, do_constant_folding=True, opset_version=14)  # opset v14 or newer is required\n\n# check\nonnx_model = onnx.load(onnx_path)\nonnx.checker.check_model(onnx_model, full_check=True) ==> pass\n```\nTo convert to TensorRT format, simplify the ONNX model using onnx-simplifier:\n```\n$ onnxsim <path to non_simplified onnx model> <path to simplified onnx model>\n```\nA screenshot of model simplification changed values. Fifteen values were lowered as a result of simplification.\nFigure 5. onnxsim report for the PARSeq model\nAfter converting the model to a simplified ONNX format, use the trtexec tool for the conversion. This conversion is done inside the TensorRT container version 22.07.\n```\n~$ docker run -it --gpus all -v <path to onnx model repository>:/models nvcr.io/nvidia/tensorrt:22.07-py3\nroot@576df0ec3a49:/workspace# trtexec --onnx=/models/parseq_simple.onnx --fp16 \\\n--workspace=1024 --saveEngine=/models/parseq_fp16.trt --minShapes=input:1x3x32x128 \\\n--optShapes=input:4x3x32x128 --maxShapes=input:16x3x32x128\n```\nThe following code example shows the ```config.pbtxt``` file for the scene text recognition model:\n```\nname: \"parseq\"\nmax_batch_size: 16\nplatform: \"tensorrt_plan\"\ndefault_model_filename: \"parseq_exp_fp32.trt\"\n \ninput {\n\tname: \"input\"\n\tdata_type: TYPE_FP32\n\tdims: [3, 32, 128]\n}\n \noutput {\n\tname: \"output\"\n\tdata_type: TYPE_FP32\n\tdims: [26, 95]\n}\n \ninstance_group [\n\t{\n  \tcount: 1\n  \tkind: KIND_GPU\n\t}\n]\n```\nOrchestrator\nThe orchestrator module is a Python backend that maintains flow and performs pre-processing for the STDR pipeline. To do the pipeline benchmarking, we used four different images with different image sizes to create custom inputs for ```perf_analyzer```.\nWe created two versions of the pipeline, one pipeline using the ONNX Runtime CPU/ GPU backend and another using TensorRT plans, so that the pipeline can work in both GPU and non-GPU environments. We benchmarked the ```onnx_backend``` pipeline and ```tensorrt_plan``` pipeline on an NVIDIA RTX A5000 laptop GPU (16 GB) using NVIDIA Triton Inference Server.\nThe input sample for the benchmark has four different images with sizes (3x472x338), (3x3280x2625), (3x512x413), and (3x1600x1200).\nBar chart shows 1x throughput for ONNX-FP32 and 1.5x throughput for TensorRT-FP32.\nFigure 6. Triton Inference Server benchmark comparison of scene text recognition and detection on ONNX runtime and TensorRT plan\nThe orchestrator is a Python backend module that coordinates between the scene text detection and scene text recognition models. The configuration file for the orchestrator is as follows:\n```\nname: \"pipeline\"\nbackend: \"python\"\nmax_batch_size: 1\ninput [\n  {\n\tname: \"input\"\n\tdata_type: TYPE_UINT8\n\tdims: [ -1, -1, 3 ]\n  }\n]\noutput [\n  {\n\tname: \"output\"\n\tdata_type: TYPE_STRING\n\tdims: [ -1 ]\n  }\n]\n \ninstance_group [\n\t{\n  \tcount: 1\n  \tkind: KIND_GPU\n\t}\n]\n```\nSummary\nIn summary, the deployment of scene text detection and recognition systems requires careful consideration of real-world scenarios, and optimizing deep learning models for inference is crucial.\nTo ensure production-ready optimization and performance, NVIDIA offers an end-to-end software solution, NVIDIA AI Enterprise, that consists of best-in-class AI software and tools including TensorRT and Triton Inference Server for easy access to build enterprise AI applications. The solution is instrumental in achieving low latency and high-performance inference across various devices.\nBy using these technologies, you can build efficient and robust scene-text-OCR systems for a range of applications."}], "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/": [{"text": "The article discusses the implementation of a robust scene text detection and recognition (STDR) pipeline using deep learning algorithms like CRAFT for text detection and PARSeq for text recognition. It emphasizes the importance of having full control of the model for incremental learning and fine-tuning to cater to specific use cases and datasets. The pipeline is crucial for scene understanding, AI-based inspection, and document processing platforms, requiring accuracy and low latency. Tools like NVIDIA TensorRT, ONNX Runtime, and Triton Inference Server are used for model optimization and high-performance inference serving. The orchestrator module coordinates between text detection and recognition, processing input images, cropping text fields, and generating response JSON. The article also mentions the use of pretrained models trained on various datasets and incremental learning techniques for fine-tuning. Overall, the implementation focuses on achieving accurate text detection and recognition for diverse industries and challenges.", "text_components": ["Robust Scene Text Detection and Recognition: Implementation\nTo make scene text detection and recognition work on irregular text or for specific use cases, you must have full control of your model so that you can do incremental learning or fine-tuning as per your use cases and datasets. Keep in mind that this pipeline is the main building block of scene understanding, AI-based inspection, and document processing platforms. It should be accurate and have low latency.\nThe first post in this series, Robust Scene Text Detection and Recognition: Introduction, discussed the importance of robust scene text detection and recognition (STDR) in various industries and the challenges. The third post, Robust Scene Text Detection and Recognition: Inference Optimization, covers production-ready optimization and performance for your STDR pipeline.\nFor this post, we decided to use state-of-the-art, highly accurate, deep-learning models. To ensure accuracy and keep low end-to-end latency, we performed model inference optimization using tools and frameworks like NVIDIA TensorRT and ONNX Runtime. To ensure standard model deployment and execution, in addition to high-performing inference with scalability, we decided to use NVIDIA Triton Inference Server.\nTo train our models, we used Docker container images from the NGC catalog, a hub for GPU-optimized AI and ML software. NGC containers leverage the power of NVIDIA GPUs and can run in virtual machines (VMs) configured with NVIDIA virtual GPU (vGPU) software in NVIDIA vGPU and GPU pass-through deployments. These containers are pre-configured with optimized libraries for SDKs like PyTorch and TensorRT.\nTo enable high-performance inference across the cloud, on-premises, and at the edge, we also made use of the Triton Inference Server Docker container. This container enables multiple models from different frameworks to be executed simultaneously on a single GPU or CPU. On a multi-GPU server, Triton Inference Server automatically creates an instance of each model on each GPU to maximize utilization.\nThere are three building blocks of the STDR pipeline:\nScene text detection\nScene text recognition\nOrchestration\nA diagram of an end-to-end OCR pipeline using Triton Inference Server.\nFigure 1. Architecture of a scene text detection pipeline\nOCR pipeline flow diagram showing text recognition on the product label.\nFigure 2. Scene text detection flow of text detection and recognition", "Scene text detection\nIn the current pipeline, there are the following options for text detection algorithms:\nFCENet\nCRAFT\nTextFuseNet\nYou can train and fine-tune FCENet and TextFuseNet for particular use cases. However, CRAFT cannot be trained or fine-tuned as Clova AI has not published the training code for IP reasons. Our general-purpose pipeline uses the pretrained CRAFT model, which is trained on synthText, IC13, and IC17. For more information, see Character Region Awareness for Text Detection.\nDiagram shows the region score, affinity score, UpConv blocks and Conv stages.\nFigure 3. Schematic illustration of CRAFT architecture\nThis network uses a fully convolutional network architecture based on the VGG-16 model, which encodes the input into a distinct feature representation. The decoding segment of CRAFT is similar to that of UNet and includes skip connections that aggregate low-level features.\nCRAFT predicts two separate scores for each character:\nRegion score: Provides information regarding the area of the character.\nAffinity score: Reveals the degree to which characters are combined into a single entity.\nEach column shows the input image and the region score and affinity maps.\nFigure 4. Two scene text detection photos with region score and affinity maps (Image: Character Region Awareness for Text Detection )", "Scene text recognition\nIn this post, we use the state-of-the-art Scene Text Recognition with the Permuted Autoregressive Sequence (PARseq) algorithm. For more information, see Scene Text Recognition with Permuted Autoregressive Sequence Models.\nDiagram shows the position queries, visio-lingual decoder, input context, permutations, input image, output logits, and ground-truth label.\nFigure 5. PARSeq architecture and training overview\nPublished, pretrained models are trained on several datasets like MJSynth and SynthText, COCO-Text, RCTW17, Uber-Text, ArT, LSVT, and MLT19. We also used incremental learning techniques to fine-tune pretrained models on custom datasets.", "Orchestrator\nThe orchestrator module is the control unit of the pipeline. This module is responsible for coordinating between scene text detection and scene text recognition.\nIt receives an input image from a request.\nThat image is pre-processed and sent to the scene text detection module.\nThe detection module returns the locations of the text fields present in the input image.\nThe Orchestrator crops out the text fields from the input image into a list of ```ndarrays```.\nIt creates batches from the cropped text images of a predefined batch size and sends one batch at a time to the text recognition module.\nThe recognition module returns STR output with a confidence score for each of the cropped text images within that batch.\nThe orchestrator maintains a track of each text field location, corresponding STR output, and confidence score. Using all this information, it creates a response JSON.\nDiagram shows the scene image input for scene text detection on a TensorRT backend, the orchestrator on a Python backend, and scene text recognition on a TensorRT backend, all leading to the output JSON.\nFigure 6. Orchestrator flow", "Summary\nIn this post, we discussed the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning. We used the CRAFT algorithm for text detection and the PARSeq algorithm for text recognition. We designed a distinct orchestration module to facilitate coordination between text detection and recognition. This post also highlighted the use of NVIDIA TensorRT, ONNX Runtime, and NVIDIA Triton Inference Server for model optimization and high-performance inference serving.\nFor more information, see the Robust Scene Text Detection and Recognition: Introduction and Robust Scene Text Detection and Recognition: Inference Optimization posts in this series."], "document_title": "Robust Scene Text Detection and Recognition: Implementation", "document_url": "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/", "document_date": "2024-01-16T17:01:00", "document_date_modified": "2024-01-25T18:17:33", "document_full_text": "Robust Scene Text Detection and Recognition: Implementation\nTo make scene text detection and recognition work on irregular text or for specific use cases, you must have full control of your model so that you can do incremental learning or fine-tuning as per your use cases and datasets. Keep in mind that this pipeline is the main building block of scene understanding, AI-based inspection, and document processing platforms. It should be accurate and have low latency.\nThe first post in this series, Robust Scene Text Detection and Recognition: Introduction, discussed the importance of robust scene text detection and recognition (STDR) in various industries and the challenges. The third post, Robust Scene Text Detection and Recognition: Inference Optimization, covers production-ready optimization and performance for your STDR pipeline.\nFor this post, we decided to use state-of-the-art, highly accurate, deep-learning models. To ensure accuracy and keep low end-to-end latency, we performed model inference optimization using tools and frameworks like NVIDIA TensorRT and ONNX Runtime. To ensure standard model deployment and execution, in addition to high-performing inference with scalability, we decided to use NVIDIA Triton Inference Server.\nTo train our models, we used Docker container images from the NGC catalog, a hub for GPU-optimized AI and ML software. NGC containers leverage the power of NVIDIA GPUs and can run in virtual machines (VMs) configured with NVIDIA virtual GPU (vGPU) software in NVIDIA vGPU and GPU pass-through deployments. These containers are pre-configured with optimized libraries for SDKs like PyTorch and TensorRT.\nTo enable high-performance inference across the cloud, on-premises, and at the edge, we also made use of the Triton Inference Server Docker container. This container enables multiple models from different frameworks to be executed simultaneously on a single GPU or CPU. On a multi-GPU server, Triton Inference Server automatically creates an instance of each model on each GPU to maximize utilization.\nThere are three building blocks of the STDR pipeline:\nScene text detection\nScene text recognition\nOrchestration\nA diagram of an end-to-end OCR pipeline using Triton Inference Server.\nFigure 1. Architecture of a scene text detection pipeline\nOCR pipeline flow diagram showing text recognition on the product label.\nFigure 2. Scene text detection flow of text detection and recognition\nScene text detection\nIn the current pipeline, there are the following options for text detection algorithms:\nFCENet\nCRAFT\nTextFuseNet\nYou can train and fine-tune FCENet and TextFuseNet for particular use cases. However, CRAFT cannot be trained or fine-tuned as Clova AI has not published the training code for IP reasons. Our general-purpose pipeline uses the pretrained CRAFT model, which is trained on synthText, IC13, and IC17. For more information, see Character Region Awareness for Text Detection.\nDiagram shows the region score, affinity score, UpConv blocks and Conv stages.\nFigure 3. Schematic illustration of CRAFT architecture\nThis network uses a fully convolutional network architecture based on the VGG-16 model, which encodes the input into a distinct feature representation. The decoding segment of CRAFT is similar to that of UNet and includes skip connections that aggregate low-level features.\nCRAFT predicts two separate scores for each character:\nRegion score: Provides information regarding the area of the character.\nAffinity score: Reveals the degree to which characters are combined into a single entity.\nEach column shows the input image and the region score and affinity maps.\nFigure 4. Two scene text detection photos with region score and affinity maps (Image: Character Region Awareness for Text Detection )\nScene text recognition\nIn this post, we use the state-of-the-art Scene Text Recognition with the Permuted Autoregressive Sequence (PARseq) algorithm. For more information, see Scene Text Recognition with Permuted Autoregressive Sequence Models.\nDiagram shows the position queries, visio-lingual decoder, input context, permutations, input image, output logits, and ground-truth label.\nFigure 5. PARSeq architecture and training overview\nPublished, pretrained models are trained on several datasets like MJSynth and SynthText, COCO-Text, RCTW17, Uber-Text, ArT, LSVT, and MLT19. We also used incremental learning techniques to fine-tune pretrained models on custom datasets.\nOrchestrator\nThe orchestrator module is the control unit of the pipeline. This module is responsible for coordinating between scene text detection and scene text recognition.\nIt receives an input image from a request.\nThat image is pre-processed and sent to the scene text detection module.\nThe detection module returns the locations of the text fields present in the input image.\nThe Orchestrator crops out the text fields from the input image into a list of ```ndarrays```.\nIt creates batches from the cropped text images of a predefined batch size and sends one batch at a time to the text recognition module.\nThe recognition module returns STR output with a confidence score for each of the cropped text images within that batch.\nThe orchestrator maintains a track of each text field location, corresponding STR output, and confidence score. Using all this information, it creates a response JSON.\nDiagram shows the scene image input for scene text detection on a TensorRT backend, the orchestrator on a Python backend, and scene text recognition on a TensorRT backend, all leading to the output JSON.\nFigure 6. Orchestrator flow\nSummary\nIn this post, we discussed the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning. We used the CRAFT algorithm for text detection and the PARSeq algorithm for text recognition. We designed a distinct orchestration module to facilitate coordination between text detection and recognition. This post also highlighted the use of NVIDIA TensorRT, ONNX Runtime, and NVIDIA Triton Inference Server for model optimization and high-performance inference serving.\nFor more information, see the Robust Scene Text Detection and Recognition: Introduction and Robust Scene Text Detection and Recognition: Inference Optimization posts in this series."}], "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/": [{"text": "The article discusses the importance of robust scene text detection and recognition (STDR) for various industries, such as healthcare, manufacturing, banking, and automotive. It highlights the challenges faced in STDR, including creating accurate models that can handle variations in text size, orientation, color, background, and language. Meeting performance expectations is another challenge, as deploying complex deep learning algorithms can be computationally expensive and lead to latency issues. Additionally, ensuring data privacy and security is crucial when deploying STDR solutions in real-world scenarios. The article emphasizes the need for thorough testing, evaluation, and fine-tuning to optimize performance and strike a balance between accuracy, latency, and resources. The implementation of state-of-the-art deep learning algorithms and techniques, such as incremental learning and fine-tuning, can help address these challenges. Specialized optimization tools like ONNX Runtime, NVIDIA TensorRT SDK, and Triton Inference Server can also be used to reduce latency and improve performance. The article concludes by highlighting the importance of considering specific requirements and constraints of the deployment scenario to achieve optimal performance in STDR applications.", "text_components": ["Robust Scene Text Detection and Recognition: Introduction\nIdentification and recognition of text from natural scenes and images become important for use cases like video caption text recognition, detecting signboards from vehicle-mounted cameras, information retrieval, scene understanding, vehicle number plate recognition, and recognizing text on products.\nMost of these use cases require near real-time performance. The common technique for text extraction includes using an optical character recognition (OCR) system. However, most of the free and commercially available OCR systems are trained to recognize text from documents. There are many challenges when it comes to recognizing text from natural scenes or captioned videos like image perspective, reflections, blurriness, and so on.\nThe next post in this series, Robust Scene Text Detection and Recognition: Implementation, discusses the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning. The third post, Robust Scene Text Detection and Recognition: Inference Optimization, covers production-ready optimization and performance for your STDR pipeline.\nTypically, the text extraction process involves the following steps:\nText fields are detected from the bigger scene by text detection algorithms.\nThis text is extracted and recognized using a custom OCR technique.\nThe recognition of irregular text in natural scene images can be challenging due to the variability in text appearance, such as curvature, orientation, and distortion. To overcome this, sophisticated deep-learning architectures and fine-grained annotations are often required.\nHowever, these can lead to optimization and latency challenges when creating and deploying these algorithms. Despite these challenges, advancements in computer vision have made significant strides in text detection and recognition, providing a powerful tool for various industries. To further optimize inference, you can use specialized optimization tools to reduce latency and improve performance.\nIn this post, we describe these challenges and our approach for optimization and acceleration of inference. We emphasize that deploying a scene text detection and recognition (STDR) pipeline requires careful consideration of real-world scenarios and conditions. To meet these needs, we have used state-of-the-art deep learning algorithms and leveraged techniques like incremental learning and fine-tuning for specific use cases.\nTo ensure low latency, we used the following model inference optimization tools:\nONNX Runtime is a cross-platform machine-learning model accelerator that offers flexibility for integrating hardware-specific libraries. It can be used with models from PyTorch, TensorFlow and Keras, TensorFlow Lite, scikit-learn, and other frameworks.\nNVIDIA TensorRT SDK is used for high-performance deep learning inference, providing a deep learning inference optimizer and runtime that guarantees low latency and high throughput for inference applications.\nNVIDIA Triton Inference Server is used for high-performance inference serving across cloud, on-premises, and edge devices.\nTensorRT and Triton Inference Server are included in NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform.", "STDR applications\nRecognizing text from images and videos is used in various industries.\nHealthcare and Life Sciences: Scene text detection and recognition are used in the healthcare industry to scan and store the medical history of patients on a computer, including reports, X-rays, previous diseases, treatments, diagnostics, and hospital records. It is also required in medical device and drug manufacturing for logistics and warehouse operations.\nPicture of four medicine bottles with prescription labels.\nFigure 1. Sample of medicine package and bottles (Image: The Times )\nManufacturing Supply Chain/Logistics: Scene text detection and recognition play a crucial role in the food, drink, and cosmetics industries for quality control throughout the supply chain. It is used to track products and read product codes, batch codes, expiry dates, and serial numbers. This information can be used to ensure compliance with safety and anti-counterfeiting laws and to locate products within the supply chain at any given time. OCR is often used in conjunction with barcoding to maximize information collection accuracy.\nWarehouse shelves full of boxes with package labels.\nFigure 2. Samples of warehouse packages (Image: shelving.com)\nBanking: Scene text detection and recognition is widely used in the banking industry to automate know-your-customer (KYC) documents like birth certificates, marriage certificates, and so on.\nAutomotive and utilities: Self-driving cars and utility line maintenance drives often require scene images to be identified and data to be extracted (for example, street names, establishment names, utility pole numbers, and transformer and generator details). Usually, the text appears for a fraction of a time as the vehicle is moving, creating a motion blur. In that case, manual detection becomes impossible.", "STDR challenges\nThe biggest challenge in detecting and extracting text from complex images taken from videos and mobile phones is that the text in such images is often irregular and overlayed on varied backgrounds like glass, plastics, rubber, and so on.\nAlso, even if the machine learning model is developed with decent accuracy, the expectation is that the model should process images live or in near real time. Thus, catering to both accuracy and performance expectations requires highly refined models that can work optimally in the cloud as well as edge devices. These challenges are described in detail in this post.", "Creating robust models\nOften, the leading reason for accuracy concerns in scene text models is the number of variations in the input data. Here are some of the data variations.\nText size-scale-blur: Text in natural scenes can appear in various sizes and scales. The distance from the camera also plays an important role in scaling the text. The angle from the camera brings the perspective distortions. Also, lighting conditions create the reflections and shadows around the text. The moving objects or camera movements add to the blur effects. All these conditions contribute to the size-scale-blur distortions in images.\nText orientation, color, and font: Text may appear horizontally, vertically, diagonally, and even circularly. This variation in text orientation can make it difficult for algorithms to correctly detect and recognize text. The color, transparency, and font style used also cause challenges when not reflected by the data used in training.\nBackground and overlays: Text in natural scenes can appear with various backgrounds, such as buildings, trees, vehicles, and so on, and is often overlayed on glass, metal objects, plastics, or stickers. It can also be embossed or debossed onto various kinds of materials.\nMultiple languages: Real-world images contain text in multiple scripts and languages. Often, signage or restaurant menus are written in mixed languages.\nAnother typical challenge in ML projects is the availability of labeled data to train the model. However, for this pipeline, we used a pretrained CRAFT model for text detection, which is trained on the SynthText, IC13, and IC17 datasets.\nFor text recognition, we used the PARseq model, which is trained on various datasets ( MJSynth, SynthText, COCO-Text, RCTW17, Uber-Text, ArT, LSVT, MLT19, and ReCTS, TextOCR ) and finetuned with in-house data.", "Meeting the performance expectations\nDeploying a scene text detection solution can also present various challenges.\nComputational resources: Today, modern STDR systems use complex deep learning algorithms. These models have an abundance of parameters, making them computationally expensive to run. Consequently, it can be difficult to deploy these solutions on devices with limited computational resources, such as smartphones or Internet of Things (IoT) devices.\nLatency and response time: In many scenarios, scene text detection and recognition must be real-time to be effective. Deep learning models can offer excellent accuracy, but their high number of parameters can lead to increased inference time compared to models having a low number of parameters, resulting in unacceptable latency and response time. To optimize accuracy, state-of-the-art algorithms must be used, while inference time can be reduced through optimization techniques such as quantization, lowering precision, and pruning. These optimizations may reduce the accuracy of the model.\nData privacy and security: The privacy and security of the data used for training and running the model are important when deploying the solutions in real-world scenarios. The model needs to be protected from malicious attacks and data breaches. Compliance with data privacy regulations must be ensured.\nThe deployment of scene text detection solutions demands meticulous consideration of the real-world scenarios and conditions in which the solution will be employed. This process is a crucial step that necessitates thorough testing, evaluation, and fine-tuning.\nConsider a package delivery company that requires a label-reading application on a conveyor belt. In this case, high accuracy is critical, as any error can cause delays and result in additional costs for the company. The speed of the conveyor belt is another essential factor to consider, as it affects the overall time required to process the packages.\nAchieving high accuracy may require complex deep-learning models that can be computationally expensive and impact system latency. To optimize performance, it\u2019s important to consider the specific requirements and constraints of the deployment scenario, such as conveyor belt speed and computational resources, and adjust the deep learning models accordingly to strike a balance between accuracy, latency, and resources.", "Summary\nIn this post, we discussed the importance of robust scene text detection and recognition (STDR) in various industries. We highlighted the challenges faced in STDR, including creating accurate models, meeting performance expectations, and dealing with real-world scenarios and conditions.\nFor more information, see the next posts in this series:\nRobust Scene Text Detection and Recognition: Implementation\nRobust Scene Text Detection and Recognition: Inference Optimization"], "document_title": "Robust Scene Text Detection and Recognition: Introduction", "document_url": "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/", "document_date": "2024-01-16T17:00:00", "document_date_modified": "2024-01-25T18:17:34", "document_full_text": "Robust Scene Text Detection and Recognition: Introduction\nIdentification and recognition of text from natural scenes and images become important for use cases like video caption text recognition, detecting signboards from vehicle-mounted cameras, information retrieval, scene understanding, vehicle number plate recognition, and recognizing text on products.\nMost of these use cases require near real-time performance. The common technique for text extraction includes using an optical character recognition (OCR) system. However, most of the free and commercially available OCR systems are trained to recognize text from documents. There are many challenges when it comes to recognizing text from natural scenes or captioned videos like image perspective, reflections, blurriness, and so on.\nThe next post in this series, Robust Scene Text Detection and Recognition: Implementation, discusses the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning. The third post, Robust Scene Text Detection and Recognition: Inference Optimization, covers production-ready optimization and performance for your STDR pipeline.\nTypically, the text extraction process involves the following steps:\nText fields are detected from the bigger scene by text detection algorithms.\nThis text is extracted and recognized using a custom OCR technique.\nThe recognition of irregular text in natural scene images can be challenging due to the variability in text appearance, such as curvature, orientation, and distortion. To overcome this, sophisticated deep-learning architectures and fine-grained annotations are often required.\nHowever, these can lead to optimization and latency challenges when creating and deploying these algorithms. Despite these challenges, advancements in computer vision have made significant strides in text detection and recognition, providing a powerful tool for various industries. To further optimize inference, you can use specialized optimization tools to reduce latency and improve performance.\nIn this post, we describe these challenges and our approach for optimization and acceleration of inference. We emphasize that deploying a scene text detection and recognition (STDR) pipeline requires careful consideration of real-world scenarios and conditions. To meet these needs, we have used state-of-the-art deep learning algorithms and leveraged techniques like incremental learning and fine-tuning for specific use cases.\nTo ensure low latency, we used the following model inference optimization tools:\nONNX Runtime is a cross-platform machine-learning model accelerator that offers flexibility for integrating hardware-specific libraries. It can be used with models from PyTorch, TensorFlow and Keras, TensorFlow Lite, scikit-learn, and other frameworks.\nNVIDIA TensorRT SDK is used for high-performance deep learning inference, providing a deep learning inference optimizer and runtime that guarantees low latency and high throughput for inference applications.\nNVIDIA Triton Inference Server is used for high-performance inference serving across cloud, on-premises, and edge devices.\nTensorRT and Triton Inference Server are included in NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform.\nSTDR applications\nRecognizing text from images and videos is used in various industries.\nHealthcare and Life Sciences: Scene text detection and recognition are used in the healthcare industry to scan and store the medical history of patients on a computer, including reports, X-rays, previous diseases, treatments, diagnostics, and hospital records. It is also required in medical device and drug manufacturing for logistics and warehouse operations.\nPicture of four medicine bottles with prescription labels.\nFigure 1. Sample of medicine package and bottles (Image: The Times )\nManufacturing Supply Chain/Logistics: Scene text detection and recognition play a crucial role in the food, drink, and cosmetics industries for quality control throughout the supply chain. It is used to track products and read product codes, batch codes, expiry dates, and serial numbers. This information can be used to ensure compliance with safety and anti-counterfeiting laws and to locate products within the supply chain at any given time. OCR is often used in conjunction with barcoding to maximize information collection accuracy.\nWarehouse shelves full of boxes with package labels.\nFigure 2. Samples of warehouse packages (Image: shelving.com)\nBanking: Scene text detection and recognition is widely used in the banking industry to automate know-your-customer (KYC) documents like birth certificates, marriage certificates, and so on.\nAutomotive and utilities: Self-driving cars and utility line maintenance drives often require scene images to be identified and data to be extracted (for example, street names, establishment names, utility pole numbers, and transformer and generator details). Usually, the text appears for a fraction of a time as the vehicle is moving, creating a motion blur. In that case, manual detection becomes impossible.\nSTDR challenges\nThe biggest challenge in detecting and extracting text from complex images taken from videos and mobile phones is that the text in such images is often irregular and overlayed on varied backgrounds like glass, plastics, rubber, and so on.\nAlso, even if the machine learning model is developed with decent accuracy, the expectation is that the model should process images live or in near real time. Thus, catering to both accuracy and performance expectations requires highly refined models that can work optimally in the cloud as well as edge devices. These challenges are described in detail in this post.\nCreating robust models\nOften, the leading reason for accuracy concerns in scene text models is the number of variations in the input data. Here are some of the data variations.\nText size-scale-blur: Text in natural scenes can appear in various sizes and scales. The distance from the camera also plays an important role in scaling the text. The angle from the camera brings the perspective distortions. Also, lighting conditions create the reflections and shadows around the text. The moving objects or camera movements add to the blur effects. All these conditions contribute to the size-scale-blur distortions in images.\nText orientation, color, and font: Text may appear horizontally, vertically, diagonally, and even circularly. This variation in text orientation can make it difficult for algorithms to correctly detect and recognize text. The color, transparency, and font style used also cause challenges when not reflected by the data used in training.\nBackground and overlays: Text in natural scenes can appear with various backgrounds, such as buildings, trees, vehicles, and so on, and is often overlayed on glass, metal objects, plastics, or stickers. It can also be embossed or debossed onto various kinds of materials.\nMultiple languages: Real-world images contain text in multiple scripts and languages. Often, signage or restaurant menus are written in mixed languages.\nAnother typical challenge in ML projects is the availability of labeled data to train the model. However, for this pipeline, we used a pretrained CRAFT model for text detection, which is trained on the SynthText, IC13, and IC17 datasets.\nFor text recognition, we used the PARseq model, which is trained on various datasets ( MJSynth, SynthText, COCO-Text, RCTW17, Uber-Text, ArT, LSVT, MLT19, and ReCTS, TextOCR ) and finetuned with in-house data.\nMeeting the performance expectations\nDeploying a scene text detection solution can also present various challenges.\nComputational resources: Today, modern STDR systems use complex deep learning algorithms. These models have an abundance of parameters, making them computationally expensive to run. Consequently, it can be difficult to deploy these solutions on devices with limited computational resources, such as smartphones or Internet of Things (IoT) devices.\nLatency and response time: In many scenarios, scene text detection and recognition must be real-time to be effective. Deep learning models can offer excellent accuracy, but their high number of parameters can lead to increased inference time compared to models having a low number of parameters, resulting in unacceptable latency and response time. To optimize accuracy, state-of-the-art algorithms must be used, while inference time can be reduced through optimization techniques such as quantization, lowering precision, and pruning. These optimizations may reduce the accuracy of the model.\nData privacy and security: The privacy and security of the data used for training and running the model are important when deploying the solutions in real-world scenarios. The model needs to be protected from malicious attacks and data breaches. Compliance with data privacy regulations must be ensured.\nThe deployment of scene text detection solutions demands meticulous consideration of the real-world scenarios and conditions in which the solution will be employed. This process is a crucial step that necessitates thorough testing, evaluation, and fine-tuning.\nConsider a package delivery company that requires a label-reading application on a conveyor belt. In this case, high accuracy is critical, as any error can cause delays and result in additional costs for the company. The speed of the conveyor belt is another essential factor to consider, as it affects the overall time required to process the packages.\nAchieving high accuracy may require complex deep-learning models that can be computationally expensive and impact system latency. To optimize performance, it\u2019s important to consider the specific requirements and constraints of the deployment scenario, such as conveyor belt speed and computational resources, and adjust the deep learning models accordingly to strike a balance between accuracy, latency, and resources.\nSummary\nIn this post, we discussed the importance of robust scene text detection and recognition (STDR) in various industries. We highlighted the challenges faced in STDR, including creating accurate models, meeting performance expectations, and dealing with real-world scenarios and conditions.\nFor more information, see the next posts in this series:\nRobust Scene Text Detection and Recognition: Implementation\nRobust Scene Text Detection and Recognition: Inference Optimization"}], "https://developer.nvidia.com/blog/experience-real-time-audio-and-video-communication-with-nvidia-maxine/": [{"text": "NVIDIA Maxine offers real-time AI-enhanced audio and video communication through features like Voice Font, Live Portrait, and Eye Contact. The platform provides developers and businesses with low-code implementation options for GPU-accelerated AI microservices and SDKs. The latest release includes improvements to Live Portrait and Voice Font, with features available in the NGC catalog. Maxine also offers Studio Voice for enhancing audio quality and speech-driven Live Portrait for animating 2D photos with speech. The platform is designed to enhance communication in video conferences, live streams, and offline videos. Users can access the latest Maxine features through NVIDIA AI Enterprise and provide feedback through early access programs. Maxine aims to revolutionize video conferencing and editing with state-of-the-art AI models and tools for creating high-quality effects using standard microphones and cameras.", "text_components": ["Experience Real-Time Audio and Video Communication with NVIDIA Maxine\u00a0\nThe NVIDIA Maxine developer platform redefines video conferencing and editing by providing developers and businesses with a variety of low-code implementation options. These include GPU-accelerated AI microservices , SDKs, and NVIDIA-hosted API endpoints for AI enhancement of audio and video streams in real time.\nThe latest Maxine developer platform release introduces early access to Voice Font, improvements to video-driven Live Portrait, and improvements to the popular Maxine Eye Contact feature. In addition, the Video Live Portrait and Voice Font features are now available in the NVIDIA NGC catalog. You can now experience Maxine pretrained generative AI models in action on NVIDIA-accelerated cloud infrastructure.\nNVIDIA AI Foundation Models such as Maxine demonstrate how enterprises can now connect their applications to read-to-integrate NVIDIA Foundations API endpoints and quickly create and deploy performance-optimized AI models with a reduced TCO.\nNVIDIA Maxine Live Portrait webpage as seen on NVIDIA AI Foundation Models.\nFigure 1. You can now experience NVIDIA Maxine Live Portrait and Voice Font\nThe Maxine team is also offering select partners the opportunity to give feedback on early versions of Maxine\u2019s new Studio Voice and Speech-driven Live Portrait features. For more information, visit Maxine Microservices Early Access Program and Maxine SDK Early Access Program.\n\u201cWith the Maxine developer platform, you can now experience state-of-the-art Maxine features on NVIDIA AI Foundations,\u201d said Rochelle Pereira, Director of Engineering, Maxine Developer Platform. \u201cYou can design your deployment plan on a CSP of your choice or NVIDIA DGX Cloud, and choose your integration touch point ranging from microservice containers to SDK libraries or even ready-to-integrate NVIDIA AI Foundation Endpoints. Enhancing real-time audio and video communication in your application workflow just got a lot easier.\u201d", "New feature highlights\nNVIDIA Maxine enables clear communications and increased presence for speakers in video conferences, live streams, and offline video. Maxine\u2019s state-of-the-art AI models create high-quality effects that can be achieved with standard microphones and cameras.", "Natural eye movement\nThe new production Maxine Eye Contact now has smoother transitions in gaze redirection and fine-grained controls for more natural eye movement. Eye Contact is available for developers to evaluate through the Maxine Early Access Program and for production use through NVIDIA AI Enterprise.", "2D photo animation\nThe newest Maxine release also sees improvements to video-driven Live Portrait including increases in robustness and background stability. Maxine Live Portrait has been a game-changer, enabling 2D photo animation driven by video. This new Maxine release also introduces speech-driven Live Portrait that enables speech as a new driving modality.\nYou can now animate 2D photos with speech, providing a sense of presence even when conditions don\u2019t permit real-time video streaming. Combined with NVIDIA Riva translation service and NVIDIA Maxine Voice Font, speech-driven Live Portrait ushers in new possibilities in the realm of 2D animation.", "Speech-driven personas\nNVIDIA Maxine video and speech Live Portrait animation AI microservices are ideal for anyone who does not want to appear on camera. Create a unique persona for an individual or a company using a stylized or photorealistic portrait photo. Speech-driven Live Portrait is available to select partners for feedback.\nVideo 1. Animate 2D portraits in real time using speech, no meshes or rigging required", "Voice customization\nWith the new Maxine Voice Font feature, a generative AI model now available in the Maxine Early Access Program, you can customize your voice to a desired timbre. Generate a unique voice for a brand or replicate your voice for use with other translation microservices. This enables you to speak in different languages in your own voice, for example. The feature can convert audio samples into a digital voice with just 30 seconds of reference audio.\nCheck out the samples below to experience Voice Font.\nOrigin audio sample:\nReference audio sample:\nVoice Font output, applying reference audio voice to origin audio:\nAlso available to select development partners for feedback is the latest NVIDIA Maxine AI enhancement capability. With Studio Voice, you can enhance a recording from an inexpensive microphone with the characteristics of a high-end studio microphone. Studio Voice removes speech frequency degradations caused by low-quality microphones. Additionally, characteristics like dynamic range and bandwidth extension are added using a pretrained neural net, giving the resulting audio a rich and vibrant sound.\nGet a sneak preview of Studio Voice with the samples below:\nInput voice on basic microphone:\nStudio Voice output:", "Summary\nWith NVIDIA Maxine, you can use AI to enhance your audio and video communication in real time. The latest Maxine release is available exclusively through NVIDIA AI Enterprise. This enables users to access enterprise support, production-ready tools including NVIDIA Triton Inference Server, and more. Try the newest NVIDIA Maxine features.\nFor early access to the latest NVIDIA Maxine features and to provide feedback, apply for the Maxine Microservices Early Access Program or Maxine SDK Early Access Program. To provide feedback on speech-driven Live Portrait or Studio Voice (not yet released), contact Greg Jones, Maxine Product Management, at gjones@nvidia.com.\nYou can also provide feedback through the NVIDIA Maxine and NVIDIA Broadcast App Survey to help improve Maxine features in upcoming releases."], "document_title": "Experience Real-Time Audio and Video Communication with NVIDIA Maxine\u00a0", "document_url": "https://developer.nvidia.com/blog/experience-real-time-audio-and-video-communication-with-nvidia-maxine/", "document_date": "2024-01-10T19:00:00", "document_date_modified": "2024-01-25T18:17:37", "document_full_text": "Experience Real-Time Audio and Video Communication with NVIDIA Maxine\u00a0\nThe NVIDIA Maxine developer platform redefines video conferencing and editing by providing developers and businesses with a variety of low-code implementation options. These include GPU-accelerated AI microservices , SDKs, and NVIDIA-hosted API endpoints for AI enhancement of audio and video streams in real time.\nThe latest Maxine developer platform release introduces early access to Voice Font, improvements to video-driven Live Portrait, and improvements to the popular Maxine Eye Contact feature. In addition, the Video Live Portrait and Voice Font features are now available in the NVIDIA NGC catalog. You can now experience Maxine pretrained generative AI models in action on NVIDIA-accelerated cloud infrastructure.\nNVIDIA AI Foundation Models such as Maxine demonstrate how enterprises can now connect their applications to read-to-integrate NVIDIA Foundations API endpoints and quickly create and deploy performance-optimized AI models with a reduced TCO.\nNVIDIA Maxine Live Portrait webpage as seen on NVIDIA AI Foundation Models.\nFigure 1. You can now experience NVIDIA Maxine Live Portrait and Voice Font\nThe Maxine team is also offering select partners the opportunity to give feedback on early versions of Maxine\u2019s new Studio Voice and Speech-driven Live Portrait features. For more information, visit Maxine Microservices Early Access Program and Maxine SDK Early Access Program.\n\u201cWith the Maxine developer platform, you can now experience state-of-the-art Maxine features on NVIDIA AI Foundations,\u201d said Rochelle Pereira, Director of Engineering, Maxine Developer Platform. \u201cYou can design your deployment plan on a CSP of your choice or NVIDIA DGX Cloud, and choose your integration touch point ranging from microservice containers to SDK libraries or even ready-to-integrate NVIDIA AI Foundation Endpoints. Enhancing real-time audio and video communication in your application workflow just got a lot easier.\u201d\nNew feature highlights\nNVIDIA Maxine enables clear communications and increased presence for speakers in video conferences, live streams, and offline video. Maxine\u2019s state-of-the-art AI models create high-quality effects that can be achieved with standard microphones and cameras.\nNatural eye movement\nThe new production Maxine Eye Contact now has smoother transitions in gaze redirection and fine-grained controls for more natural eye movement. Eye Contact is available for developers to evaluate through the Maxine Early Access Program and for production use through NVIDIA AI Enterprise.\n2D photo animation\nThe newest Maxine release also sees improvements to video-driven Live Portrait including increases in robustness and background stability. Maxine Live Portrait has been a game-changer, enabling 2D photo animation driven by video. This new Maxine release also introduces speech-driven Live Portrait that enables speech as a new driving modality.\nYou can now animate 2D photos with speech, providing a sense of presence even when conditions don\u2019t permit real-time video streaming. Combined with NVIDIA Riva translation service and NVIDIA Maxine Voice Font, speech-driven Live Portrait ushers in new possibilities in the realm of 2D animation.\nSpeech-driven personas\nNVIDIA Maxine video and speech Live Portrait animation AI microservices are ideal for anyone who does not want to appear on camera. Create a unique persona for an individual or a company using a stylized or photorealistic portrait photo. Speech-driven Live Portrait is available to select partners for feedback.\nVideo 1. Animate 2D portraits in real time using speech, no meshes or rigging required\nVoice customization\nWith the new Maxine Voice Font feature, a generative AI model now available in the Maxine Early Access Program, you can customize your voice to a desired timbre. Generate a unique voice for a brand or replicate your voice for use with other translation microservices. This enables you to speak in different languages in your own voice, for example. The feature can convert audio samples into a digital voice with just 30 seconds of reference audio.\nCheck out the samples below to experience Voice Font.\nOrigin audio sample:\nReference audio sample:\nVoice Font output, applying reference audio voice to origin audio:\nAlso available to select development partners for feedback is the latest NVIDIA Maxine AI enhancement capability. With Studio Voice, you can enhance a recording from an inexpensive microphone with the characteristics of a high-end studio microphone. Studio Voice removes speech frequency degradations caused by low-quality microphones. Additionally, characteristics like dynamic range and bandwidth extension are added using a pretrained neural net, giving the resulting audio a rich and vibrant sound.\nGet a sneak preview of Studio Voice with the samples below:\nInput voice on basic microphone:\nStudio Voice output:\nSummary\nWith NVIDIA Maxine, you can use AI to enhance your audio and video communication in real time. The latest Maxine release is available exclusively through NVIDIA AI Enterprise. This enables users to access enterprise support, production-ready tools including NVIDIA Triton Inference Server, and more. Try the newest NVIDIA Maxine features.\nFor early access to the latest NVIDIA Maxine features and to provide feedback, apply for the Maxine Microservices Early Access Program or Maxine SDK Early Access Program. To provide feedback on speech-driven Live Portrait or Studio Voice (not yet released), contact Greg Jones, Maxine Product Management, at gjones@nvidia.com.\nYou can also provide feedback through the NVIDIA Maxine and NVIDIA Broadcast App Survey to help improve Maxine features in upcoming releases."}], "https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/": [{"text": "The article discusses the importance of accurate pose estimation for robotic agents to interact with objects, focusing on NVIDIA's Deep Object Pose Estimation (DOPE) model. DOPE estimates the six degrees of freedom pose of objects from RGB images, enabling robotic manipulation in various scenarios. The model is trained on synthetic data, reducing data collection costs and handling object occlusion. Techniques like domain randomization are used to bridge the reality gap between synthetic and real-world data. The article details the architecture of DOPE, data generation methods using NVIDIA Isaac Sim, training processes, inference, and evaluation workflows. DOPE's performance is compared to other pose estimation models like PoseCNN and BB8, showing superior results. The article also introduces the use of DOPE in NVIDIA Isaac ROS for GPU-accelerated pose estimation on platforms like Jetson and NVIDIA GPUs. The article provides insights into generating synthetic data, training DOPE models, and evaluating their performance for various object manipulation tasks.", "text_components": ["Generate Synthetic Data for Deep Object Pose Estimation Training with NVIDIA Isaac ROS\nFor robotic agents to interact with objects in their environment, they must know the position and orientation of objects around them. This information describes the six degrees of freedom (DOF) pose of a rigid body in 3D space, detailing the translational and rotational state.\nAccurate pose estimation is necessary to determine how to orient a robotic arm to grasp or place objects in a specific way. Use cases include robotic manipulation for pick-and-place operations, especially applicable in warehouse scenarios for tasks like box packing, part loading, and food packaging. Knowing an object\u2019s pose is also crucial for robot-to-human handoff and is useful in healthcare, retail, and household scenarios.\nNVIDIA developed Deep Object Pose Estimation (DOPE) to find the six DOF pose of an object. In this post, we show how to generate synthetic data to train a DOPE model for an object.", "Deep Object Pose Estimation\nFour images of a collection of different objects. Each image shows 3D bounding boxes and corresponding pose axes around each object.\nFigure 1. DOPE estimating six DOF poses of objects from an RGB image\nDOPE is a one-shot DNN developed by NVIDIA that estimates six DOF poses of objects of interest from an RGB image to enable robotic manipulation of objects in an environment. It is trained only on synthetic data and requires a textured 3D model. It provides enough accuracy for real-world grasping and gripper manipulation, with a tolerance of 2 cm.\nDOPE is an instance-level model, meaning a DOPE model must be trained specifically for each object type within a class. For example, we can\u2019t train a single DOPE model to detect all types of chairs and instead must train one model per chair type.\nAs another example, if an application is detecting four geometrically similar boxes of different colors, four instances of DOPE models are required for inference\u2014one trained specifically on each colored box.", "Advantages of DOPE\nIt can be trained entirely on synthetic data, reducing data collection and annotation costs.\nHandles object occlusion.\nReduces the reality gap challenge by combining domain randomized and photorealistic synthetic data for training.\nIt works on different camera intrinsics without retraining through using the Perspective-n-point (PnP) algorithm.\nDOPE is supported in NVIDIA Isaac ROS to provide GPU-accelerated object pose estimation.", "Reality gap challenge\nNetworks trained only on synthetic data often perform poorly on real-world data. Techniques like fine-tuning or domain randomization help improve performance.\nDomain randomization is the method of varying parameters like scene lighting, scale, pose, color, and texture of objects in a simulation environment. This is done to provide a sufficient variety of domain parameters to the neural network to improve generalization to real-world environments. This way, real data appears as just another variation to the network.\nDOPE bridges the reality gap by combining domain randomized and photorealistic synthetic data for training, and generalizes well to real-world use cases.", "Architecture\nA diagram showing an overview of the DOPE network architecture. It shows VGG19 as a block followed by multiple convolution layers.\nFigure 2. Overview of DOPE network architecture\nDOPE is a one-shot fully convolutional neural network, inspired by convolutional pose machines (CPMs) and a multi-person pose estimator. The architecture consists of a standard CNN such as VGG19 or RESNET with additional convolution layers.\nFor a comprehensive understanding of the DOPE architecture and data generation pipeline, refer to Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects.", "Dataset\nNVIDIA provides pretrained DOPE models trained on the NVIDIA Household Objects for Pose Estimation (HOPE) dataset. It is a collection of 28 toy grocery objects in varying environments and is part of the Benchmark for 6D Object Pose Estimation.\nBeing instance-level, DOPE must be trained with a dataset targeting objects of interest relevant to the application. To generate a dataset for training DOPE, a 3D model of the object is required. 3D object models can be generated using BundleSDF. The method, developed by NVIDIA, uses monocular RGBD cameras and removes the need for expensive 3D sensors.", "Data generation\nSynthetic data can be generated for DOPE using NVIDIA Isaac Sim for domain randomization. We focus on two datasets\u2014MESH and DOME\u2014and implement randomization techniques similar to those shown for these datasets in the NViSII paper.\nThese datasets add flying distractors to the scene around the object of interest and randomize lighting conditions, distractors\u2019 colors, and materials. DOME uses fewer distractors than MESH and provides more realistic backgrounds.\nTwo example images from the DOME dataset and MESH dataset, respectively.\nFigure 3. Example of DOME data (left) and MESH data (right)\nInformation on how to use Isaac Sim to create training data for DOPE is available in NVIDIA docs.\nYou can specify the number of images you want to generate of each type (MESH and DOME). A good MESH / DOME split depends on the use case. Experiment to find heuristics that work well for your model (for instance, 25 / 75 between MESH / DOME). If you\u2019re generating data and training DOPE on a single object, a training dataset of around 20k images is generally enough.\nThe generated dataset includes images and annotated JSON files. Each JSON file contains information about the object, including object class, position, orientation, and visibility in the corresponding image. Visibility represents how much of the object is visible (in the case of occlusions) and can be used to filter images for training.\nThis data generation method using Isaac Sim can also write data in a format similar to the YCB Video Dataset, which can then be used to train other 6D pose estimation models.", "Object symmetry\nDOPE is trained on cuboid corners that bound an object of interest. Rotational symmetries in this object could result in multiple frames that are identical pixel-wise but marked by different cuboid corners.\nWatch this Deep Object Pose video on GitHub to learn more.\nThe Isaac Sim data generation method doesn\u2019t explicitly handle rotational symmetries at the moment. However, NVIDIA also provides synthetic data generation scripts using NViSII that can handle symmetry.", "Training DOPE\nAfter you\u2019ve generated your training dataset, NVIDIA provides a script to train DOPE. You can point the script to your training data and specify the batch size and number of epochs you want to train your model for.\nThe script saves useful training information (including loss graphs and belief maps) which you can view using TensorBoard.", "Inference and evaluation\nAfter you\u2019ve trained your DOPE model, you can run inference on a test dataset. Depending on the images in your test data, you can specify configuration parameters in the provided configuration files or write your own.\nInclude the physical dimensions of the object of interest in the object config file (I used a 3D viewer online to load the 3D model and find the dimensions). The inference workflow uses these dimensions to generate results with bounding boxes around the detected objects.\nAn example output from DOPE. The image shows an object with a 3D bounding box around it, oriented accurately using the pose estimate from DOPE.\nFigure 4. Qualitative result showing a bounding box around an object with an accurate pose from DOPE\nAfter running inference, we provide an evaluation workflow to evaluate the performance of your model quantitatively. Ground truth data, predicted results from the inference step, and a 3D model of the object of interest (in .obj format) are required for evaluation. The object\u2019s 3D model is rendered to calculate the 3D error between ground truth and predicted results.\nThe ADD metric is used and we provide two options for calculating error:\nAverage distance (ADD) is the average distance calculated using the closest point distance between the predicted pose and the ground-truth pose.\nCuboid distance calculates the average distance using the eight cuboid points of the 3D models (ground truth) and predicted cuboid points. This is faster to calculate than ADD but less accurate.\nWith domain randomized data alone for an arbitrary object, the highest area under the curve (AUC) observed was 66.64 for 300k images. 62.94 AUC was observed when using a dataset of 600k photorealistic images alone. Accuracy was highest when domain randomized and photorealistic synthetic images were combined (77.00 AUC).\nImage showing the Accuracy-threshold curve for DOPE compared to PoseCNN. The highest value of 76.06 was observed when using DOPE with domain randomized and photorealistic data. This value is observed to be 66.07 for the same object using PoseCNN.\nFigure 5. Accuracy-threshold curve for DOPE compared with PoseCNN for an object on the YCB-Video dataset\nDOPE has been trained on synthetic images only. Yet it still performs well on scenes captured with a different camera, even when there are occlusions and extreme lighting changes. Its performance is better than PoseCNN and BB8, which have been trained on real data or a combination of synthetic and real data.\nFor a direct comparison, five objects were selected from the YCB dataset, and DOPE achieved a higher AUC than PoseCNN for four of the five objects.\nMore details can be found in the DOPE paper. Check out our GitHub for information on inference and evaluation.", "Using \u200cIsaac ROS pose estimation\nIsaac ROS provides a ROS 2 package for pose estimation using DOPE. It performs GPU-accelerated inference using NVIDIA Triton or NVIDIA TensorRT with Isaac ROS DNN Inference.\nAfter training your DOPE model, you can run inference using this package on NVIDIA Jetson or a system with an NVIDIA GPU.\nYou can also perform inference on live images from a camera stream, however, this is a compute-intensive task. Pose estimation is done at a lower frame rate than the camera input rate. Our DOPE graph runs at 39.8 FPS on an NVIDIA Jetson AGX Orin and 89.2 FPS on an NVIDIA RTX 4060 Ti\u2014based on the Isaac ROS Benchmark workflow.\nGIF showing the camera view of a bottle of mustard being moved around and Isaac ROS DOPE being used to estimate the pose of this bottle. The output pose results are visualized as 3D axis on an RViz window.\nFig 6. Running Isaac ROS DOPE with live camera input and visualizing pose on RViz\nThe graph includes three components and steps:\nThe DNN image encoder node turns a raw image into a resized, normalized tensor.\nTensorRT node converts an input tensor into a tensor of belief maps.\nThe DOPE decoder node converts a belief map into an array of poses.\nLearn more about the performance of different Isaac ROS packages and benchmarking methodology in the performance summary. Check out Isaac ROS Pose Estimation on GitHub."], "document_title": "Generate Synthetic Data for Deep Object Pose Estimation Training with NVIDIA Isaac ROS", "document_url": "https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/", "document_date": "2024-01-18T21:45:18", "document_date_modified": "2024-01-25T18:17:30", "document_full_text": "Generate Synthetic Data for Deep Object Pose Estimation Training with NVIDIA Isaac ROS\nFor robotic agents to interact with objects in their environment, they must know the position and orientation of objects around them. This information describes the six degrees of freedom (DOF) pose of a rigid body in 3D space, detailing the translational and rotational state.\nAccurate pose estimation is necessary to determine how to orient a robotic arm to grasp or place objects in a specific way. Use cases include robotic manipulation for pick-and-place operations, especially applicable in warehouse scenarios for tasks like box packing, part loading, and food packaging. Knowing an object\u2019s pose is also crucial for robot-to-human handoff and is useful in healthcare, retail, and household scenarios.\nNVIDIA developed Deep Object Pose Estimation (DOPE) to find the six DOF pose of an object. In this post, we show how to generate synthetic data to train a DOPE model for an object.\nDeep Object Pose Estimation\nFour images of a collection of different objects. Each image shows 3D bounding boxes and corresponding pose axes around each object.\nFigure 1. DOPE estimating six DOF poses of objects from an RGB image\nDOPE is a one-shot DNN developed by NVIDIA that estimates six DOF poses of objects of interest from an RGB image to enable robotic manipulation of objects in an environment. It is trained only on synthetic data and requires a textured 3D model. It provides enough accuracy for real-world grasping and gripper manipulation, with a tolerance of 2 cm.\nDOPE is an instance-level model, meaning a DOPE model must be trained specifically for each object type within a class. For example, we can\u2019t train a single DOPE model to detect all types of chairs and instead must train one model per chair type.\nAs another example, if an application is detecting four geometrically similar boxes of different colors, four instances of DOPE models are required for inference\u2014one trained specifically on each colored box.\nAdvantages of DOPE\nIt can be trained entirely on synthetic data, reducing data collection and annotation costs.\nHandles object occlusion.\nReduces the reality gap challenge by combining domain randomized and photorealistic synthetic data for training.\nIt works on different camera intrinsics without retraining through using the Perspective-n-point (PnP) algorithm.\nDOPE is supported in NVIDIA Isaac ROS to provide GPU-accelerated object pose estimation.\nReality gap challenge\nNetworks trained only on synthetic data often perform poorly on real-world data. Techniques like fine-tuning or domain randomization help improve performance.\nDomain randomization is the method of varying parameters like scene lighting, scale, pose, color, and texture of objects in a simulation environment. This is done to provide a sufficient variety of domain parameters to the neural network to improve generalization to real-world environments. This way, real data appears as just another variation to the network.\nDOPE bridges the reality gap by combining domain randomized and photorealistic synthetic data for training, and generalizes well to real-world use cases.\nArchitecture\nA diagram showing an overview of the DOPE network architecture. It shows VGG19 as a block followed by multiple convolution layers.\nFigure 2. Overview of DOPE network architecture\nDOPE is a one-shot fully convolutional neural network, inspired by convolutional pose machines (CPMs) and a multi-person pose estimator. The architecture consists of a standard CNN such as VGG19 or RESNET with additional convolution layers.\nFor a comprehensive understanding of the DOPE architecture and data generation pipeline, refer to Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects.\nDataset\nNVIDIA provides pretrained DOPE models trained on the NVIDIA Household Objects for Pose Estimation (HOPE) dataset. It is a collection of 28 toy grocery objects in varying environments and is part of the Benchmark for 6D Object Pose Estimation.\nBeing instance-level, DOPE must be trained with a dataset targeting objects of interest relevant to the application. To generate a dataset for training DOPE, a 3D model of the object is required. 3D object models can be generated using BundleSDF. The method, developed by NVIDIA, uses monocular RGBD cameras and removes the need for expensive 3D sensors.\nData generation\nSynthetic data can be generated for DOPE using NVIDIA Isaac Sim for domain randomization. We focus on two datasets\u2014MESH and DOME\u2014and implement randomization techniques similar to those shown for these datasets in the NViSII paper.\nThese datasets add flying distractors to the scene around the object of interest and randomize lighting conditions, distractors\u2019 colors, and materials. DOME uses fewer distractors than MESH and provides more realistic backgrounds.\nTwo example images from the DOME dataset and MESH dataset, respectively.\nFigure 3. Example of DOME data (left) and MESH data (right)\nInformation on how to use Isaac Sim to create training data for DOPE is available in NVIDIA docs.\nYou can specify the number of images you want to generate of each type (MESH and DOME). A good MESH / DOME split depends on the use case. Experiment to find heuristics that work well for your model (for instance, 25 / 75 between MESH / DOME). If you\u2019re generating data and training DOPE on a single object, a training dataset of around 20k images is generally enough.\nThe generated dataset includes images and annotated JSON files. Each JSON file contains information about the object, including object class, position, orientation, and visibility in the corresponding image. Visibility represents how much of the object is visible (in the case of occlusions) and can be used to filter images for training.\nThis data generation method using Isaac Sim can also write data in a format similar to the YCB Video Dataset, which can then be used to train other 6D pose estimation models.\nObject symmetry\nDOPE is trained on cuboid corners that bound an object of interest. Rotational symmetries in this object could result in multiple frames that are identical pixel-wise but marked by different cuboid corners.\nWatch this Deep Object Pose video on GitHub to learn more.\nThe Isaac Sim data generation method doesn\u2019t explicitly handle rotational symmetries at the moment. However, NVIDIA also provides synthetic data generation scripts using NViSII that can handle symmetry.\nTraining DOPE\nAfter you\u2019ve generated your training dataset, NVIDIA provides a script to train DOPE. You can point the script to your training data and specify the batch size and number of epochs you want to train your model for.\nThe script saves useful training information (including loss graphs and belief maps) which you can view using TensorBoard.\nInference and evaluation\nAfter you\u2019ve trained your DOPE model, you can run inference on a test dataset. Depending on the images in your test data, you can specify configuration parameters in the provided configuration files or write your own.\nInclude the physical dimensions of the object of interest in the object config file (I used a 3D viewer online to load the 3D model and find the dimensions). The inference workflow uses these dimensions to generate results with bounding boxes around the detected objects.\nAn example output from DOPE. The image shows an object with a 3D bounding box around it, oriented accurately using the pose estimate from DOPE.\nFigure 4. Qualitative result showing a bounding box around an object with an accurate pose from DOPE\nAfter running inference, we provide an evaluation workflow to evaluate the performance of your model quantitatively. Ground truth data, predicted results from the inference step, and a 3D model of the object of interest (in .obj format) are required for evaluation. The object\u2019s 3D model is rendered to calculate the 3D error between ground truth and predicted results.\nThe ADD metric is used and we provide two options for calculating error:\nAverage distance (ADD) is the average distance calculated using the closest point distance between the predicted pose and the ground-truth pose.\nCuboid distance calculates the average distance using the eight cuboid points of the 3D models (ground truth) and predicted cuboid points. This is faster to calculate than ADD but less accurate.\nWith domain randomized data alone for an arbitrary object, the highest area under the curve (AUC) observed was 66.64 for 300k images. 62.94 AUC was observed when using a dataset of 600k photorealistic images alone. Accuracy was highest when domain randomized and photorealistic synthetic images were combined (77.00 AUC).\nImage showing the Accuracy-threshold curve for DOPE compared to PoseCNN. The highest value of 76.06 was observed when using DOPE with domain randomized and photorealistic data. This value is observed to be 66.07 for the same object using PoseCNN.\nFigure 5. Accuracy-threshold curve for DOPE compared with PoseCNN for an object on the YCB-Video dataset\nDOPE has been trained on synthetic images only. Yet it still performs well on scenes captured with a different camera, even when there are occlusions and extreme lighting changes. Its performance is better than PoseCNN and BB8, which have been trained on real data or a combination of synthetic and real data.\nFor a direct comparison, five objects were selected from the YCB dataset, and DOPE achieved a higher AUC than PoseCNN for four of the five objects.\nMore details can be found in the DOPE paper. Check out our GitHub for information on inference and evaluation.\nUsing \u200cIsaac ROS pose estimation\nIsaac ROS provides a ROS 2 package for pose estimation using DOPE. It performs GPU-accelerated inference using NVIDIA Triton or NVIDIA TensorRT with Isaac ROS DNN Inference.\nAfter training your DOPE model, you can run inference using this package on NVIDIA Jetson or a system with an NVIDIA GPU.\nYou can also perform inference on live images from a camera stream, however, this is a compute-intensive task. Pose estimation is done at a lower frame rate than the camera input rate. Our DOPE graph runs at 39.8 FPS on an NVIDIA Jetson AGX Orin and 89.2 FPS on an NVIDIA RTX 4060 Ti\u2014based on the Isaac ROS Benchmark workflow.\nGIF showing the camera view of a bottle of mustard being moved around and Isaac ROS DOPE being used to estimate the pose of this bottle. The output pose results are visualized as 3D axis on an RViz window.\nFig 6. Running Isaac ROS DOPE with live camera input and visualizing pose on RViz\nThe graph includes three components and steps:\nThe DNN image encoder node turns a raw image into a resized, normalized tensor.\nTensorRT node converts an input tensor into a tensor of belief maps.\nThe DOPE decoder node converts a belief map into an array of poses.\nLearn more about the performance of different Isaac ROS packages and benchmarking methodology in the performance summary. Check out Isaac ROS Pose Estimation on GitHub."}], "https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/": [{"text": "The article discusses the importance of automatic speech recognition (ASR) customization in enhancing phone customer service interactions. Traditional language understanding systems struggle with background noise, accents, and dialects, making it challenging to gain a caller's trust. PolyAI explored third-party and in-house ASR solutions, ultimately developing an in-house solution using NVIDIA Riva for better accuracy and performance. By fine-tuning their ASR models with 20 hours of data, PolyAI achieved significant improvements in word error rate compared to out-of-the-box solutions. The in-house ASR models performed better across various datasets, showcasing their versatility and effectiveness in understanding specific phrases and keywords. Customized ASR models are essential for automating customer interactions over the phone and providing a natural and helpful voice experience. PolyAI's journey highlights the benefits and challenges of building in-house ASR solutions and the importance of choosing the right model for accurate transcription.", "text_components": ["Enhancing Phone Customer Service with ASR Customization\nAt the core of understanding people correctly and having natural conversations is automatic speech recognition (ASR). To make customer-led voice assistants and automate customer service interactions over the phone, companies must solve the unique challenge of gaining a caller\u2019s trust through qualities such as understanding, empathy, and clarity.\nTelephony-bound voice is inherently challenging from a speech recognition perspective. Background noise, poor call quality, and various dialects and accents make understanding a caller\u2019s words difficult. Traditional language understanding systems have limited support for voice in general, and how a person speaks differs fundamentally from how they type or text.\nIn this post, we discuss PolyAI \u2019s exploration journey with third-party, out-of-the-box, and in-house customized NVIDIA Riva ASR solutions. The goal is to deliver voice experiences that let callers speak however they like, providing helpful and natural responses at every turn of the conversation. The in-house fine-tuned Riva ASR models resulted in notable accuracy improvement on a variety of different validation real-world customer call datasets.", "Out-of-the-box ASR challenges for effective customer interactions\nOut-of-the-box ASR tools are typically prepared for non-noisy environments and speakers who clearly enunciate and have expected accents. These systems can\u2019t predict what a caller will say, how they might say it, or their speaking tempo. While out-of-the-box solutions can be useful, they can\u2019t be tailored to specific business needs and objectives.\nTo achieve accurate voice assistants that handle customer interactions efficiently, organizations require an ASR system that can be fine-tuned to significantly improve word error rate (WER).", "Advantages and challenges of building an in-house ASR solution\nTo truly understand people from different places, with different accents, and in noisy environments, conversational systems can use multiple ASR systems, phoneme matching, biasing keywords, and post-processing tools.\nThe machine learning team at PolyAI rigorously tested numerous ASR systems, often on multiple models, and applied spoken language understanding (SLU) principles to improve transcription accuracy (Figure 1). This work significantly improved the accuracy of speech recognition in real customer phone calls.\nOptimizing the caller experience further required the development of an in-house solution.\nDiagram shows stack components: ASR systems, phoneme matching, biasing keywords, and post-processing tools.\nFigure 1. PolyAI tech stack\nThe PolyAI tech stack enables voice assistants to accurately understand alphanumeric inputs and people from different places, with different accents, and in noisy environments.\nDeveloping an in-house solution approach offers the following advantages:\nBetter accuracy and performance with flexible fine-tuning of model parameters on extensive data and voice activity detector (VAD) adaptation for the specific ways in which people talk with the system.\nFull compliance with a bring-your-own-cloud (BYOC) approach that delivers the model and the whole conversational system to clients with zero data transfers to third-party providers.\nWith great benefits comes a unique set of challenges. Building an in-house solution requires heavy investment in the following areas:\nExpensive pretraining data: Most models require large quantities of good quality, annotated, pretraining data.\nLatency optimization: This area is often overlooked in the research process. Contrary to chat conversation, voice conversation operates on milliseconds. Every millisecond counts. Adding latency at the start of the conversation gives even less time when calling the large language models (LLM) or text-to-speech (TTS) models.", "Choosing and finetuning ASR models for an in-house solution\nAfter a substantial search for an ASR solution that addresses building in-house solution challenges, PolyAI decided to use NVIDIA Riva for the following reasons:\nCutting-edge accuracy of pretrained models trained on a substantial volume of conversational speech data.\nEnhanced accuracy with full model customization, including acoustic model customization for different accents, noisy environments, or poor audio quality.\nHigh inference performance based on tight coupling with NVIDIA Triton Inference Server and battle-tested to handle machine learning servicing.\nInitial trials with an in-house ASR model provided valuable insights into the fine-tuning process. This led to the development of a robust and flexible fine-tuning methodology, incorporating diverse validation sets to ensure optimal performance.", "Conversational system for testing out-of-the-box and in-house ASR solutions\nTypical conversational systems use public switched telephone networks (PSTN) or session initiation protocol (SIP) connections to transfer calls into the tech stack.\nCall information from these systems is then sent to third-party ASR cloud service providers or in-house ASR solutions. For PolyAI\u2019s testing of ASR solutions (Figure 2), after a call is transcribed, it is sent to a PolyAI voice assistant, where natural language models generate a response. The response is then transferred back into the audio wave through in-house TTS or third-party providers.\nDiagram includes a telephony gateway, audio gateway, natural language models, and text-to-speech.\nFigure 2. PolyAI architecture for testing ASR solutions", "Creating a real-world ASR testing dataset\nPolyAI identified 20 hours of the most challenging conversations split equally between UK and US region calls to test the accuracy of third-party, out-of-the-box, and in-house ASR solutions. These were the calls with noisy environments and ones where other ASR models\u2014in-house or third-party providers\u2014had previously failed.\nThese failure calls varied from single-word utterances, such as \u2018yes\u2019 or \u2018no\u2019 answers, to much longer responses. PolyAI manually annotated them and established a word error rate (WER) below 1%, essential when dealing with fine-tuning ASR models.", "Notable accuracy improvement of an in-house customized ASR solution\nFine-tuning two in-house ASR models using only 20 hours of data already resulted in a notable mean WER improvement for the US English model, reducing it by ~8.4% compared to the best model from CSP (Table 1). The importance of choosing the right model should be noted since different CSP out-of-the-box ASR models resulted in 44.51% mean WER.\nEven more remarkable is that the WER median of in-house US English ASR solution reached 0%. This achievement was validated across various data sets, ensuring the fine-tuning was not overfitting a specific use case. This versatility allows the model to perform well across different projects where people use specific keywords, enabling the accurate understanding of particular phrases and enhancing overall median performance.\nUS English\nProvider\nModel\nLanguage\nWER Mean [%]\nWER MEdian [%]\n0\nPoly AI\nFine-Tuned\nEn-US\n20.32\n0.00\n1\nPoly AI\nFine-TUned\nEn-All\n22.19\n7.14\n2\nCSP\nBest\nEn-US\n22.22\n7.69\n9\nCSP\nWorst\nEn-US\n44.51\n33.33\nTable 1. PolyAI in-house US English ASR solution achieved better accuracy with acoustic model fine-tunings than third-party out-of-the-box ASR A similar pattern is observed with the UK English ASR solution (Table 2).\nUK English\nProvider\nModel\nLanguage\nWER Mean [%]\nWER MEdian [%]\n0\nPoly AI\nFine-Tuned\nEn-UK\n20.99\n8.33\n1\nPoly AI\nFine-TUned\nEn-All\n22.77\n10.00\n2\nCSP\nBest\nEn-UK\n25.15\n14.29\n9\nCSP\nWorst\nEn-UK\n33.46\n25.00\nTable 2. PolyAI in-house UK English ASR solution achieved better accuracy with acoustic model fine-tunings than third-party out-of-the-box ASR Only 20 hours of fine-tuning data demonstrates the potential for further fine-tuning. More importantly, the in-house fine-tuned ASR model kept the same score when evaluated on a variety of different validation datasets as when it was in its original pretrained state.", "Summary\nFor effectively automating customer interactions over the phone, fully customized ASR models play a pivotal role in solving the challenges of the voice channel, including background noise, poor call quality, and various dialects and accents. Dive deeper into PolyAI\u2019s ASR transformative journey and explore the possibilities of speech AI and NVIDIA Riva by checking out Speech AI Day sessions.\nPolyAI, part of NVIDIA Inception, provides a customer-led conversational platform for enterprises. To reimagine customer service with a best-in-class voice experience, see PolyAI\u2019s product and sign up for a free trial. Join the conversation on Speech AI in the NVIDIA Riva forum."], "document_title": "Enhancing Phone Customer Service with ASR Customization", "document_url": "https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/", "document_date": "2024-01-09T17:00:00", "document_date_modified": "2024-01-25T18:17:37", "document_full_text": "Enhancing Phone Customer Service with ASR Customization\nAt the core of understanding people correctly and having natural conversations is automatic speech recognition (ASR). To make customer-led voice assistants and automate customer service interactions over the phone, companies must solve the unique challenge of gaining a caller\u2019s trust through qualities such as understanding, empathy, and clarity.\nTelephony-bound voice is inherently challenging from a speech recognition perspective. Background noise, poor call quality, and various dialects and accents make understanding a caller\u2019s words difficult. Traditional language understanding systems have limited support for voice in general, and how a person speaks differs fundamentally from how they type or text.\nIn this post, we discuss PolyAI \u2019s exploration journey with third-party, out-of-the-box, and in-house customized NVIDIA Riva ASR solutions. The goal is to deliver voice experiences that let callers speak however they like, providing helpful and natural responses at every turn of the conversation. The in-house fine-tuned Riva ASR models resulted in notable accuracy improvement on a variety of different validation real-world customer call datasets.\nOut-of-the-box ASR challenges for effective customer interactions\nOut-of-the-box ASR tools are typically prepared for non-noisy environments and speakers who clearly enunciate and have expected accents. These systems can\u2019t predict what a caller will say, how they might say it, or their speaking tempo. While out-of-the-box solutions can be useful, they can\u2019t be tailored to specific business needs and objectives.\nTo achieve accurate voice assistants that handle customer interactions efficiently, organizations require an ASR system that can be fine-tuned to significantly improve word error rate (WER).\nAdvantages and challenges of building an in-house ASR solution\nTo truly understand people from different places, with different accents, and in noisy environments, conversational systems can use multiple ASR systems, phoneme matching, biasing keywords, and post-processing tools.\nThe machine learning team at PolyAI rigorously tested numerous ASR systems, often on multiple models, and applied spoken language understanding (SLU) principles to improve transcription accuracy (Figure 1). This work significantly improved the accuracy of speech recognition in real customer phone calls.\nOptimizing the caller experience further required the development of an in-house solution.\nDiagram shows stack components: ASR systems, phoneme matching, biasing keywords, and post-processing tools.\nFigure 1. PolyAI tech stack\nThe PolyAI tech stack enables voice assistants to accurately understand alphanumeric inputs and people from different places, with different accents, and in noisy environments.\nDeveloping an in-house solution approach offers the following advantages:\nBetter accuracy and performance with flexible fine-tuning of model parameters on extensive data and voice activity detector (VAD) adaptation for the specific ways in which people talk with the system.\nFull compliance with a bring-your-own-cloud (BYOC) approach that delivers the model and the whole conversational system to clients with zero data transfers to third-party providers.\nWith great benefits comes a unique set of challenges. Building an in-house solution requires heavy investment in the following areas:\nExpensive pretraining data: Most models require large quantities of good quality, annotated, pretraining data.\nLatency optimization: This area is often overlooked in the research process. Contrary to chat conversation, voice conversation operates on milliseconds. Every millisecond counts. Adding latency at the start of the conversation gives even less time when calling the large language models (LLM) or text-to-speech (TTS) models.\nChoosing and finetuning ASR models for an in-house solution\nAfter a substantial search for an ASR solution that addresses building in-house solution challenges, PolyAI decided to use NVIDIA Riva for the following reasons:\nCutting-edge accuracy of pretrained models trained on a substantial volume of conversational speech data.\nEnhanced accuracy with full model customization, including acoustic model customization for different accents, noisy environments, or poor audio quality.\nHigh inference performance based on tight coupling with NVIDIA Triton Inference Server and battle-tested to handle machine learning servicing.\nInitial trials with an in-house ASR model provided valuable insights into the fine-tuning process. This led to the development of a robust and flexible fine-tuning methodology, incorporating diverse validation sets to ensure optimal performance.\nConversational system for testing out-of-the-box and in-house ASR solutions\nTypical conversational systems use public switched telephone networks (PSTN) or session initiation protocol (SIP) connections to transfer calls into the tech stack.\nCall information from these systems is then sent to third-party ASR cloud service providers or in-house ASR solutions. For PolyAI\u2019s testing of ASR solutions (Figure 2), after a call is transcribed, it is sent to a PolyAI voice assistant, where natural language models generate a response. The response is then transferred back into the audio wave through in-house TTS or third-party providers.\nDiagram includes a telephony gateway, audio gateway, natural language models, and text-to-speech.\nFigure 2. PolyAI architecture for testing ASR solutions\nCreating a real-world ASR testing dataset\nPolyAI identified 20 hours of the most challenging conversations split equally between UK and US region calls to test the accuracy of third-party, out-of-the-box, and in-house ASR solutions. These were the calls with noisy environments and ones where other ASR models\u2014in-house or third-party providers\u2014had previously failed.\nThese failure calls varied from single-word utterances, such as \u2018yes\u2019 or \u2018no\u2019 answers, to much longer responses. PolyAI manually annotated them and established a word error rate (WER) below 1%, essential when dealing with fine-tuning ASR models.\nNotable accuracy improvement of an in-house customized ASR solution\nFine-tuning two in-house ASR models using only 20 hours of data already resulted in a notable mean WER improvement for the US English model, reducing it by ~8.4% compared to the best model from CSP (Table 1). The importance of choosing the right model should be noted since different CSP out-of-the-box ASR models resulted in 44.51% mean WER.\nEven more remarkable is that the WER median of in-house US English ASR solution reached 0%. This achievement was validated across various data sets, ensuring the fine-tuning was not overfitting a specific use case. This versatility allows the model to perform well across different projects where people use specific keywords, enabling the accurate understanding of particular phrases and enhancing overall median performance.\nUS English\nProvider\nModel\nLanguage\nWER Mean [%]\nWER MEdian [%]\n0\nPoly AI\nFine-Tuned\nEn-US\n20.32\n0.00\n1\nPoly AI\nFine-TUned\nEn-All\n22.19\n7.14\n2\nCSP\nBest\nEn-US\n22.22\n7.69\n9\nCSP\nWorst\nEn-US\n44.51\n33.33\nTable 1. PolyAI in-house US English ASR solution achieved better accuracy with acoustic model fine-tunings than third-party out-of-the-box ASR A similar pattern is observed with the UK English ASR solution (Table 2).\nUK English\nProvider\nModel\nLanguage\nWER Mean [%]\nWER MEdian [%]\n0\nPoly AI\nFine-Tuned\nEn-UK\n20.99\n8.33\n1\nPoly AI\nFine-TUned\nEn-All\n22.77\n10.00\n2\nCSP\nBest\nEn-UK\n25.15\n14.29\n9\nCSP\nWorst\nEn-UK\n33.46\n25.00\nTable 2. PolyAI in-house UK English ASR solution achieved better accuracy with acoustic model fine-tunings than third-party out-of-the-box ASR Only 20 hours of fine-tuning data demonstrates the potential for further fine-tuning. More importantly, the in-house fine-tuned ASR model kept the same score when evaluated on a variety of different validation datasets as when it was in its original pretrained state.\nSummary\nFor effectively automating customer interactions over the phone, fully customized ASR models play a pivotal role in solving the challenges of the voice channel, including background noise, poor call quality, and various dialects and accents. Dive deeper into PolyAI\u2019s ASR transformative journey and explore the possibilities of speech AI and NVIDIA Riva by checking out Speech AI Day sessions.\nPolyAI, part of NVIDIA Inception, provides a customer-led conversational platform for enterprises. To reimagine customer service with a best-in-class voice experience, see PolyAI\u2019s product and sign up for a free trial. Join the conversation on Speech AI in the NVIDIA Riva forum."}], "https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/": [{"text": "The article discusses the challenges of drug discovery and the importance of finding molecules with specific properties to be effective as therapeutics. It introduces NVIDIA BioNeMo's MolMIM, a generative AI model that can generate molecules based on user-defined scoring functions. MolMIM uses a numerical optimization algorithm called CMA-ES to navigate the chemical space and generate molecules that meet the desired criteria. The article also mentions the upcoming release of an update to BioNeMo's Cloud API, which will include an accelerated version of DiffDock, an AI model for predicting the three-dimensional structure of protein-ligand complexes. This update will allow researchers to predict complex structures more than 2.5 times faster than the current implementation. Researchers can access MolMIM and DiffDock through BioNeMo's Cloud API, which is set to launch on January 19, 2024. The article encourages interested individuals to sign up for early access to the API or start using BioNeMo Framework for model training.", "text_components": ["New Models MolMIM and DiffDock Power Molecule Generation and Molecular Docking in NVIDIA BioNeMo\nThe search for viable drugs is one of the most formidable challenges at the intersection of science, technology, and medicine. Mathematically, the odds of randomly stumbling across a good therapeutic candidate are staggeringly small. This is owed primarily to the astronomically large number of ways that just a handful of atoms can be connected together to make what appear at first glance to be drug-like compounds.\nUpon deeper inspection of these molecules, the vast majority would make for unsuitable therapeutics. A clinically viable drug must possess a multitude of characteristics or properties, any one of which\u2014if missing or out of range\u2014could render a drug ineffective or even toxic. Examples of properties that drug hunters seek include those that characterize a drug\u2019s binding affinity, solubility, membrane permeability, molecular weight, and stability, just to name a few.\nIn essence, the pursuit of drug candidates is a multiple-objective optimization problem.\nGenerative AI models like NVIDIA BioNeMo \u2019s MolMIM are designed to directly address the challenge of finding molecules with the right properties. Using MolMIM, researchers can generate molecules that maximize a user-specified scoring function, or oracle function for short. MolMIM performs controlled generation, navigating the learned internal representation of chemical space guided by the oracle function that the user provided.\nDiagram shows the workflow for molecule generation from seed embeddings to a molecule optimized for the Oracle function.\nFigure 1. Molecule generation workflow with MolMIM\nResearchers can define any objective function they wish, even functions that compute multiple objectives. Under the hood, MolMIM uses a gradient-free numerical optimization algorithm called CMA-ES to navigate the latent space of the model. The generative process is iterative:\nAt each iteration, MolMIM generates a batch of molecules.\nArmed with the user-provided oracle function, the properties of the molecules are calculated.\nThe latent vectors are then updated using the covariance matrix adaptation evolution strategy (CMA-ES) to generate the next batch of molecules.\nThe algorithm continues until convergence.\nThe optimized molecular generation workflow is incredibly simple using the BioNeMo Cloud API with BioNeMo\u2019s Python client library. To define an oracle function, all that you have to do is write a Python function that follows a simple call signature. The function should take a list of SMILES and return a NumPy array of scores of the same length. With the oracle function defined, an optimizer can be instantiated from the BioNeMo Python client and molecule generation can begin!\nMolMIM is available in early access through BioNeMo\u2019s Cloud API, which launches January 19, 2024.\nAlso coming to BioNeMo\u2019s Cloud API is an update accelerating DiffDock, an AI model that predicts the three-dimensional structure of a protein-ligand complex, a crucial step in the drug discovery process. With BioNeMo\u2019s updated version of DiffDock, researchers can predict the three-dimensional pose of a protein-ligand complex more than 2.5x faster than a baseline implementation on identical hardware. For more information, see Build Generative AI Pipelines for Drug Discovery with NVIDIA BioNeMo Service.\nSign up for early access to the NVIDIA BioNeMo Cloud API or get started immediately with BioNeMo Framework for model training."], "document_title": "New Models MolMIM and DiffDock Power Molecule Generation and Molecular Docking in NVIDIA BioNeMo", "document_url": "https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/", "document_date": "2024-01-08T19:00:00", "document_date_modified": "2024-01-25T18:17:38", "document_full_text": "New Models MolMIM and DiffDock Power Molecule Generation and Molecular Docking in NVIDIA BioNeMo\nThe search for viable drugs is one of the most formidable challenges at the intersection of science, technology, and medicine. Mathematically, the odds of randomly stumbling across a good therapeutic candidate are staggeringly small. This is owed primarily to the astronomically large number of ways that just a handful of atoms can be connected together to make what appear at first glance to be drug-like compounds.\nUpon deeper inspection of these molecules, the vast majority would make for unsuitable therapeutics. A clinically viable drug must possess a multitude of characteristics or properties, any one of which\u2014if missing or out of range\u2014could render a drug ineffective or even toxic. Examples of properties that drug hunters seek include those that characterize a drug\u2019s binding affinity, solubility, membrane permeability, molecular weight, and stability, just to name a few.\nIn essence, the pursuit of drug candidates is a multiple-objective optimization problem.\nGenerative AI models like NVIDIA BioNeMo \u2019s MolMIM are designed to directly address the challenge of finding molecules with the right properties. Using MolMIM, researchers can generate molecules that maximize a user-specified scoring function, or oracle function for short. MolMIM performs controlled generation, navigating the learned internal representation of chemical space guided by the oracle function that the user provided.\nDiagram shows the workflow for molecule generation from seed embeddings to a molecule optimized for the Oracle function.\nFigure 1. Molecule generation workflow with MolMIM\nResearchers can define any objective function they wish, even functions that compute multiple objectives. Under the hood, MolMIM uses a gradient-free numerical optimization algorithm called CMA-ES to navigate the latent space of the model. The generative process is iterative:\nAt each iteration, MolMIM generates a batch of molecules.\nArmed with the user-provided oracle function, the properties of the molecules are calculated.\nThe latent vectors are then updated using the covariance matrix adaptation evolution strategy (CMA-ES) to generate the next batch of molecules.\nThe algorithm continues until convergence.\nThe optimized molecular generation workflow is incredibly simple using the BioNeMo Cloud API with BioNeMo\u2019s Python client library. To define an oracle function, all that you have to do is write a Python function that follows a simple call signature. The function should take a list of SMILES and return a NumPy array of scores of the same length. With the oracle function defined, an optimizer can be instantiated from the BioNeMo Python client and molecule generation can begin!\nMolMIM is available in early access through BioNeMo\u2019s Cloud API, which launches January 19, 2024.\nAlso coming to BioNeMo\u2019s Cloud API is an update accelerating DiffDock, an AI model that predicts the three-dimensional structure of a protein-ligand complex, a crucial step in the drug discovery process. With BioNeMo\u2019s updated version of DiffDock, researchers can predict the three-dimensional pose of a protein-ligand complex more than 2.5x faster than a baseline implementation on identical hardware. For more information, see Build Generative AI Pipelines for Drug Discovery with NVIDIA BioNeMo Service.\nSign up for early access to the NVIDIA BioNeMo Cloud API or get started immediately with BioNeMo Framework for model training."}], "https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/": [{"text": "NVIDIA has announced that SDXL Turbo, LCM-LoRA, and Stable Video Diffusion models are now accelerated with NVIDIA TensorRT, allowing GeForce RTX GPU owners to generate images in real-time and save time when generating videos. SDXL Turbo can produce up to four images per second, LCM-LoRA reduces the number of sampling steps needed to produce an image, and Stable Video Diffusion runs up to 40% faster with TensorRT. These enhancements improve workflow efficiency and speed up the image and video generation process. Users can download the models on Hugging Face and access the Stable Diffusion Web UI TensorRT extension on GitHub. Additionally, developers can enter the NVIDIA Generative AI on RTX PCs Developer Contest for a chance to win prizes including a GeForce RTX 4090 GPU and a full GTC conference pass.", "text_components": ["New Stable Diffusion Models Accelerated with NVIDIA TensorRT\nAt CES, NVIDIA shared that SDXL Turbo, LCM-LoRA, and Stable Video Diffusion are all being accelerated by NVIDIA TensorRT. These enhancements allow GeForce RTX GPU owners to generate images in real-time and save minutes generating videos, vastly improving workflows.\nVideo 1. Accelerate Stable Diffusion with NVIDIA RTX GPUs", "SDXL Turbo\nSDXL Turbo achieves state-of-the-art performance with a new distillation technology, enabling single-step image generation. NVIDIA hardware, accelerated by Tensor Cores and TensorRT, can produce up to four images per second, giving you access to real-time SDXL image generation for the first time ever. For more information about non-commercial and commercial use, see the Stability AI Membership page.\nDownload the SDXL Turbo model on Hugging Face.", "LCM-LoRA\nLow-Rank Adaptation (LoRA) is a training technique for fine-tuning Stable Diffusion models. Combined with the latent consistency model (LCM), a LoRA checkpoint enables you to drastically reduce the number of sampling steps needed to produce a Stable Diffusion image. This improves speed dramatically at the cost of an image quality hit. LCM-LoRA can run ~9x faster because it uses only four steps (compared to 50 steps traditionally) and is accelerated by TensorRT optimizations.\nDownload the LCM-LoRA model on Hugging Face.", "Stable Video Diffusion\nStable Video Diffusion by Stability AI is their first foundation model for generative video based on the image model Stable Diffusion. Stable Video Diffusion runs up to 40% faster with TensorRT, potentially saving up to minutes per generation. For more information about non-commercial and commercial use, see the Stability AI Membership page.\nThe Stable Video Diffusion model will be available for download soon.", "Get started with Stable Diffusion\nTo download the Stable Diffusion Web UI TensorRT extension, see the NVIDIA/Stable-Diffusion-WebUI-TensorRT GitHub repo. The newly released update to this extension includes TensorRT acceleration for SDXL, SDXL Turbo, and LCM-LoRA.\nFor a demo showcasing the acceleration of a Stable Diffusion pipeline, see NVIDIA/TensorRT. For more information about the Automatic 1111 TensorRT extension, see TensorRT Extension for Stable Diffusion Web UI.\nHave an idea for a generative AI-powered Windows app or plugin? Enter the NVIDIA Generative AI on RTX PCs Developer Contest and you could win a GeForce RTX 4090 GPU, a full GTC in-person conference pass, and more."], "document_title": "New Stable Diffusion Models Accelerated with NVIDIA TensorRT", "document_url": "https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/", "document_date": "2024-01-08T16:31:00", "document_date_modified": "2024-01-25T18:17:39", "document_full_text": "New Stable Diffusion Models Accelerated with NVIDIA TensorRT\nAt CES, NVIDIA shared that SDXL Turbo, LCM-LoRA, and Stable Video Diffusion are all being accelerated by NVIDIA TensorRT. These enhancements allow GeForce RTX GPU owners to generate images in real-time and save minutes generating videos, vastly improving workflows.\nVideo 1. Accelerate Stable Diffusion with NVIDIA RTX GPUs\nSDXL Turbo\nSDXL Turbo achieves state-of-the-art performance with a new distillation technology, enabling single-step image generation. NVIDIA hardware, accelerated by Tensor Cores and TensorRT, can produce up to four images per second, giving you access to real-time SDXL image generation for the first time ever. For more information about non-commercial and commercial use, see the Stability AI Membership page.\nDownload the SDXL Turbo model on Hugging Face.\nLCM-LoRA\nLow-Rank Adaptation (LoRA) is a training technique for fine-tuning Stable Diffusion models. Combined with the latent consistency model (LCM), a LoRA checkpoint enables you to drastically reduce the number of sampling steps needed to produce a Stable Diffusion image. This improves speed dramatically at the cost of an image quality hit. LCM-LoRA can run ~9x faster because it uses only four steps (compared to 50 steps traditionally) and is accelerated by TensorRT optimizations.\nDownload the LCM-LoRA model on Hugging Face.\nStable Video Diffusion\nStable Video Diffusion by Stability AI is their first foundation model for generative video based on the image model Stable Diffusion. Stable Video Diffusion runs up to 40% faster with TensorRT, potentially saving up to minutes per generation. For more information about non-commercial and commercial use, see the Stability AI Membership page.\nThe Stable Video Diffusion model will be available for download soon.\nGet started with Stable Diffusion\nTo download the Stable Diffusion Web UI TensorRT extension, see the NVIDIA/Stable-Diffusion-WebUI-TensorRT GitHub repo. The newly released update to this extension includes TensorRT acceleration for SDXL, SDXL Turbo, and LCM-LoRA.\nFor a demo showcasing the acceleration of a Stable Diffusion pipeline, see NVIDIA/TensorRT. For more information about the Automatic 1111 TensorRT extension, see TensorRT Extension for Stable Diffusion Web UI.\nHave an idea for a generative AI-powered Windows app or plugin? Enter the NVIDIA Generative AI on RTX PCs Developer Contest and you could win a GeForce RTX 4090 GPU, a full GTC in-person conference pass, and more."}], "https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/": [{"text": "The article discusses the benefits of using generative AI and large language models on Windows PCs with NVIDIA RTX GPUs for various applications such as gaming, creativity, productivity, and developer experiences. It introduces developer tools like NVIDIA TensorRT-LLM for building text-based generative AI projects, including model quantization and pre-optimized models for compatibility with PC GPUs. The article also covers minimum system requirements and provides resources for building visual generative AI projects using NVIDIA TensorRT SDK. It includes developer demos and reference applications for optimizing models like Stable Diffusion and accelerating AI pipelines. The article encourages developers to explore these tools and resources to easily integrate generative AI capabilities into their applications on NVIDIA RTX PCs and participate in the NVIDIA Generative AI on NVIDIA RTX Developer Contest for a chance to win prizes.", "text_components": ["Get Started with Generative AI Development for Windows PCs with NVIDIA RTX\nGenerative AI and large language models (LLMs) are changing human-computer interaction as we know it. Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences. This post discusses several NVIDIA end-to-end developer tools for creating and deploying both text-based and visual LLM applications on NVIDIA RTX AI-ready PCs.", "Developer tools for building text-based generative AI projects\nNVIDIA TensorRT-LLM is an open-source large language model (LLM) inference library. It provides an easy-to-use Python API to define LLMs and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. NVIDIA TensorRT-LLM also contains components to create Python and C++ runtimes to run inference with the generated TensorRT engines.\nTo get started with TensorRT-LLM, visit the NVIDIA/TensorRT-LLM GitHub repo. Check out the TensorRT-LLM for Windows developer environment setup details.\nIn desktop applications, model quantization is crucial for compatibility with PC GPUs, which often have limited VRAM. TensorRT-LLM facilitates this process through its support for model quantization, enabling models to occupy a smaller memory footprint with the help of the TensorRT-LLM Quantization Toolkit.\nTo start exploring post-training quantization using TensorRT-LLM Quantization Toolkit, see the TensorRT-LLM Quantization Toolkit Installation Guide on GitHub.", "Model compatibility and pre-optimized models\nTensorRT-LLM provides the capability to define models through its Python API and is pre-equipped to support a diverse range of LLMs. Quantized model weights are available, specifically optimized for NVIDIA RTX PCs on NVIDIA GPU Cloud (NGC), enabling rapid deployment of these models.\nModel Name\nModel Location\nLlama 2 7B \u2013 Int4-AWQ\nDownload\nLlama 2 13B \u2013 Int4-AWQ\nDownload\nCode Llama 13B \u2013 Int4-AWQ\nDownload\nMistral 7B \u2013 Int4-AWQ\nDownload\nTable 1. Pre-optimized text-based LLMs that run on Windows PC for NVIDIA RTX with the NVIDIA TensorRT-LLM backend You can also build TensorRT engines for a wide variety of models supported by TensorRT-LLM. Visit TensorRT-LLM/examples on GitHub to see all supported models.", "Developer resources and reference applications\nCheck out these reference projects for more information:\nTRT-LLM RAG on Windows: This repository demonstrates a retrieval-augmented generation (RAG) pipeline, using ```llama_index``` on Windows with Llama 2 13B \u2013 int4, TensorRT-LLM, and FAISS.\nOpenAI API Spec Web Server: Drop-in replacement REST API compatible with OpenAI API spec using TensorRT-LLM as the inference backend.", "Minimum system requirements\nSupported GPU architectures for TensorRT-LLM include NVIDIA Ampere and above, with a minimum of 8GB RAM. It is suggested to use Windows 11 and above, for an optimal experience.", "Developer tools for building visual generative AI projects\nNVIDIA TensorRT SDK is a high-performance deep learning inference optimizer. It provides layer fusion, precision calibration, kernel auto-tuning, and other capabilities that significantly boost the efficiency and speed of deep learning models. This makes it indispensable for real-time applications and resource-intensive models like Stable Diffusion, substantially accelerating performance. Get started with NVIDIA TensorRT.\nFor broader guidance on how to integrate TensorRT into your applications, see Getting Started with NVIDIA AI for Your Applications. Learn how to profile your pipeline to pinpoint where optimization is critical and where minor changes can have a significant impact. Accelerate your AI pipeline by choosing a machine learning framework, and discover SDKs for video, graphic design, photography, and audio.", "Developer demos and reference applications\nCheck out these resources for more information:\nHow to Optimize Models like Stable Diffusion with TensorRT: This demo notebook showcases the acceleration of Stable Diffusion inference pipeline using TensorRT through Hugging Face.\nExample TRT Pipeline for Stable Diffusion: An example of how TensorRT can be used to accelerate the text-to-image Stable Diffusion inference pipeline.\nTensorRT Extension for Stable Diffusion Web UI: A working example of TensorRT accelerating the most popular Stable Diffusion web UI.", "Summary\nUse the resources in this post to easily add generative AI capabilities to applications powered by the existing installed base of 100 million NVIDIA RTX PCs.\nShare what you develop with the NVIDIA developer community by entering the NVIDIA Generative AI on NVIDIA RTX Developer Contest for a chance to win a GeForce RTX 4090 GPU, a full in-person NVIDIA GTC conference pass, and more."], "document_title": "Get Started with Generative AI Development for Windows PCs with NVIDIA RTX", "document_url": "https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/", "document_date": "2024-01-08T16:30:00", "document_date_modified": "2024-01-25T18:17:39", "document_full_text": "Get Started with Generative AI Development for Windows PCs with NVIDIA RTX\nGenerative AI and large language models (LLMs) are changing human-computer interaction as we know it. Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences. This post discusses several NVIDIA end-to-end developer tools for creating and deploying both text-based and visual LLM applications on NVIDIA RTX AI-ready PCs.\nDeveloper tools for building text-based generative AI projects\nNVIDIA TensorRT-LLM is an open-source large language model (LLM) inference library. It provides an easy-to-use Python API to define LLMs and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. NVIDIA TensorRT-LLM also contains components to create Python and C++ runtimes to run inference with the generated TensorRT engines.\nTo get started with TensorRT-LLM, visit the NVIDIA/TensorRT-LLM GitHub repo. Check out the TensorRT-LLM for Windows developer environment setup details.\nIn desktop applications, model quantization is crucial for compatibility with PC GPUs, which often have limited VRAM. TensorRT-LLM facilitates this process through its support for model quantization, enabling models to occupy a smaller memory footprint with the help of the TensorRT-LLM Quantization Toolkit.\nTo start exploring post-training quantization using TensorRT-LLM Quantization Toolkit, see the TensorRT-LLM Quantization Toolkit Installation Guide on GitHub.\nModel compatibility and pre-optimized models\nTensorRT-LLM provides the capability to define models through its Python API and is pre-equipped to support a diverse range of LLMs. Quantized model weights are available, specifically optimized for NVIDIA RTX PCs on NVIDIA GPU Cloud (NGC), enabling rapid deployment of these models.\nModel Name\nModel Location\nLlama 2 7B \u2013 Int4-AWQ\nDownload\nLlama 2 13B \u2013 Int4-AWQ\nDownload\nCode Llama 13B \u2013 Int4-AWQ\nDownload\nMistral 7B \u2013 Int4-AWQ\nDownload\nTable 1. Pre-optimized text-based LLMs that run on Windows PC for NVIDIA RTX with the NVIDIA TensorRT-LLM backend You can also build TensorRT engines for a wide variety of models supported by TensorRT-LLM. Visit TensorRT-LLM/examples on GitHub to see all supported models.\nDeveloper resources and reference applications\nCheck out these reference projects for more information:\nTRT-LLM RAG on Windows: This repository demonstrates a retrieval-augmented generation (RAG) pipeline, using ```llama_index``` on Windows with Llama 2 13B \u2013 int4, TensorRT-LLM, and FAISS.\nOpenAI API Spec Web Server: Drop-in replacement REST API compatible with OpenAI API spec using TensorRT-LLM as the inference backend.\nMinimum system requirements\nSupported GPU architectures for TensorRT-LLM include NVIDIA Ampere and above, with a minimum of 8GB RAM. It is suggested to use Windows 11 and above, for an optimal experience.\nDeveloper tools for building visual generative AI projects\nNVIDIA TensorRT SDK is a high-performance deep learning inference optimizer. It provides layer fusion, precision calibration, kernel auto-tuning, and other capabilities that significantly boost the efficiency and speed of deep learning models. This makes it indispensable for real-time applications and resource-intensive models like Stable Diffusion, substantially accelerating performance. Get started with NVIDIA TensorRT.\nFor broader guidance on how to integrate TensorRT into your applications, see Getting Started with NVIDIA AI for Your Applications. Learn how to profile your pipeline to pinpoint where optimization is critical and where minor changes can have a significant impact. Accelerate your AI pipeline by choosing a machine learning framework, and discover SDKs for video, graphic design, photography, and audio.\nDeveloper demos and reference applications\nCheck out these resources for more information:\nHow to Optimize Models like Stable Diffusion with TensorRT: This demo notebook showcases the acceleration of Stable Diffusion inference pipeline using TensorRT through Hugging Face.\nExample TRT Pipeline for Stable Diffusion: An example of how TensorRT can be used to accelerate the text-to-image Stable Diffusion inference pipeline.\nTensorRT Extension for Stable Diffusion Web UI: A working example of TensorRT accelerating the most popular Stable Diffusion web UI.\nSummary\nUse the resources in this post to easily add generative AI capabilities to applications powered by the existing installed base of 100 million NVIDIA RTX PCs.\nShare what you develop with the NVIDIA developer community by entering the NVIDIA Generative AI on NVIDIA RTX Developer Contest for a chance to win a GeForce RTX 4090 GPU, a full in-person NVIDIA GTC conference pass, and more."}], "https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/": [{"text": "Convai is a developer platform that allows creators to design advanced non-playable characters (NPCs) with multimodal perception abilities for both virtual and real worlds. By leveraging NVIDIA technologies such as ACE, Riva, and Audio2Face, Convai enables lifelike NPCs with context-aware responsiveness, emotional awareness, and dynamic interactions. These AI-driven characters can understand human commands, interact with objects, engage in conversations, and perform actions based on player interactions. Convai also allows NPCs to display emotions through facial expressions, voices, and gestures, adding a layer of realism and immersion to gaming experiences. The platform has attracted over 15,000 users and offers tutorials and early access programs for creators interested in developing characters with Convai. Overall, Convai's innovative features and technologies are reshaping NPC interactions and enhancing storytelling in gaming.", "text_components": ["Spotlight: Convai Reinvents Non-Playable Character Interactions\nConvai is a versatile developer platform for designing characters with advanced multimodal perception abilities. These characters are designed to integrate seamlessly into both the virtual and real worlds.\nWhether you\u2019re a creator, game designer, or developer, Convai enables you to quickly modify a non-playable character (NPC), from backstory and knowledge to voice and personality. You can do this through the playground user interface or programmatically through the API. Within minutes, creators can witness their characters responding in their own unique style while embodying a spatially-aware NPC capable of performing a wide range of actions.\nConvai believes in pushing the state of the art and enabling its users to experience the best in gaming. Some of the fundamental blocks for realistic AI NPCs are facial animations, lip synchronization, and emotion control for the character\u2019s voices, all of which needs to be generated in real time during gameplay.\nGraphic of examples of features supported by Convai technologies, including Ask Anything, Digital Humans, and Intelligent NPC.\nFigure 1. Examples of features supported by Convai technologies\nConvai tapped into NVIDIA Avatar Cloud Engine (ACE), which delivers AI models and microservices that developers can integrate into their pipelines. They used NVIDIA Audio2Face to power the facial animations of their characters, and NVIDIA Riva for speech-to-text and text-to-speech.\nThe team is currently planning to leverage NVIDIA NeMo and NVIDIA Triton Inference Server, which is part of the NVIDIA AI Enterprise software platform, to develop and deploy its custom LLMs with the lowest possible latency.\n\u201cGenerative AI-powered characters in virtual worlds unlock various use cases and experiences that were previously impossible,\u201d said Convai Founder and CEO Purnendu Mukherjee. \u201cConvai is leveraging NVIDIA ACE technologies such as Riva automatic speech recognition and Audio2Face to enable lifelike non-playable characters with low latency response times and high fidelity natural animation.\u201d\nConvai also developed an extension in NVIDIA Omniverse that enables users to connect their 3D character assets with intelligent conversation agents. Developers can use this extension to take the backstory and voice they have created in Convai and add it to a customizable 3D character in Omniverse. Once in Omniverse, users can have a conversation with the animated character using their computer\u2019s microphone input.\nVideo 1. Learn how to use the Convai extension for NVIDIA Omniverse", "Heighten immersive interactions with AI-driven Convai characters\nTraditional NPCs in gaming suffer from many limitations that directly impact the overall player experience. Examples include rigid conversational patterns and a lack of contextual awareness. To address these limitations, Convai has developed a proprietary AI-driven suite of technologies.", "Context-aware responsiveness\nThis innovative approach consists of enabling adaptive and context-aware NPC responsiveness. Characters are equipped with spatial cognition and scene understanding, enabling dynamic comprehension of their environment. Consequently, NPCs perceive situational elements, objects, characters, and their corresponding states in real time.\nVisual from the NVIDIA Kairos demo of labeled items in the ramen shop that AI NPCs are aware of.\nFigure 2. Environment-aware NPCs\nThis heightened awareness, alongside understanding human commands through NVIDIA Riva ASR, empowers NPCs to identify and interact with objects, engage in conversations about their surroundings, and perform actions based on their motivation or as prompted by the player\u2019s conversations.\nOther known issues with traditional NPC interactions often come from rigid scripts and predictable behavior. Convai champions more organic interactions by integrating LLM-driven conversation systems with behavior trees that define the default behaviors of the characters. This leads to more dynamic engagements and intelligent responses, while staying true to the original story and gameplay the designer intended.\nVideo 2. NVIDIA ACE and Convai are helping developers bring digital avatars to life with generative AI Characters feel more lifelike if conversations have consequences involving not only responses but also actions. Examples include interacting with the environment or other characters. Convai NPCs can navigate complex verbal instructions and convert them to natural actions in the game engine. Need to grab a snack from the vending machine? No problem\u2014Convai\u2019s complex action sequences enable NPCs to easily handle multistep tasks.\nTo get started creating characters that respond dynamically to various actions, check out the Convai tutorials.", "Emotional awareness\nTo make the gaming experience even more immersive, Convai characters can perceive emotions and emote naturally. NPCs are emotionally aware, reflecting their feelings through pertinent facial expressions, voices, and gestures. This adds a layer of realism and immersion, making your NPCs feel genuinely alive as they respond with emotional awareness, which adapts throughout the conversation.\nTo learn more, check out the Convai tutorial on facial expressions. You can also apply to join the Convai early action program for emotional voices.\nNPCs are not limited to interacting with the player\u2014they can also interact with fellow NPCs, making the world come alive with real-time generated dialogue content. Creators can decide the topic and the flow of the conversation between the NPCs, making the conversation pertinent to the world they are crafting, while leaving some flexibility in the storyline for conversations personalized to the player\u2019s experience. For more details, check out the Convai tutorial video.", "Summary\nConvai\u2019s forward-thinking features and approaches transform NPC interactions and elevate gaming with nuanced storytelling, diverse personalities, emotional responsiveness, and contextual accuracy. Currently, more than 15K people have signed up to use Convai. The latest Convai technology also opens the door for multiple emerging use cases, such as building brand representatives and tutors. NVIDIA showcased Convai and the latest NVIDIA ACE technologies in the NVIDIA Kairos demo that debuted at CES 2024.\nLearn more about Convai technologies."], "document_title": "Spotlight: Convai Reinvents Non-Playable Character Interactions", "document_url": "https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/", "document_date": "2024-01-08T16:30:00", "document_date_modified": "2024-01-25T18:17:40", "document_full_text": "Spotlight: Convai Reinvents Non-Playable Character Interactions\nConvai is a versatile developer platform for designing characters with advanced multimodal perception abilities. These characters are designed to integrate seamlessly into both the virtual and real worlds.\nWhether you\u2019re a creator, game designer, or developer, Convai enables you to quickly modify a non-playable character (NPC), from backstory and knowledge to voice and personality. You can do this through the playground user interface or programmatically through the API. Within minutes, creators can witness their characters responding in their own unique style while embodying a spatially-aware NPC capable of performing a wide range of actions.\nConvai believes in pushing the state of the art and enabling its users to experience the best in gaming. Some of the fundamental blocks for realistic AI NPCs are facial animations, lip synchronization, and emotion control for the character\u2019s voices, all of which needs to be generated in real time during gameplay.\nGraphic of examples of features supported by Convai technologies, including Ask Anything, Digital Humans, and Intelligent NPC.\nFigure 1. Examples of features supported by Convai technologies\nConvai tapped into NVIDIA Avatar Cloud Engine (ACE), which delivers AI models and microservices that developers can integrate into their pipelines. They used NVIDIA Audio2Face to power the facial animations of their characters, and NVIDIA Riva for speech-to-text and text-to-speech.\nThe team is currently planning to leverage NVIDIA NeMo and NVIDIA Triton Inference Server, which is part of the NVIDIA AI Enterprise software platform, to develop and deploy its custom LLMs with the lowest possible latency.\n\u201cGenerative AI-powered characters in virtual worlds unlock various use cases and experiences that were previously impossible,\u201d said Convai Founder and CEO Purnendu Mukherjee. \u201cConvai is leveraging NVIDIA ACE technologies such as Riva automatic speech recognition and Audio2Face to enable lifelike non-playable characters with low latency response times and high fidelity natural animation.\u201d\nConvai also developed an extension in NVIDIA Omniverse that enables users to connect their 3D character assets with intelligent conversation agents. Developers can use this extension to take the backstory and voice they have created in Convai and add it to a customizable 3D character in Omniverse. Once in Omniverse, users can have a conversation with the animated character using their computer\u2019s microphone input.\nVideo 1. Learn how to use the Convai extension for NVIDIA Omniverse\nHeighten immersive interactions with AI-driven Convai characters\nTraditional NPCs in gaming suffer from many limitations that directly impact the overall player experience. Examples include rigid conversational patterns and a lack of contextual awareness. To address these limitations, Convai has developed a proprietary AI-driven suite of technologies.\nContext-aware responsiveness\nThis innovative approach consists of enabling adaptive and context-aware NPC responsiveness. Characters are equipped with spatial cognition and scene understanding, enabling dynamic comprehension of their environment. Consequently, NPCs perceive situational elements, objects, characters, and their corresponding states in real time.\nVisual from the NVIDIA Kairos demo of labeled items in the ramen shop that AI NPCs are aware of.\nFigure 2. Environment-aware NPCs\nThis heightened awareness, alongside understanding human commands through NVIDIA Riva ASR, empowers NPCs to identify and interact with objects, engage in conversations about their surroundings, and perform actions based on their motivation or as prompted by the player\u2019s conversations.\nOther known issues with traditional NPC interactions often come from rigid scripts and predictable behavior. Convai champions more organic interactions by integrating LLM-driven conversation systems with behavior trees that define the default behaviors of the characters. This leads to more dynamic engagements and intelligent responses, while staying true to the original story and gameplay the designer intended.\nVideo 2. NVIDIA ACE and Convai are helping developers bring digital avatars to life with generative AI Characters feel more lifelike if conversations have consequences involving not only responses but also actions. Examples include interacting with the environment or other characters. Convai NPCs can navigate complex verbal instructions and convert them to natural actions in the game engine. Need to grab a snack from the vending machine? No problem\u2014Convai\u2019s complex action sequences enable NPCs to easily handle multistep tasks.\nTo get started creating characters that respond dynamically to various actions, check out the Convai tutorials.\nEmotional awareness\nTo make the gaming experience even more immersive, Convai characters can perceive emotions and emote naturally. NPCs are emotionally aware, reflecting their feelings through pertinent facial expressions, voices, and gestures. This adds a layer of realism and immersion, making your NPCs feel genuinely alive as they respond with emotional awareness, which adapts throughout the conversation.\nTo learn more, check out the Convai tutorial on facial expressions. You can also apply to join the Convai early action program for emotional voices.\nNPCs are not limited to interacting with the player\u2014they can also interact with fellow NPCs, making the world come alive with real-time generated dialogue content. Creators can decide the topic and the flow of the conversation between the NPCs, making the conversation pertinent to the world they are crafting, while leaving some flexibility in the storyline for conversations personalized to the player\u2019s experience. For more details, check out the Convai tutorial video.\nSummary\nConvai\u2019s forward-thinking features and approaches transform NPC interactions and elevate gaming with nuanced storytelling, diverse personalities, emotional responsiveness, and contextual accuracy. Currently, more than 15K people have signed up to use Convai. The latest Convai technology also opens the door for multiple emerging use cases, such as building brand representatives and tutors. NVIDIA showcased Convai and the latest NVIDIA ACE technologies in the NVIDIA Kairos demo that debuted at CES 2024.\nLearn more about Convai technologies."}], "https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/": [{"text": "The article discusses the importance of incorporating large language models (LLMs) into various applications and the benefits of running LLMs locally on Windows PCs with NVIDIA RTX Systems. It highlights the announcement of developer tools by NVIDIA at CES 2024 to accelerate LLM inference and development on NVIDIA RTX Systems. These tools include support for popular community models, native connectors for application frameworks, and an OpenAI Chat API wrapper for easy switching between running LLM applications in the cloud or locally on Windows PCs. The article also introduces two open-source developer reference applications: a retrieval augmented generation project and a continue.dev Visual Studio Code extension. Running LLMs locally offers advantages in terms of cost, always-on availability, performance, and data privacy. The article details developer workflows for LLMs on NVIDIA RTX, including accessing pre-optimized models, training and customizing models, and optimizing models for best performance with TensorRT-LLM. It encourages developers to get started with LLM-based applications on Windows PCs with NVIDIA RTX and participate in the NVIDIA Generative AI developer contest to win prizes.", "text_components": ["Supercharging LLM Applications on Windows PCs with NVIDIA RTX Systems\nLarge language models (LLMs) are fundamentally changing the way we interact with computers. These models are being incorporated into a wide range of applications, from internet search to office productivity tools. They are advancing real-time content generation, text summarization, customer service chatbots, and question-answering use cases.\nToday, LLM-powered applications are running predominantly in the cloud. However, many use cases that would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.\nAT CES 2024, NVIDIA announced several developer tools to accelerate LLM inference and development on NVIDIA RTX Systems for Windows PCs. You can now use NVIDIA end-to-end developer tools to create and deploy LLM applications on NVIDIA RTX AI-ready PCs.", "Support for community models and native connectors\nNVIDIA just announced optimized support for popular community models, including Phi-2, in addition to existing support for Llama2, Mistral-7B, and Code Llama on NVIDIA RTX systems. These models provide extensive developer choice, along with best-in-class performance using the NVIDIA TensorRT-LLM inference backend.\nNVIDIA collaborated with the open-source community to develop native connectors for TensorRT-LLM to popular application frameworks such as LlamaIndex. These connectors offer seamless integration on Windows PCs to commonly used application development tools. View the example for the LlamaIndex implementation of the connector here.\nWe\u2019ve also developed an OpenAI Chat API wrapper for TensorRT-LLM so that you can easily switch between running LLM applications on the cloud or on local Windows PCs by just changing one line of code. Now, you can use a similar workflow with the same popular community frameworks, whether they are designing applications in the cloud or on a local PC with NVIDIA RTX.\nThese latest advancements can now all be accessed through two recently launched open-source developer reference applications:\nA retrieval augmented generation (RAG) project running entirely on Windows PC with an NVIDIA RTX GPU and using TensorRT-LLM and LlamaIndex.\nA reference project that runs the popular continue.dev plugin entirely on a local Windows PC, with a web server for OpenAI Chat API compatibility.", "RAG on Windows using TensorRT-LLM and LlamaIndex\nThe RAG pipeline consists of the Llama-2 13B model, TensorRT-LLM, LlamaIndex, and the FAISS vector search library. You can now easily talk to your data with this reference application. Figure 1 shows a dataset that consists of NVIDIA GeForce News.\nGet started with this application now.\nvideo showing the retrieval augmented generation project dashboard\nVideo 1. A retrieval augmented generation reference application running entirely on Windows PC with an NVIDIA RTX System", "Continue.dev Visual Studio Code extension on PC with CodeLlama-13B\nOriginally, the continue.dev plugin was designed to provide LLM-powered code assistance using ChatGPT in the cloud. It works natively with the Visual Studio Code integrated development environment. Using the OpenAI Chat API wrapper for TensorRT-LLM, with just one line of code change, this plugin now uses a Code Llama-13B model running locally on an NVIDIA RTX-enabled PC. This offers an easy path for fast, local LLM inferencing.\nTry this reference project on GitHub now.", "Benefits of running LLMs locally\nRunning LLMs locally on PCs offers several advantages:\nCost: No cloud-hosted API or infrastructure costs for LLM inference. Directly access your compute resources.\nAlways-on: Availability of LLM capabilities everywhere you go, without relying on high-bandwidth network connectivity.\nPerformance: Latency is independent of network quality, offering lower latency as the entire model is running locally. This can be important for real-time use cases such as gaming or video conferencing. NVIDIA RTX offers the fastest PC accelerator with up to 1300 TOPS.\nData privacy: Private and proprietary data can always stay on the device.\nWith over 100M systems shipped, NVIDIA RTX offers a large installed base of users for new LLM-powered applications.", "Developer workflows for LLMs on NVIDIA RTX\nYou can now seamlessly run LLMs on NVIDIA RTX AI-ready PCs with the following options:\nAccess pre-optimized models on HuggingFace, NGC, and NVIDIA AI Foundations.\nTrain or customize models on custom data in NVIDIA DGX Cloud with NVIDIA NeMo Framework.\nQuantize and optimize the models for best performance on NVIDIA RTX with TensorRT-LLM.\nThis workflow is powered by the NVIDIA AI platform, alongside popular development tools such as NVIDIA AI Workbench to seamlessly migrate between cloud and PC.\nAI Workbench provides you with the flexibility to collaborate on and migrate generative AI projects between GPU-enabled environments in just a few clicks. Projects can start locally on a PC or workstation and can then be scaled out anywhere for training: data center, public cloud, or NVIDIA DGX Cloud. You can then bring models back to a local NVIDIA RTX system for inference and lightweight customization with TensorRT-LLM.\nAI Workbench will be released as a beta later this month.", "Get started\nWith the latest updates, you can now use popular community models and frameworks in the same workflow to build applications that run either in the cloud or locally on Windows PC with NVIDIA RTX. Easily add LLM capabilities to applications powered by the existing 100M installed base of NVIDIA RTX PCs.\nFor more information about developing LLM-based applications and projects now, see Get Started with Generative AI Development on Windows PC with NVIDIA RTX Systems.\nHave an idea for a generative AI-powered Windows app or plugin? Enter the NVIDIA Generative AI on NVIDIA RTX developer contest and you could win a GeForce RTX 4090 GPU, a full GTC in-person conference pass, and more."], "document_title": "Supercharging LLM Applications on Windows PCs with NVIDIA RTX Systems", "document_url": "https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/", "document_date": "2024-01-08T16:30:00", "document_date_modified": "2024-01-25T18:17:40", "document_full_text": "Supercharging LLM Applications on Windows PCs with NVIDIA RTX Systems\nLarge language models (LLMs) are fundamentally changing the way we interact with computers. These models are being incorporated into a wide range of applications, from internet search to office productivity tools. They are advancing real-time content generation, text summarization, customer service chatbots, and question-answering use cases.\nToday, LLM-powered applications are running predominantly in the cloud. However, many use cases that would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.\nAT CES 2024, NVIDIA announced several developer tools to accelerate LLM inference and development on NVIDIA RTX Systems for Windows PCs. You can now use NVIDIA end-to-end developer tools to create and deploy LLM applications on NVIDIA RTX AI-ready PCs.\nSupport for community models and native connectors\nNVIDIA just announced optimized support for popular community models, including Phi-2, in addition to existing support for Llama2, Mistral-7B, and Code Llama on NVIDIA RTX systems. These models provide extensive developer choice, along with best-in-class performance using the NVIDIA TensorRT-LLM inference backend.\nNVIDIA collaborated with the open-source community to develop native connectors for TensorRT-LLM to popular application frameworks such as LlamaIndex. These connectors offer seamless integration on Windows PCs to commonly used application development tools. View the example for the LlamaIndex implementation of the connector here.\nWe\u2019ve also developed an OpenAI Chat API wrapper for TensorRT-LLM so that you can easily switch between running LLM applications on the cloud or on local Windows PCs by just changing one line of code. Now, you can use a similar workflow with the same popular community frameworks, whether they are designing applications in the cloud or on a local PC with NVIDIA RTX.\nThese latest advancements can now all be accessed through two recently launched open-source developer reference applications:\nA retrieval augmented generation (RAG) project running entirely on Windows PC with an NVIDIA RTX GPU and using TensorRT-LLM and LlamaIndex.\nA reference project that runs the popular continue.dev plugin entirely on a local Windows PC, with a web server for OpenAI Chat API compatibility.\nRAG on Windows using TensorRT-LLM and LlamaIndex\nThe RAG pipeline consists of the Llama-2 13B model, TensorRT-LLM, LlamaIndex, and the FAISS vector search library. You can now easily talk to your data with this reference application. Figure 1 shows a dataset that consists of NVIDIA GeForce News.\nGet started with this application now.\nvideo showing the retrieval augmented generation project dashboard\nVideo 1. A retrieval augmented generation reference application running entirely on Windows PC with an NVIDIA RTX System\nContinue.dev Visual Studio Code extension on PC with CodeLlama-13B\nOriginally, the continue.dev plugin was designed to provide LLM-powered code assistance using ChatGPT in the cloud. It works natively with the Visual Studio Code integrated development environment. Using the OpenAI Chat API wrapper for TensorRT-LLM, with just one line of code change, this plugin now uses a Code Llama-13B model running locally on an NVIDIA RTX-enabled PC. This offers an easy path for fast, local LLM inferencing.\nTry this reference project on GitHub now.\nBenefits of running LLMs locally\nRunning LLMs locally on PCs offers several advantages:\nCost: No cloud-hosted API or infrastructure costs for LLM inference. Directly access your compute resources.\nAlways-on: Availability of LLM capabilities everywhere you go, without relying on high-bandwidth network connectivity.\nPerformance: Latency is independent of network quality, offering lower latency as the entire model is running locally. This can be important for real-time use cases such as gaming or video conferencing. NVIDIA RTX offers the fastest PC accelerator with up to 1300 TOPS.\nData privacy: Private and proprietary data can always stay on the device.\nWith over 100M systems shipped, NVIDIA RTX offers a large installed base of users for new LLM-powered applications.\nDeveloper workflows for LLMs on NVIDIA RTX\nYou can now seamlessly run LLMs on NVIDIA RTX AI-ready PCs with the following options:\nAccess pre-optimized models on HuggingFace, NGC, and NVIDIA AI Foundations.\nTrain or customize models on custom data in NVIDIA DGX Cloud with NVIDIA NeMo Framework.\nQuantize and optimize the models for best performance on NVIDIA RTX with TensorRT-LLM.\nThis workflow is powered by the NVIDIA AI platform, alongside popular development tools such as NVIDIA AI Workbench to seamlessly migrate between cloud and PC.\nAI Workbench provides you with the flexibility to collaborate on and migrate generative AI projects between GPU-enabled environments in just a few clicks. Projects can start locally on a PC or workstation and can then be scaled out anywhere for training: data center, public cloud, or NVIDIA DGX Cloud. You can then bring models back to a local NVIDIA RTX system for inference and lightweight customization with TensorRT-LLM.\nAI Workbench will be released as a beta later this month.\nGet started\nWith the latest updates, you can now use popular community models and frameworks in the same workflow to build applications that run either in the cloud or locally on Windows PC with NVIDIA RTX. Easily add LLM capabilities to applications powered by the existing 100M installed base of NVIDIA RTX PCs.\nFor more information about developing LLM-based applications and projects now, see Get Started with Generative AI Development on Windows PC with NVIDIA RTX Systems.\nHave an idea for a generative AI-powered Windows app or plugin? Enter the NVIDIA Generative AI on NVIDIA RTX developer contest and you could win a GeForce RTX 4090 GPU, a full GTC in-person conference pass, and more."}], "https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/": [{"text": "NVIDIA's ACE microservices, including Audio2Face and Riva ASR, are revolutionizing the creation of lifelike digital avatars in games. These technologies allow for dynamic character responses to gamer input, with improved emotional support, lip sync, and language support. Partnering with Convai, NVIDIA showcased the Kairos demo, demonstrating the potential for AI-powered NPCs to have open-ended conversations, spatial awareness, and interactions with the game world. Top developers in the gaming industry, such as Tencent and NetEase Games, are embracing NVIDIA ACE to create digital avatars with individual, lifelike personalities. These advancements in AI technology are making games more intelligent and immersive, leading to new gameplay experiences previously thought impossible. Developers can access and deploy these technologies through NVIDIA AI Enterprise, enabling the integration of intelligent NPCs into their games today.", "text_components": ["Building Lifelike Digital Avatars with NVIDIA ACE Microservices\nGenerative AI technologies are revolutionizing how games are produced and played. Game developers are exploring how these technologies can accelerate their content pipelines and provide new gameplay experiences previously thought impossible. One area of focus, digital avatars, will have a transformative impact on how gamers will interact with non-playable characters (NPCs).\nHistorically, NPCs have predetermined responses and facial animations, where players can only communicate within a limited set of options. These player interactions tend to be transactional, short-lived, and oftentimes skipped.\nHowever, with NVIDIA Avatar Cloud Engine (ACE), middleware, tool, and game developers can take four state-of-the-art AI models and implement them into an end-to-end digital avatar solution. The ACE models use a flexible combination of local and cloud resources that transform gamer input into a dynamic character response. The models include the following:\nNVIDIA Riva Automatic Speech Recognition (Riva ASR) for transcribing human speech.\nNVIDIA Riva Text-to-Speech (Riva TTS) to generate audible speech.\nNVIDIA Audio2Face (A2F) to generate facial expressions and lip movements.\nNVIDIA NeMo Large Language Model (NeMo LLM) to understand player text and transcribed voice and generate a response.\nNVIDIA announced that the A2F and Riva ASR microservices are now available for middleware, tool, and game developers looking to enhance game studio NPCs.\nAnnouncement slide for Audio2Face and Riva ASR with simplified diagrams showing how audio input is transformed into a visual character speaking and transcribed text, respectively.\nFigure 1. New NVIDIA ACE microservices", "Explore microservices through NVIDIA AI Foundry\nIf you have an NVIDIA AI Enterprise license, you can access the microservices now and then deploy them on DGX Cloud or any CSP or private cloud.\nThe A2F microservice now has emotional support and quality improvements including lip sync. NVIDIA Riva ASR supports more languages than ever\u2014Italian, EU Spanish, German, and Mandarin\u2014with the overall accuracy being much improved.\nLater this month, you can go to NVIDIA AI Foundation Models to explore, experience, and evaluate these available AI models directly from a browser or through API endpoints running a fully accelerated stack. You can deploy anywhere with NVIDIA AI Enterprise.", "Kairos demo evolves with new technologies from Convai\nIn collaboration with Convai, NVIDIA showed the latest version of the Kairos demo to showcase how next-generation AI NPCs will revolutionize gaming. Convai is an NPC developer platform that makes it easy for you to enable characters in 3D worlds to have human-like conversation, perception, and action abilities.\n\u201cGenerative-AI-powered characters in virtual worlds unlock various use cases and experiences that were previously impossible. Convai is leveraging Riva ASR and A2F to enable lifelike non-playable characters (NPC) with low latency response times and high fidelity natural animation,\u201d said Purnendu Mukherjee, founder and CEO at Convai.\nOpen-ended conversations with NPCs open up a world of possibilities for interactivity in games. However, conversations should have consequences that could lead to potential actions. To carry out actions from NPCs, they must be aware of the world around them and be able to interact dynamically.\nWith our partner Convai and their latest releases, we take our collaborative demo to the next level, enabling these AI NPCs with the following new features:\nSpatial awareness: Enables game characters to interact and describe the world throughout conversations.\nActions: Enables game characters to interact with items in the game world based on the conversation, for example, delivering a bottle of sake when requested.\nNPC-to-NPC Interaction: Enables game characters to have generated conversations without the player\u2019s interaction.\nConvai has integrated the new NVIDIA ACE microservices, Audio2Face and Riva ASR. Game characters now get improved lip sync, better expression, and accurate speech detection when listening to the player.\nDiagram shows Convai character API workflow with NVIDIA microservices and the Convai Universal Engine plugin.\nFigure 2. NVIDIA ACE in the Convai pipeline\nHere\u2019s a look at the new Kairos demo.\nVideo 1. Kairos Demo Convai is working closely with NVIDIA to deliver the next generation of AI-powered digital characters. For more information about getting started with their platform, see Playground Walkthrough.", "Top digital avatar developers embrace NVIDIA ACE\nNVIDIA is working with top developers in the gaming ecosystem to create digital avatars that use ACE technologies, including Charisma.AI, Inworld, miHoYo, NetEase Games, OurPalm, Tencent, Ubisoft, and UneeQ.\n\u201cThis is a milestone moment for AI in games,\u201d said Tencent Games. \u201cNVIDIA ACE and Tencent Games will help lay the foundation that will bring digital avatars with individual, lifelike personalities and interactions to video games.\u201d\n\u201cFor years NVIDIA has been the pied piper of gaming technologies, delivering new and innovative ways to create games. NVIDIA is making games more intelligent and playable through the adoption of gaming AI technologies, which ultimately creates a more immersive experience,\u201d said Zhipeng Hu, senior vice president of NetEase and head of the LeiHuo business group.\nLearn more about the NVIDIA Audio2Face and NVIDIA Riva ASR microservices, explore the technologies on NVIDIA AI Foundation Models, and deploy anywhere with NVIDIA AI Enterprise to begin integrating intelligent NPCs into your solutions today."], "document_title": "Building Lifelike Digital Avatars with NVIDIA ACE Microservices", "document_url": "https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/", "document_date": "2024-01-08T16:30:00", "document_date_modified": "2024-01-25T18:17:41", "document_full_text": "Building Lifelike Digital Avatars with NVIDIA ACE Microservices\nGenerative AI technologies are revolutionizing how games are produced and played. Game developers are exploring how these technologies can accelerate their content pipelines and provide new gameplay experiences previously thought impossible. One area of focus, digital avatars, will have a transformative impact on how gamers will interact with non-playable characters (NPCs).\nHistorically, NPCs have predetermined responses and facial animations, where players can only communicate within a limited set of options. These player interactions tend to be transactional, short-lived, and oftentimes skipped.\nHowever, with NVIDIA Avatar Cloud Engine (ACE), middleware, tool, and game developers can take four state-of-the-art AI models and implement them into an end-to-end digital avatar solution. The ACE models use a flexible combination of local and cloud resources that transform gamer input into a dynamic character response. The models include the following:\nNVIDIA Riva Automatic Speech Recognition (Riva ASR) for transcribing human speech.\nNVIDIA Riva Text-to-Speech (Riva TTS) to generate audible speech.\nNVIDIA Audio2Face (A2F) to generate facial expressions and lip movements.\nNVIDIA NeMo Large Language Model (NeMo LLM) to understand player text and transcribed voice and generate a response.\nNVIDIA announced that the A2F and Riva ASR microservices are now available for middleware, tool, and game developers looking to enhance game studio NPCs.\nAnnouncement slide for Audio2Face and Riva ASR with simplified diagrams showing how audio input is transformed into a visual character speaking and transcribed text, respectively.\nFigure 1. New NVIDIA ACE microservices\nExplore microservices through NVIDIA AI Foundry\nIf you have an NVIDIA AI Enterprise license, you can access the microservices now and then deploy them on DGX Cloud or any CSP or private cloud.\nThe A2F microservice now has emotional support and quality improvements including lip sync. NVIDIA Riva ASR supports more languages than ever\u2014Italian, EU Spanish, German, and Mandarin\u2014with the overall accuracy being much improved.\nLater this month, you can go to NVIDIA AI Foundation Models to explore, experience, and evaluate these available AI models directly from a browser or through API endpoints running a fully accelerated stack. You can deploy anywhere with NVIDIA AI Enterprise.\nKairos demo evolves with new technologies from Convai\nIn collaboration with Convai, NVIDIA showed the latest version of the Kairos demo to showcase how next-generation AI NPCs will revolutionize gaming. Convai is an NPC developer platform that makes it easy for you to enable characters in 3D worlds to have human-like conversation, perception, and action abilities.\n\u201cGenerative-AI-powered characters in virtual worlds unlock various use cases and experiences that were previously impossible. Convai is leveraging Riva ASR and A2F to enable lifelike non-playable characters (NPC) with low latency response times and high fidelity natural animation,\u201d said Purnendu Mukherjee, founder and CEO at Convai.\nOpen-ended conversations with NPCs open up a world of possibilities for interactivity in games. However, conversations should have consequences that could lead to potential actions. To carry out actions from NPCs, they must be aware of the world around them and be able to interact dynamically.\nWith our partner Convai and their latest releases, we take our collaborative demo to the next level, enabling these AI NPCs with the following new features:\nSpatial awareness: Enables game characters to interact and describe the world throughout conversations.\nActions: Enables game characters to interact with items in the game world based on the conversation, for example, delivering a bottle of sake when requested.\nNPC-to-NPC Interaction: Enables game characters to have generated conversations without the player\u2019s interaction.\nConvai has integrated the new NVIDIA ACE microservices, Audio2Face and Riva ASR. Game characters now get improved lip sync, better expression, and accurate speech detection when listening to the player.\nDiagram shows Convai character API workflow with NVIDIA microservices and the Convai Universal Engine plugin.\nFigure 2. NVIDIA ACE in the Convai pipeline\nHere\u2019s a look at the new Kairos demo.\nVideo 1. Kairos Demo Convai is working closely with NVIDIA to deliver the next generation of AI-powered digital characters. For more information about getting started with their platform, see Playground Walkthrough.\nTop digital avatar developers embrace NVIDIA ACE\nNVIDIA is working with top developers in the gaming ecosystem to create digital avatars that use ACE technologies, including Charisma.AI, Inworld, miHoYo, NetEase Games, OurPalm, Tencent, Ubisoft, and UneeQ.\n\u201cThis is a milestone moment for AI in games,\u201d said Tencent Games. \u201cNVIDIA ACE and Tencent Games will help lay the foundation that will bring digital avatars with individual, lifelike personalities and interactions to video games.\u201d\n\u201cFor years NVIDIA has been the pied piper of gaming technologies, delivering new and innovative ways to create games. NVIDIA is making games more intelligent and playable through the adoption of gaming AI technologies, which ultimately creates a more immersive experience,\u201d said Zhipeng Hu, senior vice president of NetEase and head of the LeiHuo business group.\nLearn more about the NVIDIA Audio2Face and NVIDIA Riva ASR microservices, explore the technologies on NVIDIA AI Foundation Models, and deploy anywhere with NVIDIA AI Enterprise to begin integrating intelligent NPCs into your solutions today."}], "https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/": [{"text": "Many CUDA applications running on multi-GPU platforms only use one GPU, leading to unnecessary initialization of all GPUs. To improve performance, unwanted GPUs can be isolated from the CUDA process using tools like ```cgroups``` on Linux systems. This ensures that only needed GPUs are made available to the CUDA process, reducing initialization time. The article discusses low-level and higher-level approaches to GPU isolation using ```cgroups```, as well as the use of ```CUDA_VISIBLE_DEVICES``` for similar functionality. The bubblewrap utility can also be used for GPU isolation. Performance tests show significant improvements in CUDA initialization times when only required GPUs are exposed to the process compared to when all GPUs are available. Overall, GPU isolation using ```cgroups``` can be beneficial in scenarios where not all GPUs on the system are needed for a particular CUDA application.", "text_components": ["Improving CUDA Initialization Times Using cgroups in Certain Scenarios\nMany CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.\nThis post discusses the various methods to accomplish this and their performance benefits.", "GPU isolation\nGPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach.\nAnother method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.", "Isolating GPUs using cgroups V1\nControl groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it.\nThe following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post.\n```\n# Create a mountpoint for the cgroup hierarchy as root\n$> cd /mnt\n$> mkdir cgroupV1Device\n\n# Use mount command to mount the hierarchy and attach the device subsystem to it\n$> mount -t cgroup -o devices devices cgroupV1Device\n$> cd cgroupV1Device\n# Now create a gpu subgroup directory to restrict/allow GPU access\n$> mkdir gpugroup\n$> cd gpugroup\n# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\n$> ls gpugroup\ntasks      devices.deny     devices.allow\n\n# Launch a shell from where the CUDA process will be executed. Gets the shells PID\n$> echo $$\n\n# Write this PID into the tasks files in the gpugroups folder\n$> echo <PID> tasks\n\n# List the device numbers of nvidia devices with the ls command\n$> ls -l /dev/nvidia*\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\n\n# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\n$> echo 'c 195:1 rmw' > devices.deny\n\n# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\n# To provide the CUDA process access to GPU1, we should write the following to devices.allow\n\n$> echo 'c 195:1 rmw' > devices.allow\n```\nWhen you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.\n```\numount /mnt/cgroupV1Device\n```\nTo allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here\u2019s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.\nIn the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:\n```\n$> echo <PID> tasks\n```\nNow add GPU5 and GPU6 to the denied list:\n```\n$> echo 'c 195:5 rmw' > devices.deny\n$> echo 'c 195:6 rmw' > devices.deny\n```\nAt this point, the CUDA process can\u2019t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.\nThe access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.", "Isolating GPUs using the bubblewrap utility\nThe bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:\n```\n# install bubblewrap utility on Debian-like systems\n$>sudo apt-get install -y bubblewrap\n\n# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\n\n#!/bin/sh\n# bwrap.sh\nGPU=$1;shift   # 0, 1, 2, 3, ..\nif [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi\nbwrap \\\n        --bind / / \\\n        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\n        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\n        \"$@\"\n\n\n# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\n$> ./bwrap.sh 0 ./test_cuda_app <args>\n```\nMore than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.", "Performance benefits of GPU isolation\nIn this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs.\nBar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\nFigure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system", "Summary\nGPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.\nFor more information, see the following resources:\nControl Groups version 1 \u2014 The Linux Kernel documentation\ncuInit"], "document_title": "Improving CUDA Initialization Times Using cgroups in Certain Scenarios", "document_url": "https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/", "document_date": "2024-01-05T22:14:41", "document_date_modified": "2024-01-11T19:49:33", "document_full_text": "Improving CUDA Initialization Times Using cgroups in Certain Scenarios\nMany CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.\nThis post discusses the various methods to accomplish this and their performance benefits.\nGPU isolation\nGPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach.\nAnother method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.\nIsolating GPUs using cgroups V1\nControl groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it.\nThe following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post.\n```\n# Create a mountpoint for the cgroup hierarchy as root\n$> cd /mnt\n$> mkdir cgroupV1Device\n\n# Use mount command to mount the hierarchy and attach the device subsystem to it\n$> mount -t cgroup -o devices devices cgroupV1Device\n$> cd cgroupV1Device\n# Now create a gpu subgroup directory to restrict/allow GPU access\n$> mkdir gpugroup\n$> cd gpugroup\n# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\n$> ls gpugroup\ntasks      devices.deny     devices.allow\n\n# Launch a shell from where the CUDA process will be executed. Gets the shells PID\n$> echo $$\n\n# Write this PID into the tasks files in the gpugroups folder\n$> echo <PID> tasks\n\n# List the device numbers of nvidia devices with the ls command\n$> ls -l /dev/nvidia*\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\n\n# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\n$> echo 'c 195:1 rmw' > devices.deny\n\n# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\n# To provide the CUDA process access to GPU1, we should write the following to devices.allow\n\n$> echo 'c 195:1 rmw' > devices.allow\n```\nWhen you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.\n```\numount /mnt/cgroupV1Device\n```\nTo allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here\u2019s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.\nIn the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:\n```\n$> echo <PID> tasks\n```\nNow add GPU5 and GPU6 to the denied list:\n```\n$> echo 'c 195:5 rmw' > devices.deny\n$> echo 'c 195:6 rmw' > devices.deny\n```\nAt this point, the CUDA process can\u2019t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.\nThe access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\nIsolating GPUs using the bubblewrap utility\nThe bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:\n```\n# install bubblewrap utility on Debian-like systems\n$>sudo apt-get install -y bubblewrap\n\n# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\n\n#!/bin/sh\n# bwrap.sh\nGPU=$1;shift   # 0, 1, 2, 3, ..\nif [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi\nbwrap \\\n        --bind / / \\\n        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\n        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\n        \"$@\"\n\n\n# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\n$> ./bwrap.sh 0 ./test_cuda_app <args>\n```\nMore than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.\nPerformance benefits of GPU isolation\nIn this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs.\nBar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\nFigure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system\nSummary\nGPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.\nFor more information, see the following resources:\nControl Groups version 1 \u2014 The Linux Kernel documentation\ncuInit"}], "https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/": [{"text": "The article discusses the challenges of deploying machine learning models in a production environment and introduces Metaflow and NVIDIA Triton Inference Server as solutions. Metaflow helps in developing production-grade ML workflows, while NVIDIA Triton Inference Server is a model serving framework that efficiently handles various types of models on CPUs and GPUs. The article emphasizes the importance of aligning the training and serving stacks to maintain full lineage and debuggability. A practical example of training and serving a tree-based model using Metaflow and NVIDIA Triton Inference Server is provided, showing significant performance improvements compared to Python-based API servers. The article also mentions the support for serving LLMs with NVIDIA Triton Inference Server. Lastly, the article recommends experimenting with the mentioned AI stack in one's own environment and provides resources for further information.", "text_components": ["Develop ML and AI with Metaflow and Deploy with NVIDIA Triton Inference Server\nThere are many ways to deploy ML models to production. Sometimes, a model is run once per day to refresh forecasts in a database. Sometimes, it powers a small-scale but critical decision-making dashboard or speech-to-text on a mobile device. These days, the model can also be a custom large language model (LLM) backing a novel AI-driven product experience.\nOften, the model is exposed to its environment through an API endpoint with microservices, enabling the model to be queried in real time. While this may sound straightforward, as there are plenty of frameworks for building and deploying microservices in general, serving models in a serious production setting is surprisingly non-trivial.\nConsider the following typical challenges (Table 1).\nModel training\nCan you train and deploy models continuously without human intervention?\nCan you develop new models easily, test deployments locally, and experiment confidently?\nCan you manage features during development and deployment consistently?\nModel deployment\nCan the deployment handle various types of models you want to use?\nCan the model produce responses quickly enough to support the desired product experience?\nWhat happens if the responses are bad? Can you trace the lineage and the model that produced them?\nInfrastructure\nIs the deployment highly available enough to support the SLA target?\nDoes the deployment use hardware resources efficiently?\nCan you monitor the requests and responses easily?\nDoes the deployment integrate with your existing infrastructure and policies?\nCan you deploy models in a cost-efficient manner?\nCan the deployment scale to enough requests per second?\nDoes the infrastructure provide secure access to sensitive datasets and models?\nCan infrastructure scale to meet compute needs? Does cost go to zero when it isn\u2019t being used?\nCan users tune performance knobs, such as GPU card types and amounts, to reduce TCO?\nTable 1. Challenges of model serving in a production environment", "From prototype to production\nTo address these challenges holistically, consider the full lifecycle of an ML system from the early stages of development to the deployment (and back).\nDiagram lists steps starting with develop locally, explore with notebooks, create a workflow, version everything, scale vertically, scale horizontally, access data quickly, schedule execution, package model and libraries, monitor model, and deploy endpoint. The debug step circles back to the prototype starting point.\nFigure 1. The end-to-end feedback loop between prototype and production\nWhile you could cover the journey by adopting a separate tool for each step, a smoother developer experience and a faster time to deployment can be achieved by providing a consistent API that connects the dots.\nWith this vision in mind, Netflix started developing a Python library called Metaflow in 2017, which was open-sourced in 2019. Since then, it has been adopted by thousands of leading ML and AI organizations across industries from real estate and drones to gaming and healthcare.\nMetaflow covers all the concerns in the first part of the journey: how to develop production-grade reactive ML workflows, access data and train models easily at scale, and keep track of all the work comprehensively.\nVideo 1. Triggering a Metaflow flow based on an external event Today, you can adopt Metaflow as open source, or have it deployed in your cloud account with Outerbounds, a fully managed ML and AI platform, which layers additional security, scalability, and developer productivity features on top of the open-source package.\nWith Metaflow, you can address the first three challenges related to developing and producing models. To deploy the models for real-time inference, you need a model serving stack. That\u2019s where NVIDIA Triton Inference Server comes into play.\nNVIDIA Triton Inference Server is an open-source model serving framework developed by NVIDIA. It supports a wide variety of models, which it can handle efficiently both on CPUs and GPUs.\nOuterbounds and NVIDIA are collaborating to make the NVIDIA inference stack more easily accessible for a wide variety of ML and AI use cases. The combination of the two open-source frameworks enables you to develop machine learning and AI-powered models and systems quickly and deploy them as high-performance, production-grade services.", "Deploy on NVIDIA Triton Inference Server\nTo support production AI at an enterprise level, NVIDIA Triton Inference Server is included in the NVIDIA AI Enterprise software platform, which offers enterprise-grade security, support and stability.\nWhile a number of model serving frameworks are available, both as open-source and as managed services, NVIDIA Triton Inference Server stands out for several reasons:\nIt is highly performant, thanks to it being implemented in C++ and capable of using GPUs efficiently. This makes it a great choice for latency\u2013 and throughput-sensitive applications.\nIt is versatile, capable of handling many different model families thanks to its pluggable backends.\nIt is tested and tempered, thanks to years of development and large-scale usage driven by NVIDIA.\nThese features make NVIDIA Triton Inference Server a particularly capable model serving stack to which workflows can push trained models.\nMetaflow helps you prototype models and the workflows around them and test them at scale, while keeping track of all the work performed. When the workflow shows enough promise, it is straightforward to integrate it to surrounding software systems and orchestrate it reliably in production.\nWhile Metaflow exposes all the necessary functionality through a simple, developer-focused Python API, this is powered by a thick stack of infrastructure. The stack integrates with data stores like Amazon S3, facilitates large-scale compute on Kubernetes, and uses production-grade workflow orchestrators like Argo Workflows.\nWhen models have been trained successfully, the responsibility shifts to NVIDIA Triton Inference Server. Similar to the training infrastructure, the inference side requires a surprisingly non-trivial stack of infrastructure.", "Infrastructure for model serving\nIt is not too hard to implement a simple service that exposes a simple model, such as a logistic regression model, over HTTPS. A basic version of something like this is doable in a few hundred lines of Python using a framework like FastAPI.\nHowever, an improvised model serving solution like this is not particularly performant. Python is an expressive language but it does not excel at processing requests quickly. It is not scalable without extra infrastructure: A single FastAPI process can handle only so many requests per second. Also, the solution is not versatile out of the box, if you want to replace the logistic regression model with a more sophisticated deep regression model, for example.\nYou could try to address these shortcomings incrementally. But as the solution grows more complex, so does the surface area of bugs, security issues, and other glitches, which is motivation for a more tempered solution.\nNVIDIA Triton Inference Server addresses these challenges by deconstructing the stack into the following key components.\nA frontend that is responsible for receiving requests over HTTP or gRPC and routing them to the backend.\nOne or more backend layers that are responsible for interacting with a particular model family.\nNVIDIA Triton Inference Server supports pluggable backends, implementations of which exist for ONNX, Python-native models, tree-based models, LLMs, and a number of other model types. This makes it possible to handle versatile models with one stack.\nThe combination of a high-performance frontend, capable of handling tens of thousands of requests per second, and a backend optimized for a certain model type, provides a low-latency path from request to response. With NVIDIA Triton Inference Server, the whole request-handling path can stay in native code, reducing request latency and increasing throughput relative to a Python-based model serving solution.\nLow-level server optimizations are important for applications that leverage custom LLMs, which require serving large deep learning models with low-latency inferences. To further optimize LLM inference, NVIDIA has introduced TensorRT-LLM. TensorRT-LLM is an SDK that makes it significantly easier for Python developers to build production-quality LLM servers. TensorRT-LLM works out-of-the-box as a backend with NVIDIA Triton Inference Server.\nNo matter the scale or server backend, an individual NVIDIA Triton Inference Server instance is typically deployed with a container orchestrator like Kubernetes. A separate layer, a deployment orchestrator, is needed to manage the instances, autoscale the cluster on demand, manage model lifecycle, request routing, and other infrastructure concerns, listed in the last two rows of Table 1.", "Integrating the training and the serving stacks\nWhile both training and serving require infrastructure stacks of their own, you want to align them tightly for a few reasons.\nFirst, deploying a model should be a routine operation, which shouldn\u2019t require many extra lines of code or worse, manual effort.\nSecond, you want to maintain a full lineage about deployed models, so you can understand the whole chain from raw data and preprocessing to trained models and finally deployed models producing real-time inferences. This feature comes in handy when dealing with A/B experiments, deploying hundreds of parallel models, or debugging responses from an endpoint.\nIn the workflow guiding this post, you deploy models to NVIDIA Triton Inference Server in a manner that carries the version information across the stacks. This way, you can backtrack inferences all the way to the raw data.\nWorkflow diagram shows that each stage has a separate ID for tracking.\nFigure 2. End-to-end lineage from API response to data transformations and modeling\nEnd-to-end lineage and debuggability means that when an endpoint hosting a model responds to a request, you can trace the prediction back to the workflow that produced the model and data it was trained on.\nIn practice, every response contains an NVIDIA Triton Inference Server deployment ID, which maps to a Metaflow run ID, which in turn enables you to inspect the data used to produce the model.", "Example: Train and serve a tree-based model\nTo show the end-to-end workflow in action, here\u2019s a practical example. You can follow along using the /triton-metaflow-starter-pack GitHub repo.\nThe example workflow addresses a fraud detection problem, a classification task for predicting loan defaults. It trains many Scikit-learn models in parallel, selects the best one, and pushes the model to a cloud-based model registry used by NVIDIA Triton Inference Server.\nScreenshot from the Metaflow UI shows the workflow steps: start, preprocess, train, eval, deploy, and end.\nFigure 3. The Metaflow UI, an interface for monitoring ML workflows\nThe Metaflow UI enables the monitoring and visualizations of your workflow runs, organizing workflows by a run ID and seamlessly tracking all artifacts that the workflow runs produce.\nTo give you an idea what the workflow code looks like, the following code example defines the first three steps: start, preprocess, and train.\n```\nclass FraudClassifierTreeSelection(FlowSpec):\n\n     @step\n     def start(self):\n     self.next(self.preprocess)\n\n     @batch(cpu=1, memory=8000)\n     @card\n     @step\n     def preprocess(self):\n     self.compute_features()\n     self.setup_model_grid(model_list=[\"Random Forest\"])\n     self.next(self.train, foreach=\"model_grid\")\n\n     @batch(cpu=4, memory=16000)\n     @card\n     @step\n     def train(self):\n     self.model_name, self.model_grid = self.input\n     self.best_model = self.smote_pipe(\n          self.model_grid, self.X_train_full, self.y_train_full\n          )\n     self.next(self.eval)\n\t...\n```\nThe train step executes a number of parallel training tasks using Metaflow\u2019s foreach construct. In this case, you execute the tasks in the cloud using AWS Batch, by specifying a ```@batch``` decorator. Run the workflow manually with the following command:\n```\npython train/flow.py run \\\n--model-repo s3://outerbounds-datasets/triton/tree-models/\n```\nVideo 2. Deploy Triton Artifacts from Metaflow Demo When Metaflow is deployed in your environment, you don\u2019t have to write any other configuration or Dockerfiles besides writing and executing this workflow. The workflows can be deployed using a single command to run on a schedule or be triggered by events in the broader system.", "Preparing models for NVIDIA Triton Inference Server\nThe deploy step of the workflow takes care of preparing a model for deployment with NVIDIA Triton Inference Server. This happens through the following steps:\nSet the backend and other properties in NVIDIA Triton Inference Server\u2019s model configuration. Some of these properties depend on what happens during training, so you dynamically create the file and include it in the artifact package sent to cloud storage during model training runs.\nNVIDIA Triton Inference Server also needs the model representation itself. Because you\u2019re concerned about efficient serving, serialize the model using Treelite, a pattern that works with Scikit-learn tree models, XGBoost, and LightGBM. Place the resulting checkpoint.tl file in the model repository, and NVIDIA Triton Inference Server knows what to do from there.\nSerialized model files are pushed to Amazon S3 using Metaflow\u2019s built-in optimized Amazon S3 client, naming them based on a unique workflow run ID that can be used to access all information used to train the model. This way, you maintain a full lineage from inference responses back to data processing and training workflows.", "NVIDIA Triton Inference Server performance\nFor serving tree models, NVIDIA Triton Inference Server works best with the FIL backend. We ran a simple benchmark to see how the inference latency compares to a baseline Python-based API server using FastAPI as a frontend and Uvicorn as a backend. You can reproduce the results using the triton-metaflow-starter-pack GitHub repo.\nThe results demonstrate that hosting a tree-based model using the Triton server with a FIL backend outperforms a FastAPI server with a Uvicorn backend by over an order of magnitude.\nFigure 4. Response latencies (boxes show the quartiles of response latencies; whiskers show the rest of the distribution; circles are specific request times determined to be outliers)\nThe response time that we observed (not counting the network overhead) was 0.44 ms \u00b1 0.64 ms for NVIDIA Triton Inference Server compared to 5.15 ms \u00b1 0.9 ms for FastAPI. This is more than an order of magnitude difference.\nThe benchmark was run on a CoreWeave server with eight Xeon cores, demonstrating that NVIDIA Triton Inference Server can provide big speedups in any environment, and not only on NVIDIA GPUs.\nWe plan to compare more interesting server combinations, explore NVIDIA Triton Inference Server optimizations such as dynamic request batching, and extend to the real-world complexity of network overhead.", "NVIDIA Triton Inference Server for production inference\nSecurity, reliability, and enterprise-grade support are critical for production AI.\nNVIDIA AI Enterprise is a production-ready inference platform that includes NVIDIA Triton Inference Server. It is designed to accelerate time-to-value with enterprise-grade security, support, and API stability to ensure performance and high availability.\nYou can use this inference platform to relieve the burden of maintaining and securing the complex software platform of AI.", "Fine-tuning and serving an LLM\nIn 2023, I\u2019d be remiss to discuss model serving without mentioning LLMs. NVIDIA Triton Inference Server has extensive support for deep learning models and increasingly sophisticated support for serving LLMs. Yet, serving LLMs efficiently is a deep and quickly evolving topic.\nTo give you a small sample of serving LLMs with NVIDIA Triton Inference Server, I used this workflow to fine-tune a Llama2 model from HuggingFace to produce QLoRAs, and serve the resulting model with NVIDIA Triton Inference Server. For more information, see Fine-tuning a Large Language Model using Metaflow, featuring LLaMA and LoRA and Better, Faster, Stronger LLM Fine-tuning.\nAs I did previously, I constructed the NVIDIA Triton Inference Server configuration on the fly after the workflow finishes model training. This example is meant as a proof-of-concept.", "Next steps\nStart experimenting with the end-to-end AI stack highlighted in this post in your own environment. For more information, see the following resources:\nInstalling Metaflow\nOuterbounds\nNVIDIA Triton Inference Server is available with NVIDIA AI Enterprise under free 90-day software evaluation licenses\nIf you have any questions or feedback, join thousands of ML/AI developers and engineers in the Metaflow community Slack channel."], "document_title": "Develop ML and AI with Metaflow and Deploy with NVIDIA Triton Inference Server", "document_url": "https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/", "document_date": "2024-01-05T19:23:39", "document_date_modified": "2024-01-11T19:49:34", "document_full_text": "Develop ML and AI with Metaflow and Deploy with NVIDIA Triton Inference Server\nThere are many ways to deploy ML models to production. Sometimes, a model is run once per day to refresh forecasts in a database. Sometimes, it powers a small-scale but critical decision-making dashboard or speech-to-text on a mobile device. These days, the model can also be a custom large language model (LLM) backing a novel AI-driven product experience.\nOften, the model is exposed to its environment through an API endpoint with microservices, enabling the model to be queried in real time. While this may sound straightforward, as there are plenty of frameworks for building and deploying microservices in general, serving models in a serious production setting is surprisingly non-trivial.\nConsider the following typical challenges (Table 1).\nModel training\nCan you train and deploy models continuously without human intervention?\nCan you develop new models easily, test deployments locally, and experiment confidently?\nCan you manage features during development and deployment consistently?\nModel deployment\nCan the deployment handle various types of models you want to use?\nCan the model produce responses quickly enough to support the desired product experience?\nWhat happens if the responses are bad? Can you trace the lineage and the model that produced them?\nInfrastructure\nIs the deployment highly available enough to support the SLA target?\nDoes the deployment use hardware resources efficiently?\nCan you monitor the requests and responses easily?\nDoes the deployment integrate with your existing infrastructure and policies?\nCan you deploy models in a cost-efficient manner?\nCan the deployment scale to enough requests per second?\nDoes the infrastructure provide secure access to sensitive datasets and models?\nCan infrastructure scale to meet compute needs? Does cost go to zero when it isn\u2019t being used?\nCan users tune performance knobs, such as GPU card types and amounts, to reduce TCO?\nTable 1. Challenges of model serving in a production environment\nFrom prototype to production\nTo address these challenges holistically, consider the full lifecycle of an ML system from the early stages of development to the deployment (and back).\nDiagram lists steps starting with develop locally, explore with notebooks, create a workflow, version everything, scale vertically, scale horizontally, access data quickly, schedule execution, package model and libraries, monitor model, and deploy endpoint. The debug step circles back to the prototype starting point.\nFigure 1. The end-to-end feedback loop between prototype and production\nWhile you could cover the journey by adopting a separate tool for each step, a smoother developer experience and a faster time to deployment can be achieved by providing a consistent API that connects the dots.\nWith this vision in mind, Netflix started developing a Python library called Metaflow in 2017, which was open-sourced in 2019. Since then, it has been adopted by thousands of leading ML and AI organizations across industries from real estate and drones to gaming and healthcare.\nMetaflow covers all the concerns in the first part of the journey: how to develop production-grade reactive ML workflows, access data and train models easily at scale, and keep track of all the work comprehensively.\nVideo 1. Triggering a Metaflow flow based on an external event Today, you can adopt Metaflow as open source, or have it deployed in your cloud account with Outerbounds, a fully managed ML and AI platform, which layers additional security, scalability, and developer productivity features on top of the open-source package.\nWith Metaflow, you can address the first three challenges related to developing and producing models. To deploy the models for real-time inference, you need a model serving stack. That\u2019s where NVIDIA Triton Inference Server comes into play.\nNVIDIA Triton Inference Server is an open-source model serving framework developed by NVIDIA. It supports a wide variety of models, which it can handle efficiently both on CPUs and GPUs.\nOuterbounds and NVIDIA are collaborating to make the NVIDIA inference stack more easily accessible for a wide variety of ML and AI use cases. The combination of the two open-source frameworks enables you to develop machine learning and AI-powered models and systems quickly and deploy them as high-performance, production-grade services.\nDeploy on NVIDIA Triton Inference Server\nTo support production AI at an enterprise level, NVIDIA Triton Inference Server is included in the NVIDIA AI Enterprise software platform, which offers enterprise-grade security, support and stability.\nWhile a number of model serving frameworks are available, both as open-source and as managed services, NVIDIA Triton Inference Server stands out for several reasons:\nIt is highly performant, thanks to it being implemented in C++ and capable of using GPUs efficiently. This makes it a great choice for latency\u2013 and throughput-sensitive applications.\nIt is versatile, capable of handling many different model families thanks to its pluggable backends.\nIt is tested and tempered, thanks to years of development and large-scale usage driven by NVIDIA.\nThese features make NVIDIA Triton Inference Server a particularly capable model serving stack to which workflows can push trained models.\nMetaflow helps you prototype models and the workflows around them and test them at scale, while keeping track of all the work performed. When the workflow shows enough promise, it is straightforward to integrate it to surrounding software systems and orchestrate it reliably in production.\nWhile Metaflow exposes all the necessary functionality through a simple, developer-focused Python API, this is powered by a thick stack of infrastructure. The stack integrates with data stores like Amazon S3, facilitates large-scale compute on Kubernetes, and uses production-grade workflow orchestrators like Argo Workflows.\nWhen models have been trained successfully, the responsibility shifts to NVIDIA Triton Inference Server. Similar to the training infrastructure, the inference side requires a surprisingly non-trivial stack of infrastructure.\nInfrastructure for model serving\nIt is not too hard to implement a simple service that exposes a simple model, such as a logistic regression model, over HTTPS. A basic version of something like this is doable in a few hundred lines of Python using a framework like FastAPI.\nHowever, an improvised model serving solution like this is not particularly performant. Python is an expressive language but it does not excel at processing requests quickly. It is not scalable without extra infrastructure: A single FastAPI process can handle only so many requests per second. Also, the solution is not versatile out of the box, if you want to replace the logistic regression model with a more sophisticated deep regression model, for example.\nYou could try to address these shortcomings incrementally. But as the solution grows more complex, so does the surface area of bugs, security issues, and other glitches, which is motivation for a more tempered solution.\nNVIDIA Triton Inference Server addresses these challenges by deconstructing the stack into the following key components.\nA frontend that is responsible for receiving requests over HTTP or gRPC and routing them to the backend.\nOne or more backend layers that are responsible for interacting with a particular model family.\nNVIDIA Triton Inference Server supports pluggable backends, implementations of which exist for ONNX, Python-native models, tree-based models, LLMs, and a number of other model types. This makes it possible to handle versatile models with one stack.\nThe combination of a high-performance frontend, capable of handling tens of thousands of requests per second, and a backend optimized for a certain model type, provides a low-latency path from request to response. With NVIDIA Triton Inference Server, the whole request-handling path can stay in native code, reducing request latency and increasing throughput relative to a Python-based model serving solution.\nLow-level server optimizations are important for applications that leverage custom LLMs, which require serving large deep learning models with low-latency inferences. To further optimize LLM inference, NVIDIA has introduced TensorRT-LLM. TensorRT-LLM is an SDK that makes it significantly easier for Python developers to build production-quality LLM servers. TensorRT-LLM works out-of-the-box as a backend with NVIDIA Triton Inference Server.\nNo matter the scale or server backend, an individual NVIDIA Triton Inference Server instance is typically deployed with a container orchestrator like Kubernetes. A separate layer, a deployment orchestrator, is needed to manage the instances, autoscale the cluster on demand, manage model lifecycle, request routing, and other infrastructure concerns, listed in the last two rows of Table 1.\nIntegrating the training and the serving stacks\nWhile both training and serving require infrastructure stacks of their own, you want to align them tightly for a few reasons.\nFirst, deploying a model should be a routine operation, which shouldn\u2019t require many extra lines of code or worse, manual effort.\nSecond, you want to maintain a full lineage about deployed models, so you can understand the whole chain from raw data and preprocessing to trained models and finally deployed models producing real-time inferences. This feature comes in handy when dealing with A/B experiments, deploying hundreds of parallel models, or debugging responses from an endpoint.\nIn the workflow guiding this post, you deploy models to NVIDIA Triton Inference Server in a manner that carries the version information across the stacks. This way, you can backtrack inferences all the way to the raw data.\nWorkflow diagram shows that each stage has a separate ID for tracking.\nFigure 2. End-to-end lineage from API response to data transformations and modeling\nEnd-to-end lineage and debuggability means that when an endpoint hosting a model responds to a request, you can trace the prediction back to the workflow that produced the model and data it was trained on.\nIn practice, every response contains an NVIDIA Triton Inference Server deployment ID, which maps to a Metaflow run ID, which in turn enables you to inspect the data used to produce the model.\nExample: Train and serve a tree-based model\nTo show the end-to-end workflow in action, here\u2019s a practical example. You can follow along using the /triton-metaflow-starter-pack GitHub repo.\nThe example workflow addresses a fraud detection problem, a classification task for predicting loan defaults. It trains many Scikit-learn models in parallel, selects the best one, and pushes the model to a cloud-based model registry used by NVIDIA Triton Inference Server.\nScreenshot from the Metaflow UI shows the workflow steps: start, preprocess, train, eval, deploy, and end.\nFigure 3. The Metaflow UI, an interface for monitoring ML workflows\nThe Metaflow UI enables the monitoring and visualizations of your workflow runs, organizing workflows by a run ID and seamlessly tracking all artifacts that the workflow runs produce.\nTo give you an idea what the workflow code looks like, the following code example defines the first three steps: start, preprocess, and train.\n```\nclass FraudClassifierTreeSelection(FlowSpec):\n\n     @step\n     def start(self):\n     self.next(self.preprocess)\n\n     @batch(cpu=1, memory=8000)\n     @card\n     @step\n     def preprocess(self):\n     self.compute_features()\n     self.setup_model_grid(model_list=[\"Random Forest\"])\n     self.next(self.train, foreach=\"model_grid\")\n\n     @batch(cpu=4, memory=16000)\n     @card\n     @step\n     def train(self):\n     self.model_name, self.model_grid = self.input\n     self.best_model = self.smote_pipe(\n          self.model_grid, self.X_train_full, self.y_train_full\n          )\n     self.next(self.eval)\n\t...\n```\nThe train step executes a number of parallel training tasks using Metaflow\u2019s foreach construct. In this case, you execute the tasks in the cloud using AWS Batch, by specifying a ```@batch``` decorator. Run the workflow manually with the following command:\n```\npython train/flow.py run \\\n--model-repo s3://outerbounds-datasets/triton/tree-models/\n```\nVideo 2. Deploy Triton Artifacts from Metaflow Demo When Metaflow is deployed in your environment, you don\u2019t have to write any other configuration or Dockerfiles besides writing and executing this workflow. The workflows can be deployed using a single command to run on a schedule or be triggered by events in the broader system.\nPreparing models for NVIDIA Triton Inference Server\nThe deploy step of the workflow takes care of preparing a model for deployment with NVIDIA Triton Inference Server. This happens through the following steps:\nSet the backend and other properties in NVIDIA Triton Inference Server\u2019s model configuration. Some of these properties depend on what happens during training, so you dynamically create the file and include it in the artifact package sent to cloud storage during model training runs.\nNVIDIA Triton Inference Server also needs the model representation itself. Because you\u2019re concerned about efficient serving, serialize the model using Treelite, a pattern that works with Scikit-learn tree models, XGBoost, and LightGBM. Place the resulting checkpoint.tl file in the model repository, and NVIDIA Triton Inference Server knows what to do from there.\nSerialized model files are pushed to Amazon S3 using Metaflow\u2019s built-in optimized Amazon S3 client, naming them based on a unique workflow run ID that can be used to access all information used to train the model. This way, you maintain a full lineage from inference responses back to data processing and training workflows.\nNVIDIA Triton Inference Server performance\nFor serving tree models, NVIDIA Triton Inference Server works best with the FIL backend. We ran a simple benchmark to see how the inference latency compares to a baseline Python-based API server using FastAPI as a frontend and Uvicorn as a backend. You can reproduce the results using the triton-metaflow-starter-pack GitHub repo.\nThe results demonstrate that hosting a tree-based model using the Triton server with a FIL backend outperforms a FastAPI server with a Uvicorn backend by over an order of magnitude.\nFigure 4. Response latencies (boxes show the quartiles of response latencies; whiskers show the rest of the distribution; circles are specific request times determined to be outliers)\nThe response time that we observed (not counting the network overhead) was 0.44 ms \u00b1 0.64 ms for NVIDIA Triton Inference Server compared to 5.15 ms \u00b1 0.9 ms for FastAPI. This is more than an order of magnitude difference.\nThe benchmark was run on a CoreWeave server with eight Xeon cores, demonstrating that NVIDIA Triton Inference Server can provide big speedups in any environment, and not only on NVIDIA GPUs.\nWe plan to compare more interesting server combinations, explore NVIDIA Triton Inference Server optimizations such as dynamic request batching, and extend to the real-world complexity of network overhead.\nNVIDIA Triton Inference Server for production inference\nSecurity, reliability, and enterprise-grade support are critical for production AI.\nNVIDIA AI Enterprise is a production-ready inference platform that includes NVIDIA Triton Inference Server. It is designed to accelerate time-to-value with enterprise-grade security, support, and API stability to ensure performance and high availability.\nYou can use this inference platform to relieve the burden of maintaining and securing the complex software platform of AI.\nFine-tuning and serving an LLM\nIn 2023, I\u2019d be remiss to discuss model serving without mentioning LLMs. NVIDIA Triton Inference Server has extensive support for deep learning models and increasingly sophisticated support for serving LLMs. Yet, serving LLMs efficiently is a deep and quickly evolving topic.\nTo give you a small sample of serving LLMs with NVIDIA Triton Inference Server, I used this workflow to fine-tune a Llama2 model from HuggingFace to produce QLoRAs, and serve the resulting model with NVIDIA Triton Inference Server. For more information, see Fine-tuning a Large Language Model using Metaflow, featuring LLaMA and LoRA and Better, Faster, Stronger LLM Fine-tuning.\nAs I did previously, I constructed the NVIDIA Triton Inference Server configuration on the fly after the workflow finishes model training. This example is meant as a proof-of-concept.\nNext steps\nStart experimenting with the end-to-end AI stack highlighted in this post in your own environment. For more information, see the following resources:\nInstalling Metaflow\nOuterbounds\nNVIDIA Triton Inference Server is available with NVIDIA AI Enterprise under free 90-day software evaluation licenses\nIf you have any questions or feedback, join thousands of ML/AI developers and engineers in the Metaflow community Slack channel."}], "https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/": [{"text": "Advances in camera and display technologies have made it possible to capture and play video at 8K resolution with 60 FPS. NVIDIA's Ada Lovelace GPU architecture enables efficient video encoding at 8K60 by utilizing multiple NVENC engines. Split-frame encoding (SFE) technique splits frames and encodes them with different NVENC engines, doubling or tripling encoding performance. SFE can be enabled or disabled using the NVENCODE API, allowing for explicit control over the number of NVENCs used. Benchmarking tests show that two-way SFE can achieve a performance scaling of 1.8x and three-way SFE up to 2.95x. While SFE may result in a slight compression efficiency penalty, it remains relatively low, especially for HQ tuning information. Users can decide whether they prioritize performance or compression efficiency based on their specific use case. Overall, SFE with NVIDIA Ada Lovelace architecture offers significant performance benefits for 8K video encoding and beyond.", "text_components": ["Video Encoding at 8K60 with Split-Frame Encoding and NVIDIA Ada Lovelace Architecture\nCapturing video footage and playing games at 8K resolution with 60 frames per second (FPS) is now possible, thanks to advances in camera and display technologies. Major leading multimedia companies including RED Digital Cinema, Nikon, and Canon have already introduced 8K60 cameras for both the consumer and professional markets.\nOn the display side, with the newest HDMI 2.1 standard, 8K60 is now widely available, supporting both gaming monitors and smart TVs. While 8K60 provides \u200cstunning image quality and sharpness, it comes with the significant cost of consuming more data for both transfer and storage.\nFast codecs are therefore paramount in bridging the gap between the sensors and the display. To make 8K60 widely available, NVIDIA Ada Lovelace GPU architecture provides multiple NVENC engines to accelerate video encoding performance while maintaining high image quality. (Two NVENCs are provided with NVIDIA RTX 4090 and 4080 and three NVENCs with NVIDIA RTX 6000 Ada Lovelace or L40.)\nIn practice, this can double or triple the encoding performance with a single GPU when compared to previous generations, enabling 8K60 video encoding and beyond.\nThis post showcases how the multiple available NVENCs in NVIDIA Ada Lovelace architecture are leveraged using a split-frame encoding (SFE) technique to achieve 8K60 video encoding performance. We explore how this SFE technique works at 4K and 8K resolutions and how to enable it through the NVENCODE API. Finally, we present several benchmarks that show, in practice, the massive performance benefits of this technique.", "Split-frame encoding\nSFE is a technique that enables exploiting multiple NVENCs present in NVIDIA Ada Lovelace GPUs when encoding a single video sequence by splitting the frames and encoding each partial frame with different NVENC engines. It was introduced in NVIDIA Video Codec SDK 12.0. SFE can effectively split the encoding work across the available NVENCs (Figure 1). However, until now, SFE was implicitly enabled based on the encoding preset, tuning information, and resolution to support 8K live encoding in HEVC or AV1. (Note that 8K is not supported on H.264.) To learn more, see Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture.\nDiagram of two-way split-frame encoding with NVENCODE API, NVENC 0, and NVENC 1 (top to bottom).\nFigure 1. Two-way split-frame encoding\nWith NVIDIA Video Codec SDK 12.1, you can enable or disable the SFE feature. This means that SFE can now be used to take advantage of two or even three NVENCs present within the NVIDIA RTX 4090 and the NVIDIA RTX 6000 Ada Generation, respectively, without resolution, preset, and tuning information restrictions. This enables the application to double or even triple the encoding performance when encoding a single video sequence by using a two-way or three-way SFE. Such performance is especially important when encoding 8K, which is a particularly demanding use case.", "SFE at 4K and 8K resolution\nHow SFE is applied can vary depending on the resolution and selected video codec. When using HEVC with SFE turned off, expect only a single slice to be used. When two-way or three-way SFE is used, two or three slices, respectively, are used. These horizontally separate each frame. It applies to both 4K and 8K resolutions (Figure 2). Additionally, the same applies to AV1 when encoding video up to 4K resolution. However, AV1 uses tiles instead of slices to create these independent frame partitions.\nree images showing generated frame partitions when encoding HEVC at 4K and 8K and AV1 at 4K for the several SFE configurations. HEVC 4K/8K and AV1 4K with no SFE (left); HEVC 4K/8K and AV1 4K with two-way SFE (center); HEVC 4K/8K and AV1 4K with three-way SFE (right).\nFigure 2. Generated frame partitions when encoding HEVC at 4K and 8K and AV1 at 4K for the several SFE configurations: HEVC 4K/8K and AV1 4K with no SFE (left); HEVC 4K/8K and AV1 4K with two-way SFE (center); HEVC 4K/8K and AV1 4K with three-way SFE (right)\nWhen encoding 8K video with AV1, consider that the maximum tile resolution defined by the standard is 4096 x 2304 pixels. This means that when encoding 8K video, each frame will be split into four tiles, each with a quarter of the resolution (3840 x 2160 pixels). When SFE is used, to achieve the same performance benefits as for HEVC, each tile will be further split horizontally, for eight or 12 tiles, for two-way and three-way SFE, respectively (Figure 3).\nThree images showing generated frame partitions when encoding AV1 at 8K for the several SFE configurations: AV1 8K with no SFE (left); AV1 8K with two-way SFE (center); AV1 8K with three-way SFE (right).\nFigure 3. Generated frame partitions when encoding AV1 at 8K for the several SFE configurations: AV1 8K with no SFE (left); AV1 8K with two-way SFE (center); AV1 8K with three-way SFE (right) Table 1 summarizes the number of partial frames and resolution to expect per codec and input video resolution.\nCodec\nVideo resolution\nNumber of partial frames and resolution\nNo SFE\nTwo-way SFE\nThree-way SFE\nHEVC\n4K\n1x 3840 x 2160\n2x 3840 x 1080\n3x 3840 x 720\n8K\n1x 7680 x 4320\n2x 7680 x 2160\n3x 7680 x 1440\nAV1\n4K\n1x 3840 x 2160\n2x 3840 x 1080\n3x 3840 x 720\n8K\n4x 3840 x 2160\n8x 3840 x 1080\n12x 3840 x 720\nTable 1. Summary of number of partial frames and respective resolution per codec when encoding 4K and 8K videos", "Enabling split-frame encoding\nWith the API update of Video Codec SDK 12.1, in the latest NVENCODER API header, you can find ```NV_ENC_SPLIT_ENCODE_MODE```. This enables control over SFE, as shown in Table 2. It is now quite easy to configure SFE using either implicit or explicit modes. ```NV_ENC_SPLIT_AUTO_MODE``` and ```NV_ENC_SPLIT_AUTO_FORCED_MODE``` provide a way to use the SFE implicit mode. To learn more, see Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture.\nThe remaining options refer to explicit SFE configuration. These include forcing SFE to be disabled, two-way, or three-way. To force two-way or three-way SFE requires an NVIDIA GPU with the appropriate number of NVENC engines.\nNV_ENC_SPLIT_ENCODE_MODE\nSFE type\nDescription\nNV_ENC_SPLIT_AUTO_MODE (0)\nAuto Mode (default)\nTwo-way SFE will be implicitly triggered based on input video resolution and encoding parameters\nNV_ENC_SPLIT_AUTO_FORCED_MODE (1)\nForce Auto Mode\nNV_ENC_SPLIT_TWO_FORCED_MODE (2)\nForce two-way SFE\nThe respective SFE configuration will be used regardless of the input video and encoding parameters\nNV_ENC_SPLIT_THREE_FORCED_MODE (3)\nForce three-way SFE\nNV_ENC_SPLIT_DISABLE_MODE (15)\nForce no SFE\nTable 2. Selected SFE type and description per ```NV_ENC_SPLIT_ENCODE_MODE``` option The latest Video Codec SDK encoding sample ```AppEncMultiInstance``` also highlights how to add explicit SFE control to an application.", "Performance and compression efficiency benchmarking\nSeveral configurations and input 8K videos were tested, which are listed in Table 3.\nBenchmarking configuration\nGPU\nGPU RTX 6000 Ada Generation (3 NVENCs)\nInput videos\n7 videos (4 gaming and 3 natural)\nEncoders\nHEVC and AV1\nPresets\nP1 (fastest), P4 (medium) and P7 (slowest)\nTuning Information\nLow latency (LL) and high quality (HQ)\nBitrates\n15, 20, 60, 150, and 250 Mbps\nTable 3. Benchmarking configuration summary Two types of benchmarks were performed:\nTranscoding Performance: Transcoding was used to minimize the influence of system bottlenecks (file I/O and memory copies between CPU and GPU). To test transcoding, the original 8K videos were pre-encoded with very high bitrates. During transcoding, NVDEC decodes the video. It is encoded by one to three NVENCs, when no split, two-way SFE, and three-way SFE are used, respectively. The performance results are shown in Figures 4 and 5 for HEVC and AV1, respectively.\nCompression Efficiency Penalty: By splitting encoding work across several NVENCs, a compression efficiency penalty is expected. To measure this penalty, BD-RATE was used across several benchmark configurations to compare the compression efficiency between no-split, two-way SFE, and three-way SFE. This metric indicates the average compression efficiency penalty for the same objective quality. The objective quality metric used in these benchmarks was PSNR. The compression efficiency penalty results are shown in Figures 6 and 7 for HEVC and AV1, respectively.\nBar chart showing average performance benchmarking results for 8K transcoding using HEVC.\nFigure 4. Average performance benchmarking results for 8K transcoding using HEVC Bar chart showing average performance benchmarking results for 8K transcoding using AV1.\nFigure 5. Average performance benchmarking results for 8K transcoding using AV1 When using two-way SFE, expect an average performance scaling of about 1.8x for both HEVC and AV1. Three-way SFE can achieve a performance scaling of up to 2.95x for HEVC and 2.31x for AV1. In practice, this enables 8K60 video encoding with NVIDIA RTX 6000 Ada Generation, using both HEVC and AV1, with LL and HQ tuning information at a medium preset (P4).\nGiven that one to three NVENCs and a single NVDEC are used, NVDEC may become the bottleneck when transcoding 8K. For this reason, the fastest preset (P1) can result in the FPS reaching a maximum of about 120 FPS on average. This is the average maximum performance achieved by a single NVDEC at 8K.\nYou can observe better scaling as long as NVDEC isn\u2019t the bottleneck. This is the case for slower presets, such as P4 and P7, where the performance scaling is much better in comparison to P1.\nChart showing average compression efficiency penalty results for 8K encoding using HEVC.\nFigure 6. Average compression efficiency penalty results for 8K encoding using HEVC Chart showing average compression efficiency penalty results for 8K encoding using AV1.\nFigure 7. Average compression efficiency penalty results for 8K encoding using AV1 In general, the compression efficiency penalty isn\u2019t expected to exceed 2% for two-way SFE and 4% for three-way SFE when using BD-RATE (PSNR) to measure quality. This penalty is more noticeable for HQ tuning information than for LL. Additionally, according to the benchmarks performed, this penalty is slightly more prominent when using HEVC compared to AV1.\nAlthough this compression efficiency penalty is still relatively low compared to the performance tradeoff, it\u2019s up to the user to determine if the required use case benefits from more performance or compression efficiency. Regardless, the NVENCODE API provides full control over SFE not only for 8K but also for lower resolutions.", "Summary\nSplit-frame encoding (SFE) is a breakthrough feature that unlocks video encoding capabilities at 8K60 and beyond. It empowers users to harness the power of multiple NVENCs within NVIDIA Ada Lovelace architecture GPUs for encoding a single video sequence. This post has explained the performance advantages of two-way SFE (using two NVENCs) and three-way SFE (using three NVENCs). The latest NVIDIA Video Codec SDK provides explicit control over SFE for optimal customization."], "document_title": "Video Encoding at 8K60 with Split-Frame Encoding and NVIDIA Ada Lovelace Architecture", "document_url": "https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/", "document_date": "2024-01-05T19:00:00", "document_date_modified": "2024-01-22T22:08:04", "document_full_text": "Video Encoding at 8K60 with Split-Frame Encoding and NVIDIA Ada Lovelace Architecture\nCapturing video footage and playing games at 8K resolution with 60 frames per second (FPS) is now possible, thanks to advances in camera and display technologies. Major leading multimedia companies including RED Digital Cinema, Nikon, and Canon have already introduced 8K60 cameras for both the consumer and professional markets.\nOn the display side, with the newest HDMI 2.1 standard, 8K60 is now widely available, supporting both gaming monitors and smart TVs. While 8K60 provides \u200cstunning image quality and sharpness, it comes with the significant cost of consuming more data for both transfer and storage.\nFast codecs are therefore paramount in bridging the gap between the sensors and the display. To make 8K60 widely available, NVIDIA Ada Lovelace GPU architecture provides multiple NVENC engines to accelerate video encoding performance while maintaining high image quality. (Two NVENCs are provided with NVIDIA RTX 4090 and 4080 and three NVENCs with NVIDIA RTX 6000 Ada Lovelace or L40.)\nIn practice, this can double or triple the encoding performance with a single GPU when compared to previous generations, enabling 8K60 video encoding and beyond.\nThis post showcases how the multiple available NVENCs in NVIDIA Ada Lovelace architecture are leveraged using a split-frame encoding (SFE) technique to achieve 8K60 video encoding performance. We explore how this SFE technique works at 4K and 8K resolutions and how to enable it through the NVENCODE API. Finally, we present several benchmarks that show, in practice, the massive performance benefits of this technique.\nSplit-frame encoding\nSFE is a technique that enables exploiting multiple NVENCs present in NVIDIA Ada Lovelace GPUs when encoding a single video sequence by splitting the frames and encoding each partial frame with different NVENC engines. It was introduced in NVIDIA Video Codec SDK 12.0. SFE can effectively split the encoding work across the available NVENCs (Figure 1). However, until now, SFE was implicitly enabled based on the encoding preset, tuning information, and resolution to support 8K live encoding in HEVC or AV1. (Note that 8K is not supported on H.264.) To learn more, see Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture.\nDiagram of two-way split-frame encoding with NVENCODE API, NVENC 0, and NVENC 1 (top to bottom).\nFigure 1. Two-way split-frame encoding\nWith NVIDIA Video Codec SDK 12.1, you can enable or disable the SFE feature. This means that SFE can now be used to take advantage of two or even three NVENCs present within the NVIDIA RTX 4090 and the NVIDIA RTX 6000 Ada Generation, respectively, without resolution, preset, and tuning information restrictions. This enables the application to double or even triple the encoding performance when encoding a single video sequence by using a two-way or three-way SFE. Such performance is especially important when encoding 8K, which is a particularly demanding use case.\nSFE at 4K and 8K resolution\nHow SFE is applied can vary depending on the resolution and selected video codec. When using HEVC with SFE turned off, expect only a single slice to be used. When two-way or three-way SFE is used, two or three slices, respectively, are used. These horizontally separate each frame. It applies to both 4K and 8K resolutions (Figure 2). Additionally, the same applies to AV1 when encoding video up to 4K resolution. However, AV1 uses tiles instead of slices to create these independent frame partitions.\nree images showing generated frame partitions when encoding HEVC at 4K and 8K and AV1 at 4K for the several SFE configurations. HEVC 4K/8K and AV1 4K with no SFE (left); HEVC 4K/8K and AV1 4K with two-way SFE (center); HEVC 4K/8K and AV1 4K with three-way SFE (right).\nFigure 2. Generated frame partitions when encoding HEVC at 4K and 8K and AV1 at 4K for the several SFE configurations: HEVC 4K/8K and AV1 4K with no SFE (left); HEVC 4K/8K and AV1 4K with two-way SFE (center); HEVC 4K/8K and AV1 4K with three-way SFE (right)\nWhen encoding 8K video with AV1, consider that the maximum tile resolution defined by the standard is 4096 x 2304 pixels. This means that when encoding 8K video, each frame will be split into four tiles, each with a quarter of the resolution (3840 x 2160 pixels). When SFE is used, to achieve the same performance benefits as for HEVC, each tile will be further split horizontally, for eight or 12 tiles, for two-way and three-way SFE, respectively (Figure 3).\nThree images showing generated frame partitions when encoding AV1 at 8K for the several SFE configurations: AV1 8K with no SFE (left); AV1 8K with two-way SFE (center); AV1 8K with three-way SFE (right).\nFigure 3. Generated frame partitions when encoding AV1 at 8K for the several SFE configurations: AV1 8K with no SFE (left); AV1 8K with two-way SFE (center); AV1 8K with three-way SFE (right) Table 1 summarizes the number of partial frames and resolution to expect per codec and input video resolution.\nCodec\nVideo resolution\nNumber of partial frames and resolution\nNo SFE\nTwo-way SFE\nThree-way SFE\nHEVC\n4K\n1x 3840 x 2160\n2x 3840 x 1080\n3x 3840 x 720\n8K\n1x 7680 x 4320\n2x 7680 x 2160\n3x 7680 x 1440\nAV1\n4K\n1x 3840 x 2160\n2x 3840 x 1080\n3x 3840 x 720\n8K\n4x 3840 x 2160\n8x 3840 x 1080\n12x 3840 x 720\nTable 1. Summary of number of partial frames and respective resolution per codec when encoding 4K and 8K videos\nEnabling split-frame encoding\nWith the API update of Video Codec SDK 12.1, in the latest NVENCODER API header, you can find ```NV_ENC_SPLIT_ENCODE_MODE```. This enables control over SFE, as shown in Table 2. It is now quite easy to configure SFE using either implicit or explicit modes. ```NV_ENC_SPLIT_AUTO_MODE``` and ```NV_ENC_SPLIT_AUTO_FORCED_MODE``` provide a way to use the SFE implicit mode. To learn more, see Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture.\nThe remaining options refer to explicit SFE configuration. These include forcing SFE to be disabled, two-way, or three-way. To force two-way or three-way SFE requires an NVIDIA GPU with the appropriate number of NVENC engines.\nNV_ENC_SPLIT_ENCODE_MODE\nSFE type\nDescription\nNV_ENC_SPLIT_AUTO_MODE (0)\nAuto Mode (default)\nTwo-way SFE will be implicitly triggered based on input video resolution and encoding parameters\nNV_ENC_SPLIT_AUTO_FORCED_MODE (1)\nForce Auto Mode\nNV_ENC_SPLIT_TWO_FORCED_MODE (2)\nForce two-way SFE\nThe respective SFE configuration will be used regardless of the input video and encoding parameters\nNV_ENC_SPLIT_THREE_FORCED_MODE (3)\nForce three-way SFE\nNV_ENC_SPLIT_DISABLE_MODE (15)\nForce no SFE\nTable 2. Selected SFE type and description per ```NV_ENC_SPLIT_ENCODE_MODE``` option The latest Video Codec SDK encoding sample ```AppEncMultiInstance``` also highlights how to add explicit SFE control to an application.\nPerformance and compression efficiency benchmarking\nSeveral configurations and input 8K videos were tested, which are listed in Table 3.\nBenchmarking configuration\nGPU\nGPU RTX 6000 Ada Generation (3 NVENCs)\nInput videos\n7 videos (4 gaming and 3 natural)\nEncoders\nHEVC and AV1\nPresets\nP1 (fastest), P4 (medium) and P7 (slowest)\nTuning Information\nLow latency (LL) and high quality (HQ)\nBitrates\n15, 20, 60, 150, and 250 Mbps\nTable 3. Benchmarking configuration summary Two types of benchmarks were performed:\nTranscoding Performance: Transcoding was used to minimize the influence of system bottlenecks (file I/O and memory copies between CPU and GPU). To test transcoding, the original 8K videos were pre-encoded with very high bitrates. During transcoding, NVDEC decodes the video. It is encoded by one to three NVENCs, when no split, two-way SFE, and three-way SFE are used, respectively. The performance results are shown in Figures 4 and 5 for HEVC and AV1, respectively.\nCompression Efficiency Penalty: By splitting encoding work across several NVENCs, a compression efficiency penalty is expected. To measure this penalty, BD-RATE was used across several benchmark configurations to compare the compression efficiency between no-split, two-way SFE, and three-way SFE. This metric indicates the average compression efficiency penalty for the same objective quality. The objective quality metric used in these benchmarks was PSNR. The compression efficiency penalty results are shown in Figures 6 and 7 for HEVC and AV1, respectively.\nBar chart showing average performance benchmarking results for 8K transcoding using HEVC.\nFigure 4. Average performance benchmarking results for 8K transcoding using HEVC Bar chart showing average performance benchmarking results for 8K transcoding using AV1.\nFigure 5. Average performance benchmarking results for 8K transcoding using AV1 When using two-way SFE, expect an average performance scaling of about 1.8x for both HEVC and AV1. Three-way SFE can achieve a performance scaling of up to 2.95x for HEVC and 2.31x for AV1. In practice, this enables 8K60 video encoding with NVIDIA RTX 6000 Ada Generation, using both HEVC and AV1, with LL and HQ tuning information at a medium preset (P4).\nGiven that one to three NVENCs and a single NVDEC are used, NVDEC may become the bottleneck when transcoding 8K. For this reason, the fastest preset (P1) can result in the FPS reaching a maximum of about 120 FPS on average. This is the average maximum performance achieved by a single NVDEC at 8K.\nYou can observe better scaling as long as NVDEC isn\u2019t the bottleneck. This is the case for slower presets, such as P4 and P7, where the performance scaling is much better in comparison to P1.\nChart showing average compression efficiency penalty results for 8K encoding using HEVC.\nFigure 6. Average compression efficiency penalty results for 8K encoding using HEVC Chart showing average compression efficiency penalty results for 8K encoding using AV1.\nFigure 7. Average compression efficiency penalty results for 8K encoding using AV1 In general, the compression efficiency penalty isn\u2019t expected to exceed 2% for two-way SFE and 4% for three-way SFE when using BD-RATE (PSNR) to measure quality. This penalty is more noticeable for HQ tuning information than for LL. Additionally, according to the benchmarks performed, this penalty is slightly more prominent when using HEVC compared to AV1.\nAlthough this compression efficiency penalty is still relatively low compared to the performance tradeoff, it\u2019s up to the user to determine if the required use case benefits from more performance or compression efficiency. Regardless, the NVENCODE API provides full control over SFE not only for 8K but also for lower resolutions.\nSummary\nSplit-frame encoding (SFE) is a breakthrough feature that unlocks video encoding capabilities at 8K60 and beyond. It empowers users to harness the power of multiple NVENCs within NVIDIA Ada Lovelace architecture GPUs for encoding a single video sequence. This post has explained the performance advantages of two-way SFE (using two NVENCs) and three-way SFE (using three NVENCs). The latest NVIDIA Video Codec SDK provides explicit control over SFE for optimal customization."}], "https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/": [{"text": "H2O.ai and NVIDIA are collaborating to provide an end-to-end workflow for generative AI and data science in financial services. They aim to accelerate the development and deployment of large language models (LLMs) for various applications beyond natural language processing, including image generation. The integration of H2O.ai's LLM Studio and Driverless AI AutoML with the NVIDIA AI Enterprise platform enables organizations to build and deploy their own customized models for tasks such as fraud detection and investment analysis. The use of generative AI and LLMs can help financial institutions stay ahead of markets with real-time calculations and tap into alternative data sources for deeper insights. The convergence of generative AI and predictive analytics is empowering firms to outperform markets, minimize risks, and enhance customer experiences through chatbots and intelligent automation. The partnership between H2O.ai and NVIDIA provides tools for accelerated machine learning analytics and enables the development of financial domain-specific applications in a shorter period. Ultimately, the collaboration aims to democratize AI and provide organizations with the flexibility and control to develop their own AI models for maximum accuracy and productivity in the financial services sector.", "text_components": ["Accelerating Inference on End-to-End Workflows with H2O.ai and NVIDIA\nData scientists are combining generative AI and predictive analytics to build the next generation of AI applications. In financial services, AI modeling and inference can be used for solutions such as alternative data for investment analysis, AI intelligent document automation, and fraud detection in trading, banking, and payments.\nH2O.ai and NVIDIA are working together to provide an end-to-end workflow for generative AI and data science, using the NVIDIA AI Enterprise platform and H2O.ai\u2019s LLM Studio and Driverless AI AutoML. H2O.ai also uses NVIDIA AI Enterprise to deploy next-generation AI inference, including large language models (LLMs) for safe and trusted enterprise-grade FinanceGPTs and custom applications at scale.\nThis integration aims to help organizations develop and deploy their own LLMs and customized models for various applications, beyond natural language processing (NLP), including image generation. These models enable the use of multiple modalities of content\u2014such as text, audio, video, images, and code\u2014to generate new content for broader applications.\nThere\u2019s a pressing need for FSI firms to use generative AI and accelerate the innovation that leads to new product opportunities and reduced costs in operational areas. Both regulated (banks, broker-dealers, asset managers, and insurers) and unregulated (hedge funds and proprietary traders) financial institutions are working on big-picture solutions with the convergence of generative AI and data science applications.\nThis post provides an overview of different data science use cases and the tools to build integrated applications for the following progression areas as financial institutions adopt the latest AI models:\nNewer generative AI and LLMs use cases\nAccelerated inference for trading and risk\nIntelligent automation and chatbot experiences\nData science and accelerated machine learning analytics on NVIDIA Triton Inference Server\nConverged generative AI and predictive analytics", "Generative AI and LLMs on accelerated inferencing for trading and risk\nThe holy grail of investing is to stay ahead of markets with real-time calculations and tap into alternative data sources, which are typically unstructured. 70% or more of organizational information is unstructured and is currently not tapped by organizations, as detailed in an IDC report on unstructured data.\nToday, capital markets typically rely on traditional sources of tabular data called market data, which is stored in columns and rows. Newer, alternative data sources must be tapped to gain an information edge.", "Alternative data sources\nAlternative data is a category of unstructured information obtained from non-traditional sources with context unavailable through traditional market data. It can provide deeper insights, extend the fundamental analysis that is the qualitative process, and capture the behavioral finance element. Sources of alternative data can include any of the following:\nEarnings call transcripts\nFederal Reserve meeting notes\nSocial media\nNews\nSatellite imagery\nFinancial filings\nGIF shows an earnings transcript sentiment generated from AI.\nFigure 1. Generative AI for transcript sentiment\nAlternative data uses this information to provide a deeper understanding of a company\u2019s financial health or counterparty (Figure 2) for analyzing financial filings, which can be used in trading and risk management decisions. Investment leaders must contend with analyzing this unstructured alternative to make timely, informed decisions and stay ahead of the market.\nTwo screenshots of a public filing and an AI-generated summary.\nFigure 2. Generative AI analyzing SEC 10-K filings", "Impact on trading and risk\nWith generative AI and predictive analytics converging, financial firms are using NVIDIA and H2O.ai integrations to build their own generative AI NLP models and LLMs for alternative data to serve internal needs and external customers. As a result, they are helping their customers outperform markets and work on their fiduciary duty to achieve excess returns and alpha while minimizing risk for decision-making.\nOne key use case for NLP LLMs with generative AI has been to isolate signals from unstructured data which is usually noisy and then index to structured data to drive insights through sentiment analysis, question and answer, and summarization (for example, your FirmGPT) for a universe of ticker symbols. This process can provide structured signals similar to fundamental analysis in financial markets that can be used for downstream models to meet risk and return goals.\nH2O.ai\u2019s h2oGPT LLM integrated with NVIDIA Triton Inference Server, part of the NVIDIA AI Enterprise platform, can provide quick, generative AI LLMOps ability to data scientists to train and productionalize applications at a lower cost of operation since customers can train and deploy multiple models within their enterprises.\nOrganizations can use foundational LLMs on unstructured sources of information, such as financial news, along with customization techniques for understanding the financial domain better.\nEnd users can interface with customized models to source and validate ground truth responses by using retrieval-augmented generation (RAG).\nH2O.ai\u2019s LLM solutions running on NVIDIA Triton Inference Server showed accelerated inference with more tokens and lower latency.\nBar chart showing a comparison of output tokens, with Triton Inference Server having ~600 output tokens/second and vLLMSystem having ~550 output tokens/second.\nFigure 3. Higher output tokens with NVIDIA Triton Inference Server in H20.ai\u2019s environment\nFigure 3 shows the reduced total cost of ownership (TCO) with increased return on investment (ROI) for deploying and operating multiple LLMs. For more information, see NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200.", "Generative AI for intelligent automation and chatbot experiences\nGenerative AI LLMs are also used for chatbots for internal employees and external customer experiences. Chatbots help with organizational productivity and operations, generating cost savings, increasing profit margins, and driving enhanced customer experience with customer churn.\nIn customer experiences through generative AI, the goal is to enable hyper-personalization and satisfaction (Figure 4), increasing the net promoter score (NPS) and customer satisfaction score (CSAT) for your organization.\nFor internal employee experiences, AI tools enhance productivity by helping to relay information to management for decision-making insights. Externally, AI tools can be used to cater to customers through agent assistants.\nScreenshot of the H20.ai chatbot.\nFigure 4. Generative AI next best conversation", "Customer Spotlight: Commonwealth Bank of Australia\nYou can use H20.ai guardrails and NVIDIA NeMo Guardrails with tools such as langchain to develop more powerful custom GPT models with the maximum flexible options. H20.ai also has model governance, guardrails, and explainability tools that help with model explanations, all for use by customers in regulated environments.\nH2O.ai has democratized generative AI so that enterprises can safely build their own state-of-the-art LLMs. Commonwealth Bank of Australia has used this to build its own GPT, tailored to what its customers need.\nBoth NVIDIA and H20.ai believe every organization needs to own its LLM and GPT just as they need to own their brand, data, algorithms, and models. Open-source generative AI is bringing transformation to democratizing value from AI while preserving data, code, and content ownership.\n\u201cWe have a shared vision with H2O.ai for democratizing AI responsibly and, through our partnership, we are now able to create ground-up generative AI solutions that allow us to have true control of the way we use data, techniques, and training. Responsible AI is not only a question of bias and explainability. It is also about the accountability you take: for the way you develop it and the outcomes you deliver,\u201d said Dan Jermyn, chief decision scientist at the Commonwealth Bank of Australia.", "Data science and accelerated machine learning analytics on NVIDIA Triton Inference Server\nAI is revolutionizing risk assessment for credit, fraud modeling, and risk management using predictive techniques and algorithms such as XGBoost. Accelerated machine learning is applied in financial services for use cases such as predicting limit order book prices, underwriting credit products, and detecting fraud in financial transactions.\nA fraud prediction pipeline contains not just one model but a series tied together, enabling more dynamic abilities to provide insights into prospective customers. Many fraud models are based on behavioral attributes for various segments (Figure 5):\nGeographic location vs. where the transaction happened\nTransaction type (card present vs. card not present)\nAverage transaction size\nAverage transaction frequency over varied time windows and time aggregations\nFor example, 30/60/90 days vs. weekly/monthly/quarterly/annually\nThese are based on various business rules or predictive feature characteristics. For example, if the same merchant is running increasingly large transactions over a short period, then the model algorithm can flag the transaction for potential fraud.\nIn anti-money laundering (AML) and transaction fraud, the key is to know your customer (KYC) and know your customer\u2019s customer (Customer). You establish their identity and understand the behavioral attributes of customers and other agents, including fraud perpetrators.\nIn each set of models, you understand more about the customers and actors using AI techniques, such as intelligent automation. This lets you understand whether the customer\u2019s identity was breached to assign a better score to the transaction. Better scores help with the reliability of the transaction and provide a safe and secure experience to customers. Transactions are stopped based on the identity verification key for KYC and behavioral attributes, and whether fraudsters or real customers are performing the transaction.\nScreenshot of the H20.ai interface for fraud detection.\nFigure 5. Transaction fraud detection AI\nAML and fraud detection empower financial institutions to stay ahead of emerging threats. Multiple factors play a key role in preventing fraud in financial applications:\nIntelligent document automation based on generative AI models where more diverse information is processed for identity\nBehavioral attributes\nTraditional data science\nH2O.ai Driverless AI has integrated NVIDIA Triton Inference Server, RAPIDS cuML, and RAPIDS cuDF (Figure 6), enabling financial institutions to accelerate their fraud detection applications.\nDiagram shows how data scientists would use the H20.ai and NVIDIA AI architecture.\nFigure 6. H2O.ai Driverless AI architecture with RAPIDS and NVIDIA Triton Inference Server (Source: H20.ai )", "Converged generative AI and predictive analytics\nWe are increasingly seeing a convergence of data science and AI requirements from end customers. AI applications based on LLMs, even if they are based on unstructured, multi-modal data like NLP, image, and audio (Figure 7), have to be tied to tabular data science solutions. End financial solutions use a combination of data science techniques and accelerated predictive analytics such as XGBoost with upcoming generative AI solutions.\nThere is a convergence of new generative AI greenfield solutions with legacy brownfield data science solutions to make end-user applications more productive and AI-savvy.\nScreenshot of the interface for viewing transcript, AI-generated summary, and sentiment.\nFigure 7. Converged AI audio summarization and sentiment", "Customer spotlight: North American Bancard\nThis is reflected at North American Bancard (NAB), which is exploring opportunities to seamlessly integrate more of H2O.ai\u2019s next-gen LLM-powered applications into its operations driven by a shared alignment on goals and vision. This partnership promises to yield significant benefits in optimizing NAB\u2019s operations, streamlining its business processes, and automating internal workflows. Ultimately, this enables greater fulfillment and productivity.\n\u201cH2O.ai consistently demonstrates a vested interest in our continued success and is ready to explore new ways to leverage LLM-powered solutions to enable greater fulfillment and productivity,\u201d said Jeffrey Vagg, chief data and analytics officer at NAB.\nVagg envisions that generative AI has the potential to elevate NAB\u2019s performance, enhance merchant support, and bolster fraud detection capabilities, effectively identifying malicious actors attempting to exploit vulnerabilities within the payments industry.\n\u201cWe believe every organization can safely create and own their own LLMs to bring transformation to their customers,\u201d Vagg added. The depth of trust that NAB instills in H2O.ai is proving instrumental in navigating the intricacies of generative AI.\nWorking with H20.ai, NVIDIA provides those key elements needed for customer success and aligned goals for creating their own LLMs and customization.", "Blocking issues and solutions\nFinancial firms tend to fall into two camps: regulated and unregulated. Regulated institutions are typically large financial institutions like banks that deploy AI at scale across multiple business units. On the other end, unregulated institutions include hedge funds and proprietary trading firms, which are more advanced in adopting technologies. Though institutions initially lagged in technology adoption, the gap is narrowing.\nAnother blocker for AI adoption in financial services has been the ability to customize AI models to the financial domain and financial applications to deliver better accuracy. Traditionally, legacy data science applications are based on tabular data and is required for banks to productionalize these models in a compliant manner.\nBoth regulated and unregulated institutions want to be able to harness their data science resources, which are scarce and widely sought after. They want to build financial AI applications with maximum accuracy for the financial domain and make the best use of their time and productivity.\nAs the customer stories show, both customers have adopted AI LLM Ops and MLOPs solutions and need productivity tools to get to enterprise production use cases faster in financial services. This needs development productivity accelerators that can bring together such diverse solutions, integrating generative AI with data science and analytic models into a single application workflow.\nH20.ai and NVIDIA provide the ability to develop integrated financial domain-specific applications in a shorter period, harnessing the ability of NVIDIA AI Enterprise with H20.ai LLM Studio and H20.ai Driverless. NVIDIA AI Enterprise supports accelerated, high-performance inference with\nNVIDIA NeMo\nNVIDIA Triton Inference Server\nNVIDIA TensorRT\nNVIDIA TensorRT-LLM\nOther NVIDIA AI software\nDiagram of AI tools on H20.ai and NVIDIA platforms.\nFigure 8. Converged AI ecosystem powered by H2O.ai and NVIDIA\nH2O.ai\u2019s LLM Studio is an open-source, no-code solution that can empower organizations to fine-tune and evaluate LLMs. Financial institutions can customize models to their domain with NVIDIA NeMo, a framework that provides the ability to fine-tune and supervise models with various customizations such as prompt learning, instruction tuning, and RLHF (Figure 8).\nh2oGPT APIs are sustainably built on open source\u2013the most responsible way forward for regulated industries like financial services.\nDevelopers have total control and customization over their AI models. This level of customization and control is unmatched by anything in the market today.\nModels and a customization framework can efficiently accomplish tasks for your domain with your own intellectual property ready for distribution and monetization at a fraction of the cost to customize, run, and operate against other proprietary LLM models.\nCustomers can use h2oGPT built-in guardrails along with NeMo Guardrails, to empower enterprise AI use cases.", "NVIDIA NeMo Retriever\nWith H20.ai\u2019s built-in RAG, you can seamlessly integrate generative AI models into your existing data store. H20.ai, along with NVIDIA NeMo Retriever, can use NVIDIA-optimized RAG capabilities.\nNeMo Retriever is a generative AI microservice that lets enterprises connect custom LLMs to enterprise data to deliver highly accurate responses for their AI applications and provide more accurate responses. It is part of the NVIDIA AI Enterprise software platform.\nDevelopers using the microservice can connect their AI applications to business data wherever it resides, bringing the compute and software tooling applications to your data. This can help companies develop and run NVIDIA-accelerated inference applications on virtually any data center or cloud.", "Summary\nNVIDIA and H20.ai together offers the maximum flexibility and tools to develop generative AI applications to end customers. This offers a quick pathway to develop and productionalize converged AI and data science applications at a fast pace, solving a significant bottleneck to enterprises.\nAs a result, you can develop your own LLMs where the intellectual property and the ability to monetize and redistribute resides with you. You can turn existing investments, data, and resources into revenue centers, realizing returns out of AI investments and subsequently investing in further AI projects.\nTogether, this maximizes ROI, lowers TCO, and offers enhanced productivity with the flexibility and choice in tooling and framework to build converged applications, offering unprecedented value to our end customers."], "document_title": "Accelerating Inference on End-to-End Workflows with H2O.ai and NVIDIA", "document_url": "https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/", "document_date": "2024-01-04T14:00:00", "document_date_modified": "2024-01-11T19:49:35", "document_full_text": "Accelerating Inference on End-to-End Workflows with H2O.ai and NVIDIA\nData scientists are combining generative AI and predictive analytics to build the next generation of AI applications. In financial services, AI modeling and inference can be used for solutions such as alternative data for investment analysis, AI intelligent document automation, and fraud detection in trading, banking, and payments.\nH2O.ai and NVIDIA are working together to provide an end-to-end workflow for generative AI and data science, using the NVIDIA AI Enterprise platform and H2O.ai\u2019s LLM Studio and Driverless AI AutoML. H2O.ai also uses NVIDIA AI Enterprise to deploy next-generation AI inference, including large language models (LLMs) for safe and trusted enterprise-grade FinanceGPTs and custom applications at scale.\nThis integration aims to help organizations develop and deploy their own LLMs and customized models for various applications, beyond natural language processing (NLP), including image generation. These models enable the use of multiple modalities of content\u2014such as text, audio, video, images, and code\u2014to generate new content for broader applications.\nThere\u2019s a pressing need for FSI firms to use generative AI and accelerate the innovation that leads to new product opportunities and reduced costs in operational areas. Both regulated (banks, broker-dealers, asset managers, and insurers) and unregulated (hedge funds and proprietary traders) financial institutions are working on big-picture solutions with the convergence of generative AI and data science applications.\nThis post provides an overview of different data science use cases and the tools to build integrated applications for the following progression areas as financial institutions adopt the latest AI models:\nNewer generative AI and LLMs use cases\nAccelerated inference for trading and risk\nIntelligent automation and chatbot experiences\nData science and accelerated machine learning analytics on NVIDIA Triton Inference Server\nConverged generative AI and predictive analytics\nGenerative AI and LLMs on accelerated inferencing for trading and risk\nThe holy grail of investing is to stay ahead of markets with real-time calculations and tap into alternative data sources, which are typically unstructured. 70% or more of organizational information is unstructured and is currently not tapped by organizations, as detailed in an IDC report on unstructured data.\nToday, capital markets typically rely on traditional sources of tabular data called market data, which is stored in columns and rows. Newer, alternative data sources must be tapped to gain an information edge.\nAlternative data sources\nAlternative data is a category of unstructured information obtained from non-traditional sources with context unavailable through traditional market data. It can provide deeper insights, extend the fundamental analysis that is the qualitative process, and capture the behavioral finance element. Sources of alternative data can include any of the following:\nEarnings call transcripts\nFederal Reserve meeting notes\nSocial media\nNews\nSatellite imagery\nFinancial filings\nGIF shows an earnings transcript sentiment generated from AI.\nFigure 1. Generative AI for transcript sentiment\nAlternative data uses this information to provide a deeper understanding of a company\u2019s financial health or counterparty (Figure 2) for analyzing financial filings, which can be used in trading and risk management decisions. Investment leaders must contend with analyzing this unstructured alternative to make timely, informed decisions and stay ahead of the market.\nTwo screenshots of a public filing and an AI-generated summary.\nFigure 2. Generative AI analyzing SEC 10-K filings\nImpact on trading and risk\nWith generative AI and predictive analytics converging, financial firms are using NVIDIA and H2O.ai integrations to build their own generative AI NLP models and LLMs for alternative data to serve internal needs and external customers. As a result, they are helping their customers outperform markets and work on their fiduciary duty to achieve excess returns and alpha while minimizing risk for decision-making.\nOne key use case for NLP LLMs with generative AI has been to isolate signals from unstructured data which is usually noisy and then index to structured data to drive insights through sentiment analysis, question and answer, and summarization (for example, your FirmGPT) for a universe of ticker symbols. This process can provide structured signals similar to fundamental analysis in financial markets that can be used for downstream models to meet risk and return goals.\nH2O.ai\u2019s h2oGPT LLM integrated with NVIDIA Triton Inference Server, part of the NVIDIA AI Enterprise platform, can provide quick, generative AI LLMOps ability to data scientists to train and productionalize applications at a lower cost of operation since customers can train and deploy multiple models within their enterprises.\nOrganizations can use foundational LLMs on unstructured sources of information, such as financial news, along with customization techniques for understanding the financial domain better.\nEnd users can interface with customized models to source and validate ground truth responses by using retrieval-augmented generation (RAG).\nH2O.ai\u2019s LLM solutions running on NVIDIA Triton Inference Server showed accelerated inference with more tokens and lower latency.\nBar chart showing a comparison of output tokens, with Triton Inference Server having ~600 output tokens/second and vLLMSystem having ~550 output tokens/second.\nFigure 3. Higher output tokens with NVIDIA Triton Inference Server in H20.ai\u2019s environment\nFigure 3 shows the reduced total cost of ownership (TCO) with increased return on investment (ROI) for deploying and operating multiple LLMs. For more information, see NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200.\nGenerative AI for intelligent automation and chatbot experiences\nGenerative AI LLMs are also used for chatbots for internal employees and external customer experiences. Chatbots help with organizational productivity and operations, generating cost savings, increasing profit margins, and driving enhanced customer experience with customer churn.\nIn customer experiences through generative AI, the goal is to enable hyper-personalization and satisfaction (Figure 4), increasing the net promoter score (NPS) and customer satisfaction score (CSAT) for your organization.\nFor internal employee experiences, AI tools enhance productivity by helping to relay information to management for decision-making insights. Externally, AI tools can be used to cater to customers through agent assistants.\nScreenshot of the H20.ai chatbot.\nFigure 4. Generative AI next best conversation\nCustomer Spotlight: Commonwealth Bank of Australia\nYou can use H20.ai guardrails and NVIDIA NeMo Guardrails with tools such as langchain to develop more powerful custom GPT models with the maximum flexible options. H20.ai also has model governance, guardrails, and explainability tools that help with model explanations, all for use by customers in regulated environments.\nH2O.ai has democratized generative AI so that enterprises can safely build their own state-of-the-art LLMs. Commonwealth Bank of Australia has used this to build its own GPT, tailored to what its customers need.\nBoth NVIDIA and H20.ai believe every organization needs to own its LLM and GPT just as they need to own their brand, data, algorithms, and models. Open-source generative AI is bringing transformation to democratizing value from AI while preserving data, code, and content ownership.\n\u201cWe have a shared vision with H2O.ai for democratizing AI responsibly and, through our partnership, we are now able to create ground-up generative AI solutions that allow us to have true control of the way we use data, techniques, and training. Responsible AI is not only a question of bias and explainability. It is also about the accountability you take: for the way you develop it and the outcomes you deliver,\u201d said Dan Jermyn, chief decision scientist at the Commonwealth Bank of Australia.\nData science and accelerated machine learning analytics on NVIDIA Triton Inference Server\nAI is revolutionizing risk assessment for credit, fraud modeling, and risk management using predictive techniques and algorithms such as XGBoost. Accelerated machine learning is applied in financial services for use cases such as predicting limit order book prices, underwriting credit products, and detecting fraud in financial transactions.\nA fraud prediction pipeline contains not just one model but a series tied together, enabling more dynamic abilities to provide insights into prospective customers. Many fraud models are based on behavioral attributes for various segments (Figure 5):\nGeographic location vs. where the transaction happened\nTransaction type (card present vs. card not present)\nAverage transaction size\nAverage transaction frequency over varied time windows and time aggregations\nFor example, 30/60/90 days vs. weekly/monthly/quarterly/annually\nThese are based on various business rules or predictive feature characteristics. For example, if the same merchant is running increasingly large transactions over a short period, then the model algorithm can flag the transaction for potential fraud.\nIn anti-money laundering (AML) and transaction fraud, the key is to know your customer (KYC) and know your customer\u2019s customer (Customer). You establish their identity and understand the behavioral attributes of customers and other agents, including fraud perpetrators.\nIn each set of models, you understand more about the customers and actors using AI techniques, such as intelligent automation. This lets you understand whether the customer\u2019s identity was breached to assign a better score to the transaction. Better scores help with the reliability of the transaction and provide a safe and secure experience to customers. Transactions are stopped based on the identity verification key for KYC and behavioral attributes, and whether fraudsters or real customers are performing the transaction.\nScreenshot of the H20.ai interface for fraud detection.\nFigure 5. Transaction fraud detection AI\nAML and fraud detection empower financial institutions to stay ahead of emerging threats. Multiple factors play a key role in preventing fraud in financial applications:\nIntelligent document automation based on generative AI models where more diverse information is processed for identity\nBehavioral attributes\nTraditional data science\nH2O.ai Driverless AI has integrated NVIDIA Triton Inference Server, RAPIDS cuML, and RAPIDS cuDF (Figure 6), enabling financial institutions to accelerate their fraud detection applications.\nDiagram shows how data scientists would use the H20.ai and NVIDIA AI architecture.\nFigure 6. H2O.ai Driverless AI architecture with RAPIDS and NVIDIA Triton Inference Server (Source: H20.ai )\nConverged generative AI and predictive analytics\nWe are increasingly seeing a convergence of data science and AI requirements from end customers. AI applications based on LLMs, even if they are based on unstructured, multi-modal data like NLP, image, and audio (Figure 7), have to be tied to tabular data science solutions. End financial solutions use a combination of data science techniques and accelerated predictive analytics such as XGBoost with upcoming generative AI solutions.\nThere is a convergence of new generative AI greenfield solutions with legacy brownfield data science solutions to make end-user applications more productive and AI-savvy.\nScreenshot of the interface for viewing transcript, AI-generated summary, and sentiment.\nFigure 7. Converged AI audio summarization and sentiment\nCustomer spotlight: North American Bancard\nThis is reflected at North American Bancard (NAB), which is exploring opportunities to seamlessly integrate more of H2O.ai\u2019s next-gen LLM-powered applications into its operations driven by a shared alignment on goals and vision. This partnership promises to yield significant benefits in optimizing NAB\u2019s operations, streamlining its business processes, and automating internal workflows. Ultimately, this enables greater fulfillment and productivity.\n\u201cH2O.ai consistently demonstrates a vested interest in our continued success and is ready to explore new ways to leverage LLM-powered solutions to enable greater fulfillment and productivity,\u201d said Jeffrey Vagg, chief data and analytics officer at NAB.\nVagg envisions that generative AI has the potential to elevate NAB\u2019s performance, enhance merchant support, and bolster fraud detection capabilities, effectively identifying malicious actors attempting to exploit vulnerabilities within the payments industry.\n\u201cWe believe every organization can safely create and own their own LLMs to bring transformation to their customers,\u201d Vagg added. The depth of trust that NAB instills in H2O.ai is proving instrumental in navigating the intricacies of generative AI.\nWorking with H20.ai, NVIDIA provides those key elements needed for customer success and aligned goals for creating their own LLMs and customization.\nBlocking issues and solutions\nFinancial firms tend to fall into two camps: regulated and unregulated. Regulated institutions are typically large financial institutions like banks that deploy AI at scale across multiple business units. On the other end, unregulated institutions include hedge funds and proprietary trading firms, which are more advanced in adopting technologies. Though institutions initially lagged in technology adoption, the gap is narrowing.\nAnother blocker for AI adoption in financial services has been the ability to customize AI models to the financial domain and financial applications to deliver better accuracy. Traditionally, legacy data science applications are based on tabular data and is required for banks to productionalize these models in a compliant manner.\nBoth regulated and unregulated institutions want to be able to harness their data science resources, which are scarce and widely sought after. They want to build financial AI applications with maximum accuracy for the financial domain and make the best use of their time and productivity.\nAs the customer stories show, both customers have adopted AI LLM Ops and MLOPs solutions and need productivity tools to get to enterprise production use cases faster in financial services. This needs development productivity accelerators that can bring together such diverse solutions, integrating generative AI with data science and analytic models into a single application workflow.\nH20.ai and NVIDIA provide the ability to develop integrated financial domain-specific applications in a shorter period, harnessing the ability of NVIDIA AI Enterprise with H20.ai LLM Studio and H20.ai Driverless. NVIDIA AI Enterprise supports accelerated, high-performance inference with\nNVIDIA NeMo\nNVIDIA Triton Inference Server\nNVIDIA TensorRT\nNVIDIA TensorRT-LLM\nOther NVIDIA AI software\nDiagram of AI tools on H20.ai and NVIDIA platforms.\nFigure 8. Converged AI ecosystem powered by H2O.ai and NVIDIA\nH2O.ai\u2019s LLM Studio is an open-source, no-code solution that can empower organizations to fine-tune and evaluate LLMs. Financial institutions can customize models to their domain with NVIDIA NeMo, a framework that provides the ability to fine-tune and supervise models with various customizations such as prompt learning, instruction tuning, and RLHF (Figure 8).\nh2oGPT APIs are sustainably built on open source\u2013the most responsible way forward for regulated industries like financial services.\nDevelopers have total control and customization over their AI models. This level of customization and control is unmatched by anything in the market today.\nModels and a customization framework can efficiently accomplish tasks for your domain with your own intellectual property ready for distribution and monetization at a fraction of the cost to customize, run, and operate against other proprietary LLM models.\nCustomers can use h2oGPT built-in guardrails along with NeMo Guardrails, to empower enterprise AI use cases.\nNVIDIA NeMo Retriever\nWith H20.ai\u2019s built-in RAG, you can seamlessly integrate generative AI models into your existing data store. H20.ai, along with NVIDIA NeMo Retriever, can use NVIDIA-optimized RAG capabilities.\nNeMo Retriever is a generative AI microservice that lets enterprises connect custom LLMs to enterprise data to deliver highly accurate responses for their AI applications and provide more accurate responses. It is part of the NVIDIA AI Enterprise software platform.\nDevelopers using the microservice can connect their AI applications to business data wherever it resides, bringing the compute and software tooling applications to your data. This can help companies develop and run NVIDIA-accelerated inference applications on virtually any data center or cloud.\nSummary\nNVIDIA and H20.ai together offers the maximum flexibility and tools to develop generative AI applications to end customers. This offers a quick pathway to develop and productionalize converged AI and data science applications at a fast pace, solving a significant bottleneck to enterprises.\nAs a result, you can develop your own LLMs where the intellectual property and the ability to monetize and redistribute resides with you. You can turn existing investments, data, and resources into revenue centers, realizing returns out of AI investments and subsequently investing in further AI projects.\nTogether, this maximizes ROI, lowers TCO, and offers enhanced productivity with the flexibility and choice in tooling and framework to build converged applications, offering unprecedented value to our end customers."}], "https://developer.nvidia.com/blog/q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update/": [{"text": "In 2019, NVIDIA released a path tracing update for the 1997 game Quake II, showcasing cutting-edge graphics on consumer GPUs. NVIDIA engineer Alexey Panteleev worked on the project, which started from a tech demo by Christoph Schied. The update features accurate lighting, reflection, and refraction effects. Quake II's enduring popularity led to a remastered version with new levels by MachineGames. Path tracing is becoming a standard in games, with titles like Cyberpunk 2077 using it. Modders can achieve similar results with RTX Remix. Bringing path tracing to Minecraft with RTX involved similar principles but also required different approaches due to the game's dynamic, user-modifiable nature. Interested players can try Quake II RTX on Steam and learn more about NVIDIA RTX Path Tracing SDK and RTX Remix for modding.", "text_components": ["Q&A: Looking Back to When 1997\u2019s Quake II Got a Path Tracing Update\nIn 2019, if you wanted to check out the cutting edge in video game graphics, you needed an NVIDIA GeForce RTX 20 Series GPU and a copy of a game that was released in 1997, Quake II. With those pieces in hand, you could be among the first players in the world to see path tracing running in real-time on a consumer GPU. \u201cSomehow a game from 1997 convinced me it was time to upgrade,\u201d enthused PC Gamer critic Wes Fenlon.\nAs I look back at the history of NVIDIA RTX, now with over 500 RTX games and applications powered by DLSS, ray tracing, and AI-enhanced technologies, I thought it would be a good time to sit down and talk with Alexey Panteleev, an NVIDIA engineer who worked closely on the Quake II RTX project.\nWhy did you choose Quake II as the subject for NVIDIA RTX\u2019s first demonstration of path tracing?\nWe didn\u2019t choose Quake II, it chose us. The project started from Q2VKPT, a tech demo created by Christoph Schied, who integrated a simple real-time path tracer into Quake II. We saw it as something new and cool and pushed the project to the finish line.\nWhen people download and play Quake II RTX, what details would you like them to look closely at?\nMy personal favorite detail in Quake II RTX is the reflection and refraction system. It uses a novel algorithm for denoising combined reflections and refractions without making them blurry or ghosty.\nOther than that, it\u2019s all about the lighting. Look at how accurate all the lighting and shadowing effects are in the game world! Every lamp on every level is casting physically correct soft shadows.\nVideo. Official trailer for Quake II RTX Why do you think Quake II has such enduring interest from players? It just got 28 new campaign levels from MachineGames this year!\nQuake II is one of the well-known classic games, and it\u2019s no wonder that it\u2019s got a huge fan base. After the remastered version of the original Quake was released in 2021 and warmly welcomed by the public, it made total sense for MachineGames to continue working on its sequel.\nHave you seen the work that Reddit user mSteward207 has done to make the Quake II remaster compatible with the NVIDIA RTX renderer? Any thoughts or suggestions to share?\nYes, I saw those screenshots. More importantly, I received and merged some pull requests on GitHub that improved the compatibility of Quake II RTX with the maps from the remaster. I\u2019m not ready to say what exactly is left to make all the remastered maps and models work really well with the NVIDIA RTX renderer at this time, but I guess there\u2019s some content work to be done, such as authoring PBR materials.\nDo you believe that path tracing will ultimately be the standard for lighting and rendering in games?\nIt\u2019s already being used in some of this year\u2019s biggest games, including Cyberpunk 2077: Phantom Liberty and Alan Wake II. The most recent product from Lightspeed Studios, RTX Remix, is designed to bring path-traced lighting to many retro games, too. Perhaps most games will have a realistic rendering that features path-traced lighting, and some stylized games just won\u2019t need it. We\u2019ll see.\nHow can modders of classic games get similar results?\nUse RTX Remix! It\u2019s still going through active development, but we\u2019ve seen some impressive results from the modding community already.\nWas the process of bringing path tracing to Minecraft with RTX the same, or were there different approaches to the two games?\nThe principles of real-time path tracing used in Quake II RTX and Minecraft with RTX are largely the same, with some interesting differences that the cubic world of Minecraft enables, such as using an irradiance cache. Minecraft is also more challenging to render because its worlds are by design dynamic and user-modifiable, so you cannot bake anything at all or make map-specific tweaks.", "Next steps\nTo play Quake II RTX for yourself, go to Steam. For more information about NVIDIA RTX, see the NVIDIA RTX Path Tracing SDK. To get started with the ultimate modding platform, see RTX Remix and our array of tools for game developers."], "document_title": "Q&A: Looking Back to When 1997\u2019s Quake II Got a Path Tracing Update", "document_url": "https://developer.nvidia.com/blog/q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update/", "document_date": "2023-12-20T22:15:39", "document_date_modified": "2023-12-21T19:01:14", "document_full_text": "Q&A: Looking Back to When 1997\u2019s Quake II Got a Path Tracing Update\nIn 2019, if you wanted to check out the cutting edge in video game graphics, you needed an NVIDIA GeForce RTX 20 Series GPU and a copy of a game that was released in 1997, Quake II. With those pieces in hand, you could be among the first players in the world to see path tracing running in real-time on a consumer GPU. \u201cSomehow a game from 1997 convinced me it was time to upgrade,\u201d enthused PC Gamer critic Wes Fenlon.\nAs I look back at the history of NVIDIA RTX, now with over 500 RTX games and applications powered by DLSS, ray tracing, and AI-enhanced technologies, I thought it would be a good time to sit down and talk with Alexey Panteleev, an NVIDIA engineer who worked closely on the Quake II RTX project.\nWhy did you choose Quake II as the subject for NVIDIA RTX\u2019s first demonstration of path tracing?\nWe didn\u2019t choose Quake II, it chose us. The project started from Q2VKPT, a tech demo created by Christoph Schied, who integrated a simple real-time path tracer into Quake II. We saw it as something new and cool and pushed the project to the finish line.\nWhen people download and play Quake II RTX, what details would you like them to look closely at?\nMy personal favorite detail in Quake II RTX is the reflection and refraction system. It uses a novel algorithm for denoising combined reflections and refractions without making them blurry or ghosty.\nOther than that, it\u2019s all about the lighting. Look at how accurate all the lighting and shadowing effects are in the game world! Every lamp on every level is casting physically correct soft shadows.\nVideo. Official trailer for Quake II RTX Why do you think Quake II has such enduring interest from players? It just got 28 new campaign levels from MachineGames this year!\nQuake II is one of the well-known classic games, and it\u2019s no wonder that it\u2019s got a huge fan base. After the remastered version of the original Quake was released in 2021 and warmly welcomed by the public, it made total sense for MachineGames to continue working on its sequel.\nHave you seen the work that Reddit user mSteward207 has done to make the Quake II remaster compatible with the NVIDIA RTX renderer? Any thoughts or suggestions to share?\nYes, I saw those screenshots. More importantly, I received and merged some pull requests on GitHub that improved the compatibility of Quake II RTX with the maps from the remaster. I\u2019m not ready to say what exactly is left to make all the remastered maps and models work really well with the NVIDIA RTX renderer at this time, but I guess there\u2019s some content work to be done, such as authoring PBR materials.\nDo you believe that path tracing will ultimately be the standard for lighting and rendering in games?\nIt\u2019s already being used in some of this year\u2019s biggest games, including Cyberpunk 2077: Phantom Liberty and Alan Wake II. The most recent product from Lightspeed Studios, RTX Remix, is designed to bring path-traced lighting to many retro games, too. Perhaps most games will have a realistic rendering that features path-traced lighting, and some stylized games just won\u2019t need it. We\u2019ll see.\nHow can modders of classic games get similar results?\nUse RTX Remix! It\u2019s still going through active development, but we\u2019ve seen some impressive results from the modding community already.\nWas the process of bringing path tracing to Minecraft with RTX the same, or were there different approaches to the two games?\nThe principles of real-time path tracing used in Quake II RTX and Minecraft with RTX are largely the same, with some interesting differences that the cubic world of Minecraft enables, such as using an irradiance cache. Minecraft is also more challenging to render because its worlds are by design dynamic and user-modifiable, so you cannot bake anything at all or make map-specific tweaks.\nNext steps\nTo play Quake II RTX for yourself, go to Steam. For more information about NVIDIA RTX, see the NVIDIA RTX Path Tracing SDK. To get started with the ultimate modding platform, see RTX Remix and our array of tools for game developers."}], "https://developer.nvidia.com/blog/qa-real-time-ray-tracing-in-a-cinematic-scene/": [{"text": "Real-time ray tracing has made significant advancements in recent years, with cinematic-quality rendering now achievable in real-time at 45 frames per second. NVIDIA's Project Sol showcased visuals at a level of fidelity previously seen only in high-end animated films. The difference between path tracing and ray tracing lies in the comprehensive nature of path tracing, which simulates the paths of many rays per pixel to produce more realistic lighting and shading. NVIDIA accelerated the timeline for real-time ray tracing with the introduction of RT and Tensor Cores and the development of DLSS. The next major challenge for graphics technology is achieving flawless real-time path tracing to enhance realism and visual fidelity in games. Developers starting with real-time ray tracing should focus on how it can enhance the overall gaming experience and strategically use ray-traced effects such as shadows, reflections, and global illumination to elevate their game's visual impact.", "text_components": ["Q&A: Real-Time Ray Tracing in a Cinematic Scene\nSix years ago, real-time ray tracing was seen as a pipe dream. Back then, cinematic-quality rendering required computer farms to slowly bake every frame overnight\u2014a painstaking process.\nBy 2018, this level of performance was achievable in real-time, at 45 frames per second, enabling applications like video games to take a massive leap in graphical quality.\nAs part of our RTX 500 celebration, we wanted to take a look back at NVIDIA\u2019s Project Sol. This real-time cinematic series showed off visuals at a level of fidelity that audiences had only ever seen in high-end animated films. Check them out below:\nVideo. Project Sol: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX\nAdditional videos in the series\nVideo \u2013 Project Sol Part 2: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX\nVideo \u2013 Project Sol Part 3: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX\nWe recently caught up with Gabriele Leone, a senior design director at NVIDIA and the creative director behind the Project Sol series.\nWhen you first started working on the Project Sol real-time cinematic, what were your expectations for the final project, and how close did you get to your initial vision?\nWorking on Project Sol was an exhilarating experience, especially because it was our first dive into developing real-time content with consumer GPUs supporting real-time ray tracing. Our prior project, Reflections, was a collaboration with Epic that relied on the power of an NVIDIA DGX Station. But with Project Sol, we shifted to using just a single GeForce RTX 2080 GPU, which was a remarkable leap forward. We did come quite close to realizing our vision, although, as with any project, we faced our share of deadlines and constraints. But overall, it was a fantastic journey.\nWhat is the difference between path tracing and ray tracing?\nPath tracing and ray tracing are both rendering techniques, but they have key differences. Ray tracing is a technique for generating an image by tracing the path of light as pixels in an image plane, simulating effects like reflections and shadows. Path tracing, on the other hand, is a more comprehensive method that extends ray tracing by simulating the paths of many rays per pixel to produce more realistic lighting and shading. While ray tracing is great for producing visually accurate images, path tracing takes it further by accounting for the complex interactions of light in a scene.\nWhy was it so common for people to say that real-time ray tracing was at least 10 years away? How did NVIDIA speed up that timeline?\nThe notion that real-time ray tracing was over a decade away was widespread at the time, with some estimates even stretching to 15 years. NVIDIA approached this challenge from all fronts, both in hardware and software. The introduction of RT and Tensor Cores and the development of DLSS played pivotal roles. It\u2019s amazing to reflect on the tremendous strides made in just five years, accelerating what many thought was a distant future. Now we have 500 RTX games and apps using RT or DLSS.\nIn the Project Sol series, the hero\u2019s face is never fully lit. Is that due to challenges with realistic skin lighting?\nThe choice not to fully reveal the hero\u2019s face was more a result of our project timeline constraints than technical challenges with skin lighting. Character development, particularly with realistic facial features, requires considerable time and resources, which we didn\u2019t have at our disposal for this project. So, it wasn\u2019t as much about the difficulties in rendering skin, but more about prioritizing project elements within the given time frame.\nWhat artists or studios have influenced you?\nMy artistic influences are more rooted in the games I grew up playing than specific studios. Titles like Warcraft 2 and 3, Starcraft: Brood War, Counter-Strike 1.6, Monkey Island 2 and 3, Broken Sword, Silent Hill 1 and 2, Super Smash Bros. Melee, and The Legend of Zelda: Ocarina of Time and Wind Waker deeply influenced me. These games shaped my love for art and real-time graphics. In terms of artists, Dal\u00ed, Craig Mullins, and Sparth have been significant influences.\nWhat\u2019s the next major challenge for graphics technology?\nThe next big frontier in graphics technology is achieving flawless real-time path tracing. This involves perfecting all possible effects under every conceivable condition, representing a significant leap in the realism and visual fidelity of real-time graphics.\nThanks to \u200ccontinuous advancements in RTX GPUs, which are becoming increasingly powerful, coupled with software innovations like DLSS 3.5, this path-traced reality is becoming more tangible. Games such as Alan Wake 2 and Cyberpunk 2077: Ultimate Edition serve as prime examples of the direction in which we\u2019re headed, indicating an even more exciting future for graphics technology.\nWhat are your tips for developers who are just starting to add real-time ray tracing to their development pipeline?\nFor developers starting with real-time ray tracing, I\u2019d advise focusing on how it can enhance the overall gaming experience. Ray tracing opens new possibilities that weren\u2019t possible before. However, simply turning on RTX won\u2019t unlock its full potential. Consider how ray-traced shadows, reflections, and global illumination can be strategically used. Thoughtful art direction, aligned with these capabilities, can significantly elevate your game\u2019s visual impact.\nLearn more about the NVIDIA RTX platform and our tools for game developers."], "document_title": "Q&A: Real-Time Ray Tracing in a Cinematic Scene", "document_url": "https://developer.nvidia.com/blog/qa-real-time-ray-tracing-in-a-cinematic-scene/", "document_date": "2023-12-20T21:10:44", "document_date_modified": "2023-12-20T23:52:41", "document_full_text": "Q&A: Real-Time Ray Tracing in a Cinematic Scene\nSix years ago, real-time ray tracing was seen as a pipe dream. Back then, cinematic-quality rendering required computer farms to slowly bake every frame overnight\u2014a painstaking process.\nBy 2018, this level of performance was achievable in real-time, at 45 frames per second, enabling applications like video games to take a massive leap in graphical quality.\nAs part of our RTX 500 celebration, we wanted to take a look back at NVIDIA\u2019s Project Sol. This real-time cinematic series showed off visuals at a level of fidelity that audiences had only ever seen in high-end animated films. Check them out below:\nVideo. Project Sol: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX\nAdditional videos in the series\nVideo \u2013 Project Sol Part 2: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX\nVideo \u2013 Project Sol Part 3: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX\nWe recently caught up with Gabriele Leone, a senior design director at NVIDIA and the creative director behind the Project Sol series.\nWhen you first started working on the Project Sol real-time cinematic, what were your expectations for the final project, and how close did you get to your initial vision?\nWorking on Project Sol was an exhilarating experience, especially because it was our first dive into developing real-time content with consumer GPUs supporting real-time ray tracing. Our prior project, Reflections, was a collaboration with Epic that relied on the power of an NVIDIA DGX Station. But with Project Sol, we shifted to using just a single GeForce RTX 2080 GPU, which was a remarkable leap forward. We did come quite close to realizing our vision, although, as with any project, we faced our share of deadlines and constraints. But overall, it was a fantastic journey.\nWhat is the difference between path tracing and ray tracing?\nPath tracing and ray tracing are both rendering techniques, but they have key differences. Ray tracing is a technique for generating an image by tracing the path of light as pixels in an image plane, simulating effects like reflections and shadows. Path tracing, on the other hand, is a more comprehensive method that extends ray tracing by simulating the paths of many rays per pixel to produce more realistic lighting and shading. While ray tracing is great for producing visually accurate images, path tracing takes it further by accounting for the complex interactions of light in a scene.\nWhy was it so common for people to say that real-time ray tracing was at least 10 years away? How did NVIDIA speed up that timeline?\nThe notion that real-time ray tracing was over a decade away was widespread at the time, with some estimates even stretching to 15 years. NVIDIA approached this challenge from all fronts, both in hardware and software. The introduction of RT and Tensor Cores and the development of DLSS played pivotal roles. It\u2019s amazing to reflect on the tremendous strides made in just five years, accelerating what many thought was a distant future. Now we have 500 RTX games and apps using RT or DLSS.\nIn the Project Sol series, the hero\u2019s face is never fully lit. Is that due to challenges with realistic skin lighting?\nThe choice not to fully reveal the hero\u2019s face was more a result of our project timeline constraints than technical challenges with skin lighting. Character development, particularly with realistic facial features, requires considerable time and resources, which we didn\u2019t have at our disposal for this project. So, it wasn\u2019t as much about the difficulties in rendering skin, but more about prioritizing project elements within the given time frame.\nWhat artists or studios have influenced you?\nMy artistic influences are more rooted in the games I grew up playing than specific studios. Titles like Warcraft 2 and 3, Starcraft: Brood War, Counter-Strike 1.6, Monkey Island 2 and 3, Broken Sword, Silent Hill 1 and 2, Super Smash Bros. Melee, and The Legend of Zelda: Ocarina of Time and Wind Waker deeply influenced me. These games shaped my love for art and real-time graphics. In terms of artists, Dal\u00ed, Craig Mullins, and Sparth have been significant influences.\nWhat\u2019s the next major challenge for graphics technology?\nThe next big frontier in graphics technology is achieving flawless real-time path tracing. This involves perfecting all possible effects under every conceivable condition, representing a significant leap in the realism and visual fidelity of real-time graphics.\nThanks to \u200ccontinuous advancements in RTX GPUs, which are becoming increasingly powerful, coupled with software innovations like DLSS 3.5, this path-traced reality is becoming more tangible. Games such as Alan Wake 2 and Cyberpunk 2077: Ultimate Edition serve as prime examples of the direction in which we\u2019re headed, indicating an even more exciting future for graphics technology.\nWhat are your tips for developers who are just starting to add real-time ray tracing to their development pipeline?\nFor developers starting with real-time ray tracing, I\u2019d advise focusing on how it can enhance the overall gaming experience. Ray tracing opens new possibilities that weren\u2019t possible before. However, simply turning on RTX won\u2019t unlock its full potential. Consider how ray-traced shadows, reflections, and global illumination can be strategically used. Thoughtful art direction, aligned with these capabilities, can significantly elevate your game\u2019s visual impact.\nLearn more about the NVIDIA RTX platform and our tools for game developers."}], "https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/": [{"text": "Basecamp Research has made a breakthrough in functional annotation with HiFi-NN, an AI model that outperforms existing models in enzyme annotation by over 15%. The model was trained using proprietary biological data collected through global expeditions to create the BaseGraph knowledge graph. HiFi-NN annotates protein sequences with enzyme commission numbers, surpassing other state-of-the-art DL models in precision and recall. The model's performance is attributed to using the hierarchical nature of enzyme function and supplementing the training set with diverse sequences. HiFi-NN's speed and accuracy make it particularly useful for annotating protein sequences from functional dark matter. Basecamp Research's partnership with Johnson Matthey demonstrates the potential of computational advancements in industrial applications. Functional annotation driven by machine learning is crucial for drug discovery, industrial biotechnology, and evolutionary biology. Basecamp Research is integrating its workflow with NVIDIA BioNeMo for drug discovery, enabling organizations to tailor and deploy AI models for various purposes.", "text_components": ["Breakthrough in Functional Annotation with HiFi-NN\nEnzymes are vital biological catalysts for a multitude of processes, from cellular metabolism to industrial manufacturing. The applications of artificial intelligence for enzyme generation is an exciting field of research with direct applications in the life sciences. Advances in these scientific challenges are a critical necessity to further advance drug discovery, environmental science, and bioengineering.\nCurrently, only a tiny fraction of Earth\u2019s vast array of life forms has been sequenced, hindering the broader application and generalization of machine learning algorithms within the complex realm of sequence design. Improved methods for functional annotation are a vital component in enzyme research, enabling the identification and characterization of the functions of newly discovered enzymes. This is key to understanding complex biological processes and enhancing the data used for generative workflows.\nBasecamp Research, a London-based Bio-AI company and NVIDIA Inception member, recently used NVIDIA GPUs to train a Hierarchically Fine-tuned Nearest Neighbor method (HiFi-NN). This approach has shown significant improvements over existing models in recall, precision, and F1 scores, surpassing the state-of-the-art (SoTA) in enzyme annotation by over 15%.", "Unique data collected in global expeditions\nBasecamp Research tackles the most complex biological design challenges across the biotech industry. Existing datasets have significant shortcomings:\nSmall representativity, covering only 0.001% of life on earth\nNo consistent metadata\nLack of stakeholder consent and engagement before data collection\nBasecamp opted to develop its proprietary biological data resource through biodiversity partnerships with nature parks across five continents and 23 countries. They sent their scientists on worldwide expeditions to discover new genomes, enzymes, and biological relationships from the most extreme and extraordinary biomes.\nIn under two years, they created BaseGraph, the largest knowledge graph of natural biodiversity, containing over 5.5B relationships with a genomic context exceeding 70 kilobases per protein. Their extensive long-read sequencing is complemented by comprehensive metadata collection, enabling them to link proteins of interest to specific reactions and desired process conditions.\nBasecamp Research\u2019s AI strategy is data-centric for two reasons: their proprietary data enhances model performance in a rapidly commoditizing algorithmic landscape and significantly compensates for the lack of diversity in publicly available data.\nTheir knowledge graph, built from the ground up, captures and re-creates the complexities of four billion years of protein evolution in nature. This data advantage enables their AI and product teams to outperform SoTA annotation and design models, addressing complex design challenges in biotech industries, from gene-writing therapeutics to plastic degradation.", "In silico functional annotation\nTo tackle the challenge of in silico functional annotation of proteins and enzymes, Basecamp Research\u2019s deep learning (DL) team developed HiFi-NN search. HiFi-NN annotates protein sequences with enzyme commission (EC) numbers beating the current bioinformatics tool of choice ( blastp ), as well as other SoTA DL models, including CLEAN, in precision and recall (Table 1).\nMethod\nRecall\nPrecision\nF1-score\nECPred\n0.0197\n0.0197\n0.0197\nDEEPpre\n0.0403\n0.0415\n0.0386\nDeepEC\n0.0724\n0.1184\n0.0846\nProteInfer\n0.1382\n0.2434\n0.1662\nProteinVec\n0.2961\n0.4901\n0.3378\nBLASTp\n0.3750\n0.5083\n0.3852\nDeepECtransformer\n0.3026\n0.5263\n0.3511\nCLEAN\n0.4671\n0.5844\n0.4947\nHiFi-NN (Swissprot)\n0.5724\n0.5505\n0.5304\nHiFi-NN (Swissprot + 3M curated sequences)\n0.5921\n0.6657\n0.6015\nTable 1. HiFi-NN results compared to existing tools and other SoTA DL models BCR 3M refers to the version of the model retrained with 3M selected, environmentally diverse sequences from Basecamp Research\u2019s BaseGraph.\nBar chart comparing the Recall, Precision, and F1 scores of seven bioinformatics models, with HiFi-NN models outperforming others.\nFigure 1. HiFi-NN results compared to SoTA models in Enzyme Functional Annotation on the price-149 enzyme benchmarking dataset\nDeveloped in collaboration with advisors Noelia Ferruz and Kevin Yang, HiFi-NN was accepted by NeurIPS, a prestigious AI conference, for presentation at their Machine Learning for Structural Biology workshop in December 2023.\nThe model uses contrastive learning of EC numbers and the inherent hierarchy of the enzyme commission annotation system for natural augmentation. Trained on eight NVIDIA A100 GPUs on a Lambda Labs instance with CUDA version 11.8 and NCCL version 2.14.3, it employs PyTorch Lightning for distributed-data parallel training. Experiment management and tracking are done through the Hydra framework and Weights and Biases. The model boasts over 3M parameters.\nHiFi-NN\u2019s superior performance in SoTA annotation methods is attributed to using the hierarchical nature of enzyme function represented in the EC numbering system and supplementing the training set with proprietary sequences from Basecamp\u2019s knowledge graph.", "Proprietary sequences\nThe proprietary Basecamp sequences that HiFi-NN is supplemented with originate from environments spanning five continents, and a 110 o C temperature range to ensure as much sequence and environmental diversity in the training set as possible. As a result, HiFi-NN outperforms all SoTA models on benchmarking datasets and performs particularly well on protein sequences from functional dark matter, that is, those sequences with low similarity to any known enzymes.\nIn fact, the Basecamp team used HiFi-NN to annotate a large representative portion of the MGnify microbial protein database that was previously not annotated.\nIn addition to outperforming all previous annotation models, HiFi-NN is also particularly easy to use and generates annotation labels at great speed. For example, it annotates the entire human proteome within 24 minutes on a single NVIDIA A100 GPU.", "Fast enzyme identification with Johnson Matthey\nBreakthroughs like HiFi-NN, which enhance our capacity to predict the physical properties of biological entities, are set to reduce the need for extensive screening of candidates using resource-heavy lab methods.\nBasecamp Research\u2019s partnership with an FTSE100 chemicals company, Johnson Matthey, underscores the importance of computational advancements in addressing industrial challenges. A notable project with this partner involved their researchers spending over a year and a half testing thousands of enzyme variants in their labs without success.\nJohnson Matthey\u2019s objective was to find an enzyme with broad specificity capable of processing multiple bulky substrates, a more complex task compared to working with smaller substrates. Within a week, Basecamp Research employed its entirely in silico techniques to identify an enzyme (Figure 2) meeting these criteria, positioning it for potential commercialization.\nThe research group\u2019s leader expressed admiration for Basecamp Research\u2019s capability to rapidly discover and develop an enzyme, a task that had eluded them for years. This success laid the groundwork for an expanded collaborative effort on various enzyme development initiatives.\nVisual representation of a three-dimensional biomolecular structure.\nFigure 2. Basecamp Research\u2019s proprietary protein", "Bolstering AI development in life sciences\nFunctional annotation plays a pivotal role for researchers, especially in practical scenarios:\nIn drug discovery, it aids in the creation of targeted treatments by elucidating enzyme interactions within the body.\nIn the realm of industrial biotechnology, it enables the bespoke design of enzymes, tailored for specific industrial applications, promoting more environmentally friendly production methods.\nIt also offers crucial insights into evolutionary biology by revealing the developmental trajectory of enzymes across different species.\nIn essence, functional annotation, driven by machine learning, transcends its role as a mere scientific instrument. it acts as a catalyst for innovation and exploration across diverse sectors, including healthcare and environmental science.\nBasecamp Research, along with other life sciences entities, is integrating its workflow with NVIDIA BioNeMo, a generative AI platform geared towards drug discovery. This platform streamlines and expedites the training of models. With BioNeMo, organizations can tailor and deploy AI models for various purposes, including 3D protein structure prediction, de novo protein and small molecule generation, property prediction, and molecular docking.\nIf you\u2019re interested in exploring BioNeMo, opportunities are available through the BioNeMo Framework beta release or the API early access program., For more information, see the NVIDIA BioNeMo product page."], "document_title": "Breakthrough in Functional Annotation with HiFi-NN", "document_url": "https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/", "document_date": "2023-12-19T19:00:00", "document_date_modified": "2023-12-18T19:51:41", "document_full_text": "Breakthrough in Functional Annotation with HiFi-NN\nEnzymes are vital biological catalysts for a multitude of processes, from cellular metabolism to industrial manufacturing. The applications of artificial intelligence for enzyme generation is an exciting field of research with direct applications in the life sciences. Advances in these scientific challenges are a critical necessity to further advance drug discovery, environmental science, and bioengineering.\nCurrently, only a tiny fraction of Earth\u2019s vast array of life forms has been sequenced, hindering the broader application and generalization of machine learning algorithms within the complex realm of sequence design. Improved methods for functional annotation are a vital component in enzyme research, enabling the identification and characterization of the functions of newly discovered enzymes. This is key to understanding complex biological processes and enhancing the data used for generative workflows.\nBasecamp Research, a London-based Bio-AI company and NVIDIA Inception member, recently used NVIDIA GPUs to train a Hierarchically Fine-tuned Nearest Neighbor method (HiFi-NN). This approach has shown significant improvements over existing models in recall, precision, and F1 scores, surpassing the state-of-the-art (SoTA) in enzyme annotation by over 15%.\nUnique data collected in global expeditions\nBasecamp Research tackles the most complex biological design challenges across the biotech industry. Existing datasets have significant shortcomings:\nSmall representativity, covering only 0.001% of life on earth\nNo consistent metadata\nLack of stakeholder consent and engagement before data collection\nBasecamp opted to develop its proprietary biological data resource through biodiversity partnerships with nature parks across five continents and 23 countries. They sent their scientists on worldwide expeditions to discover new genomes, enzymes, and biological relationships from the most extreme and extraordinary biomes.\nIn under two years, they created BaseGraph, the largest knowledge graph of natural biodiversity, containing over 5.5B relationships with a genomic context exceeding 70 kilobases per protein. Their extensive long-read sequencing is complemented by comprehensive metadata collection, enabling them to link proteins of interest to specific reactions and desired process conditions.\nBasecamp Research\u2019s AI strategy is data-centric for two reasons: their proprietary data enhances model performance in a rapidly commoditizing algorithmic landscape and significantly compensates for the lack of diversity in publicly available data.\nTheir knowledge graph, built from the ground up, captures and re-creates the complexities of four billion years of protein evolution in nature. This data advantage enables their AI and product teams to outperform SoTA annotation and design models, addressing complex design challenges in biotech industries, from gene-writing therapeutics to plastic degradation.\nIn silico functional annotation\nTo tackle the challenge of in silico functional annotation of proteins and enzymes, Basecamp Research\u2019s deep learning (DL) team developed HiFi-NN search. HiFi-NN annotates protein sequences with enzyme commission (EC) numbers beating the current bioinformatics tool of choice ( blastp ), as well as other SoTA DL models, including CLEAN, in precision and recall (Table 1).\nMethod\nRecall\nPrecision\nF1-score\nECPred\n0.0197\n0.0197\n0.0197\nDEEPpre\n0.0403\n0.0415\n0.0386\nDeepEC\n0.0724\n0.1184\n0.0846\nProteInfer\n0.1382\n0.2434\n0.1662\nProteinVec\n0.2961\n0.4901\n0.3378\nBLASTp\n0.3750\n0.5083\n0.3852\nDeepECtransformer\n0.3026\n0.5263\n0.3511\nCLEAN\n0.4671\n0.5844\n0.4947\nHiFi-NN (Swissprot)\n0.5724\n0.5505\n0.5304\nHiFi-NN (Swissprot + 3M curated sequences)\n0.5921\n0.6657\n0.6015\nTable 1. HiFi-NN results compared to existing tools and other SoTA DL models BCR 3M refers to the version of the model retrained with 3M selected, environmentally diverse sequences from Basecamp Research\u2019s BaseGraph.\nBar chart comparing the Recall, Precision, and F1 scores of seven bioinformatics models, with HiFi-NN models outperforming others.\nFigure 1. HiFi-NN results compared to SoTA models in Enzyme Functional Annotation on the price-149 enzyme benchmarking dataset\nDeveloped in collaboration with advisors Noelia Ferruz and Kevin Yang, HiFi-NN was accepted by NeurIPS, a prestigious AI conference, for presentation at their Machine Learning for Structural Biology workshop in December 2023.\nThe model uses contrastive learning of EC numbers and the inherent hierarchy of the enzyme commission annotation system for natural augmentation. Trained on eight NVIDIA A100 GPUs on a Lambda Labs instance with CUDA version 11.8 and NCCL version 2.14.3, it employs PyTorch Lightning for distributed-data parallel training. Experiment management and tracking are done through the Hydra framework and Weights and Biases. The model boasts over 3M parameters.\nHiFi-NN\u2019s superior performance in SoTA annotation methods is attributed to using the hierarchical nature of enzyme function represented in the EC numbering system and supplementing the training set with proprietary sequences from Basecamp\u2019s knowledge graph.\nProprietary sequences\nThe proprietary Basecamp sequences that HiFi-NN is supplemented with originate from environments spanning five continents, and a 110 o C temperature range to ensure as much sequence and environmental diversity in the training set as possible. As a result, HiFi-NN outperforms all SoTA models on benchmarking datasets and performs particularly well on protein sequences from functional dark matter, that is, those sequences with low similarity to any known enzymes.\nIn fact, the Basecamp team used HiFi-NN to annotate a large representative portion of the MGnify microbial protein database that was previously not annotated.\nIn addition to outperforming all previous annotation models, HiFi-NN is also particularly easy to use and generates annotation labels at great speed. For example, it annotates the entire human proteome within 24 minutes on a single NVIDIA A100 GPU.\nFast enzyme identification with Johnson Matthey\nBreakthroughs like HiFi-NN, which enhance our capacity to predict the physical properties of biological entities, are set to reduce the need for extensive screening of candidates using resource-heavy lab methods.\nBasecamp Research\u2019s partnership with an FTSE100 chemicals company, Johnson Matthey, underscores the importance of computational advancements in addressing industrial challenges. A notable project with this partner involved their researchers spending over a year and a half testing thousands of enzyme variants in their labs without success.\nJohnson Matthey\u2019s objective was to find an enzyme with broad specificity capable of processing multiple bulky substrates, a more complex task compared to working with smaller substrates. Within a week, Basecamp Research employed its entirely in silico techniques to identify an enzyme (Figure 2) meeting these criteria, positioning it for potential commercialization.\nThe research group\u2019s leader expressed admiration for Basecamp Research\u2019s capability to rapidly discover and develop an enzyme, a task that had eluded them for years. This success laid the groundwork for an expanded collaborative effort on various enzyme development initiatives.\nVisual representation of a three-dimensional biomolecular structure.\nFigure 2. Basecamp Research\u2019s proprietary protein\nBolstering AI development in life sciences\nFunctional annotation plays a pivotal role for researchers, especially in practical scenarios:\nIn drug discovery, it aids in the creation of targeted treatments by elucidating enzyme interactions within the body.\nIn the realm of industrial biotechnology, it enables the bespoke design of enzymes, tailored for specific industrial applications, promoting more environmentally friendly production methods.\nIt also offers crucial insights into evolutionary biology by revealing the developmental trajectory of enzymes across different species.\nIn essence, functional annotation, driven by machine learning, transcends its role as a mere scientific instrument. it acts as a catalyst for innovation and exploration across diverse sectors, including healthcare and environmental science.\nBasecamp Research, along with other life sciences entities, is integrating its workflow with NVIDIA BioNeMo, a generative AI platform geared towards drug discovery. This platform streamlines and expedites the training of models. With BioNeMo, organizations can tailor and deploy AI models for various purposes, including 3D protein structure prediction, de novo protein and small molecule generation, property prediction, and molecular docking.\nIf you\u2019re interested in exploring BioNeMo, opportunities are available through the BioNeMo Framework beta release or the API early access program., For more information, see the NVIDIA BioNeMo product page."}], "https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2023/": [{"text": "The article highlights the most popular NVIDIA Technical Blog posts of 2023, focusing on breakthrough research in generative AI, large language models (LLMs), robotics, and virtual worlds. Some key highlights include advancements in creating 3D assets for virtual worlds using generative AI, improving human connection in video conferences with NVIDIA Maxine Eye Contact, and boosting LLM inference on NVIDIA H100 GPUs with TensorRT-LLM. The article also discusses the development of AI-powered robots and smart vision systems with the Jetson Orin Nano Developer Kit, as well as tools for developing safe and secure conversational systems with NeMo Guardrails. Additionally, it delves into the architecture of the NVIDIA Grace CPU Superchip and the use of AV1 codec and NVIDIA Ada Lovelace architecture to enhance video quality and performance. The article emphasizes the importance of staying up-to-date with NVIDIA's latest developments through their Developer Newsletter and social media channels.", "text_components": ["Most Popular NVIDIA Technical Blog Posts of 2023: Generative AI, LLMs, Robotics, and Virtual Worlds Breakthroughs\nAs we approach the end of another exciting year at NVIDIA, it\u2019s time to look back at the most popular stories from the NVIDIA Technical Blog in 2023.\nGroundbreaking research and developments in fields such as generative AI, large language models (LLMs), high-performance computing (HPC), and robotics are leading the way in transformative AI solutions and capturing the interest of our readers. Other top posts explore advancements in video technology and video conferencing, enhancing the user experience, alongside breakthroughs in AI security.\nThe following are some of the highlights from 2023.\nA group of different animals standing together.\nRapidly Generate 3D Assets for Virtual Worlds with Generative AI\nNew generative AI technologies on NVIDIA Omniverse enhance 3D asset creation in virtual environments. These advancements aim to make the creation of virtual worlds on the metaverse faster and easier.\nPerson in a video conference using Eye Contact feature with eye contact directly at the camera.\nImprove Human Connection in Video Conferences with NVIDIA Maxine Eye Contact\nNVIDIA Maxine Eye Contact revolutionizes video conferencing by using AI to adjust your gaze toward the camera in real time. It also maintains natural eye color and adapts to different head positions and gaze directions, creating a more authentic and connected virtual interaction.\nTensorRTLLM illustration.\nNVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\nNVIDIA TensorRT-LLM, a component of the NVIDIA NeMo framework, is tailored to boost LLM inference on NVIDIA H100 GPUs. This open-source library offers optimized processing and supports multi-GPU and multi-node setups, enabling efficient and scalable deployment of LLMs in generative AI applications.\nNVIDIA Jetson Orin Nano Developer Kit\nDevelop AI-Powered Robots, Smart Vision Systems, and More with NVIDIA Jetson Orin Nano Developer Kit\nThe latest NVIDIA Jetson Orin Nano Developer Kit is a powerful tool for developing AI-powered robots and smart vision systems. Offering a huge boost in AI performance over the prior generation, it is compatible with all NVIDIA Jetson Orin Nano and NX modules for prototyping edge AI products.\nNeMo Guardrails illustration.\nNVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems\nA toolkit for developing safe and trustworthy LLM conversational systems, NeMo Guardrails enables developers to implement rules that maintain safe and relevant conversations. It integrates with LLMs like ChatGPT, is built on the NVIDIA Colang language, and is available through NVIDIA AI Foundations.\nLLM workflow demo.\nAn Introduction to Large Language Models: Prompt Engineering and P-Tuning\nThis introduction to LLMs covers key techniques like prompt engineering and tuning. It discusses how LLMs function, their role in AI applications like text generation, and the significance of creating effective prompts and optimizing performance in various scenarios.\nTwo men working at a desktop computer in an office.\nNVIDIA AI Red Team: An Introduction\nThe NVIDIA AI Red Team details its approach to assessing and mitigating risks in AI and machine learning systems from an information security standpoint. A group of security professionals and data scientists, they aim to identify and address risks related to technical vulnerabilities, harm and abuse scenarios, and other security challenges in ML systems.\nGrace CPU Superchip illustration.\nNVIDIA Grace CPU Superchip Architecture In-Depth\nTake an in-depth look at the architecture and features of the NVIDIA Grace CPU Superchip. Offering major advancements in compute density and power efficiency, the Grace CPU excels in memory bandwidth and data movement efficiency, making it a powerhouse for HPC and AI workloads.\nA side-by-side comparison of two versions of a graphic.\nImproving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture\nImprove video quality and performance using AV1 codec and the NVIDIA Ada Lovelace architecture. This integration enhances video encoding and decoding, improving compression efficiency, quality, and increased throughput, making it ideal for various video applications.\nTensorRT-LLM improves ease of use and extensibility through an open-source modular Python API for defining, optimizing, and executing new architectures and enhancements as LLMs evolve, and can be customized easily.\nNVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\nTensorRT-LLM consists of the TensorRT deep learning compiler and includes optimized kernels, pre\u2013 and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs.\nSubscribe to the Developer Newsletter and stay in the loop on 2024 content tailored to your interests. Follow us on Instagram, Twitter, YouTube, and Discord for the latest developer news."], "document_title": "Most Popular NVIDIA Technical Blog Posts of 2023: Generative AI, LLMs, Robotics, and Virtual Worlds Breakthroughs", "document_url": "https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2023/", "document_date": "2023-12-19T17:50:21", "document_date_modified": "2024-01-11T19:49:37", "document_full_text": "Most Popular NVIDIA Technical Blog Posts of 2023: Generative AI, LLMs, Robotics, and Virtual Worlds Breakthroughs\nAs we approach the end of another exciting year at NVIDIA, it\u2019s time to look back at the most popular stories from the NVIDIA Technical Blog in 2023.\nGroundbreaking research and developments in fields such as generative AI, large language models (LLMs), high-performance computing (HPC), and robotics are leading the way in transformative AI solutions and capturing the interest of our readers. Other top posts explore advancements in video technology and video conferencing, enhancing the user experience, alongside breakthroughs in AI security.\nThe following are some of the highlights from 2023.\nA group of different animals standing together.\nRapidly Generate 3D Assets for Virtual Worlds with Generative AI\nNew generative AI technologies on NVIDIA Omniverse enhance 3D asset creation in virtual environments. These advancements aim to make the creation of virtual worlds on the metaverse faster and easier.\nPerson in a video conference using Eye Contact feature with eye contact directly at the camera.\nImprove Human Connection in Video Conferences with NVIDIA Maxine Eye Contact\nNVIDIA Maxine Eye Contact revolutionizes video conferencing by using AI to adjust your gaze toward the camera in real time. It also maintains natural eye color and adapts to different head positions and gaze directions, creating a more authentic and connected virtual interaction.\nTensorRTLLM illustration.\nNVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\nNVIDIA TensorRT-LLM, a component of the NVIDIA NeMo framework, is tailored to boost LLM inference on NVIDIA H100 GPUs. This open-source library offers optimized processing and supports multi-GPU and multi-node setups, enabling efficient and scalable deployment of LLMs in generative AI applications.\nNVIDIA Jetson Orin Nano Developer Kit\nDevelop AI-Powered Robots, Smart Vision Systems, and More with NVIDIA Jetson Orin Nano Developer Kit\nThe latest NVIDIA Jetson Orin Nano Developer Kit is a powerful tool for developing AI-powered robots and smart vision systems. Offering a huge boost in AI performance over the prior generation, it is compatible with all NVIDIA Jetson Orin Nano and NX modules for prototyping edge AI products.\nNeMo Guardrails illustration.\nNVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems\nA toolkit for developing safe and trustworthy LLM conversational systems, NeMo Guardrails enables developers to implement rules that maintain safe and relevant conversations. It integrates with LLMs like ChatGPT, is built on the NVIDIA Colang language, and is available through NVIDIA AI Foundations.\nLLM workflow demo.\nAn Introduction to Large Language Models: Prompt Engineering and P-Tuning\nThis introduction to LLMs covers key techniques like prompt engineering and tuning. It discusses how LLMs function, their role in AI applications like text generation, and the significance of creating effective prompts and optimizing performance in various scenarios.\nTwo men working at a desktop computer in an office.\nNVIDIA AI Red Team: An Introduction\nThe NVIDIA AI Red Team details its approach to assessing and mitigating risks in AI and machine learning systems from an information security standpoint. A group of security professionals and data scientists, they aim to identify and address risks related to technical vulnerabilities, harm and abuse scenarios, and other security challenges in ML systems.\nGrace CPU Superchip illustration.\nNVIDIA Grace CPU Superchip Architecture In-Depth\nTake an in-depth look at the architecture and features of the NVIDIA Grace CPU Superchip. Offering major advancements in compute density and power efficiency, the Grace CPU excels in memory bandwidth and data movement efficiency, making it a powerhouse for HPC and AI workloads.\nA side-by-side comparison of two versions of a graphic.\nImproving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture\nImprove video quality and performance using AV1 codec and the NVIDIA Ada Lovelace architecture. This integration enhances video encoding and decoding, improving compression efficiency, quality, and increased throughput, making it ideal for various video applications.\nTensorRT-LLM improves ease of use and extensibility through an open-source modular Python API for defining, optimizing, and executing new architectures and enhancements as LLMs evolve, and can be customized easily.\nNVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\nTensorRT-LLM consists of the TensorRT deep learning compiler and includes optimized kernels, pre\u2013 and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs.\nSubscribe to the Developer Newsletter and stay in the loop on 2024 content tailored to your interests. Follow us on Instagram, Twitter, YouTube, and Discord for the latest developer news."}], "https://developer.nvidia.com/blog/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/": [{"text": "NVIDIA cuQuantum is an SDK that accelerates quantum computing workflows on NVIDIA GPUs. The latest version, cuQuantum 23.10, includes updates to cuTensorNet and cuStateVec, with new features like support for NVIDIA Grace Hopper systems. cuTensorNet provides high-level APIs for quantum simulator developers to optimize their capabilities, achieving significant speedups compared to other methods on the same hardware. cuStateVec introduces new APIs for host-to-device state vector swap, allowing for larger state vector simulations with fewer devices. Grace Hopper systems outperform other CPU and GPU combinations, providing cost and energy savings. cuQuantum documentation and benchmark suite on GitHub are available to help users get started. Overall, cuQuantum aims to deliver quantum circuit simulations at the speed of light on NVIDIA GPUs and CPUs, enabling developers to leverage GPU acceleration for their workloads.", "text_components": ["Accelerate Quantum Circuit Simulation with NVIDIA cuQuantum 23.10\nNVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use it to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.\ncuQuantum aims to deliver at the speed of light on NVIDIA GPUs and CPUs for quantum circuit simulations. Users of quantum computing frameworks can leverage cuQuantum-backed simulators to realize GPU acceleration for their workloads.", "What\u2019s new in cuQuantum 23.10?\ncuQuantum 23.10 includes updates to both NVIDIA cuTensorNet and NVIDIA cuStateVec. New features include support for NVIDIA Grace Hopper systems. For more information, see the cuQuantum 23.10 release notes.", "Tensor network high-level APIs and gradients\ncuTensorNet provides high-level APIs to facilitate quantum simulator developers to program in an intuitive way to make the most of their capabilities. This technology enables developers to abstract away specific tensor network knowledge when creating simulators. This makes building a tensor-network-based quantum simulator simpler, as it covers expectations, measurements, samples, and other elements.\nThe chart shows that at a range of scales, cuTensorNet outperforms TensorCircuit with cotengra for pathfinding and PyTorch and JAX for contractions by as much as 4-5.9x on the same NVIDIA H100 GPUs.\nFig 1. Running the QAOA algorithm (p=1) on cuTensorNet high-level APIs compared with TensorCircuit\u2019s cotengra pathfinder and both PyTorch and Jax contractions. We show significant speedup that scales better and is up 4-5.9x faster on the same H100 Hardware\nWe\u2019ve introduced experimental support for gradient calculations from a given tensor network aimed at accelerating quantum machine learning with tensor networks. This enables drastic speedups for QML and adjoint differentiation-based workflows with cuTensorNet.", "Scale up state vector simulations with fewer devices\ncuStateVec introduces new APIs for host-to-device state vector swap, which enables the use of CPU memory with GPUs to further scale simulations. Now, 40 qubit state vector simulations only require 16 NVIDIA Grace Hopper systems instead of 128 NVIDIA H100 80GB GPUs. As you can see by the speedup offered by these systems, NVIDIA Grace Hopper drastically outperforms the NVIDIA Hopper GPU architecture when used with other CPUs, and CPU-only implementations. This leads to substantial cost and energy savings for each workload.\nA graph showing the runtime for 36 qubit algorithms performed with DGX H100 80 GB + Intel Xeon 8480CL, GH200 (one GPU, one CPU), and Intel Xeon 8480CL.\nFigure 2. cuQuantum on one NVIDIA Grace Hopper Superchip efficiently simulates a 36-qubit system that otherwise requires eight NVIDIA H100 GPUs and is faster than other CPU + GPU combinations. GH200 is 5.1-8.8x faster than Intel Xeon Platinum 8480CL Additional API-level and kernel-level optimizations have been made to improve performance further. Grace Hopper systems provide better runtimes than other CPU and Hopper systems. The chip-to-chip interconnect and better CPU afford faster runtimes.\nA graph showing the speedup for 33-qubit Quantum Fourier Transform performed with DGX H100 80 GB, GH200 (one GPU), and Intel Xeon 8480CL.\nFigure 3. cuQuantum on NVIDIA GH200 is 94x faster at running 33-qubit Quantum Fourier Transform simulations than Intel Xeon 8480CL dual-socket. NVIDIA H100, which launches with the same Intel CPU is 61x faster", "Getting started with cuQuantum\ncuQuantum offers documentation to help with getting starte d. If you are running \u200ca CSP, we encourage users to check out marketplace listings for each major CSP.\nAfter you have set up your environment, we recommend checking out our benchmark suite on GitHub and validating that you are engaging GPUs in your benchmarks.\nPlease reach out with questions, requests, or issues on GitHub."], "document_title": "Accelerate Quantum Circuit Simulation with NVIDIA cuQuantum 23.10", "document_url": "https://developer.nvidia.com/blog/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/", "document_date": "2023-12-18T21:44:27", "document_date_modified": "2024-01-04T21:33:19", "document_full_text": "Accelerate Quantum Circuit Simulation with NVIDIA cuQuantum 23.10\nNVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use it to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.\ncuQuantum aims to deliver at the speed of light on NVIDIA GPUs and CPUs for quantum circuit simulations. Users of quantum computing frameworks can leverage cuQuantum-backed simulators to realize GPU acceleration for their workloads.\nWhat\u2019s new in cuQuantum 23.10?\ncuQuantum 23.10 includes updates to both NVIDIA cuTensorNet and NVIDIA cuStateVec. New features include support for NVIDIA Grace Hopper systems. For more information, see the cuQuantum 23.10 release notes.\nTensor network high-level APIs and gradients\ncuTensorNet provides high-level APIs to facilitate quantum simulator developers to program in an intuitive way to make the most of their capabilities. This technology enables developers to abstract away specific tensor network knowledge when creating simulators. This makes building a tensor-network-based quantum simulator simpler, as it covers expectations, measurements, samples, and other elements.\nThe chart shows that at a range of scales, cuTensorNet outperforms TensorCircuit with cotengra for pathfinding and PyTorch and JAX for contractions by as much as 4-5.9x on the same NVIDIA H100 GPUs.\nFig 1. Running the QAOA algorithm (p=1) on cuTensorNet high-level APIs compared with TensorCircuit\u2019s cotengra pathfinder and both PyTorch and Jax contractions. We show significant speedup that scales better and is up 4-5.9x faster on the same H100 Hardware\nWe\u2019ve introduced experimental support for gradient calculations from a given tensor network aimed at accelerating quantum machine learning with tensor networks. This enables drastic speedups for QML and adjoint differentiation-based workflows with cuTensorNet.\nScale up state vector simulations with fewer devices\ncuStateVec introduces new APIs for host-to-device state vector swap, which enables the use of CPU memory with GPUs to further scale simulations. Now, 40 qubit state vector simulations only require 16 NVIDIA Grace Hopper systems instead of 128 NVIDIA H100 80GB GPUs. As you can see by the speedup offered by these systems, NVIDIA Grace Hopper drastically outperforms the NVIDIA Hopper GPU architecture when used with other CPUs, and CPU-only implementations. This leads to substantial cost and energy savings for each workload.\nA graph showing the runtime for 36 qubit algorithms performed with DGX H100 80 GB + Intel Xeon 8480CL, GH200 (one GPU, one CPU), and Intel Xeon 8480CL.\nFigure 2. cuQuantum on one NVIDIA Grace Hopper Superchip efficiently simulates a 36-qubit system that otherwise requires eight NVIDIA H100 GPUs and is faster than other CPU + GPU combinations. GH200 is 5.1-8.8x faster than Intel Xeon Platinum 8480CL Additional API-level and kernel-level optimizations have been made to improve performance further. Grace Hopper systems provide better runtimes than other CPU and Hopper systems. The chip-to-chip interconnect and better CPU afford faster runtimes.\nA graph showing the speedup for 33-qubit Quantum Fourier Transform performed with DGX H100 80 GB, GH200 (one GPU), and Intel Xeon 8480CL.\nFigure 3. cuQuantum on NVIDIA GH200 is 94x faster at running 33-qubit Quantum Fourier Transform simulations than Intel Xeon 8480CL dual-socket. NVIDIA H100, which launches with the same Intel CPU is 61x faster\nGetting started with cuQuantum\ncuQuantum offers documentation to help with getting starte d. If you are running \u200ca CSP, we encourage users to check out marketplace listings for each major CSP.\nAfter you have set up your environment, we recommend checking out our benchmark suite on GitHub and validating that you are engaging GPUs in your benchmarks.\nPlease reach out with questions, requests, or issues on GitHub."}], "https://developer.nvidia.com/blog/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/": [{"text": "The article discusses the importance of teaching autonomous vehicles (AVs) to understand human driving behavior in order to coexist safely with human drivers. The NVIDIA Research team has developed Trajeglish, a traffic modeling approach that tokenizes motion in a similar way to language models tokenize words and phrases. Trajeglish accurately predicts multi-vehicle driving scenarios by considering the interactions between different agents and trajectories. In a competition, Trajeglish outperformed 16 other traffic models by producing the most realistic traffic trajectories, especially in scenarios with dense interactions. The model improved the overall realism of scenarios by 3.3% and topped the interaction component by 9.9%. By modeling human behavior in traffic scenarios, AV developers can enhance the fidelity of their simulations, leading to more efficient training, testing, and validation processes. This approach allows AVs to better interpret human-based communication on the road, ensuring safe and efficient interactions between autonomous and human drivers.", "text_components": ["Teaching AVs the Language of Human Driving Behavior with Trajeglish\nMuch of the communication between drivers goes beyond turn signals and brake lights. Motioning another car to proceed, looking over to see if another driver is paying attention\u2014even the friendly Jeep wave\u2014all rely on human-based communication rather than vehicle technology.\nAs autonomous vehicles (AV) must coexist with human drivers for the foreseeable future, they must be able to interpret this behavior to make safe decisions that don\u2019t interrupt the flow of traffic.\nTo address this challenge in training, developers must be able to predict how the future motion of other vehicles is affected by an AV\u2019s actions. In a recently published paper, the NVIDIA Research team outlines Trajeglish, an approach to traffic modeling that tokenizes motion in the same way language models tokenize words and phrases to simulate realistic multi-vehicle driving scenarios.\nWhen compared with 16 other traffic models in the first iteration (V0) of the Waymo Sim Agents Challenge, this tokenization approach resulted in the most realistic traffic trajectories, showing a 3.3% improvement over the previous state-of-the-art model.\nTrajeglish models multi-agent traffic scenarios by breaking each scenario down into tokens, in the same way a language model breaks down a paragraph into words and phrases. By doing so, it can consider each agent and trajectory in relation to each other, predicting motions that cover the full range of possible interactions given their initial locations.\nEight different traffic scenarios showing various predicted trajectories based on the initial positions of the vehicles.\nFigure 1. Scenarios modeled by Trajeglish given only the initial timestep of the driving log. The initial state used to prompt the model is shown in black.\nWhen given only the initial timestep of a real-world scenario, Trajeglish closely models the log data to realistically simulate how other vehicles react to the actions of the AV.", "Modeling human behavior\nSimulating human driving behavior is relatively straightforward in single-lane highway scenarios, where there are few intersections, objects, or pedestrians.\nModeling multiple vehicles in urban settings, however, is significantly more difficult given the increase in traffic and road variety. To build traffic models that generalize to a wider range of scenarios, recent approaches aim to imitate driving behavior observed in driving logs.\nDoing so in simulation requires sampling realistic actions for an agent at each timestep that are compatible with actions chosen by all other agents at the current timestep\u2014a relationship known as intra-timestep dependence.\nWhile actors in the real world behave independently, intra-timestep dependence in traffic modeling is necessary as driving logs are recorded at discrete timesteps, thus any interaction between timesteps appears as coordinated behavior. Communication that is not generally recorded in log data, such as eye contact or turn signals, also contributes to the appearance of coordination among actors in a recorded scenario.\nTrajeglish explicitly models this intra-timestep dependence. It achieves this by tokenizing a given scenario\u2014in the same manner as a language model\u2014enabling the model to predict only the likely trajectories, or tokens, based on the context of the scene. Trajeglish then models the next actions in the timestep by analyzing the distribution of all the tokenized scenarios.\nThree images showing three different timesteps, where in each one, a future position is chosen based on its proximity to the current location.\nFigure 2. Trajeglish tokenizes trajectories by iteratively finding the token with minimum corner distance to the next state.\nThis process of predicting the next token continuously builds on itself. After a chosen number of tokens are sampled, Trajeglish has enough context to predict scenarios of various lengths with an arbitrary number of agents.", "A leading approach\nTrajeglish was compared with 16 other models in the V0 leaderboard of the Waymo Sim Agents Challenge. Each model was tasked with simulating 32 scene-consistent trajectories for up to 128 agents at a time, given 1 second of initial driving information.\nThe challenge evaluated the realism of each simulation based on distribution matching. Several statistics were computed over the simulated scenarios and compared to the same statistics computed on the recorded scenarios. The closer these statistics matched each other, the higher the score.\nThe only model to use tokenization, Trajeglish produced the most realistic outcomes, according to Waymo\u2019s parameters. Qualitatively, Trajeglish dramatically improved performance in scenarios with dense interaction between agents, such as traffic jams, merging scenarios, and four-way stop intersections.\nThe Waymo leaderboard evaluated three categories in each simulation: kinematics (such as speed), interactions, or distance to the nearest vehicle, and whether the trajectory remained in the drivable area. Overall realism was a weighted average across these categories.\nAccording to these parameters, Trajeglish improved over the previous state-of-the-art model in overall realism of scenarios by 3.3% and topped the interaction component by 9.9%.\nBar charts comparing overall realism and interaction realism, with Trajeglish displaying the highest results of the top 10 models.\nFigure 3. Trajeglish results compared with other entrants in the Waymo Sim Agents Challenge. Submissions using ensembling are marked with asterisks.", "Conclusion\nHuman driving behavior can be incredibly nuanced, posing a significant challenge to recreating it in simulation. However, by taking a page from language modeling, which deals with similar complexities in human language, the task becomes more manageable.\nAs a result, AV developers can leverage higher fidelity traffic models in simulation to accelerate training, testing, and validation.\nTo learn more, read the full paper and Trajeglish: Learning the Language of Driving Scenarios project page."], "document_title": "Teaching AVs the Language of Human Driving Behavior with Trajeglish", "document_url": "https://developer.nvidia.com/blog/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/", "document_date": "2023-12-18T19:49:31", "document_date_modified": "2024-01-10T18:03:24", "document_full_text": "Teaching AVs the Language of Human Driving Behavior with Trajeglish\nMuch of the communication between drivers goes beyond turn signals and brake lights. Motioning another car to proceed, looking over to see if another driver is paying attention\u2014even the friendly Jeep wave\u2014all rely on human-based communication rather than vehicle technology.\nAs autonomous vehicles (AV) must coexist with human drivers for the foreseeable future, they must be able to interpret this behavior to make safe decisions that don\u2019t interrupt the flow of traffic.\nTo address this challenge in training, developers must be able to predict how the future motion of other vehicles is affected by an AV\u2019s actions. In a recently published paper, the NVIDIA Research team outlines Trajeglish, an approach to traffic modeling that tokenizes motion in the same way language models tokenize words and phrases to simulate realistic multi-vehicle driving scenarios.\nWhen compared with 16 other traffic models in the first iteration (V0) of the Waymo Sim Agents Challenge, this tokenization approach resulted in the most realistic traffic trajectories, showing a 3.3% improvement over the previous state-of-the-art model.\nTrajeglish models multi-agent traffic scenarios by breaking each scenario down into tokens, in the same way a language model breaks down a paragraph into words and phrases. By doing so, it can consider each agent and trajectory in relation to each other, predicting motions that cover the full range of possible interactions given their initial locations.\nEight different traffic scenarios showing various predicted trajectories based on the initial positions of the vehicles.\nFigure 1. Scenarios modeled by Trajeglish given only the initial timestep of the driving log. The initial state used to prompt the model is shown in black.\nWhen given only the initial timestep of a real-world scenario, Trajeglish closely models the log data to realistically simulate how other vehicles react to the actions of the AV.\nModeling human behavior\nSimulating human driving behavior is relatively straightforward in single-lane highway scenarios, where there are few intersections, objects, or pedestrians.\nModeling multiple vehicles in urban settings, however, is significantly more difficult given the increase in traffic and road variety. To build traffic models that generalize to a wider range of scenarios, recent approaches aim to imitate driving behavior observed in driving logs.\nDoing so in simulation requires sampling realistic actions for an agent at each timestep that are compatible with actions chosen by all other agents at the current timestep\u2014a relationship known as intra-timestep dependence.\nWhile actors in the real world behave independently, intra-timestep dependence in traffic modeling is necessary as driving logs are recorded at discrete timesteps, thus any interaction between timesteps appears as coordinated behavior. Communication that is not generally recorded in log data, such as eye contact or turn signals, also contributes to the appearance of coordination among actors in a recorded scenario.\nTrajeglish explicitly models this intra-timestep dependence. It achieves this by tokenizing a given scenario\u2014in the same manner as a language model\u2014enabling the model to predict only the likely trajectories, or tokens, based on the context of the scene. Trajeglish then models the next actions in the timestep by analyzing the distribution of all the tokenized scenarios.\nThree images showing three different timesteps, where in each one, a future position is chosen based on its proximity to the current location.\nFigure 2. Trajeglish tokenizes trajectories by iteratively finding the token with minimum corner distance to the next state.\nThis process of predicting the next token continuously builds on itself. After a chosen number of tokens are sampled, Trajeglish has enough context to predict scenarios of various lengths with an arbitrary number of agents.\nA leading approach\nTrajeglish was compared with 16 other models in the V0 leaderboard of the Waymo Sim Agents Challenge. Each model was tasked with simulating 32 scene-consistent trajectories for up to 128 agents at a time, given 1 second of initial driving information.\nThe challenge evaluated the realism of each simulation based on distribution matching. Several statistics were computed over the simulated scenarios and compared to the same statistics computed on the recorded scenarios. The closer these statistics matched each other, the higher the score.\nThe only model to use tokenization, Trajeglish produced the most realistic outcomes, according to Waymo\u2019s parameters. Qualitatively, Trajeglish dramatically improved performance in scenarios with dense interaction between agents, such as traffic jams, merging scenarios, and four-way stop intersections.\nThe Waymo leaderboard evaluated three categories in each simulation: kinematics (such as speed), interactions, or distance to the nearest vehicle, and whether the trajectory remained in the drivable area. Overall realism was a weighted average across these categories.\nAccording to these parameters, Trajeglish improved over the previous state-of-the-art model in overall realism of scenarios by 3.3% and topped the interaction component by 9.9%.\nBar charts comparing overall realism and interaction realism, with Trajeglish displaying the highest results of the top 10 models.\nFigure 3. Trajeglish results compared with other entrants in the Waymo Sim Agents Challenge. Submissions using ensembling are marked with asterisks.\nConclusion\nHuman driving behavior can be incredibly nuanced, posing a significant challenge to recreating it in simulation. However, by taking a page from language modeling, which deals with similar complexities in human language, the task becomes more manageable.\nAs a result, AV developers can leverage higher fidelity traffic models in simulation to accelerate training, testing, and validation.\nTo learn more, read the full paper and Trajeglish: Learning the Language of Driving Scenarios project page."}], "https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/": [{"text": "The article discusses Retrieval-Augmented Generation (RAG) pipelines and the various components involved in designing and deploying them, including fine-tuning, Parameter-Efficient Fine-Tuning (PEFT), prompt engineering, and RAG itself. It emphasizes the importance of measuring and improving accuracy in RAG systems, as well as connecting LLMs to data sources through frameworks like LangChain and LlamaIndex. The article also covers data preprocessing, indexing and retrieval, and LLM inference to accelerate a RAG pipeline, with suggestions for improving latency for chatbots. It highlights the use of streaming UI and smaller fine-tuned LLMs to reduce latency. By implementing RAG in enterprise solutions, organizations can provide accurate and up-to-date information to LLMs, enhancing user experiences and trust. The article provides resources and recommendations for building RAG systems and chatbots in an enterprise environment.", "text_components": ["RAG 101: Retrieval-Augmented Generation Questions Answered\nData scientists, AI engineers, MLOps engineers, and IT infrastructure professionals must consider a variety of factors when designing and deploying a RAG pipeline: from core components like LLM to evaluation approaches.\nThe key point is that RAG is a system, not just a model or set of models. This system consists of several stages, which were discussed at a high level in RAG 101: Demystifying Retrieval-Augmented Generation Pipelines. All these stages provide opportunities to make design decisions according to your needs.\nHere\u2019s a list of top questions and answers.", "When should you fine-tune the LLM vs. using RAG?\nIn the world of LLMs, choosing between fine-tuning, Parameter-Efficient Fine-Tuning (PEFT), prompt engineering, and retrieval-augmented generation (RAG) depends on the specific needs and constraints of your application.\nFine-tuning customizes a pretrained LLM for a specific domain by updating most or all of its parameters with a domain-specific dataset. This approach is resource-intensive but yields high accuracy for specialized use cases.\nPEFT modifies a pretrained LLM with fewer parameter updates, focusing on a subset of the model. It strikes a balance between accuracy and resource usage, offering improvements over prompt engineering with manageable data and computational demands.\nPrompt engineering manipulates the input to an LLM to steer its output, without altering the model\u2019s parameters. It\u2019s the least resource-intensive method, suitable for applications with limited data and computational resources.\nRAG enhances LLM prompts with information from external databases, effectively a sophisticated form of prompt engineering.\nIt\u2019s not about using one technique or another. In fact, these techniques can be used in tandem. For example, PEFT might be integrated into a RAG system for further refinement of the LLM or embedding model. The best approach depends on the application\u2019s specific requirements, balancing accuracy, resource availability, and computational constraints.\nFor more information about customization techniques that you can use to improve domain-specific accuracy, see Selecting Large Language Model Customization Techniques.\nWhen building an application with LLMs, start by implementing RAG to enhance the model\u2019s responses with external information. This approach quickly improves relevance and depth.\nLater, model customization techniques as outlined earlier, can be employed if you need more domain-specific accuracy. This two-step process balances quick deployment with RAG and targeted improvements through model customization with efficient development and continuous enhancement strategies.", "How to increase RAG accuracy without fine-tuning?\nThis question deserves not just its own post but several posts. In short, obtaining accuracy in enterprise solutions that leverage RAG is crucial, and fine-tuning is just one technique that may (or may not) improve accuracy in a RAG system.\nFirst and foremost, find a way to measure your RAG accuracy. If you don\u2019t know where you\u2019re beginning, you won\u2019t know how to improve. There are several frameworks for evaluating RAG systems, such as Ragas, ARES, and Bench.\nAfter you have done some evaluation for accuracy, there are numerous places to look to improve the accuracy that does not require fine-tuning.\nAlthough it may sound trivial, first check to make sure that your data is being parsed and loaded correctly in the first place. For example, if documents contain tables or even images, certain data loaders may miss information in documents.\nAfter data is ingested, it is chunked. This is the process of splitting text into segments. A chunk can be a fixed character length, but there are various chunking methods, such as sentence splitting and recursive chunking. How text is chunked determines how it is stored in an embedding vector for retrieval.\nOn top of this, there are many indexing and associated retrieval patterns. For example, several indexes can be constructed for various kinds of user questions and a user query can be routed according to an LLM to the appropriate index.\nThere are also a variety of retrieval strategies. The most rudimentary strategy is using cosine similarity with an index, but BM25, custom retrievers, or knowledge graphs can also improve the retrieval.\nReranking of results from the retriever can also provide additional flexibility and accuracy improvements according to unique requirements. Query transformations can work well to break down more complex questions. Even just changing the LLM\u2019s system prompt can drastically change accuracy.\nAt the end of the day, it\u2019s important to take time to experiment and measure the changes in accuracy that various approaches provide.\nRemember, models like the LLM or embedding model are merely a part of a RAG system. There are many ways to improve RAG systems to achieve high accuracy without doing any fine-tuning.", "How to connect LLMs to data sources?\nThere are a variety of frameworks for connecting LLMs to your data sources, such as LangChain and LlamaIndex. These frameworks provide a variety of features, like evaluation libraries, document loaders, and query methods. New solutions are also coming out all the time. We recommend reading about various frameworks and picking the software and components of the software that make the most sense for your application.", "Can RAG cite references for the data that it retrieves?\nYes. In fact, it improves the user experience if you can cite references for retrieved data. In the AI chatbot RAG workflow example found in the /NVIDIA/GenerativeAIExamples GitHub repo, we show how to link back to source documents.", "What type of data is needed for RAG? How to secure data?\nRight now, textual data is well supported for RAG. Support in RAG systems for other forms of data like images and tables is improving as more research into multi-modal use cases progresses. You may have to write additional tools for data preprocessing depending on your data and where it\u2019s located. There are a variety of data loaders available from LlamaHub and LangChain. For more information about building enriched pipelines with chains, see Security LLM Systems Against Prompt Injection.\nSecuring data, particularly for an enterprise, is paramount. For example, some indexed data may be intended for only a particular set of users. Role-based access control (RBAC), which restricts access to a system depending on roles, can provide data access control. For example, a user session token can be used in the request to the vector database so that information that\u2019s out of scope for that user\u2019s permissions is not returned.\nA lot of the terms for securing a model in the environment are the same as you might use for securing a database or other critical asset. Think about how your system will log activities\u2014the prompt inputs, outputs, and error results\u2014that are the results of production pipelines. These may provide a rich set of data for product training and improvement, but also a source of data leaks like PII that must be carefully managed just as you are managing the model pipelines themselves.\nAI models have many common patterns to cloud deployments. You should take every advantage of tools like RBAC, rate limiting, and other controls common in those environments to make your AI deployments more robust. Models are just one element of these powerful pipelines. For more information, see Best Practices for Securing LLM Enabled Applications\nOne aspect important in any LLM deployment is the nature of interaction with your end users. So much of RAG pipelines are centered on the natural language inputs and outputs. Consider ways to ensure that the experience meets consistent expectations through input/output moderation.\nPeople can ask questions in many different ways. You can give your LLM a helping hand through tools like NeMo Guardrails, which can provide secondary checks on inputs and outputs to ensure that your system runs in tip-top shape, addresses questions it was built for, and helpfully guides users elsewhere for questions that the LLM application isn\u2019t built to handle.", "How to accelerate a RAG pipeline?\nRAG systems consist of many components, so there are ample opportunities to accelerate a RAG pipeline:\nData preprocessing\nIndexing and retrieval\nLLM inference", "Data preprocessing\nDeduplication is the process of identifying and removing duplicate data. In the context of RAG data preprocessing, deduplication can be used to reduce the number of identical documents that must be indexed for retrieval.\nNVIDIA NeMo Data Curator uses NVIDIA GPUs to accelerate deduplication by performing min hashing, Jaccard similarity computing, and connected component analysis in parallel. This can significantly reduce the amount of time it takes to deduplicate a large dataset.\nAnother opportunity is chunking. Dividing a large text corpus into smaller, more manageable chunks must be done because the downstream embedding model can only encode sentences below the maximum length. Popular embedding models such as OpenAI can encode up to 1536 tokens. If the text has more tokens, it is simply truncated.\nNVIDIA cuDF can be used to accelerate chunking by performing parallel data frame operations on the GPU. This can significantly reduce the amount of time required to chunk a large corpus.\nLastly, you can accelerate a tokenizer on the GPU. Tokenizers are responsible for converting text into integers as tokens, which are then used by the embedding model. The process of tokenizing text can be computationally expensive, especially for large datasets.", "Indexing and retrieval\nThe generation of embeddings is frequently a recurring process since RAG is well-suited for knowledge bases that are frequently updated. Retrieval is done at inference time, so low latency is a requirement. These processes can be accelerated by NVIDIA NeMo Retriever. NeMo Retriever aims to provide state-of-the-art, commercially ready models and microservices, optimized for the lowest latency and highest throughput.", "LLM inference\nAt a minimum, an LLM is used for the generation of a fully formed response. LLMs can also be used for tasks such as query decomposition and routing.\nWith several calls to an LLM, low latency for the LLM is crucial. NVIDIA NeMo includes TensorRT-LLM for model deployment, which optimizes the LLM to achieve both ground-breaking inference acceleration and GPU efficiency.\nNVIDIA Triton Inference Server also enables the optimized LLM to be deployed for high-performance, cost-effective, and low-latency inference.", "What are solutions to improve latency for a chatbot?\nBeyond using the suggestions to accelerate your RAG pipeline like NeMo Retriever and NeMo inference container with Triton Inference Server and TensorRT-LLM, it is important to consider using streaming to improve the perceived latency of the chatbot. As responses can be long, a streaming UI displaying parts of the response as they become available can mitigate \u200cperceived latency.\nIt may be worthwhile to consider using a smaller LLM that is fine-tuned for your use case. In general, smaller LLMs have much lower latency than larger LLMs.\nSome fine-tuned 7B models have demonstrated out-performing the accuracy of GPT-4 on specific tasks, for example, SQL generation. For example, ChipNeMo, a custom LLM built for internal use at NVIDIA to help engineers generate and optimize software for chip design, uses a 13B fine-tuned model instead of a 70B-parameter model. TensorRT-LLM delivers model optimizations such as flashing, FlashAttention, PagedAttention, Distillation, and Quantization for running smaller fine-tuned models locally, which can be used to decrease the memory used by the LLM.\nLatency for LLM responses is a function of the time to first token (TTFT) and the time per output token (TPOT).\nlatency = TTFT + TPOT(number\\_of\\_tokens\\_to\\_generate)\nBoth TTFT and TPOT will be lower for a smaller LLM.", "Get started building RAG in your enterprise\nBy using RAG, you can provide up-to-date and proprietary information with ease to LLMs and build a system that increases user trust, improves user experiences, and reduces hallucinations.\nExplore the NVIDIA AI chatbot RAG workflow to get started building a chatbot that can accurately answer domain-specific questions in natural language using up-to-date information."], "document_title": "RAG 101: Retrieval-Augmented Generation Questions Answered", "document_url": "https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/", "document_date": "2023-12-18T19:44:42", "document_date_modified": "2023-12-30T00:29:51", "document_full_text": "RAG 101: Retrieval-Augmented Generation Questions Answered\nData scientists, AI engineers, MLOps engineers, and IT infrastructure professionals must consider a variety of factors when designing and deploying a RAG pipeline: from core components like LLM to evaluation approaches.\nThe key point is that RAG is a system, not just a model or set of models. This system consists of several stages, which were discussed at a high level in RAG 101: Demystifying Retrieval-Augmented Generation Pipelines. All these stages provide opportunities to make design decisions according to your needs.\nHere\u2019s a list of top questions and answers.\nWhen should you fine-tune the LLM vs. using RAG?\nIn the world of LLMs, choosing between fine-tuning, Parameter-Efficient Fine-Tuning (PEFT), prompt engineering, and retrieval-augmented generation (RAG) depends on the specific needs and constraints of your application.\nFine-tuning customizes a pretrained LLM for a specific domain by updating most or all of its parameters with a domain-specific dataset. This approach is resource-intensive but yields high accuracy for specialized use cases.\nPEFT modifies a pretrained LLM with fewer parameter updates, focusing on a subset of the model. It strikes a balance between accuracy and resource usage, offering improvements over prompt engineering with manageable data and computational demands.\nPrompt engineering manipulates the input to an LLM to steer its output, without altering the model\u2019s parameters. It\u2019s the least resource-intensive method, suitable for applications with limited data and computational resources.\nRAG enhances LLM prompts with information from external databases, effectively a sophisticated form of prompt engineering.\nIt\u2019s not about using one technique or another. In fact, these techniques can be used in tandem. For example, PEFT might be integrated into a RAG system for further refinement of the LLM or embedding model. The best approach depends on the application\u2019s specific requirements, balancing accuracy, resource availability, and computational constraints.\nFor more information about customization techniques that you can use to improve domain-specific accuracy, see Selecting Large Language Model Customization Techniques.\nWhen building an application with LLMs, start by implementing RAG to enhance the model\u2019s responses with external information. This approach quickly improves relevance and depth.\nLater, model customization techniques as outlined earlier, can be employed if you need more domain-specific accuracy. This two-step process balances quick deployment with RAG and targeted improvements through model customization with efficient development and continuous enhancement strategies.\nHow to increase RAG accuracy without fine-tuning?\nThis question deserves not just its own post but several posts. In short, obtaining accuracy in enterprise solutions that leverage RAG is crucial, and fine-tuning is just one technique that may (or may not) improve accuracy in a RAG system.\nFirst and foremost, find a way to measure your RAG accuracy. If you don\u2019t know where you\u2019re beginning, you won\u2019t know how to improve. There are several frameworks for evaluating RAG systems, such as Ragas, ARES, and Bench.\nAfter you have done some evaluation for accuracy, there are numerous places to look to improve the accuracy that does not require fine-tuning.\nAlthough it may sound trivial, first check to make sure that your data is being parsed and loaded correctly in the first place. For example, if documents contain tables or even images, certain data loaders may miss information in documents.\nAfter data is ingested, it is chunked. This is the process of splitting text into segments. A chunk can be a fixed character length, but there are various chunking methods, such as sentence splitting and recursive chunking. How text is chunked determines how it is stored in an embedding vector for retrieval.\nOn top of this, there are many indexing and associated retrieval patterns. For example, several indexes can be constructed for various kinds of user questions and a user query can be routed according to an LLM to the appropriate index.\nThere are also a variety of retrieval strategies. The most rudimentary strategy is using cosine similarity with an index, but BM25, custom retrievers, or knowledge graphs can also improve the retrieval.\nReranking of results from the retriever can also provide additional flexibility and accuracy improvements according to unique requirements. Query transformations can work well to break down more complex questions. Even just changing the LLM\u2019s system prompt can drastically change accuracy.\nAt the end of the day, it\u2019s important to take time to experiment and measure the changes in accuracy that various approaches provide.\nRemember, models like the LLM or embedding model are merely a part of a RAG system. There are many ways to improve RAG systems to achieve high accuracy without doing any fine-tuning.\nHow to connect LLMs to data sources?\nThere are a variety of frameworks for connecting LLMs to your data sources, such as LangChain and LlamaIndex. These frameworks provide a variety of features, like evaluation libraries, document loaders, and query methods. New solutions are also coming out all the time. We recommend reading about various frameworks and picking the software and components of the software that make the most sense for your application.\nCan RAG cite references for the data that it retrieves?\nYes. In fact, it improves the user experience if you can cite references for retrieved data. In the AI chatbot RAG workflow example found in the /NVIDIA/GenerativeAIExamples GitHub repo, we show how to link back to source documents.\nWhat type of data is needed for RAG? How to secure data?\nRight now, textual data is well supported for RAG. Support in RAG systems for other forms of data like images and tables is improving as more research into multi-modal use cases progresses. You may have to write additional tools for data preprocessing depending on your data and where it\u2019s located. There are a variety of data loaders available from LlamaHub and LangChain. For more information about building enriched pipelines with chains, see Security LLM Systems Against Prompt Injection.\nSecuring data, particularly for an enterprise, is paramount. For example, some indexed data may be intended for only a particular set of users. Role-based access control (RBAC), which restricts access to a system depending on roles, can provide data access control. For example, a user session token can be used in the request to the vector database so that information that\u2019s out of scope for that user\u2019s permissions is not returned.\nA lot of the terms for securing a model in the environment are the same as you might use for securing a database or other critical asset. Think about how your system will log activities\u2014the prompt inputs, outputs, and error results\u2014that are the results of production pipelines. These may provide a rich set of data for product training and improvement, but also a source of data leaks like PII that must be carefully managed just as you are managing the model pipelines themselves.\nAI models have many common patterns to cloud deployments. You should take every advantage of tools like RBAC, rate limiting, and other controls common in those environments to make your AI deployments more robust. Models are just one element of these powerful pipelines. For more information, see Best Practices for Securing LLM Enabled Applications\nOne aspect important in any LLM deployment is the nature of interaction with your end users. So much of RAG pipelines are centered on the natural language inputs and outputs. Consider ways to ensure that the experience meets consistent expectations through input/output moderation.\nPeople can ask questions in many different ways. You can give your LLM a helping hand through tools like NeMo Guardrails, which can provide secondary checks on inputs and outputs to ensure that your system runs in tip-top shape, addresses questions it was built for, and helpfully guides users elsewhere for questions that the LLM application isn\u2019t built to handle.\nHow to accelerate a RAG pipeline?\nRAG systems consist of many components, so there are ample opportunities to accelerate a RAG pipeline:\nData preprocessing\nIndexing and retrieval\nLLM inference\nData preprocessing\nDeduplication is the process of identifying and removing duplicate data. In the context of RAG data preprocessing, deduplication can be used to reduce the number of identical documents that must be indexed for retrieval.\nNVIDIA NeMo Data Curator uses NVIDIA GPUs to accelerate deduplication by performing min hashing, Jaccard similarity computing, and connected component analysis in parallel. This can significantly reduce the amount of time it takes to deduplicate a large dataset.\nAnother opportunity is chunking. Dividing a large text corpus into smaller, more manageable chunks must be done because the downstream embedding model can only encode sentences below the maximum length. Popular embedding models such as OpenAI can encode up to 1536 tokens. If the text has more tokens, it is simply truncated.\nNVIDIA cuDF can be used to accelerate chunking by performing parallel data frame operations on the GPU. This can significantly reduce the amount of time required to chunk a large corpus.\nLastly, you can accelerate a tokenizer on the GPU. Tokenizers are responsible for converting text into integers as tokens, which are then used by the embedding model. The process of tokenizing text can be computationally expensive, especially for large datasets.\nIndexing and retrieval\nThe generation of embeddings is frequently a recurring process since RAG is well-suited for knowledge bases that are frequently updated. Retrieval is done at inference time, so low latency is a requirement. These processes can be accelerated by NVIDIA NeMo Retriever. NeMo Retriever aims to provide state-of-the-art, commercially ready models and microservices, optimized for the lowest latency and highest throughput.\nLLM inference\nAt a minimum, an LLM is used for the generation of a fully formed response. LLMs can also be used for tasks such as query decomposition and routing.\nWith several calls to an LLM, low latency for the LLM is crucial. NVIDIA NeMo includes TensorRT-LLM for model deployment, which optimizes the LLM to achieve both ground-breaking inference acceleration and GPU efficiency.\nNVIDIA Triton Inference Server also enables the optimized LLM to be deployed for high-performance, cost-effective, and low-latency inference.\nWhat are solutions to improve latency for a chatbot?\nBeyond using the suggestions to accelerate your RAG pipeline like NeMo Retriever and NeMo inference container with Triton Inference Server and TensorRT-LLM, it is important to consider using streaming to improve the perceived latency of the chatbot. As responses can be long, a streaming UI displaying parts of the response as they become available can mitigate \u200cperceived latency.\nIt may be worthwhile to consider using a smaller LLM that is fine-tuned for your use case. In general, smaller LLMs have much lower latency than larger LLMs.\nSome fine-tuned 7B models have demonstrated out-performing the accuracy of GPT-4 on specific tasks, for example, SQL generation. For example, ChipNeMo, a custom LLM built for internal use at NVIDIA to help engineers generate and optimize software for chip design, uses a 13B fine-tuned model instead of a 70B-parameter model. TensorRT-LLM delivers model optimizations such as flashing, FlashAttention, PagedAttention, Distillation, and Quantization for running smaller fine-tuned models locally, which can be used to decrease the memory used by the LLM.\nLatency for LLM responses is a function of the time to first token (TTFT) and the time per output token (TPOT).\nlatency = TTFT + TPOT(number\\_of\\_tokens\\_to\\_generate)\nBoth TTFT and TPOT will be lower for a smaller LLM.\nGet started building RAG in your enterprise\nBy using RAG, you can provide up-to-date and proprietary information with ease to LLMs and build a system that increases user trust, improves user experiences, and reduces hallucinations.\nExplore the NVIDIA AI chatbot RAG workflow to get started building a chatbot that can accurately answer domain-specific questions in natural language using up-to-date information."}], "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/": [{"text": "The article discusses the benefits of retrieval-augmented generation (RAG) pipelines in enhancing large language models (LLMs) for various enterprise applications such as chatbots, customer service, and enterprise search. By augmenting LLMs with real-time data access, preserving data privacy, and mitigating hallucinations, businesses can create agile and responsive AI applications. The RAG pipeline consists of document ingestion, pre-processing, generating embeddings, storing embeddings in vector databases, LLMs, and querying. By following the RAG workflow provided in the NVIDIA Generative AI Examples GitHub repo, enterprises can build and deploy their first RAG pipeline to improve user trust, enhance user experiences, and reduce hallucinations in AI responses. By leveraging RAG, companies can provide accurate and up-to-date information to LLMs, leading to more efficient interactions and better customer service.", "text_components": ["RAG 101: Demystifying Retrieval-Augmented Generation Pipelines\nLarge language models (LLMs) have impressed the world with their unprecedented capabilities to comprehend and generate human-like responses. Their chat functionality provides a fast and natural interaction between humans and large corpora of data. For example, they can summarize and extract highlights from data or replace complex queries such as SQL queries with natural language.\nIt is tempting to assume that business value can be generated by these models with no extra effort, but this is unfortunately not often the case. Luckily, all that enterprises must do to extract value out of using LLMs is to augment the LLM with their own data. This can be done with retrieval augmented generation (RAG), which is showcased in the NVIDIA Generative AI Examples GitHub repo for developers\nBy augmenting an LLM with business data, enterprises can make their AI applications agile and responsive to new developments. For instance:\nChatbots: Many enterprises already use AI chatbots to power basic customer interactions on their websites. With RAG, companies can build a chat experience that\u2019s highly specific to their product. For example, questions about product specifications could easily be answered.\nCustomer service: Companies can empower live service representatives to easily answer customer questions with precise, up-to-date information.\nEnterprise search: Businesses have a wealth of knowledge across the organization, including technical documentation, company policies, IT support articles, and code repositories. Employees could query an internal search engine to retrieve information faster and more efficiently.\nThis post explains the benefits of using the RAG technique when building an LLM application, along with the components of a RAG pipeline. For more information after you finish this post, see RAG 101: Retrieval-Augmented Generation Questions Answered.", "Benefits of RAG\nThere are several advantages of using RAG:\nEmpowering LLM solutions with real-time data access\nPreserving data privacy\nMitigating LLM hallucinations", "Empowering LLM solutions with real-time data access\nData is constantly changing in an enterprise. AI solutions that use LLMs can remain up-to-date and current with RAG, which facilitates direct access to additional data resources. These resources can consist of real-time and personalized data.", "Preserving data privacy\nEnsuring data privacy is crucial for enterprises. With a self-hosted LLM (demonstrated in the RAG workflow), sensitive data can be kept on-premises just like the stored data.", "Mitigating LLM hallucinations\nWhen LLMs are not supplied with factual actual information, they often provide faulty, but convincing responses. This is known as hallucination, and RAG reduces the likelihood of hallucinations by providing the LLM with relevant and factional information.", "Building and deploying your first RAG pipeline\nA typical RAG pipeline consists of several phases. The process of document ingestion occurs offline, and when an online query comes in, the retrieval of relevant documents and the generation of a response occurs.\nFigure 1 shows an accelerated RAG pipeline that can be built and deployed in the /NVIDIA/GenerativeAIExamples GitHub repo.\nDiagram showing retrieval-augmented generation pipeline components.\nFigure 1. Overview of RAG pipeline components: ingest and query flows\nEach logical microservice is separated into containers available in the NGC public catalog. On a high level, the architecture of a RAG system can be distilled down to the pipelines shown in Figure 1:\nA recurring pipeline of document pre-processing, ingestion, and embedding generation\nAn inference pipeline with a user query and response generation", "Document ingestion\nFirst, raw data from diverse sources, such as databases, documents, or live feeds, is ingested into the RAG system. To pre-process this data, LangChain provides a variety of document loaders that load data of many forms from many different sources.\nThe term document loader is used loosely. Source documents do not necessarily need to be what you might think of as standard documents (PDFs, text files, and so on). For example, LangChain supports loading data from Confluence, CSV files, Outlook emails, and more. LlamaIndex also provides a variety of loaders, which can be viewed in LlamaHub.", "Document pre-processing\nAfter documents have been loaded, they are often transformed. One transformation method is text-splitting, which breaks down long text into smaller segments. This is necessary for fitting the text into the embedding model, e5-large-v2, which has a maximum token length of 512. While splitting the text sounds simple, this can be a nuanced process.", "Generating embeddings\nWhen data is ingested, it must be transformed into a format that the system can efficiently process. Generating embeddings involves converting data into high-dimensional vectors, which represent text in a numerical format.", "Storing embeddings in vector databases\nThe processed data and generated embeddings are stored in specialized databases known as vector databases. These databases are optimized to handle vectorized data, enabling rapid search and retrieval operations. Storing the data in RAPIDS RAFT accelerated vector databases like Milvus guarantees that information remains accessible and can be quickly retrieved during real-time interactions.", "LLMs\nLLMs form the foundational generative component of the RAG pipeline. These advanced, generalized language models are trained on vast datasets, enabling them to understand and generate human-like text. In the context of RAG, LLMs are used to generate fully formed responses based on the user query and contextual information retrieved from the vector DBs during user queries.", "Querying\nWhen a user submits a query, the RAG system uses the indexed data and vectors to perform efficient searches. The system identifies relevant information by comparing the query vector with the stored vectors in the vector DBs. The LLMs then use the retrieved data to craft appropriate responses.\nFast-track your deployment of this system by testing out this example workflow in the /NVIDIA/GenerativeAIExamples GitHub repo.", "Get started building RAG in your enterprise\nBy using RAG, you can provide up-to-date and proprietary information with ease to LLMs and build a system that increases user trust, improves user experiences, and reduces hallucinations.\nExplore the NVIDIA AI chatbot RAG workflow to get started building a chatbot that can accurately answer domain-specific questions in natural language using up-to-date information."], "document_title": "RAG 101: Demystifying Retrieval-Augmented Generation Pipelines", "document_url": "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/", "document_date": "2023-12-18T19:44:31", "document_date_modified": "2023-12-18T19:44:34", "document_full_text": "RAG 101: Demystifying Retrieval-Augmented Generation Pipelines\nLarge language models (LLMs) have impressed the world with their unprecedented capabilities to comprehend and generate human-like responses. Their chat functionality provides a fast and natural interaction between humans and large corpora of data. For example, they can summarize and extract highlights from data or replace complex queries such as SQL queries with natural language.\nIt is tempting to assume that business value can be generated by these models with no extra effort, but this is unfortunately not often the case. Luckily, all that enterprises must do to extract value out of using LLMs is to augment the LLM with their own data. This can be done with retrieval augmented generation (RAG), which is showcased in the NVIDIA Generative AI Examples GitHub repo for developers\nBy augmenting an LLM with business data, enterprises can make their AI applications agile and responsive to new developments. For instance:\nChatbots: Many enterprises already use AI chatbots to power basic customer interactions on their websites. With RAG, companies can build a chat experience that\u2019s highly specific to their product. For example, questions about product specifications could easily be answered.\nCustomer service: Companies can empower live service representatives to easily answer customer questions with precise, up-to-date information.\nEnterprise search: Businesses have a wealth of knowledge across the organization, including technical documentation, company policies, IT support articles, and code repositories. Employees could query an internal search engine to retrieve information faster and more efficiently.\nThis post explains the benefits of using the RAG technique when building an LLM application, along with the components of a RAG pipeline. For more information after you finish this post, see RAG 101: Retrieval-Augmented Generation Questions Answered.\nBenefits of RAG\nThere are several advantages of using RAG:\nEmpowering LLM solutions with real-time data access\nPreserving data privacy\nMitigating LLM hallucinations\nEmpowering LLM solutions with real-time data access\nData is constantly changing in an enterprise. AI solutions that use LLMs can remain up-to-date and current with RAG, which facilitates direct access to additional data resources. These resources can consist of real-time and personalized data.\nPreserving data privacy\nEnsuring data privacy is crucial for enterprises. With a self-hosted LLM (demonstrated in the RAG workflow), sensitive data can be kept on-premises just like the stored data.\nMitigating LLM hallucinations\nWhen LLMs are not supplied with factual actual information, they often provide faulty, but convincing responses. This is known as hallucination, and RAG reduces the likelihood of hallucinations by providing the LLM with relevant and factional information.\nBuilding and deploying your first RAG pipeline\nA typical RAG pipeline consists of several phases. The process of document ingestion occurs offline, and when an online query comes in, the retrieval of relevant documents and the generation of a response occurs.\nFigure 1 shows an accelerated RAG pipeline that can be built and deployed in the /NVIDIA/GenerativeAIExamples GitHub repo.\nDiagram showing retrieval-augmented generation pipeline components.\nFigure 1. Overview of RAG pipeline components: ingest and query flows\nEach logical microservice is separated into containers available in the NGC public catalog. On a high level, the architecture of a RAG system can be distilled down to the pipelines shown in Figure 1:\nA recurring pipeline of document pre-processing, ingestion, and embedding generation\nAn inference pipeline with a user query and response generation\nDocument ingestion\nFirst, raw data from diverse sources, such as databases, documents, or live feeds, is ingested into the RAG system. To pre-process this data, LangChain provides a variety of document loaders that load data of many forms from many different sources.\nThe term document loader is used loosely. Source documents do not necessarily need to be what you might think of as standard documents (PDFs, text files, and so on). For example, LangChain supports loading data from Confluence, CSV files, Outlook emails, and more. LlamaIndex also provides a variety of loaders, which can be viewed in LlamaHub.\nDocument pre-processing\nAfter documents have been loaded, they are often transformed. One transformation method is text-splitting, which breaks down long text into smaller segments. This is necessary for fitting the text into the embedding model, e5-large-v2, which has a maximum token length of 512. While splitting the text sounds simple, this can be a nuanced process.\nGenerating embeddings\nWhen data is ingested, it must be transformed into a format that the system can efficiently process. Generating embeddings involves converting data into high-dimensional vectors, which represent text in a numerical format.\nStoring embeddings in vector databases\nThe processed data and generated embeddings are stored in specialized databases known as vector databases. These databases are optimized to handle vectorized data, enabling rapid search and retrieval operations. Storing the data in RAPIDS RAFT accelerated vector databases like Milvus guarantees that information remains accessible and can be quickly retrieved during real-time interactions.\nLLMs\nLLMs form the foundational generative component of the RAG pipeline. These advanced, generalized language models are trained on vast datasets, enabling them to understand and generate human-like text. In the context of RAG, LLMs are used to generate fully formed responses based on the user query and contextual information retrieved from the vector DBs during user queries.\nQuerying\nWhen a user submits a query, the RAG system uses the indexed data and vectors to perform efficient searches. The system identifies relevant information by comparing the query vector with the stored vectors in the vector DBs. The LLMs then use the retrieved data to craft appropriate responses.\nFast-track your deployment of this system by testing out this example workflow in the /NVIDIA/GenerativeAIExamples GitHub repo.\nGet started building RAG in your enterprise\nBy using RAG, you can provide up-to-date and proprietary information with ease to LLMs and build a system that increases user trust, improves user experiences, and reduces hallucinations.\nExplore the NVIDIA AI chatbot RAG workflow to get started building a chatbot that can accurately answer domain-specific questions in natural language using up-to-date information."}], "https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/": [{"text": "The article discusses the deployment of Retrieval-Augmented Generation (RAG) applications on the NVIDIA GH200 Grace Hopper Superchip to achieve accelerated performance for large language model (LLM) applications. RAG enhances LLMs by augmenting them with external knowledge bases, improving response quality and reducing errors. However, deploying and scaling RAG applications for a large number of users presents challenges, particularly in GPU memory management. The GH200 addresses these challenges with its state-of-the-art memory capabilities, offering up to 624 GB of fast-access memory on a single GPU-CPU superchip. When optimized with NVIDIA software tools like TensorRT-LLM, the GH200 demonstrates significant speedups in tasks such as embedding generation, index build, vector search, and Llama-2-70B inference performance. These improvements enable efficient handling of new data, large batch sizes, and complex queries, making the GH200 an optimal choice for RAG applications at scale. The GH200 also excels in power efficiency, delivering superior performance per watt. Overall, the GH200 and NVIDIA software solutions play a crucial role in enabling high-performance inference for compute-intensive LLM applications using RAG.", "text_components": ["Deploying Retrieval-Augmented Generation Applications on NVIDIA GH200 Delivers Accelerated Performance\nLarge language model (LLM) applications are essential in enhancing productivity across industries through natural language. However, their effectiveness is often limited by the extent of their training data, resulting in poor performance when dealing with real-time events and new knowledge the LLM isn\u2019t trained on.\nRetrieval-augmented generation (RAG) solves these problems. By augmenting models with an external knowledge base, RAG can ground LLMs with relevant data. This enhances the quality of their response, improving accuracy, and reducing hallucinations. RAG not only extends the utility of LLMs but also provides a cheaper alternative to time-consuming re-training runs.\nDeploying and scaling RAG applications for tens of thousands to millions of users comes with its own set of challenges, specifically around GPU memory management. Developers need access to state-of-the-art infrastructure with robust memory capabilities that runs real-time RAG applications performantly and within stringent service level agreements (SLAs).\nThis is where the NVIDIA end-to-end full-stack accelerated computing solution shines. In this post, we discuss how the NVIDIA GH200 Grace Hopper Superchip can help solve these issues.\nComparing the GH200 to NVIDIA A100 Tensor Core GPUs, we observed up to a 2.7x increase in speed for embedding generation, 2.9x for index build, 3.3x for vector search time, and 5.7x for Llama-2-70B (FP8) inference performance.", "Memory challenges when deploying RAG applications at scale\nOne significant challenge for developers deploying large-scale RAG applications is managing GPU memory usage. Both GPU memory capacity and bandwidth are vital for optimal inference performance, and when limited, often act as a bottleneck for crucial tasks such as:\nHosting LLMs\nProcessing batched requests\nHandling Key-Value (KV) cache in attention mechanisms\nFacilitating efficient data transfer between the GPU and CPU", "Model hosting\nWhile larger models tend to provide more precise and comprehensive responses, they require more GPU memory. This can make deployments for models like Llama-2-70B difficult to manage.\nLlama-2-70B (FP16) has weights that take up 140 GB of GPU memory alone. Developers often resort to techniques like model sharding across multiple GPUs, which ultimately add latency and complexity.\nIn addition to hosting the LLM, the GPU must host an embedding model and a vector database. These components may not require much additional GPU memory (about 10 GB depending on the size of the model and knowledge base.) However, generating embeddings and executing vector search at scale are highly parallel operations and can significantly benefit from GPU acceleration and strong bandwidth between the GPU and CPU.", "Batch processing and KV cache\nBatching enables the GPU to handle multiple requests simultaneously in a single pass through the neural network to boost throughput effectively. However, batch size is directly proportional to KV (key-value) cache size, which represents the memory occupied by the caching of self-attention tensors to avoid redundant computation, and results in large GPU memory requirements. For example, when deploying Llama-2-70B (80 layers) with FP16 precision, batch size 32, and context size 4096, the size of the KV cache comes out to around 40 GB.\nIn a RAG application that is already GPU memory-constrained from model hosting, there exists an upper limit to the batch size during inference. While larger batch sizes do boost throughput, they can also result in higher latency. To meet SLAs, developers must optimize for this throughput and latency tradeoff according to the RAG application\u2019s intended use.\nFor instance, an application generating detailed reports from extensive enterprise data can afford higher latency, and therefore larger batch sizes, compared to an application providing real-time customer support.", "Data transfer\nRAG applications at scale require rapid data transfer between the GPU and CPU. For example, during the embedding generation process, the CPU preprocessing (for example, tokenization, cleaning, and so on) of new data takes place before the data is sent to the GPU for accelerated embedding generation. This requires strong bandwidth between the GPU and CPU, which is essential for applications where delays in processing large amounts of new information at scale can lead to outdated or irrelevant results.", "Hardware: NVIDIA GH200 Grace Hopper Superchip\nThe NVIDIA GH200 Grace Hopper Superchip tackles GPU memory deployment challenges head-on with its state-of-the-art memory capabilities.\nGH200 is a high-performance GPU-CPU superchip designed for the world\u2019s most demanding AI inference workloads. The architecture combines the performance of the NVIDIA Hopper GPU and the versatility of the NVIDIA Grace CPU in one superchip. They are connected by a high-bandwidth, memory-coherent NVIDIA NVLink-C2C interconnect. This enables the CPU and GPU to talk to each other at 900 GB/s, which is 7x the bandwidth of traditional PCIe Gen5 lanes and 5x the power efficiency.\nDiagram showcasing the GH200 architecture and the tight coupling of the CPU-GPU with NVLink-C2C 900 GB/s.\nFigure 1. GH200 Architecture\nThis design, as shown in Figure 1, expands memory capacity significantly. The GH200 features up to 480 GB of LPDDR5X CPU memory and supports up to 144 GB of HBM3e GPU memory, offering up to 624 GB of fast-access memory on a single GPU-CPU superchip. This ultimately increases developer productivity and performance, enabling concurrent, transparent access to both CPU and GPU-resident memory.\nThe expanded memory capacity simplifies algorithms and memory management. It also makes the GH200 ideal for handling large batch sizes, making the GPU-CPU superchip an optimal choice for RAG applications generating content for complex queries at scale. However, \u200c expanded memory capacity isn\u2019t the only feature that addresses GPU memory bottlenecks.\nThe GH200 features the NVIDIA Transformer Engine as part of the Hopper architecture. It can natively support FP8, which reduces the memory footprint for LLMs like Llama2-70B through quantization methods implemented in optimized software like NVIDIA TensorRT-LLM. This enables large models to fit onto a single GH200.", "Software: NVIDIA NeMo Framework, NVIDIA Triton Inference Server, NVIDIA TensorRT-LLM, and NVIDIA RAFT\nA RAG pipeline includes various software components working together in harmony.\nOptimized software tools sit within different components of the broader RAG architecture (Figure 2) from embedding generation, vector search, to LLM inference, and ensure a fully accelerated RAG pipeline that delivers the best performance. It includes elements such as the deployment software optimized for LLMs like NVIDIA NeMo Framework, NVIDIA Triton Inference Server and TensorRT-LLM, and the GPU-accelerated vector database running NVIDIA RAFT.\nDiagram of a RAG inference pipeline using the NVIDIA software services like TensorRT-LLM, Triton Inference Server, and NeMo.\nFigure 2. RAG Architecture\nWhile each component of this architecture is designed to be open-source and modular, we recommend implementing this RAG architecture with the best NVIDIA models, libraries, tools, and support for optimal performance.\nFor example, TensorRT-LLM can supercharge LLM inference beyond quantization methods by implementing techniques like tensor parallelism, which enables model weights to be split across devices when GPU memory is constrained. Triton Model Analyzer, a tool that automatically evaluates model deployment configurations in Triton Inference Server, helps developers optimize for the best dynamic batching and model concurrency parameters that maximize inference performance under strict latency constraints. The NVIDIA RAFT library also includes widely used NVIDIA CUDA-accelerated algorithms like IVF-PQ for developers to simplify GPU-accelerated vector search.\nTriton and TensorRT-LLM are part of NVIDIA AI Enterprise, which features support services along with enterprise-grade stability, security, and manageability for open-sourced containers and frameworks that support the RAG pipeline.\nFurther details on NVIDIA best practices for the RAG workflow can be found on GitHub.", "GH200 RAG inference performance benchmarks\nWhen deploying a single GH200 GPU-CPU superchip optimized with NVIDIA software throughout the RAG pipeline, we observe incredible speedups. This includes an increase of 2.7x in embedding generation, 2.9x in index build, 3.3x in vector search time, and 5.7x in Llama-2-70B inference performance (2048 input length and 128 output length) running on TensorRT-LLM relative to A100.\nA bar chart showing GH200 and H100 speedups over A100 from 2.7x in embedding generation to 2.9x in index build for GH200 versus A100 and 1.7x in embedding generation to 1.5x in index build for H100 versus A100.\nFigure 3. Accelerated RAG performance (preprocessing) for embedding generation and index build (GH200 and H100 speedup over A100)\nPreliminary measured performance per GPU, subject to change.\nEmbedding Generation Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory) | Batch Size = 1024 | Output Vectors = 85M of size 768 (251 GB) | Model = Sentence Transformer Paraphrase Multilingual MPNET Base v2 from Hugging Face | Performance scaled linearly from a measurement of 10K text chunks\nIndex Build Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory)\nFigure 3 shows GH200 speedups of 2.7x in embedding generation over A100, saving 9 hours on a sample Wikipedia dataset in the RAG preprocessing pipeline. The index build time on GH200 following the embedding generation process shows speedups of 2.9x over A100.\nThese workloads are highly parallel operations, and at scale, can benefit significantly from additional GPUs and a strong connection between the GPU and CPU to reduce data transfer time and prevent bottlenecks.\nA bar chart showing vector search and Llama-2-70B inference speedups with GH200 and H100 over A100 of 3.3x for vector search and 5.7x for Llama-2-70B inference with input length 2048 and output length 128 for GH200 versus A100 and 2.5x for vector search and 3.9x for Llama-2-70B inference for H100 versus A100.\nFigure 4. Accelerated RAG inference performance for vector search (IVF-PQ) and Llama-2-70B inference (GH200 and H100 speedup over A100)\nPreliminary measured performance per GPU, subject to change.\nVector Search Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory) | Batch Size = 10,000 | Queries = 10,000 over 85M vectors\nLlama-2-70B Inference: 1x GH200 (144 GB HBM3e GPU memory) | 2x H100 (80 GB HBM3 GPU Memory) | 4x A100 SXM (80 GB HBM2e GPU memory) | Batch Size = 64 (GH200), 96 (H100s), and 120 (A100s) | Precision = FP8 (GH200 and H100) and FP16 (A100s) | TensorRT-LLM\nThroughput is measured by output tokens per second per GPU =\n(Output Length * Batch Size) / (Total end-to-end Latency) / (# GPUs)\nAfter the preprocessing tasks of setting up the external knowledge base are complete, the GH200 further accelerates the RAG inference pipeline. For example, during vector search, the GH200 achieves a 3.3x speedup over A100 as shown in Figure 4.\nGiven the need for handling extensive contextual data and producing concise responses, RAG applications often require long input lengths and short output lengths during LLM inference. Figure 4 illustrates results under such conditions, in which GH200 achieves a speedup of 5.7x over A100 for Llama-2-70B inference given an input length of 2048 and an output length of 128. Not only does GH200 deliver superior performance, but it also excels in power efficiency, offering favorable performance per watt. We expect these results to continue to improve with future TensorRT-LLM data offloading optimizations, further leveraging GH200 NVLink-C2C capability.\nWhen testing the GH200-powered RAG pipeline in over 200 real-world sample queries, it computed embeddings for the queries, ran vector search, and retrieved the necessary information from the external knowledge base all in 0.6 seconds.", "Conclusion\nWhen deploying compute-intensive LLM applications using RAG, it\u2019s essential to consider GPU memory and GPU-CPU bandwidth to unlock high-performance inference at scale. The GH200 GPU-CPU Superchip paired with software solutions like TensorRT-LLM play a pivotal role in addressing large-scale RAG deployment challenges. It enables efficient handling of new data, large batch sizes, and complex queries, for further performance gains in memory-constrained systems.\nCheck out our cloud partners that announced NVIDIA GH200 instances like AWS, CoreWeave, Lambda, OCI, Vultr, and others, and familiarize yourself with the NVIDIA RAG example published on the NVIDIA generative AI GitHub.\nLearn more about RAG by watching our on-demand content from NVIDIA LLM Day, which includes a session \u2018Tailoring LLMs to Your Use Case\u2019 where you can learn about strategies to build RAG-based systems."], "document_title": "Deploying Retrieval-Augmented Generation Applications on NVIDIA GH200 Delivers Accelerated Performance", "document_url": "https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/", "document_date": "2023-12-18T17:00:00", "document_date_modified": "2023-12-18T15:04:25", "document_full_text": "Deploying Retrieval-Augmented Generation Applications on NVIDIA GH200 Delivers Accelerated Performance\nLarge language model (LLM) applications are essential in enhancing productivity across industries through natural language. However, their effectiveness is often limited by the extent of their training data, resulting in poor performance when dealing with real-time events and new knowledge the LLM isn\u2019t trained on.\nRetrieval-augmented generation (RAG) solves these problems. By augmenting models with an external knowledge base, RAG can ground LLMs with relevant data. This enhances the quality of their response, improving accuracy, and reducing hallucinations. RAG not only extends the utility of LLMs but also provides a cheaper alternative to time-consuming re-training runs.\nDeploying and scaling RAG applications for tens of thousands to millions of users comes with its own set of challenges, specifically around GPU memory management. Developers need access to state-of-the-art infrastructure with robust memory capabilities that runs real-time RAG applications performantly and within stringent service level agreements (SLAs).\nThis is where the NVIDIA end-to-end full-stack accelerated computing solution shines. In this post, we discuss how the NVIDIA GH200 Grace Hopper Superchip can help solve these issues.\nComparing the GH200 to NVIDIA A100 Tensor Core GPUs, we observed up to a 2.7x increase in speed for embedding generation, 2.9x for index build, 3.3x for vector search time, and 5.7x for Llama-2-70B (FP8) inference performance.\nMemory challenges when deploying RAG applications at scale\nOne significant challenge for developers deploying large-scale RAG applications is managing GPU memory usage. Both GPU memory capacity and bandwidth are vital for optimal inference performance, and when limited, often act as a bottleneck for crucial tasks such as:\nHosting LLMs\nProcessing batched requests\nHandling Key-Value (KV) cache in attention mechanisms\nFacilitating efficient data transfer between the GPU and CPU\nModel hosting\nWhile larger models tend to provide more precise and comprehensive responses, they require more GPU memory. This can make deployments for models like Llama-2-70B difficult to manage.\nLlama-2-70B (FP16) has weights that take up 140 GB of GPU memory alone. Developers often resort to techniques like model sharding across multiple GPUs, which ultimately add latency and complexity.\nIn addition to hosting the LLM, the GPU must host an embedding model and a vector database. These components may not require much additional GPU memory (about 10 GB depending on the size of the model and knowledge base.) However, generating embeddings and executing vector search at scale are highly parallel operations and can significantly benefit from GPU acceleration and strong bandwidth between the GPU and CPU.\nBatch processing and KV cache\nBatching enables the GPU to handle multiple requests simultaneously in a single pass through the neural network to boost throughput effectively. However, batch size is directly proportional to KV (key-value) cache size, which represents the memory occupied by the caching of self-attention tensors to avoid redundant computation, and results in large GPU memory requirements. For example, when deploying Llama-2-70B (80 layers) with FP16 precision, batch size 32, and context size 4096, the size of the KV cache comes out to around 40 GB.\nIn a RAG application that is already GPU memory-constrained from model hosting, there exists an upper limit to the batch size during inference. While larger batch sizes do boost throughput, they can also result in higher latency. To meet SLAs, developers must optimize for this throughput and latency tradeoff according to the RAG application\u2019s intended use.\nFor instance, an application generating detailed reports from extensive enterprise data can afford higher latency, and therefore larger batch sizes, compared to an application providing real-time customer support.\nData transfer\nRAG applications at scale require rapid data transfer between the GPU and CPU. For example, during the embedding generation process, the CPU preprocessing (for example, tokenization, cleaning, and so on) of new data takes place before the data is sent to the GPU for accelerated embedding generation. This requires strong bandwidth between the GPU and CPU, which is essential for applications where delays in processing large amounts of new information at scale can lead to outdated or irrelevant results.\nHardware: NVIDIA GH200 Grace Hopper Superchip\nThe NVIDIA GH200 Grace Hopper Superchip tackles GPU memory deployment challenges head-on with its state-of-the-art memory capabilities.\nGH200 is a high-performance GPU-CPU superchip designed for the world\u2019s most demanding AI inference workloads. The architecture combines the performance of the NVIDIA Hopper GPU and the versatility of the NVIDIA Grace CPU in one superchip. They are connected by a high-bandwidth, memory-coherent NVIDIA NVLink-C2C interconnect. This enables the CPU and GPU to talk to each other at 900 GB/s, which is 7x the bandwidth of traditional PCIe Gen5 lanes and 5x the power efficiency.\nDiagram showcasing the GH200 architecture and the tight coupling of the CPU-GPU with NVLink-C2C 900 GB/s.\nFigure 1. GH200 Architecture\nThis design, as shown in Figure 1, expands memory capacity significantly. The GH200 features up to 480 GB of LPDDR5X CPU memory and supports up to 144 GB of HBM3e GPU memory, offering up to 624 GB of fast-access memory on a single GPU-CPU superchip. This ultimately increases developer productivity and performance, enabling concurrent, transparent access to both CPU and GPU-resident memory.\nThe expanded memory capacity simplifies algorithms and memory management. It also makes the GH200 ideal for handling large batch sizes, making the GPU-CPU superchip an optimal choice for RAG applications generating content for complex queries at scale. However, \u200c expanded memory capacity isn\u2019t the only feature that addresses GPU memory bottlenecks.\nThe GH200 features the NVIDIA Transformer Engine as part of the Hopper architecture. It can natively support FP8, which reduces the memory footprint for LLMs like Llama2-70B through quantization methods implemented in optimized software like NVIDIA TensorRT-LLM. This enables large models to fit onto a single GH200.\nSoftware: NVIDIA NeMo Framework, NVIDIA Triton Inference Server, NVIDIA TensorRT-LLM, and NVIDIA RAFT\nA RAG pipeline includes various software components working together in harmony.\nOptimized software tools sit within different components of the broader RAG architecture (Figure 2) from embedding generation, vector search, to LLM inference, and ensure a fully accelerated RAG pipeline that delivers the best performance. It includes elements such as the deployment software optimized for LLMs like NVIDIA NeMo Framework, NVIDIA Triton Inference Server and TensorRT-LLM, and the GPU-accelerated vector database running NVIDIA RAFT.\nDiagram of a RAG inference pipeline using the NVIDIA software services like TensorRT-LLM, Triton Inference Server, and NeMo.\nFigure 2. RAG Architecture\nWhile each component of this architecture is designed to be open-source and modular, we recommend implementing this RAG architecture with the best NVIDIA models, libraries, tools, and support for optimal performance.\nFor example, TensorRT-LLM can supercharge LLM inference beyond quantization methods by implementing techniques like tensor parallelism, which enables model weights to be split across devices when GPU memory is constrained. Triton Model Analyzer, a tool that automatically evaluates model deployment configurations in Triton Inference Server, helps developers optimize for the best dynamic batching and model concurrency parameters that maximize inference performance under strict latency constraints. The NVIDIA RAFT library also includes widely used NVIDIA CUDA-accelerated algorithms like IVF-PQ for developers to simplify GPU-accelerated vector search.\nTriton and TensorRT-LLM are part of NVIDIA AI Enterprise, which features support services along with enterprise-grade stability, security, and manageability for open-sourced containers and frameworks that support the RAG pipeline.\nFurther details on NVIDIA best practices for the RAG workflow can be found on GitHub.\nGH200 RAG inference performance benchmarks\nWhen deploying a single GH200 GPU-CPU superchip optimized with NVIDIA software throughout the RAG pipeline, we observe incredible speedups. This includes an increase of 2.7x in embedding generation, 2.9x in index build, 3.3x in vector search time, and 5.7x in Llama-2-70B inference performance (2048 input length and 128 output length) running on TensorRT-LLM relative to A100.\nA bar chart showing GH200 and H100 speedups over A100 from 2.7x in embedding generation to 2.9x in index build for GH200 versus A100 and 1.7x in embedding generation to 1.5x in index build for H100 versus A100.\nFigure 3. Accelerated RAG performance (preprocessing) for embedding generation and index build (GH200 and H100 speedup over A100)\nPreliminary measured performance per GPU, subject to change.\nEmbedding Generation Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory) | Batch Size = 1024 | Output Vectors = 85M of size 768 (251 GB) | Model = Sentence Transformer Paraphrase Multilingual MPNET Base v2 from Hugging Face | Performance scaled linearly from a measurement of 10K text chunks\nIndex Build Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory)\nFigure 3 shows GH200 speedups of 2.7x in embedding generation over A100, saving 9 hours on a sample Wikipedia dataset in the RAG preprocessing pipeline. The index build time on GH200 following the embedding generation process shows speedups of 2.9x over A100.\nThese workloads are highly parallel operations, and at scale, can benefit significantly from additional GPUs and a strong connection between the GPU and CPU to reduce data transfer time and prevent bottlenecks.\nA bar chart showing vector search and Llama-2-70B inference speedups with GH200 and H100 over A100 of 3.3x for vector search and 5.7x for Llama-2-70B inference with input length 2048 and output length 128 for GH200 versus A100 and 2.5x for vector search and 3.9x for Llama-2-70B inference for H100 versus A100.\nFigure 4. Accelerated RAG inference performance for vector search (IVF-PQ) and Llama-2-70B inference (GH200 and H100 speedup over A100)\nPreliminary measured performance per GPU, subject to change.\nVector Search Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory) | Batch Size = 10,000 | Queries = 10,000 over 85M vectors\nLlama-2-70B Inference: 1x GH200 (144 GB HBM3e GPU memory) | 2x H100 (80 GB HBM3 GPU Memory) | 4x A100 SXM (80 GB HBM2e GPU memory) | Batch Size = 64 (GH200), 96 (H100s), and 120 (A100s) | Precision = FP8 (GH200 and H100) and FP16 (A100s) | TensorRT-LLM\nThroughput is measured by output tokens per second per GPU =\n(Output Length * Batch Size) / (Total end-to-end Latency) / (# GPUs)\nAfter the preprocessing tasks of setting up the external knowledge base are complete, the GH200 further accelerates the RAG inference pipeline. For example, during vector search, the GH200 achieves a 3.3x speedup over A100 as shown in Figure 4.\nGiven the need for handling extensive contextual data and producing concise responses, RAG applications often require long input lengths and short output lengths during LLM inference. Figure 4 illustrates results under such conditions, in which GH200 achieves a speedup of 5.7x over A100 for Llama-2-70B inference given an input length of 2048 and an output length of 128. Not only does GH200 deliver superior performance, but it also excels in power efficiency, offering favorable performance per watt. We expect these results to continue to improve with future TensorRT-LLM data offloading optimizations, further leveraging GH200 NVLink-C2C capability.\nWhen testing the GH200-powered RAG pipeline in over 200 real-world sample queries, it computed embeddings for the queries, ran vector search, and retrieved the necessary information from the external knowledge base all in 0.6 seconds.\nConclusion\nWhen deploying compute-intensive LLM applications using RAG, it\u2019s essential to consider GPU memory and GPU-CPU bandwidth to unlock high-performance inference at scale. The GH200 GPU-CPU Superchip paired with software solutions like TensorRT-LLM play a pivotal role in addressing large-scale RAG deployment challenges. It enables efficient handling of new data, large batch sizes, and complex queries, for further performance gains in memory-constrained systems.\nCheck out our cloud partners that announced NVIDIA GH200 instances like AWS, CoreWeave, Lambda, OCI, Vultr, and others, and familiarize yourself with the NVIDIA RAG example published on the NVIDIA generative AI GitHub.\nLearn more about RAG by watching our on-demand content from NVIDIA LLM Day, which includes a session \u2018Tailoring LLMs to Your Use Case\u2019 where you can learn about strategies to build RAG-based systems."}], "https://developer.nvidia.com/blog/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/": [{"text": "The article discusses the use of nested data types in ETL workflows with RAPIDS libcudf, a CUDA C++ library for columnar data processing. Nested data types, such as lists and structs, are commonly used in various applications like business intelligence and recommender systems to represent hierarchical relationships within columnar data. The article showcases data processing with nested data types, introduces row operators that enable nested data processing, and explores how data types impact performance. The article includes an example of processing JSON data using nested types in RAPIDS libcudf, demonstrating count aggregation, joining, and sorting steps. Performance data shows that more complex data types increase the runtime of sort-based processing steps, while row count and nesting depth also impact performance. The article concludes by highlighting the powerful and flexible tools provided by RAPIDS libcudf for working with nested data types and provides resources for further exploration and testing.", "text_components": ["Streamline ETL Workflows with Nested Data Types in RAPIDS libcudf\nNested data types are a convenient way to represent hierarchical relationships within columnar data. They are frequently used as part of extract, transform, load (ETL) workloads in business intelligence, recommender systems, cybersecurity, geospatial, and other applications.\nList types can be used to easily attach multiple transactions to a user without creating a new lookup table, for example. Struct types can be used to attach flexible metadata and many key-value pairs within the same column. In web and mobile applications, nested types represent raw JSON objects as elements in a column of data, enabling this data to feed into machine learning (ML) training pipelines. And many data science applications rely on nested types to model, manage, and process complex data inputs.\nIn the RAPIDS suite of accelerated data science libraries, libcudf is the CUDA C++ library for columnar data processing. RAPIDS libcudf is based on the Apache Arrow memory format and supports GPU-accelerated readers, writers, relational algebra functions and column transformations.\nIn addition to flat data types like numbers and strings, libcudf also supports nested data types such as variable-length lists, structs, and arbitrarily nested combinations of list and struct types. In the releases from 23.02 to 23.12, RAPIDS libcudf has expanded support for nested data types in algorithms including aggregations, joins, and sorting.\nThis post showcases data processing with nested data types, introduces the \u201crow operators\u201d that make nested data processing possible, and explores how nested data types impact performance.", "Data processing with nested types\nOne common workflow in database management is monitoring and managing data duplicates. RAPIDS libcudf now includes a ```C++ nested_types``` example that reads in JSON data as a libcudf table, computes the count of each distinct element from the first column, joins the count to the original table, and writes the data back as JSON. The libcudf public API enables data processing applications to work with flat types like numbers or strings and nested types like structs and lists, both with equal ease.\nThe ```C++ nested_types``` example uses the libcudf JSON reader to ingest nested data in a columnar format as a table object. The accelerated JSON reader is also available to C++ developers. JSON provides a human-readable way to create and inspect nested columns. To learn about patterns for using the JSON reader in the Python layer, see GPU-Accelerated JSON Data Processing with RAPIDS.\nThe ```read_json``` function in the ```C++ nested_types``` example accepts a ```filepath``` and returns a ```table_with_metadata``` object:\n```\ncudf::io::table_with_metadata read_json(std::string filepath)\n{\n  auto source_info = cudf::io::source_info(filepath);\n  auto builder     = cudf::io::json_reader_options::builder(source_info).lines(true);\n  auto options     = builder.build();\n  return cudf::io::read_json(options);\n}\n```\nOnce the JSON data is read and parsed into a table object, the first processing step is a count aggregation to track the number of occurrences for each distinct element. The ```count_aggregate``` function in the example populates an aggregation request, executes the aggregate function, and then constructs an output table:\n```\nstd::unique_ptr<cudf::table> count_aggregate(cudf::table_view tbl)\n{\n  // Get count for each key\n  auto keys = cudf::table_view{{tbl.column(0)}};\n  auto val  = cudf::make_numeric_column(cudf::data_type{cudf::type_id::INT32}, keys.num_rows());\n\n  cudf::groupby::groupby grpby_obj(keys);\n  std::vector<cudf::groupby::aggregation_request> requests;\n  requests.emplace_back(cudf::groupby::aggregation_request());\n  auto agg = cudf::make_count_aggregation<cudf::groupby_aggregation>();\n  requests[0].aggregations.push_back(std::move(agg));\n  requests[0].values = *val;\n  auto agg_results   = grpby_obj.aggregate(requests);\n  auto result_key    = std::move(agg_results.first);\n  auto result_val    = std::move(agg_results.second[0].results[0]);\n\n  auto left_cols = result_key->release();\n  left_cols.push_back(std::move(result_val));\n\n  // Join on keys to get\n  return std::make_unique<cudf::table>(std::move(left_cols));\n}\n```\nWith the counts data in hand, the next processing step joins this data to the original table, adding this information to inform count-based filtering and root-cause investigations in downstream analysis. The ```join_count``` function in the ```C++ nested_types``` example accepts two ```table_view``` objects, joins them on their first columns, and then constructs an output table:\n```\nstd::unique_ptr<cudf::table> join_count(cudf::table_view left, cudf::table_view right)\n{\n  auto [left_indices, right_indices] =\n    cudf::inner_join(cudf::table_view{{left.column(0)}}, cudf::table_view{{right.column(0)}});\n  auto new_left  = cudf::gather(left, cudf::device_span<int const>{*left_indices});\n  auto new_right = cudf::gather(right, cudf::device_span<int const>{*right_indices});\n\n  auto left_cols  = new_left->release();\n  auto right_cols = new_right->release();\n  left_cols.push_back(std::move(right_cols[1]));\n\n  return std::make_unique<cudf::table>(std::move(left_cols));\n}\n```\nThe last data processing step sorts the table based on the elements in the first column. Sorting is useful for providing a deterministic ordering that facilitates downstream steps like partitioning and merging. The ```sort_keys``` function in the C++ nested_types example accepts a ```table_view```, computes indices with ```sorted_order```, and then gathers the table based on the ordering:\n```\nstd::unique_ptr<cudf::table> sort_keys(cudf::table_view tbl)\n{\n  auto sort_order = cudf::sorted_order(cudf::table_view{{tbl.column(0)}});\n  return cudf::gather(tbl, *sort_order);\n}\n```\nFinally, the processed data is serialized back to disk using a GPU-accelerated JSON writer, which uses the metadata from ```read_json``` to preserve the nested struct key names from the input data. The ```write_json``` function in the ```C++ nested_types``` example accepts a ```table_view```, ```table_metadata```, and a ```filepath```:\n```\nvoid write_json(cudf::table_view tbl, cudf::io::table_metadata metadata, std::string filepath)\n{\n  auto sink_info = cudf::io::sink_info(filepath);\n  auto builder   = cudf::io::json_writer_options::builder(sink_info, tbl).lines(true);\n  builder.metadata(metadata);\n  auto options = builder.build();\n  cudf::io::write_json(options);\n}\n```\nTaken together, the ```C++ nested_types``` example makes a count of each distinct element in the first column, joins those values to the original table, and then sorts the table on the first column. Note that no part of the code in this example is specific to nested types. In fact, this example is compatible with any supported data type in libcudf, flat or nested, demonstrating the power and flexibility of libcudf nested type support.", "Introducing libcudf row operators\nUnder the hood, libcudf supports equality comparison, inequality comparison, and element hashing using a few key \u201crow operators.\u201d These row operators are reused in algorithms throughput libcudf and enable the separation of data type support from other algorithm details.\nTaking hash-based aggregations as an example, the hashing and equality operators are used when building and probing the hash tables. For sort-based aggregations, the lexicographic operator identifies one element as less than another element and is a key component of any sorting algorithm. The new row operators unlock support for nested types across the relational algebra functions in libcudf.\nFor flat types such as numeric and strings, the row operators process the value and null state for each element. Strings types add more complexity with integer offsets associating a variable number of characters with each element. For struct types, the row operators process the null state for the struct parent as well as the values and null state for each child column.\nVariable-length list types add another layer of complexity, where the row operators account for the hierarchical structure, including null state, list depth, and list length at each nested level. If the hierarchical structure is matching, list operators then consider the value and null state for each leaf element. Of the row operators, hashing and equality are simpler because they can process the data from each element in any order. However, for types that include lists, lexicographic comparison must produce consistent ordering and so requires sequential parsing of null states, hierarchy, and values.\nThe treatment for list types in the libcudf lexicographic operator is inspired by the Dremel encoding algorithm used in the Parquet format. In Dremel encoding, the list columns are represented using three data streams: the definition stream for recording null state and nesting depth, the repetition stream for recording list lengths, and the value stream for recording leaf values. The encoding gives a flat data structure that\u2019s more efficient to process than the recursive variable-length list representation in Arrow.\nOne limitation of Dremel encoding for lists is that the value stream only supports flat types. To extend support for lists containing structs, a preprocessing step replaces a nested struct column with an integer column corresponding to the rank of each struct element. This recursive preprocessing step extends the lexicographic operator type support to include any combination of lists and structs in the data type.", "How data types impact performance\nThe ```C++ nested_types``` example is compatible with any supported data type in libcudf. Comparing performance is easy using the command line interface in the example. The following performance data was collected based on timing implemented in the example and run on NVIDIA DGX H100 hardware.\nThe data type of the column impacts the overall runtime of the example, with more complex data types increasing the runtime of sort-based processing steps (Figure 1). Across a range of data types, the results show 2-5 ms runtime for count aggregation step and 10-25 ms runtime for the inner join step. Both of these steps use hash-based implementations and rely on the hashing and equality row operators.\nHowever, the sorting step shows that runtimes increased to 60-90 ms for variable-sized types that include strings or lists. The sorting step relies on the more complex lexicographic row operator. While hash-based algorithms show relatively consistent runtimes as a function of data type, sort-based algorithms show longer runtimes for variable-sized types.\nBar chart showing the runtime in ms of count_aggregate, join_count and sort_keys steps by data type, with 85% distinct elements and 20 million rows.\nFigure 1. Runtime of ```count_aggregate```, ```join_count```, and ```sort_keys``` steps by data type, with 85% distinct elements and 20 million rows\nRow count and nesting depth also impact the performance of the example, with higher row counts and simpler data types showing the highest data processing throughput. Figure 2 shows ```count_aggregate``` performance from the ```C++ nested_types``` example, where throughput generally increases as the row count increases from 100K to 20 million rows. Data types marked with \u20188\u2019 have eight levels of nesting depth. ```int``` and ```float``` refer to 64-bit types.\nNote that the input data uses structs with one child and lists with length one. The performance data shows primitive types with about 45 GB/s peak throughput, singly nested types with about 30 GB/s peak throughput, and deeply nested types with 10-25 GB/s peak throughput. Struct levels incur less overhead than list levels, and mixed struct/list nesting incurs the largest overhead.\nScatter plot showing data processing throughput in GB/s versus memory size of data in MB.\nFigure 2. Data processing throughput versus memory size of data for the ```count_aggregate``` example function. Each line shows the effect of sweeping from 100K rows to 20 million rows\nFinally, the length of list elements also impacts performance, with longer lengths showing higher throughput due to early exits in comparators. Figure 3 shows the impact of list length on data processing throughput using ```list<int>``` columns with list lengths from 1 to 16. As list length increases, the total integer leaf count and total memory size also increase, and the number of rows and total size of the offsets data are held constant.\nThe data in Figure 3 uses randomly-ordered leaf values, so the comparators will often only need to examine the first element of each list. Performance data collected from lengths increasing from 1 to 16 shows a 7x increase in throughput for the ```count_aggregate``` step and a 4x increase in throughput for the ```sort_keys``` step. The data uses 10 million rows, 64-bit integer leaf elements, 85% distinct leaf values, and list length held constant within each table.\nScatter plot showing data processing throughput in GB/s compared to list length from 1 to 16. As list length increases, the data shows data processing throughput also increases.\nFigure 3. Data processing throughput of the ```count_aggregate```, ```join_count```, and ```sort_keys``` steps for singly nested list types with varying list lengths", "Summary\nRAPIDS libcudf provides powerful, flexible, and accelerated tools for working with nested data types. Relational algebra algorithms such as aggregations, joins, and sorting are tuned and optimized for any supported nested data type, even deeply nested and mixed list and struct nested data types.\nBuild and run a few examples to get started with RAPIDS libcudf. For more information about CUDA-accelerated dataframes, see the cuDF documentation and the rapidsai/cudf GitHub repo. For easier testing and deployment, RAPIDS Docker containers are also available for releases and nightly builds. If you\u2019re already using cuDF, you can run the new ```C++ nested_types``` example by visiting rapidsai/cudf/tree/HEAD/cpp/examples/nested_types on GitHub.", "Acknowledgments\nThank you Devavret Makkar, Jake Hemstad, and the rest of the RAPIDS team for contributing to this work."], "document_title": "Streamline ETL Workflows with Nested Data Types in RAPIDS libcudf", "document_url": "https://developer.nvidia.com/blog/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/", "document_date": "2023-12-15T21:16:55", "document_date_modified": "2024-01-22T21:35:40", "document_full_text": "Streamline ETL Workflows with Nested Data Types in RAPIDS libcudf\nNested data types are a convenient way to represent hierarchical relationships within columnar data. They are frequently used as part of extract, transform, load (ETL) workloads in business intelligence, recommender systems, cybersecurity, geospatial, and other applications.\nList types can be used to easily attach multiple transactions to a user without creating a new lookup table, for example. Struct types can be used to attach flexible metadata and many key-value pairs within the same column. In web and mobile applications, nested types represent raw JSON objects as elements in a column of data, enabling this data to feed into machine learning (ML) training pipelines. And many data science applications rely on nested types to model, manage, and process complex data inputs.\nIn the RAPIDS suite of accelerated data science libraries, libcudf is the CUDA C++ library for columnar data processing. RAPIDS libcudf is based on the Apache Arrow memory format and supports GPU-accelerated readers, writers, relational algebra functions and column transformations.\nIn addition to flat data types like numbers and strings, libcudf also supports nested data types such as variable-length lists, structs, and arbitrarily nested combinations of list and struct types. In the releases from 23.02 to 23.12, RAPIDS libcudf has expanded support for nested data types in algorithms including aggregations, joins, and sorting.\nThis post showcases data processing with nested data types, introduces the \u201crow operators\u201d that make nested data processing possible, and explores how nested data types impact performance.\nData processing with nested types\nOne common workflow in database management is monitoring and managing data duplicates. RAPIDS libcudf now includes a ```C++ nested_types``` example that reads in JSON data as a libcudf table, computes the count of each distinct element from the first column, joins the count to the original table, and writes the data back as JSON. The libcudf public API enables data processing applications to work with flat types like numbers or strings and nested types like structs and lists, both with equal ease.\nThe ```C++ nested_types``` example uses the libcudf JSON reader to ingest nested data in a columnar format as a table object. The accelerated JSON reader is also available to C++ developers. JSON provides a human-readable way to create and inspect nested columns. To learn about patterns for using the JSON reader in the Python layer, see GPU-Accelerated JSON Data Processing with RAPIDS.\nThe ```read_json``` function in the ```C++ nested_types``` example accepts a ```filepath``` and returns a ```table_with_metadata``` object:\n```\ncudf::io::table_with_metadata read_json(std::string filepath)\n{\n  auto source_info = cudf::io::source_info(filepath);\n  auto builder     = cudf::io::json_reader_options::builder(source_info).lines(true);\n  auto options     = builder.build();\n  return cudf::io::read_json(options);\n}\n```\nOnce the JSON data is read and parsed into a table object, the first processing step is a count aggregation to track the number of occurrences for each distinct element. The ```count_aggregate``` function in the example populates an aggregation request, executes the aggregate function, and then constructs an output table:\n```\nstd::unique_ptr<cudf::table> count_aggregate(cudf::table_view tbl)\n{\n  // Get count for each key\n  auto keys = cudf::table_view{{tbl.column(0)}};\n  auto val  = cudf::make_numeric_column(cudf::data_type{cudf::type_id::INT32}, keys.num_rows());\n\n  cudf::groupby::groupby grpby_obj(keys);\n  std::vector<cudf::groupby::aggregation_request> requests;\n  requests.emplace_back(cudf::groupby::aggregation_request());\n  auto agg = cudf::make_count_aggregation<cudf::groupby_aggregation>();\n  requests[0].aggregations.push_back(std::move(agg));\n  requests[0].values = *val;\n  auto agg_results   = grpby_obj.aggregate(requests);\n  auto result_key    = std::move(agg_results.first);\n  auto result_val    = std::move(agg_results.second[0].results[0]);\n\n  auto left_cols = result_key->release();\n  left_cols.push_back(std::move(result_val));\n\n  // Join on keys to get\n  return std::make_unique<cudf::table>(std::move(left_cols));\n}\n```\nWith the counts data in hand, the next processing step joins this data to the original table, adding this information to inform count-based filtering and root-cause investigations in downstream analysis. The ```join_count``` function in the ```C++ nested_types``` example accepts two ```table_view``` objects, joins them on their first columns, and then constructs an output table:\n```\nstd::unique_ptr<cudf::table> join_count(cudf::table_view left, cudf::table_view right)\n{\n  auto [left_indices, right_indices] =\n    cudf::inner_join(cudf::table_view{{left.column(0)}}, cudf::table_view{{right.column(0)}});\n  auto new_left  = cudf::gather(left, cudf::device_span<int const>{*left_indices});\n  auto new_right = cudf::gather(right, cudf::device_span<int const>{*right_indices});\n\n  auto left_cols  = new_left->release();\n  auto right_cols = new_right->release();\n  left_cols.push_back(std::move(right_cols[1]));\n\n  return std::make_unique<cudf::table>(std::move(left_cols));\n}\n```\nThe last data processing step sorts the table based on the elements in the first column. Sorting is useful for providing a deterministic ordering that facilitates downstream steps like partitioning and merging. The ```sort_keys``` function in the C++ nested_types example accepts a ```table_view```, computes indices with ```sorted_order```, and then gathers the table based on the ordering:\n```\nstd::unique_ptr<cudf::table> sort_keys(cudf::table_view tbl)\n{\n  auto sort_order = cudf::sorted_order(cudf::table_view{{tbl.column(0)}});\n  return cudf::gather(tbl, *sort_order);\n}\n```\nFinally, the processed data is serialized back to disk using a GPU-accelerated JSON writer, which uses the metadata from ```read_json``` to preserve the nested struct key names from the input data. The ```write_json``` function in the ```C++ nested_types``` example accepts a ```table_view```, ```table_metadata```, and a ```filepath```:\n```\nvoid write_json(cudf::table_view tbl, cudf::io::table_metadata metadata, std::string filepath)\n{\n  auto sink_info = cudf::io::sink_info(filepath);\n  auto builder   = cudf::io::json_writer_options::builder(sink_info, tbl).lines(true);\n  builder.metadata(metadata);\n  auto options = builder.build();\n  cudf::io::write_json(options);\n}\n```\nTaken together, the ```C++ nested_types``` example makes a count of each distinct element in the first column, joins those values to the original table, and then sorts the table on the first column. Note that no part of the code in this example is specific to nested types. In fact, this example is compatible with any supported data type in libcudf, flat or nested, demonstrating the power and flexibility of libcudf nested type support.\nIntroducing libcudf row operators\nUnder the hood, libcudf supports equality comparison, inequality comparison, and element hashing using a few key \u201crow operators.\u201d These row operators are reused in algorithms throughput libcudf and enable the separation of data type support from other algorithm details.\nTaking hash-based aggregations as an example, the hashing and equality operators are used when building and probing the hash tables. For sort-based aggregations, the lexicographic operator identifies one element as less than another element and is a key component of any sorting algorithm. The new row operators unlock support for nested types across the relational algebra functions in libcudf.\nFor flat types such as numeric and strings, the row operators process the value and null state for each element. Strings types add more complexity with integer offsets associating a variable number of characters with each element. For struct types, the row operators process the null state for the struct parent as well as the values and null state for each child column.\nVariable-length list types add another layer of complexity, where the row operators account for the hierarchical structure, including null state, list depth, and list length at each nested level. If the hierarchical structure is matching, list operators then consider the value and null state for each leaf element. Of the row operators, hashing and equality are simpler because they can process the data from each element in any order. However, for types that include lists, lexicographic comparison must produce consistent ordering and so requires sequential parsing of null states, hierarchy, and values.\nThe treatment for list types in the libcudf lexicographic operator is inspired by the Dremel encoding algorithm used in the Parquet format. In Dremel encoding, the list columns are represented using three data streams: the definition stream for recording null state and nesting depth, the repetition stream for recording list lengths, and the value stream for recording leaf values. The encoding gives a flat data structure that\u2019s more efficient to process than the recursive variable-length list representation in Arrow.\nOne limitation of Dremel encoding for lists is that the value stream only supports flat types. To extend support for lists containing structs, a preprocessing step replaces a nested struct column with an integer column corresponding to the rank of each struct element. This recursive preprocessing step extends the lexicographic operator type support to include any combination of lists and structs in the data type.\nHow data types impact performance\nThe ```C++ nested_types``` example is compatible with any supported data type in libcudf. Comparing performance is easy using the command line interface in the example. The following performance data was collected based on timing implemented in the example and run on NVIDIA DGX H100 hardware.\nThe data type of the column impacts the overall runtime of the example, with more complex data types increasing the runtime of sort-based processing steps (Figure 1). Across a range of data types, the results show 2-5 ms runtime for count aggregation step and 10-25 ms runtime for the inner join step. Both of these steps use hash-based implementations and rely on the hashing and equality row operators.\nHowever, the sorting step shows that runtimes increased to 60-90 ms for variable-sized types that include strings or lists. The sorting step relies on the more complex lexicographic row operator. While hash-based algorithms show relatively consistent runtimes as a function of data type, sort-based algorithms show longer runtimes for variable-sized types.\nBar chart showing the runtime in ms of count_aggregate, join_count and sort_keys steps by data type, with 85% distinct elements and 20 million rows.\nFigure 1. Runtime of ```count_aggregate```, ```join_count```, and ```sort_keys``` steps by data type, with 85% distinct elements and 20 million rows\nRow count and nesting depth also impact the performance of the example, with higher row counts and simpler data types showing the highest data processing throughput. Figure 2 shows ```count_aggregate``` performance from the ```C++ nested_types``` example, where throughput generally increases as the row count increases from 100K to 20 million rows. Data types marked with \u20188\u2019 have eight levels of nesting depth. ```int``` and ```float``` refer to 64-bit types.\nNote that the input data uses structs with one child and lists with length one. The performance data shows primitive types with about 45 GB/s peak throughput, singly nested types with about 30 GB/s peak throughput, and deeply nested types with 10-25 GB/s peak throughput. Struct levels incur less overhead than list levels, and mixed struct/list nesting incurs the largest overhead.\nScatter plot showing data processing throughput in GB/s versus memory size of data in MB.\nFigure 2. Data processing throughput versus memory size of data for the ```count_aggregate``` example function. Each line shows the effect of sweeping from 100K rows to 20 million rows\nFinally, the length of list elements also impacts performance, with longer lengths showing higher throughput due to early exits in comparators. Figure 3 shows the impact of list length on data processing throughput using ```list<int>``` columns with list lengths from 1 to 16. As list length increases, the total integer leaf count and total memory size also increase, and the number of rows and total size of the offsets data are held constant.\nThe data in Figure 3 uses randomly-ordered leaf values, so the comparators will often only need to examine the first element of each list. Performance data collected from lengths increasing from 1 to 16 shows a 7x increase in throughput for the ```count_aggregate``` step and a 4x increase in throughput for the ```sort_keys``` step. The data uses 10 million rows, 64-bit integer leaf elements, 85% distinct leaf values, and list length held constant within each table.\nScatter plot showing data processing throughput in GB/s compared to list length from 1 to 16. As list length increases, the data shows data processing throughput also increases.\nFigure 3. Data processing throughput of the ```count_aggregate```, ```join_count```, and ```sort_keys``` steps for singly nested list types with varying list lengths\nSummary\nRAPIDS libcudf provides powerful, flexible, and accelerated tools for working with nested data types. Relational algebra algorithms such as aggregations, joins, and sorting are tuned and optimized for any supported nested data type, even deeply nested and mixed list and struct nested data types.\nBuild and run a few examples to get started with RAPIDS libcudf. For more information about CUDA-accelerated dataframes, see the cuDF documentation and the rapidsai/cudf GitHub repo. For easier testing and deployment, RAPIDS Docker containers are also available for releases and nightly builds. If you\u2019re already using cuDF, you can run the new ```C++ nested_types``` example by visiting rapidsai/cudf/tree/HEAD/cpp/examples/nested_types on GitHub.\nAcknowledgments\nThank you Devavret Makkar, Jake Hemstad, and the rest of the RAPIDS team for contributing to this work."}], "https://developer.nvidia.com/blog/advanced-api-performance-swap-chains/": [{"text": "Swap chains are crucial for rendering data output to a screen, consisting of buffers that are rendered to in rotation. Optimizing swap chains is often overlooked but can improve performance and latency. Best practices for NVIDIA GPUs include using flip-mode swap chains, setting flags for fullscreen and tearing support, controlling latency and buffer count, and using about 1-2 more swap chain buffers than the maximum frames intended to be queued. It is recommended to use ```IDXGISwapChain2::SetMaximumFrameLatency``` to set the desired latency and to ensure the most recent frame is displayed. Not recommended practices include forgetting the per-swap chain limit of three queued frames and failing to call ```ResizeBuffers``` after switching to true immediate independent flip mode. By following these recommendations and considerations, developers can ensure optimum swap chain performance and achieve high and consistent frame rates in their applications. Special thanks are given to Cody Robson, Kumaresan Gnanasekaran, Adrian Muntianu, and Meenal Nachnani for their advice and assistance.", "text_components": ["Advanced API Performance: Swap Chains\nSwap chains are an integral part of how you get rendering data output to a screen. They usually consist of some group of output-ready buffers, each of which can be rendered to one at a time in rotation. In parallel with rendering to one of a swap chain\u2019s buffers, some other buffer in the swap chain is generally read from for display output.\nThis post covers best practices when working with swap chains on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.\nIt\u2019s common to focus on the more frequently optimized parts of the rendering pipeline when looking to improve rendering performance. However, swap chains are often overlooked, leaving potential performance and latency on the table.\nThe following suggestions and considerations should provide more insight into the best ways to ensure optimum swap chain performance.", "Recommended\nUse flip-mode swap chains. This is especially important for leveraging multiplane overlay support, which provides fullscreen-like performance and latency when running in windowed mode.\nUse ```SetFullScreenState(TRUE)```, a (borderless) fullscreen window, and a non-windowed flip model swap chain to switch to true immediate independent flip mode.\nThis is the only mode that enables unlimited framerates with tearing for Direct 3D 12 when calling ```Present(0,0)```.\nFor proper unlimited frame rate support for displays that support variable refresh rates, you must also use the ```DXGI_SWAP_CHAIN_FLAG_ALLOW_TEARING``` swap chain flag, along with the ```DXGI_PRESENT_ALLOW_TEARING Present``` flag\nUse the ```DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH``` flag consciously.\nThe flag is not necessary to achieve unlimited frame rates (see earlier note) if your window size matches the current screen resolution\nIf this flag is set, trying to change the resolution using ResizeTarget before calling ```SetFullScreenState(TRUE)``` works fine and the framerate will be unlimited\nIf this flag is not set, trying to change resolution using ```ResizeTarget``` before calling ```SetFullScreenState(TRUE)``` results in no change of display resolution. Your target is stretched to the current resolution and the frame rate is limited.\nIf not in the fullscreen state (true immediate independent flip mode), control your latency and buffer count in your swap chain carefully for the desired frame rate and latency.\nUse ```IDXGISwapChain2::SetMaximumFrameLatency(MaxLatency)``` to set the desired latency, where ```MaxLatency``` is some number of frames (counted by queued ```Present``` calls).\nFor this to work, you must create your swap chain with the ```DXGI_SWAP_CHAIN_FLAG_FRAME_LATENCY_WAITABLE_OBJECT``` flag set.\nDXGI starts to block in ```Present``` after you have the ```MaxLatency``` number of present calls queued.\nIn this windowed state, a sync interval of 0 in a ```Present``` call ensures that the frame being presented is the most recent frame available for the next time desktop composition happens (combining the windowed rendered frame with the rest of the desktop), and all previously completed frames are discarded in favor of this latest one. No rendered frame is displayed until composition happens, which happens at ```VSYNC``` time. This latest finished frame is what is displayed\nUse about 1-2 more swap chain buffers than the maximum number of frames that you intend to queue (in terms of command allocators, dynamic data, and the associated frame fences). Set the maximum frame latency to this number of swap chain buffers through ```IDXGISwapChain2::SetMaximumFrameLatency(MaxLatency)```.\nThis ensures that you can limit queued frames and latency explicitly and optimally from within the application logic rather than relying on the OS to block or have it block at an unexpected time.", "Not recommended\nForgetting that, by default, there\u2019s a per\u2013swap chain limit of three queued frames before DXGI starts to block in ```Present```. This means that it blocks on the fourth Present call if there are currently three ```Present``` calls queued.\nSet the ```DXGI_SWAP_CHAIN_FLAG_FRAME_LATENCY_WAITABLE_OBJECT``` flag on swap chain creation and use ```IDXGISwapChain2::SetMaximumFrameLatency``` to modify this default value\nForgetting to call ```ResizeBuffers``` after you have switched to true immediate independent flip mode using ```SetFullScreenState(TRUE)```.", "Acknowledgments\nThanks to Cody Robson, Kumaresan Gnanasekaran, Adrian Muntianu, and Meenal Nachnani for their advice and assistance."], "document_title": "Advanced API Performance: Swap Chains", "document_url": "https://developer.nvidia.com/blog/advanced-api-performance-swap-chains/", "document_date": "2023-12-15T17:00:00", "document_date_modified": "2023-12-11T20:20:45", "document_full_text": "Advanced API Performance: Swap Chains\nSwap chains are an integral part of how you get rendering data output to a screen. They usually consist of some group of output-ready buffers, each of which can be rendered to one at a time in rotation. In parallel with rendering to one of a swap chain\u2019s buffers, some other buffer in the swap chain is generally read from for display output.\nThis post covers best practices when working with swap chains on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.\nIt\u2019s common to focus on the more frequently optimized parts of the rendering pipeline when looking to improve rendering performance. However, swap chains are often overlooked, leaving potential performance and latency on the table.\nThe following suggestions and considerations should provide more insight into the best ways to ensure optimum swap chain performance.\nRecommended\nUse flip-mode swap chains. This is especially important for leveraging multiplane overlay support, which provides fullscreen-like performance and latency when running in windowed mode.\nUse ```SetFullScreenState(TRUE)```, a (borderless) fullscreen window, and a non-windowed flip model swap chain to switch to true immediate independent flip mode.\nThis is the only mode that enables unlimited framerates with tearing for Direct 3D 12 when calling ```Present(0,0)```.\nFor proper unlimited frame rate support for displays that support variable refresh rates, you must also use the ```DXGI_SWAP_CHAIN_FLAG_ALLOW_TEARING``` swap chain flag, along with the ```DXGI_PRESENT_ALLOW_TEARING Present``` flag\nUse the ```DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH``` flag consciously.\nThe flag is not necessary to achieve unlimited frame rates (see earlier note) if your window size matches the current screen resolution\nIf this flag is set, trying to change the resolution using ResizeTarget before calling ```SetFullScreenState(TRUE)``` works fine and the framerate will be unlimited\nIf this flag is not set, trying to change resolution using ```ResizeTarget``` before calling ```SetFullScreenState(TRUE)``` results in no change of display resolution. Your target is stretched to the current resolution and the frame rate is limited.\nIf not in the fullscreen state (true immediate independent flip mode), control your latency and buffer count in your swap chain carefully for the desired frame rate and latency.\nUse ```IDXGISwapChain2::SetMaximumFrameLatency(MaxLatency)``` to set the desired latency, where ```MaxLatency``` is some number of frames (counted by queued ```Present``` calls).\nFor this to work, you must create your swap chain with the ```DXGI_SWAP_CHAIN_FLAG_FRAME_LATENCY_WAITABLE_OBJECT``` flag set.\nDXGI starts to block in ```Present``` after you have the ```MaxLatency``` number of present calls queued.\nIn this windowed state, a sync interval of 0 in a ```Present``` call ensures that the frame being presented is the most recent frame available for the next time desktop composition happens (combining the windowed rendered frame with the rest of the desktop), and all previously completed frames are discarded in favor of this latest one. No rendered frame is displayed until composition happens, which happens at ```VSYNC``` time. This latest finished frame is what is displayed\nUse about 1-2 more swap chain buffers than the maximum number of frames that you intend to queue (in terms of command allocators, dynamic data, and the associated frame fences). Set the maximum frame latency to this number of swap chain buffers through ```IDXGISwapChain2::SetMaximumFrameLatency(MaxLatency)```.\nThis ensures that you can limit queued frames and latency explicitly and optimally from within the application logic rather than relying on the OS to block or have it block at an unexpected time.\nNot recommended\nForgetting that, by default, there\u2019s a per\u2013swap chain limit of three queued frames before DXGI starts to block in ```Present```. This means that it blocks on the fourth Present call if there are currently three ```Present``` calls queued.\nSet the ```DXGI_SWAP_CHAIN_FLAG_FRAME_LATENCY_WAITABLE_OBJECT``` flag on swap chain creation and use ```IDXGISwapChain2::SetMaximumFrameLatency``` to modify this default value\nForgetting to call ```ResizeBuffers``` after you have switched to true immediate independent flip mode using ```SetFullScreenState(TRUE)```.\nAcknowledgments\nThanks to Cody Robson, Kumaresan Gnanasekaran, Adrian Muntianu, and Meenal Nachnani for their advice and assistance."}], "https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/": [{"text": "NVIDIA has released the NVIDIA TensorRT-LLM, optimized for the NVIDIA H100 Tensor Core GPU, allowing models like Llama 2 70B to execute with accelerated FP8 operations while maintaining accuracy. The H100 GPU outperforms AMD's MI300X chip when properly benchmarked, being 2x faster. A single NVIDIA DGX H100 server with eight H100 GPUs can process a Llama 2 70B model with various input and output token lengths. By setting a fixed response time, cloud services can optimize both response time and data center throughput, processing more inferences per second. Industry benchmarks like MLPerf measure performance using fixed response time metrics. NVIDIA's CUDA ecosystem enables quick and continuous optimization of software, with ongoing updates to improve AI performance. The article provides details on how to reproduce the AI inference results, comparing the performance of the DGX H100 with the AMD MI300X chip based on the configurations provided by AMD.", "text_components": ["Achieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM\nBest-in-class AI performance requires an efficient parallel computing architecture, a productive tool stack, and deeply optimized algorithms. NVIDIA released the open-source NVIDIA TensorRT-LLM, which includes the latest kernel optimizations for the NVIDIA Hopper architecture at the heart of the NVIDIA H100 Tensor Core GPU. These optimizations enable models like Llama 2 70B to execute using accelerated FP8 operations on H100 GPUs while maintaining inference accuracy.\nAt a recent launch event, AMD talked about the inference performance of the H100 GPU compared to that of its MI300X chip. The results shared did not use optimized software, and the H100, if benchmarked properly, is 2x faster.\nThe following is the actual measured performance of a single NVIDIA DGX H100 server with eight NVIDIA H100 GPUs on the Llama 2 70B model. This includes results for both \u201cBatch-1\u201d where an inference request is processed one at a time, as well as results using fixed response-time processing.\nChart of Llama 2 70B server inference performance in queries.\nFigure 1. Llama 2 70B server inference performance in queries per second with 2,048 input tokens and 128 output tokens for \u201cBatch 1\u201d and various fixed response time settings\nAMD\u2019s implied claims for H100 are measured based on the configuration taken from AMD launch presentation footnote #MI300-38. Using vLLM v.02.2.2 inference software with NVIDIA DGX H100 system, Llama 2 70B query with an input sequence length of 2,048 and output sequence length of 128. They claimed relative performance compared to DGX H100 with 8x GPU MI300X system.\nFor NVIDIA measured data, DGX H100 with 8x NVIDIA H100 Tensor Core GPUs with 80 GB HBM3 with publicly available NVIDIA TensorRT-LLM, v0.5.0 for batch 1 and v0.6.1 for latency threshold measurements. Workload details same as footnote #MI300-38.\nDGX H100 can process a single inference in 1.7 seconds using a batch size of one\u2014in other words, one inference request at a time. A batch size of one results in the fastest possible response time for serving a model. To optimize both response time and data center throughput, cloud services set a fixed response time for a particular service. This enables them to combine multiple inference requests into larger \u201cbatches\u201d and increase the overall inferences per second of the server. Industry-standard benchmarks like MLPerf also measure performance with this fixed response time metric.\nSmall tradeoffs in response time can yield x-factors in the number of inference requests that a server can process in real time. Using a fixed 2.5-second response time budget, an 8-GPU DGX H100 server can process over five Llama 2 70B inferences per second compared to less than one per second with batch one.\nAI is moving fast and the NVIDIA CUDA ecosystem enables us to optimize our stack quickly and continuously. We look forward to continuing to improve AI performance with every update of our software, so be sure to check out our performance pages and GitHub sites for the latest.", "How to reproduce these AI inference results\nDGX H100 AMD Footnote was measured by NVIDIA in vLLM based on the configurations provided by AMD in their footnotes using vLLM and its provided benchmarking script with the following command lines:\n```\n$ python benchmarks/benchmark_latency.py --model \"meta-llama/Llama-2-70b-hf\" --input-len 2048 --output-len 128 --batch-size 1 -tp 8\n```\nMI300X 8-Chip System is the inferred data based on AMD\u2019s claimed speedup over DGX H100 AMD Footnote measured vLLM results.\nDGX H100 Measured was measured by NVIDIA using publicly available versions of TensorRT-LLM available on GitHub and using the command lines outlined in the TensorRT-LLM benchmarking guide for Llama 2.\n```\n// Build TensorRT optimized Llama-2-70b for H100 fp8 tensorcore\n$ python examples/llama/build.py --remove_input_padding --enable_context_fmha --parallel_build --output_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --dtype float16 --use_gpt_attention_plugin float16 --world_size 8 --tp_size 8 --pp_size 1 --max_batch_size 14 --max_input_len 2048 --max_output_len 128 --enable_fp8 --fp8_kv_cache --strongly_typed --n_head 64 --n_kv_head 8 --n_embd 8192 --inter_size 28672 --vocab_size 32000 --n_positions 4096 --hidden_act silu --ffn_dim_multiplier 1.3 --multiple_of 4096 --n_layer 80\n\n\n// Benchmark Llama-70B\n$ mpirun -n 8 --allow-run-as-root --oversubscribe ./cpp/build/benchmarks/gptSessionBenchmark --model llama_70b --engine_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --warm_up 1 --batch_size 14 --duration 0 --num_runs 5 --input_output_len 2048,1;2048,128\n```"], "document_title": "Achieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM", "document_url": "https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/", "document_date": "2023-12-14T19:59:00", "document_date_modified": "2023-12-14T23:33:04", "document_full_text": "Achieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM\nBest-in-class AI performance requires an efficient parallel computing architecture, a productive tool stack, and deeply optimized algorithms. NVIDIA released the open-source NVIDIA TensorRT-LLM, which includes the latest kernel optimizations for the NVIDIA Hopper architecture at the heart of the NVIDIA H100 Tensor Core GPU. These optimizations enable models like Llama 2 70B to execute using accelerated FP8 operations on H100 GPUs while maintaining inference accuracy.\nAt a recent launch event, AMD talked about the inference performance of the H100 GPU compared to that of its MI300X chip. The results shared did not use optimized software, and the H100, if benchmarked properly, is 2x faster.\nThe following is the actual measured performance of a single NVIDIA DGX H100 server with eight NVIDIA H100 GPUs on the Llama 2 70B model. This includes results for both \u201cBatch-1\u201d where an inference request is processed one at a time, as well as results using fixed response-time processing.\nChart of Llama 2 70B server inference performance in queries.\nFigure 1. Llama 2 70B server inference performance in queries per second with 2,048 input tokens and 128 output tokens for \u201cBatch 1\u201d and various fixed response time settings\nAMD\u2019s implied claims for H100 are measured based on the configuration taken from AMD launch presentation footnote #MI300-38. Using vLLM v.02.2.2 inference software with NVIDIA DGX H100 system, Llama 2 70B query with an input sequence length of 2,048 and output sequence length of 128. They claimed relative performance compared to DGX H100 with 8x GPU MI300X system.\nFor NVIDIA measured data, DGX H100 with 8x NVIDIA H100 Tensor Core GPUs with 80 GB HBM3 with publicly available NVIDIA TensorRT-LLM, v0.5.0 for batch 1 and v0.6.1 for latency threshold measurements. Workload details same as footnote #MI300-38.\nDGX H100 can process a single inference in 1.7 seconds using a batch size of one\u2014in other words, one inference request at a time. A batch size of one results in the fastest possible response time for serving a model. To optimize both response time and data center throughput, cloud services set a fixed response time for a particular service. This enables them to combine multiple inference requests into larger \u201cbatches\u201d and increase the overall inferences per second of the server. Industry-standard benchmarks like MLPerf also measure performance with this fixed response time metric.\nSmall tradeoffs in response time can yield x-factors in the number of inference requests that a server can process in real time. Using a fixed 2.5-second response time budget, an 8-GPU DGX H100 server can process over five Llama 2 70B inferences per second compared to less than one per second with batch one.\nAI is moving fast and the NVIDIA CUDA ecosystem enables us to optimize our stack quickly and continuously. We look forward to continuing to improve AI performance with every update of our software, so be sure to check out our performance pages and GitHub sites for the latest.\nHow to reproduce these AI inference results\nDGX H100 AMD Footnote was measured by NVIDIA in vLLM based on the configurations provided by AMD in their footnotes using vLLM and its provided benchmarking script with the following command lines:\n```\n$ python benchmarks/benchmark_latency.py --model \"meta-llama/Llama-2-70b-hf\" --input-len 2048 --output-len 128 --batch-size 1 -tp 8\n```\nMI300X 8-Chip System is the inferred data based on AMD\u2019s claimed speedup over DGX H100 AMD Footnote measured vLLM results.\nDGX H100 Measured was measured by NVIDIA using publicly available versions of TensorRT-LLM available on GitHub and using the command lines outlined in the TensorRT-LLM benchmarking guide for Llama 2.\n```\n// Build TensorRT optimized Llama-2-70b for H100 fp8 tensorcore\n$ python examples/llama/build.py --remove_input_padding --enable_context_fmha --parallel_build --output_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --dtype float16 --use_gpt_attention_plugin float16 --world_size 8 --tp_size 8 --pp_size 1 --max_batch_size 14 --max_input_len 2048 --max_output_len 128 --enable_fp8 --fp8_kv_cache --strongly_typed --n_head 64 --n_kv_head 8 --n_embd 8192 --inter_size 28672 --vocab_size 32000 --n_positions 4096 --hidden_act silu --ffn_dim_multiplier 1.3 --multiple_of 4096 --n_layer 80\n\n\n// Benchmark Llama-70B\n$ mpirun -n 8 --allow-run-as-root --oversubscribe ./cpp/build/benchmarks/gptSessionBenchmark --model llama_70b --engine_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --warm_up 1 --batch_size 14 --duration 0 --num_runs 5 --input_output_len 2048,1;2048,128\n```"}], "https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/": [{"text": "The article discusses the advancements in generative AI research related to denoising diffusion-based models for generating images. The focus is on understanding the core mechanisms underlying these models to improve quality and computational efficiency. The process involves gradually revealing images hidden under noise by iteratively feeding them through a denoiser network. The article explains the theory behind denoising diffusion, design choices related to sampling, and training the denoiser network. It emphasizes the importance of straightening flow lines, careful stepping at different noise levels, and using higher-order solvers for accurate steps. The article also addresses issues such as standardizing input scales, predicting noise vs. the clean image, and equalizing gradient feedback magnitudes across noise levels. The results of incorporating these improvements have led to advancements in image generation quality, demonstrated by achieving world record FID metrics in competitive categories. The article provides a code snippet implementing these improvements and directs readers to the official implementation on GitHub for further study and experimentation.", "text_components": ["Generative AI Research Spotlight: Demystifying Diffusion-Based Models\nWith Internet-scale data, the computational demands of AI-generated content have grown significantly, with data centers running full steam for weeks or months to train a single model\u2014not to mention the high inference costs in generation, often offered as a service. In this context, suboptimal algorithmic design that sacrifices performance is an expensive mistake.\nMuch of the recent progress in AI-generated image, video, and audio content has been driven by denoising diffusion \u2014a technique that iteratively shapes random noise into novel samples of the data. A recent research paper published by our team, Elucidating the Design Space of Diffusion Based Models, recipient of the Outstanding Paper Award at NeurIPS 2022, identifies the simple core mechanisms underlying the seemingly complicated approaches in the literature. Starting with this clear view of the fundamentals, we then find the state-of-the-art practices for quality and computational efficiency.", "Denoising diffusion\nDenoising is the act of removing, for example, sensor noise from images or hiss from audio recordings. This post will use images as the running example, but the process applies to many other domains as well. This task is excellently suited for convolutional neural networks.\nWhat does this have to do with generating novel images? Imagine there is a large amount of noise on an image. Indeed, so much that the original image is lost. Could a denoiser be used to reveal some random image that could be hiding under all that noise? Surprisingly, the answer is yes.\nThis is the simple essence of denoising diffusion: first draw a random image of pure white noise, and then chip away at the noise level\u2014say, 2% at a time\u2014by repeatedly feeding it to a neural denoiser. Gradually, a random clean image emerges from underneath the noise. The distribution of generated content (pictures of cats and dogs? audio waveforms of spoken English phrases? video clips of driving?) is determined by the dataset that the denoiser network was trained with.\nAn image sequence where each consecutive image is produced by denoising the previous one. The first image is pure white noise, and the last image is the generation result.\nFigure 1. Denoising diffusion reveals novel images from pure noise through repeated denoising The code below is a first guess of how to implement this idea, assuming a neural network function ```denoise``` is available.\n```\n# start with an image of pure large-magnitude noise\nsigma = 80\t\t# initial noise level\nx = sigma * torch.randn(img_shape)\n\nfor step in range(256):\n    # keep 98% of current noisy image, and mix in 2% of denoising\n\tx = 0.98 * x + 0.02 * denoise(x, sigma)\n\t\n    # keep track of current noise level\n    sigma *= 0.98\n```\nIf you\u2019ve ever looked at code bases or scientific papers in the field, filled with pages of equations, you might be surprised to learn that this near-trivial piece of code is actually a theoretically valid implementation of something called a probability flow ordinary differential equation solver. While this snippet is hardly optimal, it embodies surprisingly many of the key good practices explained in the paper. The team\u2019s top-of-the-line final sampler is essentially just a few more lines.\nWhat about that function ```denoise```? At its core, it\u2019s surprisingly straightforward as well: the denoiser must output the blurry average of all possible clean images that could have been hiding under the noise. The desired output at various noise levels might look like the examples in Figure 2.\nThree examples of denoising at different noise levels.\nFigure 2. Examples of ideal denoiser outputs at different noise levels. At high noise levels, the image details are uncertain, and remain blurry in the output Training a denoiser network (typically a U-Net) with basic loss\u2014the mean square error between its output and the clean target\u2014achieves precisely this result. Fancier losses that aim for sharper output are in fact harmful, and violate the theory. Keep in mind that, even if the task is conceptually simple, most existing denoisers are not trained for it specifically.\nMuch of the apparent mathematical complexity in the literature arises from justifying why this works. The theory can be built up from various formalisms, the most popular two being Markov chains and stochastic differential equations. While each approach boils down to a denoising loop that uses a trained denoiser, they open up a vast and confusing space of different practical implementations, and a minefield of opportunities to make poor choices.\nThe paper peels back the layers of mathematical complexity, directly exposing the tangible design choices in a standardized framework where they are easy to analyze.\nThis post presents the team\u2019s key findings and intuitions through visualizations and code. We\u2019ll cover three topics:\nAn intuitive overview of the theory behind denoising diffusion\nDesign choices related to sampling (generating images when you already have a trained denoiser)\nDesign choices when training that denoiser", "What makes diffusion work?\nTo begin, this section takes a step back to the basics and builds the theory that justifies this straightforward piece of code. We find most insight in the differential equation framework, originally presented in Score-Based Generative Modeling through Stochastic Differential Equations. While the equations and mathematical concepts might look intimidating, they are not crucial for understanding the gist. It is useful to mention them occasionally to highlight that they are often just a different language for describing concrete things done in code.\nImagine an RGB image x of shape, say, ```[3, 64, 64]``` from the dataset. Begin by considering the easy direction of destroying the image by gradually adding noise onto it. (Of course, this is the opposite of the end goal.)\n```\nfor step in range(1000):\n\tx = x + 0.1 * torch.randn_like(x)\n```\nThis is in fact (suitably squinting) a stochastic differential equation (SDE) solver corresponding to a simple SDE \\text{d}\\mathbf{x} = \\text{d}\\omega_t\n. It says that the change in image x over a short time step is random white noise. Here, solving simply means simulating a specific random numerical realization of the process described by the SDE.\nA nice thing about differential equations is that they have a fruitful geometric interpretation. You can visualize this process as the image taking a random walk (the famous Brownian motion, or Wiener process) in the pixel value space. If you consider x above to be just a single number (a \u201csingle pixel image\u201d), you can plot its evolution as the following graph. The real thing is exactly the same but in a much higher dimension, so it can\u2019t be visualized on a two-dimensional monitor.\nAn animated diagram with pixel value on the vertical axis and time on the horizontal axis. A graph starting from the left edge draws a random path over time. An image with gradually increasing noise level is shown simultaneously.\nFigure 3. Gradual noise addition is a random walk in pixel value space\nStudying this evolution using many different starting images and random paths, you begin to see some order within the chaos. Think of it like stacking lots of these squiggly paths on top of each other. On average, they create a changing shape over time.\nThe complex pattern of data at the left edge (you could metaphorically imagine the two peaks corresponding to images of cats and dogs, respectively) gradually mixes and simplifies into a featureless blob at the right edge. This is the ubiquitous normal distribution, or pure white noise.\nA diagram with multiple paths and images making the random walk simultaneously. The overlapping random paths average into a smooth density.\nFigure 4. The random paths drawn by all dataset images establish a density that evolves with time\nThe high-level goal (generative modeling) is to somehow find a trick to sample novel images from the true hidden data distribution on the left in Figure 4\u2014actual new images that could have been in the dataset, but weren\u2019t. You could easily sample from the pure-noise state on the right, using ```randn```. Is it possible to then run the above noising process in the reverse direction, so as to end with random samples of clean images (Figure 5)?\nThe same diagram but with time direction reversed: the random path starts from the right edge and advances toward the left edge, and simultaneously an initial pure noise evolves into a novel generated image.\nFigure 5. A reversed random walk starting from noise, and ending at a randomly generated image Following a random path starting from the right edge, what guarantees a proper image at the left edge, rather than just more noise? Some kind of additional force is needed to gently pull the image towards the data on each step.\nThe theory of SDE\u2019s provides a beautiful solution. Without diving too much into the technicalities, it indeed enables reversing the time direction, and doing so automatically introduces an extra term for the sought-after data-attraction force. The force pulls the noisy image towards its mean-square optimal denoising. This can be estimated with a trained neural network (here, sigma is the current noise level):\n\\text{d}\\mathbf{x} = \\underbrace{\\text{d}\\omega_t}_{\\text{add noise}} - ~ \\underbrace{[\\text{denoise}(\\mathbf{x}, \\sigma) - \\mathbf{x}]/\\sigma^2 ~ \\text{d}t}_{\\text{remove noise}}\nYou can even adjust the weight of the two terms, as long as you are careful to keep the total rate of noise reduction unchanged. Taking this idea to the extreme of removing noise exclusively leads to a fully deterministic ordinary differential equation (ODE) with no random component at all. The evolution then follows a smooth trajectory, and the image simply fades in from underneath the fixed noise (Figure 6).\nReverse time evolution of the path and the image. Instead of a squiggly random path, the line is now a smooth curve.\nFigure 6. The smooth evolution induced by the deterministic ordinary differential equation\nNotice how the curved trajectory in Figure 6 connects the initial random noise at the right edge to a unique generated image at the left edge. Indeed, the ODE establishes a different trajectory for each initial noise. Think of these curves as flow lines of a fluid pushing our image around. During the generation, the task is simply to follow the flow line from the start as accurately as possible. Start from a random spot on the right, and at each step, the formula (really, the denoiser network) shows where the flow line points for the current image. Inch a bit in its direction and repeat. That\u2019s the generation process in a nutshell.\nFigure 7 shows that each step of the solver advances the time backward by some chosen amount (dt), and consults the ODE formula (and consequently, the denoiser network) to determine how to change the image over the timestep.\nA diagram showing the step direction arrow along a flow line as being constructed as a sum of a horizontal arrow corresponding to dt, and a vertical arrow corresponding to dx.\nFigure 7. Each step of the solver advances the time backward by some chosen amount (dt) and consults the ODE formula to determine how to change the image over the timestep\nSubsequent sections analyze the deterministic version exclusively, as stochasticity obscures the geometric insight afforded by the deterministic picture. With appropriate tuning, stochasticity has beneficial error correction properties, but it is tedious to use and can be seen as something of a crutch. For more details, see Elucidating the Design Space of Diffusion Based Models.", "Design choices for sampling to generate images\nAs stated in the introduction, it\u2019s the details that make or break the performance. The key difficulty is that the step direction given by the network is valid only in the immediate vicinity of the current noise level. Trying to reduce too much noise at once without stopping to re-evaluate will result in adding something into the images that shouldn\u2019t be there. This shows up as variously reduced image quality: indescript blobbiness and graininess, color and intensity artifacts, distortions and lack of coherence in faces and other higher-level details, and so on.\nIn the 1D visualization, this corresponds to taking a step that lands away from the starting flow line, as shown in Figure 8. Notice the gap that opens between the arrows (representing steps that might be taken) and the curve.\nA diagram with a highlighted curved flow line, showing two straight arrows that attempt to follow it. The endpoints land away from the curve.\nFigure 8. The linear steps (straight arrows) can be a poor approximation of the true curved flow line The common brute-force solution is to simply take a large number of very short steps so as to avoid getting thrown off. However, this is expensive because every step incurs a full evaluation of the denoiser network. It\u2019s like crawling instead of running: safe but slow.\nOur sampler design drastically reduces the number of steps required without compromising quality. The strategy is three-fold:\nDesign the ODE such that its flow lines are as straight as possible, and hence easy to follow ( noise schedules )\nIdentify what noise levels still need extra careful stepping ( time step discretization )\nTake smarter steps to get the most bang for the buck from each ( higher-order solvers )", "Straightening the flow for fewer steps\nThe key issue is the curvature of the flow lines. Had they been straight lines, they would be very easy to follow. It would be possible to take a single long straight step all the way to noise level zero, and never worry about falling off the curve. In reality, some curvature is unavoidably built into the setup. Can it be reduced?\nIt turns out that the theory developed in the previous section enables some poor choices in this regard. For example, you can build different versions of the ODE by specifying different noise schedules. Recall that the 1D visualization was built by adding the same amount of noise at each step. Had it been added at some different time-varying rate, each noise level would be reached at some different time (a different schedule). This amounts to stretching and squeezing the time axis.\nFigure 9 shows a few different ODEs that are induced by different noise schedule choices.\nAn animated diagram showing different choices of time schedule. The flow lines become warped in different ways.\nFigure 9. Different noise schedules induce different flow line curvature. In some schedules, the linear steps are better approximations to the curve than in others Notice that this has the side effect of reshaping the flow lines. In fact, the lines are almost straight in one of these schedules. This is indeed the one that the team advocates. The arrows representing steps now almost perfectly align with the curves. Consequently, it\u2019s possible to take many fewer steps compared to other choices (Figure 10).\nA diagram showing almost straight flow lines, and a pair of linear arrows that now align with them well.\nFigure 10. Our team\u2019s choice of noise schedule. While some curvature still exists at \u200clow noise levels (left edge), the flow lines are almost straight for much of the evolution Figure 10 shows a schedule with the noise level growing linearly as time progresses. Contrast the previous example of constant-rate addition, with the noise level growing fast at first but then slowing down. In other words, time becomes synonymous with noise level. Without diving into the technical details here, this particular choice gives the beautifully straightforward solver algorithm. This is Algorithm 1 in our paper, without the optional lines 6 to 8, using the proposed schedule and after some tidying up:\n```\n# a (poor) placeholder example time discretization\ntimesteps = np.linspace(80, 0, num_steps)\n\n# sample an image of random noise at first noise level\nx = torch.randn(img_shape) * timesteps[0]\n\n# iterate through pairs of adjacent noise levels\nfor t_curr, t_next in zip(timesteps[:-1], timesteps[1:]):\n\n    # fraction of noise we keep in this iteration\n\tblend = t_next / t_curr\n\t\n    # mix in the denoised image\n\tx = blend * x + (1-blend) * denoise(x, t_curr)\n```\nThe code is just a slight generalization of the listing in the introduction. It can\u2019t get much simpler than this. The algorithm is so straightforward that one wonders how it wasn\u2019t stumbled upon from heuristic grounds in 2015\u2014perhaps the idea seems too preposterous to work. Incidentally, denoising diffusion was discussed in the 2015 paper, Deep Unsupervised Learning using Nonequilibrium Thermodynamics, but under a complex mathematical guise. Its potential went unnoticed for years.", "Careful stepping at low noise levels\nThis clearly highlights another design choice, which in most treatments is obscured and entangled with the noise schedule: the choice of time steps. The linear spacing used in the previous code snippet is in fact a poor choice. Empirically (and reasoning from natural image statistics), it\u2019s clear that detail is revealed more rapidly near low noise levels. In the 1D visualization, little is happening for most of the right side of the plot, but then the flow lines take a sudden turn into one of the two basins on the left. This means that long steps are possible at high noise levels, but it is necessary to slow down when approaching low noise levels (Figure 11).\nA diagram where a chain of arrows follows a flow line, such that the arrows are longer at the right edge of the image and become progressively shorter towards the left edge.\nFigure 11. Time steps are long at high noise levels and short at low noise levels Our paper empirically studies what the relative length of steps should be at low versus high noise levels. The following code snippet arrives at a simple but robust modification for timesteps. Roughly, raise the numbers in it to the power of seven (careful to keep them scaled to the original range of 0 to 80). This strongly biases the steps towards the low noise levels:\n```\nsigma_max = 80\nsigma_min = 0.002\t# leave a microscopic bit of noise for stability\nrho = 7\n\nstep_indices = torch.arange(num_steps)\ntimesteps = (sigma_max ** (1 / rho) \\\n          + step_indices / (num_steps - 1) \\\n            * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n```", "Higher-order solvers for more accurate steps\nThe ODE viewpoint enables the use of fancier higher-order solvers, which essentially take curved instead of linear steps. This is an advantage when trying to follow \u200ccurved flow lines. The benefits are not clear-cut, as estimating \u200clocal curvature requires additional neural network evaluations. The team tested a range of approaches and consistently found the so-called second-order Heun scheme to be the best (Figure 12). This adds a couple of lines to the code (see Algorithm 1 in Elucidating the Design Space of Diffusion Based Models ) and doubles the expense per iteration, but cuts the number of required iterations to a fraction.\nThe Heun step has a nice geometric interpretation and a straightforward implementation in code. Take a tentative step as before, then take a second one, and retrace back halfway from the landing point. Notice how the final corrected step lands much closer to the actual flow line than the original one (Figure 12).\nA diagram showing a curve and arrows that attempt to follow it. The first arrow lands away from the curve, and is followed by a second arrow that continues from there. A third arrow shows a half-way point between the base of the first arrow, and the tip of the last arrow. This indicates the corrected step.\nFigure 12. The Heun step illustrated geometrically With all of these improvements combined, it now suffices to evaluate the denoiser only 30 to 80 times, as opposed to 250 to 1,000 times as in most previous work.", "Design choices for training the denoiser\nThis is now a sleek and efficient chain of denoising steps. Thus far, it\u2019s been assumed that each step can call a readily-trained denoiser ```denoise (x, sigma)``` taking in the noisy image and a number indicating its noise level. But how to\u200c parametrize and train it for best results?\nThe most basic form of theoretically valid training for such a network (here, a PyTorch module instantiated as ```denoise``` ) would look something like the following:\n```\n# WARNING: this code illustrates poor choices across the board!\n\nfor clean_image in training_data:\t# we\u2019ll ignore minibatching for brevity\n\n\t# pick a random noise level to train at\n\tsigma = np.random.uniform(0, 80)\n\n    # add noise with this level\n\tnoisy_image = clean_image + sigma * torch.randn_like(clean_image)\n\n    # feed to network under training\n    denoised_image = denoise(noisy_image, sigma)\n\n    # compute mean square loss\n\tloss = (denoised_image - clean_image).square().sum()\n\n\t# ... plus the usual backpropagation and parameter updates\n```\nThe theory requires using white noise and mean-square loss, and to touch all the noise levels that are intended for use in sampling. Within these constraints, there is a lot of freedom to rearrange the computations. The following subsections identify and address each of the serious practical problems in this code.\nNote that the network architecture itself will not be addressed. This discussion is largely orthogonal and agnostic to layer counts, shapes and sizes, use of attention or transformers, and so on. For all the results in the paper, network architectures from previous work were adopted.", "Network-friendly numerical magnitudes\nThe maximum noise level of 80 in these examples has been empirically chosen as a large enough number to completely drown out the image. Consequently, the denoiser is sometimes fed with an image whose pixel values are roughly in the range of -1 to 1 (when the noise level is very low), and sometimes with images in a range beyond -100 to 100. This raises a red flag, as neural networks are known to suffer from unstable training and poor final performance if their inputs vary vastly in scale between examples. It is necessary to standardize the scales.\nSome works combat this by modifying the ODE itself, such that the sampling process keeps the noisy image in a constant-magnitude range rather than allowing it to expand over time (a so-called variance preserving scale schedule ). Unfortunately, this distorts the flow lines again, defeating the benefits of straightening uncovered in the previous section.\nA straightforward solution that does not suffer from such numerical drawbacks follows. The noise level is known, so simply scale the noisy image to a standard magnitude before feeding it to the network. It will automatically adapt to the different scale convention through training, but the problematic range variation is eliminated.\nThe clean way to achieve this is to keep ```denoise``` unchanged from the viewpoint of external callers (the ODE solver and the training loop), but change the way it utilizes the network internally. Isolate the actual raw network layers into their own black box module ```net```, and wrap it with magnitude management code (\u201cpreconditioning\u201d) within ```denoise```:\n```\nsigma_data = 0.5\t# approximate standard deviation of ImageNet pixels\ndef denoise(noisy_image, sigma):\n    noisy_image_variance = sigma**2 + sigma_data**2\n\tscaled_noisy_image = noisy_image / noisy_image_variance ** 0.5\n\treturn net(scaled_noisy_image, sigma)\n```\nHere, the noisy image is divided by its expected standard deviation to bring it roughly to unit variance.\nAs a minor detail (not shown here), similarly also warp the noise level label input to ```net``` with a logarithmic function to make it more evenly spread around the range of -1 to 1.", "Predicting the image versus the noise\nIf you\u2019re familiar with existing diffusion methods, you may have noticed that most of them train the network to predict noise (in unit variance) instead of the clean image, explicitly scale it to the known noise level ```sigma```, and then recover the denoised image by subtraction from input.\nIt turns out that this is a good idea specifically at low noise levels, but a bad one at high noise levels. Because most image detail is suddenly revealed at relatively low noise levels, the benefits outweigh the drawbacks.\nWhy is this a good idea at low noise levels? This approach recycles the almost-clean image from the input, and only uses the network to add a small noise correction to it. Importantly, the network output is explicitly scaled down (by ```sigma``` ) to match the noise level. Consequently, if the network makes some error (as it always does), that error becomes downscaled as well, and has less of an opportunity to mess up the image. This minimizes the contribution from the unreliable learned network, and maximizes the re-use of what is already known in the input.\nWhy is this a bad idea at high noise levels? It ends up boosting the network output according to the large noise magnitude. Consequently, any small error the network makes now becomes a big error in the denoiser output.\nA better option is a continuous transition, where the network\u200c predicts a noise level dependent mixture of the (negative) noise and clean image. Then blend this with the noisy input in appropriate quantities to cancel the noise out.\nThe paper presents a principled way to calculate the blending weights as a function of the noise level. The exact statistical argument is somewhat involved, so this post won\u2019t attempt to replicate it in full. Basically, it is asking for the blend coefficients that result in minimal amplification of the network output. The implementation is quite straightforward. The last return line is replaced with the code below, where ```c_skip``` and ```c_out``` are those blend factors controlling how much of the input is recycled, and how much the network contributes, respectively.\n```\nreturn c_skip * noisy_image + c_out * net(scaled_noisy_image, sigma)\n```", "Equalizing the gradient feedback magnitudes across noise levels\nWith the denoiser internals done, this section addresses the noise level issues in our straw-man training code snippet. It was a (poor) implicit choice not to apply any noise-level-dependent scaling on the loss. It\u2019s as though the following was written:\n```\nweight = 1\nloss = weight * (denoised_image - clean_image).square().sum()\n```\nThe problem is that the value of this loss is large for some noise levels and small for others, due to the various scalings inside the denoiser. Consequently, the magnitude of the updates (gradient feedback) made to the network weights will also depend on the noise level. It\u2019s like a different learning rate was used for different noise levels, for no good reason.\nThis is yet another situation where unifying the magnitudes leads to a more stable and successful training. Fortunately, a simple data-independent statistical formula gives the expected loss magnitude for each noise level. ```weight``` is set accordingly to scale this magnitude back to 1.", "Allocating \u200ctraining efforts\nA tempting misuse of ```weight``` could be also weighing the noise levels according to their relative importance, so as to direct more network capacity where it counts. However, the same goal can be achieved without impacting the magnitudes by training more often at those important noise levels. The division of labor the team advocates is conceptually illustrated in Figure 13.\nEach noise level contributes gradient updates (the arrows) to the network weights throughout the training. Separately, we took control of the magnitudes and counts of these updates using the two respective mechanisms. By default, both the magnitude (length of the arrows) and the frequency (their number) depends on the noise level in an uncontrolled manner. The team advocates a division of labor where the loss scaling standardizes the lengths, and noise level distribution decides how often to train at each level.\nA diagram showing a panel with \u201cbefore\u201d state, and an \u201cafter\u201d state corresponding to our recommendations. In the \u201cbefore\u201d state, an uncontrolled bunch of arrows of various lengths and number emanate from different noise levels. In the \u201cafter\u201d state, the loss scaling makes them equally long, and noise level distribution sets their number.\nFigure 13. By default, both the magnitude (length of the arrows) and the frequency (their number) depends on the noise level in an uncontrolled manner\nUnsurprisingly, the code example that selects the training noise level from a uniform distribution is a poor choice. The theory offers quite little guidance in this choice, as it depends on the characteristics of the dataset. At very low noise levels, progress is minimal because predicting the noise from a noise-free image is effectively impossible (but also irrelevant). Conversely, at very high noise levels, the optimal denoising (blurry average of dataset images) is rather easy to predict. The middle provides a broad range of levels where progress can be made.\nIn practice, we chose the random training noise levels from the formula ```sigma = torch.exp(P_mean + P_std * torch.randn([]))```, where ```P_mean``` and ```P_std``` specify the average noise level for training, and the breadth of randomization around that value, respectively. This specific formula was chosen simply because it\u2019s a straightforward heuristic for drawing non-negative random values spanning multiple orders of magnitude. The values for these parameters are tuned empirically, but turn out to be fairly robust across regular image datasets.\nTo summarize, below is a minimal piece that brings together all the discussed changes to our original training code, including any omitted formulas:\n```\nP_mean = -1.2\t\t# average noise level (logarithmic)\nP_std = 1.2\t\t# spread of random noise levels\nsigma_data = 0.5\t# ImageNet standard deviation\n\ndef denoise(noisy_image, sigma):\n\t    # Input, output and skip scale\n    \tc_in = 1 / torch.sqrt(sigma_data**2 + sigma**2)\n    \tc_out = sigma * sigma_data / torch.sqrt(sigma**2 + sigma_data**2)\n    \tc_skip = sigma_data**2 / (sigma**2 + sigma_data**2)\n    \tc_noise = torch.log(sigma) / 4\t\t# noise label warp\n\n\t    # mix the input and network output to extract the clean image\n        return c_skip * noisy_image + \\\n                   c_out  * net(c_in * noisy_image, c_noise)\n\nfor clean_image in training_data:\t# we\u2019ll ignore minibatching for brevity\n       # random noise level\n        sigma = torch.exp(P_mean + P_std * torch.randn([]))\n\n       noisy_image = clean_image \\\n                            + sigma * torch.randn_like(clean_image)\n       denoised_image = denoise(noisy_image, sigma)\n\t\n\t   # weighted least squares loss\n       weight = (sigma**2 + sigma_data**2) / (sigma * sigma_data)**2\n\t   loss = weight * (denoised_image - clean_image).square().sum()\n\n\t   # ... plus backpropagation and optimizer update\n```", "Results and conclusions\nAll findings presented in this post are shown to be beneficial by thorough numerical experiments, as detailed in Elucidating the Design Space of Diffusion Based Models. The net effect of incorporating all the improvements is a significant advancement of previous work. In particular, we held the world record FID metric in the highly competitive Imagenet 64\u00d764 category for some while. Moreover, we achieved this record with a greatly reduced number of denoiser evaluations at generation time.\nWe believe these findings remain relevant into the future with other data modalities, improved network architectures, or higher-resolution images. Of course, one should still be mindful of the underlying reasoning when applying the model in a different context. For example, many constants (such as that maximum noise level of 80, or position and width of the training noise level distribution) will surely need to be adjusted when, for example, adopting latent diffusion or raising the resolution.\nTo see our official implementation along with pretrained networks, visit NVlabs/edm on GitHub. The code is a clean and minimal implementation that follows the paper notation and conventions, and could serve as an excellent starting point for experimenting and building on these ideas. Note that we include several functions and classes that reproduce previous methods for comparison, but these are not required to use or study our method. For the particularly relevant code, see:\n```generate.py```\n```edm_sampler``` implements the full sampler, including optional stochasticity\n```training/```\n```loss.EDMLoss``` for the loss function and weightings\n```networks.EDMPrecond``` for the scale management and mixture prediction\n```networks.DhariwalUNet``` for our reimplementation of the commonly used ADM network architecture\nThe team has also just recently published a follow-up research paper, Analyzing and Improving the Training Dynamics of Diffusion Models that picks up where this post ends. In this work, they achieve record-breaking generation quality through a deep dive into the design and training of the denoiser network."], "document_title": "Generative AI Research Spotlight: Demystifying Diffusion-Based Models", "document_url": "https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/", "document_date": "2023-12-14T19:58:00", "document_date_modified": "2023-12-14T21:01:17", "document_full_text": "Generative AI Research Spotlight: Demystifying Diffusion-Based Models\nWith Internet-scale data, the computational demands of AI-generated content have grown significantly, with data centers running full steam for weeks or months to train a single model\u2014not to mention the high inference costs in generation, often offered as a service. In this context, suboptimal algorithmic design that sacrifices performance is an expensive mistake.\nMuch of the recent progress in AI-generated image, video, and audio content has been driven by denoising diffusion \u2014a technique that iteratively shapes random noise into novel samples of the data. A recent research paper published by our team, Elucidating the Design Space of Diffusion Based Models, recipient of the Outstanding Paper Award at NeurIPS 2022, identifies the simple core mechanisms underlying the seemingly complicated approaches in the literature. Starting with this clear view of the fundamentals, we then find the state-of-the-art practices for quality and computational efficiency.\nDenoising diffusion\nDenoising is the act of removing, for example, sensor noise from images or hiss from audio recordings. This post will use images as the running example, but the process applies to many other domains as well. This task is excellently suited for convolutional neural networks.\nWhat does this have to do with generating novel images? Imagine there is a large amount of noise on an image. Indeed, so much that the original image is lost. Could a denoiser be used to reveal some random image that could be hiding under all that noise? Surprisingly, the answer is yes.\nThis is the simple essence of denoising diffusion: first draw a random image of pure white noise, and then chip away at the noise level\u2014say, 2% at a time\u2014by repeatedly feeding it to a neural denoiser. Gradually, a random clean image emerges from underneath the noise. The distribution of generated content (pictures of cats and dogs? audio waveforms of spoken English phrases? video clips of driving?) is determined by the dataset that the denoiser network was trained with.\nAn image sequence where each consecutive image is produced by denoising the previous one. The first image is pure white noise, and the last image is the generation result.\nFigure 1. Denoising diffusion reveals novel images from pure noise through repeated denoising The code below is a first guess of how to implement this idea, assuming a neural network function ```denoise``` is available.\n```\n# start with an image of pure large-magnitude noise\nsigma = 80\t\t# initial noise level\nx = sigma * torch.randn(img_shape)\n\nfor step in range(256):\n    # keep 98% of current noisy image, and mix in 2% of denoising\n\tx = 0.98 * x + 0.02 * denoise(x, sigma)\n\t\n    # keep track of current noise level\n    sigma *= 0.98\n```\nIf you\u2019ve ever looked at code bases or scientific papers in the field, filled with pages of equations, you might be surprised to learn that this near-trivial piece of code is actually a theoretically valid implementation of something called a probability flow ordinary differential equation solver. While this snippet is hardly optimal, it embodies surprisingly many of the key good practices explained in the paper. The team\u2019s top-of-the-line final sampler is essentially just a few more lines.\nWhat about that function ```denoise```? At its core, it\u2019s surprisingly straightforward as well: the denoiser must output the blurry average of all possible clean images that could have been hiding under the noise. The desired output at various noise levels might look like the examples in Figure 2.\nThree examples of denoising at different noise levels.\nFigure 2. Examples of ideal denoiser outputs at different noise levels. At high noise levels, the image details are uncertain, and remain blurry in the output Training a denoiser network (typically a U-Net) with basic loss\u2014the mean square error between its output and the clean target\u2014achieves precisely this result. Fancier losses that aim for sharper output are in fact harmful, and violate the theory. Keep in mind that, even if the task is conceptually simple, most existing denoisers are not trained for it specifically.\nMuch of the apparent mathematical complexity in the literature arises from justifying why this works. The theory can be built up from various formalisms, the most popular two being Markov chains and stochastic differential equations. While each approach boils down to a denoising loop that uses a trained denoiser, they open up a vast and confusing space of different practical implementations, and a minefield of opportunities to make poor choices.\nThe paper peels back the layers of mathematical complexity, directly exposing the tangible design choices in a standardized framework where they are easy to analyze.\nThis post presents the team\u2019s key findings and intuitions through visualizations and code. We\u2019ll cover three topics:\nAn intuitive overview of the theory behind denoising diffusion\nDesign choices related to sampling (generating images when you already have a trained denoiser)\nDesign choices when training that denoiser\nWhat makes diffusion work?\nTo begin, this section takes a step back to the basics and builds the theory that justifies this straightforward piece of code. We find most insight in the differential equation framework, originally presented in Score-Based Generative Modeling through Stochastic Differential Equations. While the equations and mathematical concepts might look intimidating, they are not crucial for understanding the gist. It is useful to mention them occasionally to highlight that they are often just a different language for describing concrete things done in code.\nImagine an RGB image x of shape, say, ```[3, 64, 64]``` from the dataset. Begin by considering the easy direction of destroying the image by gradually adding noise onto it. (Of course, this is the opposite of the end goal.)\n```\nfor step in range(1000):\n\tx = x + 0.1 * torch.randn_like(x)\n```\nThis is in fact (suitably squinting) a stochastic differential equation (SDE) solver corresponding to a simple SDE \\text{d}\\mathbf{x} = \\text{d}\\omega_t\n. It says that the change in image x over a short time step is random white noise. Here, solving simply means simulating a specific random numerical realization of the process described by the SDE.\nA nice thing about differential equations is that they have a fruitful geometric interpretation. You can visualize this process as the image taking a random walk (the famous Brownian motion, or Wiener process) in the pixel value space. If you consider x above to be just a single number (a \u201csingle pixel image\u201d), you can plot its evolution as the following graph. The real thing is exactly the same but in a much higher dimension, so it can\u2019t be visualized on a two-dimensional monitor.\nAn animated diagram with pixel value on the vertical axis and time on the horizontal axis. A graph starting from the left edge draws a random path over time. An image with gradually increasing noise level is shown simultaneously.\nFigure 3. Gradual noise addition is a random walk in pixel value space\nStudying this evolution using many different starting images and random paths, you begin to see some order within the chaos. Think of it like stacking lots of these squiggly paths on top of each other. On average, they create a changing shape over time.\nThe complex pattern of data at the left edge (you could metaphorically imagine the two peaks corresponding to images of cats and dogs, respectively) gradually mixes and simplifies into a featureless blob at the right edge. This is the ubiquitous normal distribution, or pure white noise.\nA diagram with multiple paths and images making the random walk simultaneously. The overlapping random paths average into a smooth density.\nFigure 4. The random paths drawn by all dataset images establish a density that evolves with time\nThe high-level goal (generative modeling) is to somehow find a trick to sample novel images from the true hidden data distribution on the left in Figure 4\u2014actual new images that could have been in the dataset, but weren\u2019t. You could easily sample from the pure-noise state on the right, using ```randn```. Is it possible to then run the above noising process in the reverse direction, so as to end with random samples of clean images (Figure 5)?\nThe same diagram but with time direction reversed: the random path starts from the right edge and advances toward the left edge, and simultaneously an initial pure noise evolves into a novel generated image.\nFigure 5. A reversed random walk starting from noise, and ending at a randomly generated image Following a random path starting from the right edge, what guarantees a proper image at the left edge, rather than just more noise? Some kind of additional force is needed to gently pull the image towards the data on each step.\nThe theory of SDE\u2019s provides a beautiful solution. Without diving too much into the technicalities, it indeed enables reversing the time direction, and doing so automatically introduces an extra term for the sought-after data-attraction force. The force pulls the noisy image towards its mean-square optimal denoising. This can be estimated with a trained neural network (here, sigma is the current noise level):\n\\text{d}\\mathbf{x} = \\underbrace{\\text{d}\\omega_t}_{\\text{add noise}} - ~ \\underbrace{[\\text{denoise}(\\mathbf{x}, \\sigma) - \\mathbf{x}]/\\sigma^2 ~ \\text{d}t}_{\\text{remove noise}}\nYou can even adjust the weight of the two terms, as long as you are careful to keep the total rate of noise reduction unchanged. Taking this idea to the extreme of removing noise exclusively leads to a fully deterministic ordinary differential equation (ODE) with no random component at all. The evolution then follows a smooth trajectory, and the image simply fades in from underneath the fixed noise (Figure 6).\nReverse time evolution of the path and the image. Instead of a squiggly random path, the line is now a smooth curve.\nFigure 6. The smooth evolution induced by the deterministic ordinary differential equation\nNotice how the curved trajectory in Figure 6 connects the initial random noise at the right edge to a unique generated image at the left edge. Indeed, the ODE establishes a different trajectory for each initial noise. Think of these curves as flow lines of a fluid pushing our image around. During the generation, the task is simply to follow the flow line from the start as accurately as possible. Start from a random spot on the right, and at each step, the formula (really, the denoiser network) shows where the flow line points for the current image. Inch a bit in its direction and repeat. That\u2019s the generation process in a nutshell.\nFigure 7 shows that each step of the solver advances the time backward by some chosen amount (dt), and consults the ODE formula (and consequently, the denoiser network) to determine how to change the image over the timestep.\nA diagram showing the step direction arrow along a flow line as being constructed as a sum of a horizontal arrow corresponding to dt, and a vertical arrow corresponding to dx.\nFigure 7. Each step of the solver advances the time backward by some chosen amount (dt) and consults the ODE formula to determine how to change the image over the timestep\nSubsequent sections analyze the deterministic version exclusively, as stochasticity obscures the geometric insight afforded by the deterministic picture. With appropriate tuning, stochasticity has beneficial error correction properties, but it is tedious to use and can be seen as something of a crutch. For more details, see Elucidating the Design Space of Diffusion Based Models.\nDesign choices for sampling to generate images\nAs stated in the introduction, it\u2019s the details that make or break the performance. The key difficulty is that the step direction given by the network is valid only in the immediate vicinity of the current noise level. Trying to reduce too much noise at once without stopping to re-evaluate will result in adding something into the images that shouldn\u2019t be there. This shows up as variously reduced image quality: indescript blobbiness and graininess, color and intensity artifacts, distortions and lack of coherence in faces and other higher-level details, and so on.\nIn the 1D visualization, this corresponds to taking a step that lands away from the starting flow line, as shown in Figure 8. Notice the gap that opens between the arrows (representing steps that might be taken) and the curve.\nA diagram with a highlighted curved flow line, showing two straight arrows that attempt to follow it. The endpoints land away from the curve.\nFigure 8. The linear steps (straight arrows) can be a poor approximation of the true curved flow line The common brute-force solution is to simply take a large number of very short steps so as to avoid getting thrown off. However, this is expensive because every step incurs a full evaluation of the denoiser network. It\u2019s like crawling instead of running: safe but slow.\nOur sampler design drastically reduces the number of steps required without compromising quality. The strategy is three-fold:\nDesign the ODE such that its flow lines are as straight as possible, and hence easy to follow ( noise schedules )\nIdentify what noise levels still need extra careful stepping ( time step discretization )\nTake smarter steps to get the most bang for the buck from each ( higher-order solvers )\nStraightening the flow for fewer steps\nThe key issue is the curvature of the flow lines. Had they been straight lines, they would be very easy to follow. It would be possible to take a single long straight step all the way to noise level zero, and never worry about falling off the curve. In reality, some curvature is unavoidably built into the setup. Can it be reduced?\nIt turns out that the theory developed in the previous section enables some poor choices in this regard. For example, you can build different versions of the ODE by specifying different noise schedules. Recall that the 1D visualization was built by adding the same amount of noise at each step. Had it been added at some different time-varying rate, each noise level would be reached at some different time (a different schedule). This amounts to stretching and squeezing the time axis.\nFigure 9 shows a few different ODEs that are induced by different noise schedule choices.\nAn animated diagram showing different choices of time schedule. The flow lines become warped in different ways.\nFigure 9. Different noise schedules induce different flow line curvature. In some schedules, the linear steps are better approximations to the curve than in others Notice that this has the side effect of reshaping the flow lines. In fact, the lines are almost straight in one of these schedules. This is indeed the one that the team advocates. The arrows representing steps now almost perfectly align with the curves. Consequently, it\u2019s possible to take many fewer steps compared to other choices (Figure 10).\nA diagram showing almost straight flow lines, and a pair of linear arrows that now align with them well.\nFigure 10. Our team\u2019s choice of noise schedule. While some curvature still exists at \u200clow noise levels (left edge), the flow lines are almost straight for much of the evolution Figure 10 shows a schedule with the noise level growing linearly as time progresses. Contrast the previous example of constant-rate addition, with the noise level growing fast at first but then slowing down. In other words, time becomes synonymous with noise level. Without diving into the technical details here, this particular choice gives the beautifully straightforward solver algorithm. This is Algorithm 1 in our paper, without the optional lines 6 to 8, using the proposed schedule and after some tidying up:\n```\n# a (poor) placeholder example time discretization\ntimesteps = np.linspace(80, 0, num_steps)\n\n# sample an image of random noise at first noise level\nx = torch.randn(img_shape) * timesteps[0]\n\n# iterate through pairs of adjacent noise levels\nfor t_curr, t_next in zip(timesteps[:-1], timesteps[1:]):\n\n    # fraction of noise we keep in this iteration\n\tblend = t_next / t_curr\n\t\n    # mix in the denoised image\n\tx = blend * x + (1-blend) * denoise(x, t_curr)\n```\nThe code is just a slight generalization of the listing in the introduction. It can\u2019t get much simpler than this. The algorithm is so straightforward that one wonders how it wasn\u2019t stumbled upon from heuristic grounds in 2015\u2014perhaps the idea seems too preposterous to work. Incidentally, denoising diffusion was discussed in the 2015 paper, Deep Unsupervised Learning using Nonequilibrium Thermodynamics, but under a complex mathematical guise. Its potential went unnoticed for years.\nCareful stepping at low noise levels\nThis clearly highlights another design choice, which in most treatments is obscured and entangled with the noise schedule: the choice of time steps. The linear spacing used in the previous code snippet is in fact a poor choice. Empirically (and reasoning from natural image statistics), it\u2019s clear that detail is revealed more rapidly near low noise levels. In the 1D visualization, little is happening for most of the right side of the plot, but then the flow lines take a sudden turn into one of the two basins on the left. This means that long steps are possible at high noise levels, but it is necessary to slow down when approaching low noise levels (Figure 11).\nA diagram where a chain of arrows follows a flow line, such that the arrows are longer at the right edge of the image and become progressively shorter towards the left edge.\nFigure 11. Time steps are long at high noise levels and short at low noise levels Our paper empirically studies what the relative length of steps should be at low versus high noise levels. The following code snippet arrives at a simple but robust modification for timesteps. Roughly, raise the numbers in it to the power of seven (careful to keep them scaled to the original range of 0 to 80). This strongly biases the steps towards the low noise levels:\n```\nsigma_max = 80\nsigma_min = 0.002\t# leave a microscopic bit of noise for stability\nrho = 7\n\nstep_indices = torch.arange(num_steps)\ntimesteps = (sigma_max ** (1 / rho) \\\n          + step_indices / (num_steps - 1) \\\n            * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n```\nHigher-order solvers for more accurate steps\nThe ODE viewpoint enables the use of fancier higher-order solvers, which essentially take curved instead of linear steps. This is an advantage when trying to follow \u200ccurved flow lines. The benefits are not clear-cut, as estimating \u200clocal curvature requires additional neural network evaluations. The team tested a range of approaches and consistently found the so-called second-order Heun scheme to be the best (Figure 12). This adds a couple of lines to the code (see Algorithm 1 in Elucidating the Design Space of Diffusion Based Models ) and doubles the expense per iteration, but cuts the number of required iterations to a fraction.\nThe Heun step has a nice geometric interpretation and a straightforward implementation in code. Take a tentative step as before, then take a second one, and retrace back halfway from the landing point. Notice how the final corrected step lands much closer to the actual flow line than the original one (Figure 12).\nA diagram showing a curve and arrows that attempt to follow it. The first arrow lands away from the curve, and is followed by a second arrow that continues from there. A third arrow shows a half-way point between the base of the first arrow, and the tip of the last arrow. This indicates the corrected step.\nFigure 12. The Heun step illustrated geometrically With all of these improvements combined, it now suffices to evaluate the denoiser only 30 to 80 times, as opposed to 250 to 1,000 times as in most previous work.\nDesign choices for training the denoiser\nThis is now a sleek and efficient chain of denoising steps. Thus far, it\u2019s been assumed that each step can call a readily-trained denoiser ```denoise (x, sigma)``` taking in the noisy image and a number indicating its noise level. But how to\u200c parametrize and train it for best results?\nThe most basic form of theoretically valid training for such a network (here, a PyTorch module instantiated as ```denoise``` ) would look something like the following:\n```\n# WARNING: this code illustrates poor choices across the board!\n\nfor clean_image in training_data:\t# we\u2019ll ignore minibatching for brevity\n\n\t# pick a random noise level to train at\n\tsigma = np.random.uniform(0, 80)\n\n    # add noise with this level\n\tnoisy_image = clean_image + sigma * torch.randn_like(clean_image)\n\n    # feed to network under training\n    denoised_image = denoise(noisy_image, sigma)\n\n    # compute mean square loss\n\tloss = (denoised_image - clean_image).square().sum()\n\n\t# ... plus the usual backpropagation and parameter updates\n```\nThe theory requires using white noise and mean-square loss, and to touch all the noise levels that are intended for use in sampling. Within these constraints, there is a lot of freedom to rearrange the computations. The following subsections identify and address each of the serious practical problems in this code.\nNote that the network architecture itself will not be addressed. This discussion is largely orthogonal and agnostic to layer counts, shapes and sizes, use of attention or transformers, and so on. For all the results in the paper, network architectures from previous work were adopted.\nNetwork-friendly numerical magnitudes\nThe maximum noise level of 80 in these examples has been empirically chosen as a large enough number to completely drown out the image. Consequently, the denoiser is sometimes fed with an image whose pixel values are roughly in the range of -1 to 1 (when the noise level is very low), and sometimes with images in a range beyond -100 to 100. This raises a red flag, as neural networks are known to suffer from unstable training and poor final performance if their inputs vary vastly in scale between examples. It is necessary to standardize the scales.\nSome works combat this by modifying the ODE itself, such that the sampling process keeps the noisy image in a constant-magnitude range rather than allowing it to expand over time (a so-called variance preserving scale schedule ). Unfortunately, this distorts the flow lines again, defeating the benefits of straightening uncovered in the previous section.\nA straightforward solution that does not suffer from such numerical drawbacks follows. The noise level is known, so simply scale the noisy image to a standard magnitude before feeding it to the network. It will automatically adapt to the different scale convention through training, but the problematic range variation is eliminated.\nThe clean way to achieve this is to keep ```denoise``` unchanged from the viewpoint of external callers (the ODE solver and the training loop), but change the way it utilizes the network internally. Isolate the actual raw network layers into their own black box module ```net```, and wrap it with magnitude management code (\u201cpreconditioning\u201d) within ```denoise```:\n```\nsigma_data = 0.5\t# approximate standard deviation of ImageNet pixels\ndef denoise(noisy_image, sigma):\n    noisy_image_variance = sigma**2 + sigma_data**2\n\tscaled_noisy_image = noisy_image / noisy_image_variance ** 0.5\n\treturn net(scaled_noisy_image, sigma)\n```\nHere, the noisy image is divided by its expected standard deviation to bring it roughly to unit variance.\nAs a minor detail (not shown here), similarly also warp the noise level label input to ```net``` with a logarithmic function to make it more evenly spread around the range of -1 to 1.\nPredicting the image versus the noise\nIf you\u2019re familiar with existing diffusion methods, you may have noticed that most of them train the network to predict noise (in unit variance) instead of the clean image, explicitly scale it to the known noise level ```sigma```, and then recover the denoised image by subtraction from input.\nIt turns out that this is a good idea specifically at low noise levels, but a bad one at high noise levels. Because most image detail is suddenly revealed at relatively low noise levels, the benefits outweigh the drawbacks.\nWhy is this a good idea at low noise levels? This approach recycles the almost-clean image from the input, and only uses the network to add a small noise correction to it. Importantly, the network output is explicitly scaled down (by ```sigma``` ) to match the noise level. Consequently, if the network makes some error (as it always does), that error becomes downscaled as well, and has less of an opportunity to mess up the image. This minimizes the contribution from the unreliable learned network, and maximizes the re-use of what is already known in the input.\nWhy is this a bad idea at high noise levels? It ends up boosting the network output according to the large noise magnitude. Consequently, any small error the network makes now becomes a big error in the denoiser output.\nA better option is a continuous transition, where the network\u200c predicts a noise level dependent mixture of the (negative) noise and clean image. Then blend this with the noisy input in appropriate quantities to cancel the noise out.\nThe paper presents a principled way to calculate the blending weights as a function of the noise level. The exact statistical argument is somewhat involved, so this post won\u2019t attempt to replicate it in full. Basically, it is asking for the blend coefficients that result in minimal amplification of the network output. The implementation is quite straightforward. The last return line is replaced with the code below, where ```c_skip``` and ```c_out``` are those blend factors controlling how much of the input is recycled, and how much the network contributes, respectively.\n```\nreturn c_skip * noisy_image + c_out * net(scaled_noisy_image, sigma)\n```\nEqualizing the gradient feedback magnitudes across noise levels\nWith the denoiser internals done, this section addresses the noise level issues in our straw-man training code snippet. It was a (poor) implicit choice not to apply any noise-level-dependent scaling on the loss. It\u2019s as though the following was written:\n```\nweight = 1\nloss = weight * (denoised_image - clean_image).square().sum()\n```\nThe problem is that the value of this loss is large for some noise levels and small for others, due to the various scalings inside the denoiser. Consequently, the magnitude of the updates (gradient feedback) made to the network weights will also depend on the noise level. It\u2019s like a different learning rate was used for different noise levels, for no good reason.\nThis is yet another situation where unifying the magnitudes leads to a more stable and successful training. Fortunately, a simple data-independent statistical formula gives the expected loss magnitude for each noise level. ```weight``` is set accordingly to scale this magnitude back to 1.\nAllocating \u200ctraining efforts\nA tempting misuse of ```weight``` could be also weighing the noise levels according to their relative importance, so as to direct more network capacity where it counts. However, the same goal can be achieved without impacting the magnitudes by training more often at those important noise levels. The division of labor the team advocates is conceptually illustrated in Figure 13.\nEach noise level contributes gradient updates (the arrows) to the network weights throughout the training. Separately, we took control of the magnitudes and counts of these updates using the two respective mechanisms. By default, both the magnitude (length of the arrows) and the frequency (their number) depends on the noise level in an uncontrolled manner. The team advocates a division of labor where the loss scaling standardizes the lengths, and noise level distribution decides how often to train at each level.\nA diagram showing a panel with \u201cbefore\u201d state, and an \u201cafter\u201d state corresponding to our recommendations. In the \u201cbefore\u201d state, an uncontrolled bunch of arrows of various lengths and number emanate from different noise levels. In the \u201cafter\u201d state, the loss scaling makes them equally long, and noise level distribution sets their number.\nFigure 13. By default, both the magnitude (length of the arrows) and the frequency (their number) depends on the noise level in an uncontrolled manner\nUnsurprisingly, the code example that selects the training noise level from a uniform distribution is a poor choice. The theory offers quite little guidance in this choice, as it depends on the characteristics of the dataset. At very low noise levels, progress is minimal because predicting the noise from a noise-free image is effectively impossible (but also irrelevant). Conversely, at very high noise levels, the optimal denoising (blurry average of dataset images) is rather easy to predict. The middle provides a broad range of levels where progress can be made.\nIn practice, we chose the random training noise levels from the formula ```sigma = torch.exp(P_mean + P_std * torch.randn([]))```, where ```P_mean``` and ```P_std``` specify the average noise level for training, and the breadth of randomization around that value, respectively. This specific formula was chosen simply because it\u2019s a straightforward heuristic for drawing non-negative random values spanning multiple orders of magnitude. The values for these parameters are tuned empirically, but turn out to be fairly robust across regular image datasets.\nTo summarize, below is a minimal piece that brings together all the discussed changes to our original training code, including any omitted formulas:\n```\nP_mean = -1.2\t\t# average noise level (logarithmic)\nP_std = 1.2\t\t# spread of random noise levels\nsigma_data = 0.5\t# ImageNet standard deviation\n\ndef denoise(noisy_image, sigma):\n\t    # Input, output and skip scale\n    \tc_in = 1 / torch.sqrt(sigma_data**2 + sigma**2)\n    \tc_out = sigma * sigma_data / torch.sqrt(sigma**2 + sigma_data**2)\n    \tc_skip = sigma_data**2 / (sigma**2 + sigma_data**2)\n    \tc_noise = torch.log(sigma) / 4\t\t# noise label warp\n\n\t    # mix the input and network output to extract the clean image\n        return c_skip * noisy_image + \\\n                   c_out  * net(c_in * noisy_image, c_noise)\n\nfor clean_image in training_data:\t# we\u2019ll ignore minibatching for brevity\n       # random noise level\n        sigma = torch.exp(P_mean + P_std * torch.randn([]))\n\n       noisy_image = clean_image \\\n                            + sigma * torch.randn_like(clean_image)\n       denoised_image = denoise(noisy_image, sigma)\n\t\n\t   # weighted least squares loss\n       weight = (sigma**2 + sigma_data**2) / (sigma * sigma_data)**2\n\t   loss = weight * (denoised_image - clean_image).square().sum()\n\n\t   # ... plus backpropagation and optimizer update\n```\nResults and conclusions\nAll findings presented in this post are shown to be beneficial by thorough numerical experiments, as detailed in Elucidating the Design Space of Diffusion Based Models. The net effect of incorporating all the improvements is a significant advancement of previous work. In particular, we held the world record FID metric in the highly competitive Imagenet 64\u00d764 category for some while. Moreover, we achieved this record with a greatly reduced number of denoiser evaluations at generation time.\nWe believe these findings remain relevant into the future with other data modalities, improved network architectures, or higher-resolution images. Of course, one should still be mindful of the underlying reasoning when applying the model in a different context. For example, many constants (such as that maximum noise level of 80, or position and width of the training noise level distribution) will surely need to be adjusted when, for example, adopting latent diffusion or raising the resolution.\nTo see our official implementation along with pretrained networks, visit NVlabs/edm on GitHub. The code is a clean and minimal implementation that follows the paper notation and conventions, and could serve as an excellent starting point for experimenting and building on these ideas. Note that we include several functions and classes that reproduce previous methods for comparison, but these are not required to use or study our method. For the particularly relevant code, see:\n```generate.py```\n```edm_sampler``` implements the full sampler, including optional stochasticity\n```training/```\n```loss.EDMLoss``` for the loss function and weightings\n```networks.EDMPrecond``` for the scale management and mixture prediction\n```networks.DhariwalUNet``` for our reimplementation of the commonly used ADM network architecture\nThe team has also just recently published a follow-up research paper, Analyzing and Improving the Training Dynamics of Diffusion Models that picks up where this post ends. In this work, they achieve record-breaking generation quality through a deep dive into the design and training of the denoiser network."}], "https://developer.nvidia.com/blog/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/": [{"text": "AI-based computer vision applications are becoming more prevalent, providing real-time insights from video feeds. NVIDIA DeepStream SDK targets Intelligent Video Analytics (IVA) with machine learning for video stream insights. Combining Edge Impulse for model development with DeepStream allows for rapid end-to-end application creation. Building complex CV applications quickly is crucial, with typical use cases like vehicle identification and quality control. Edge Impulse offers ML tools for training models, while DeepStream enables deployment on NVIDIA hardware. DeepStream simplifies CV pipeline creation using various approaches, and models from Edge Impulse can be easily deployed into DeepStream projects. Custom YOLO and Image Classification models can be built in Edge Impulse and exported for DeepStream compatibility. Configuration files in DeepStream specify model requirements and can be tailored for primary or secondary inference stages. Leveraging Edge Impulse and DeepStream streamlines the process of creating and deploying high-performance computer vision applications.", "text_components": ["Fast-Track Computer Vision Deployments with NVIDIA DeepStream and Edge Impulse\nAI-based computer vision (CV) applications are increasing, and are particularly important for extracting real-time insights from video feeds. This revolutionary technology empowers you to unlock valuable information that was once impossible to obtain without significant operator intervention, and provides new opportunities for innovation and problem-solving.\nNVIDIA DeepStream SDK targets Intelligent Video Analytics (IVA) use cases that leverage machine learning (ML) to extract insights from video streams. It uses GPU acceleration for ML and accelerated hardware for maximum preprocessing performance when running on NVIDIA hardware.\nThis post explores the potential of combining Edge Impulse for model development with the NVIDIA DeepStream SDK for deployment so that you can rapidly create end-to-end applications. Edge Impulse is a member of the NVIDIA Inception program.", "Computer vision applications\nThe ability to build complex, scalable CV applications rapidly is critical in today\u2019s environment. Typical CV applications include diverse use cases such as vehicle identification, traffic measurement, inspection systems, quality control on production lines, safety and security enhancement through surveillance, smart checkout system implementation, and process measurement.\nIntegrating machine intelligence to analyze multimedia streams in business processes can add immense value. Thanks to unparalleled accuracy and reliability, machine intelligence can help streamline operations, resulting in increased efficiency.\nPrebuilt AI models aren\u2019t always the right solution and often require fine-tuning for a specific problem that prebuilt models don\u2019t account for.\nBuilding AI-based CV applications generally requires expertise in three skill sets: MLOps, CV application development, and deployment (DevOps). Without these specialized skills, the project ROI and delivery timeline could be at risk.\nIn the past, sophisticated CV applications required highly specialized developers. This translated to long learning curves and expensive resources.\nThe combination of Edge Impulse and the NVIDIA DeepStream SDK offers a user-friendly, complementary solution stack that helps developers quickly create IVA solutions. You can easily customize applications for a specific use case, integrating NVIDIA hardware directly into your solution.\nDeepStream is free to use and Edge Impulse offers a free tier that suits many ML model-building use cases.\nEdge Impulse ML tools are at the top of the stack and used for training. The bottom half of the stack is for building models and consists of Python and C/C++ at the top, followed by Deepstream SDK, CUDA-X, and the NVIDIA computing platform as the foundation.\nFigure 1. Edge Impulse and NVIDIA DeepStream SDK solution stack", "Building CV applications with NVIDIA DeepStream\nDeepstream SDK is a component of NVIDIA Metropolis, which is designed to support video analytics at scale. You can quickly and easily create production-ready CV pipelines that can be deployed directly on NVIDIA hardware appliances.\nDeepStream apps are built using the following approaches:\nFrom the command line\nVisually using Graph Composer\nWithout code using the DeepStream reference application and config files\nWith C++ or Python code for more customization\nIf you aren\u2019t a developer, you can have a pipeline up and running using one of the first three options together with your trained ML model in less than an hour. If you need more customization, you can build a custom-coded solution from existing templates as a starting point.", "Deploying CV applications\nOnce you have created your pipeline, it can be deployed directly on NVIDIA hardware appliances. These range from edge devices, like the NVIDIA Jetson Nano, to high-performance computing (HPC) and cloud deployments, and a hybrid approach.\nYou can deploy your application to run locally on NVIDIA edge hardware with your video source directly connected for minimal latency. If you need to handle complex pipelines or accommodate multiple video sources that exceed the capability of an NVIDIA edge appliance, you can deploy the same pipeline to an NVIDIA-based cloud instance on your preferred IaaS provider.\nA hybrid approach is also possible, where the pipeline can be deployed to an NVIDIA edge appliance and inference can be performed remotely using NVIDIA Triton Inference Server.\nTriton enables remote execution of models, receiving input frames from the client and serving back the results. Triton leverages NVIDIA GPUs when present and can also perform inference on x86 with support for concurrency and dynamic batching. Triton also has native support for most popular frameworks, including TensorFlow and PyTorch.\nDeepStream supports Triton through an alternative to the Gst-nvinfer inference plugin called Gst-nvinferserver. This plugin enables you to use a Triton instance in a DeepStream application.\nIVA applications are only as good as the ML models they are built around. While many pre built models are available, use cases often require customized models and MLOps workflows. This is where having an easy-to-use MLOps platform enables speedy deployments, especially when combined with DeepStream rapid application development.", "Edge Impulse for machine learning\nEdge Impulse offers a powerful suite of tools to build ML models that can be deployed directly onto NVIDIA targets and dropped into DeepStream applications. Seamlessly integrating with NVIDIA hardware acceleration and the DeepStream SDK, Edge Impulse helps you scale your projects quickly.\nEdge Impulse guides developers at all levels throughout the process. Experienced ML professionals will appreciate the ease and convenience of bringing in data from different sources, as well as the end-to-end model-buildinging process. You can also integrate custom models with the custom learning blocks feature, which takes the heavy lifting out of MLOps.\nFor those new to machine learning, the Edge Impulse process guides you in building basic models as you use the environment. The basic model types you can use with DeepStream are YOLO object detection and classification.\nYou can also repurpose models built for tinyML targets so they work with edge use cases and the more powerful NVIDIA hardware. Many edge AI use cases involve complex applications that demand more powerful compute resources. NVIDIA hardware can help solve challenges associated with the limitations of constrained devices.\nWhile you can create your own models from scratch with Edge Impulse, it also integrates with NVIDIA TAO Toolkit so you can leverage over 100 pretrained models in the Computer Vision Model Zoo. Edge Impulse complements TAO and can be used to adapt these models to custom applications. It is a great starting point for enterprise users.\nDiagram showing models from NVIDIA TAO can be used by Edge Impulse Enterprise users\nFigure 2. NVIDIA TAO combined with Edge Impulse Enterprise", "Building models for DeepStream with Edge Impulse\nOnce you are done building your model, deploy it into DeepStream. Export your model files from Edge Impulse and drop them into your DeepStream project. Then follow the configuration steps to ensure your Edge Impulse model works with DeepStream. The process generally involves four steps (Figure 3).\nDiagram of four steps for deploying model files from Edge Impulse into NVIDIA DeepStream. Step 1: Build model in Edge Impulse. Step 2: Export model from Edge Impulse. Step 3: Convert model to DeepStream compatible ONNX. Step 4: Create inference plugin configuration file.\nFigure 3. Four steps for deploying model files from Edge Impulse into NVIDIA DeepStream", "Step 1: Build model in Edge Impulse\nStart by building either a YOLO or Image Classification model in Edge Impulse Studio. The DeepStream inference Gst-nvinfer plugin requires tensors to be in NCHW format for the input layer. Be sure to select Jetson Nano as the target and use FP32 weights.", "Step 2: Export model from Edge Impulse\nEdge Impulse can export models from the Dashboard page in Edge Impulse Studio. YOLOv5 can be exported as an ONNX with an NCHW input layer ready for use with DeepStream.\nScreenshot of Edge Impulse Studio Dashboard, indicating where to export as an ONNX.\nFigure 4. Edge Impulse Studio Dashboard showing how to export as an ONNX model An IVA pipeline in DeepStream typically consists of a primary inference (PGIE) step that performs object detection with the bounding box coordinates. Associated object classes are passed to a secondary inference step (SGIE) that classifies each object. Each is implemented as an instance of the Gst-nvinfer plugin.", "Step 3: Convert model to DeepStream compatible ONNX\nWhen using YOLO with DeepStream, a custom output layer parser is required to extract the bounding boxes and object classes from the output layers that are then passed to the next plugin. For more details about the custom YOLO output parser, see How to Use the Custom YOLO Model.\nEdge Impulse uses YOLOv5, which is a more recent, higher performance model, and has a slightly different output tensor format than YOLOv3. YOLOv3 has three output layers, each responsible for detecting objects at different scales, whereas YOLOv5 has a single output layer that uses anchor boxes to handle objects of various sizes.\nDeepStream is based on GStreamer, which was designed for multimedia use cases. NVIDIA has added features to support deep learning within a GStreamer pipeline, including additional ML-related metadata which is passed down the pipeline with Gst-Buffer and encapsulated in the NvDsBatchMeta structures with Gst-Buffer.\nDeepStream NvDsBatchMeta hierarchy diagram.\nFigure 5. DeepStream metadata hierarchy\nThe output tensor from YOLO is different from the bounding box data required by DeepStream which is held in NvDsObjectMeta. To use YOLO with DeepStream, a custom output parser is needed to transform YOLO output to meet NvDsObjectMeta\u2019s requirements at run-time. NVIDIA provides a sample plugin that works through YOLOv3.\nEdge Impulse uses YOLOv5. The differences between the output layers of YOLOv3 and YOLOv5 make YOLOv3 plugin unsuitable for use with YOLOv5 (Figure 6).\nYOLOv3 and YOLOv5 output layer visualized in Netron.\nFigure 6. A comparison of the YOLOv3 and YOLOv5 output tensor structure\nTo use the YOLOv5 model trained in Edge Impulse, a custom YOLOv5 output parser must be created to process the single output tensor. One implementation that can be used is a third-party output parser that works with the Edge Impulse ONNX exports.\nFor Image Classification models, the default TFLite Float32 provided by Edge Impulse in NHWC format and its input layer need to be converted to NCHW.\nScreenshot of Edge Impulse Studio Dashboard, indicating where to find the TFLight Float32.\nFigure 7. Edge Impulse Studio Dashboard showing where to find the TFLight Float32\nThis is easily achieved using the following ```tf2onnx``` command:\n```\npython -m tf2onnx.convert  --inputs-as-nchw serving_default_x:0  --opset 13 --tflite MODELFILE  --output OUTPUT.ONNX\n\nMODELFILE``` is the input file and ```OUTPUT.ONNX``` is the output file that specifies the input layer name generated by Edge Impulse as ```serving_default_x:0```. As a result, the input layer is transformed to meet DeepStream requirements.\nEdge Impulse\u2019s default input layer is NHWC where as Gst-nvinfer requires NCHW\nFigure 8. Edge Impulse default input layer shape compared to the DeepStream Gst-nvinfer plugin", "Step 4: Create inference plugin configuration file\nDeepStream requires the creation of plain text configuration files for each instance of the Gst-nvinfer plugin to specify runtime requirements. This includes the ONNX model file or generated TRT Engine file and the text file containing the label names. Figure 9 shows the minimum set of parameters required to use the Edge Impulse YOLOv5 and classification models.\nScreenshot of Gst-nvinfer configuration parameter for Edge Impulse.\nFigure 9. Gst-nvinfer plugin configuration parameters for models built using Edge Impulse Note that, although comments are shown inline with parameters for illustrative purposes, all configuration parameters should be separated into new lines.\nThe ```process-mode``` parameter can be used to specify whether the plugin is a primary or secondary stage. Note that the ONNX file is specified and DeepStream uses ```trtexec``` to generate the TensorRT Engine that NVIDIA TensorRT executes on NVIDIA GPUs.\nAfter creating the engine, specify it using the ```model-engine-file``` parameter. The ```model-file``` parameter can be commented out to prevent the engine from being recreated on each run, thereby saving on startup time.\nDepending on the ```model-color-mode``` (whether the model is RGB or grayscale), the parameter must be set to 0 or 2, respectively. This will correspond to the color depth set in Edge Impulse Studio.\nThe preceding example shows how the model used as the primary inference plugin. The model can also be used as the second-stage classifier by setting the ```process-mode``` property as follows:\n```\nprocess-mode=2 #SGIE\n```\nThe example in Figure 9 also shows the minimal configuration files needed for a two-stage pipeline where the YOLO model first detects objects, then individually classifies them in the second stage classifier. For the YOLO model, the default YOLO label file can be edited and the labels replaced with labels from the custom model with each label on a new line, per the YOLO standard format.\nIn the case of the classification model, labels are separated by semicolons. During run time, the models will be indexed accordingly from these files and the text you specify will be displayed.\nDeepStream can be used by referencing the configuration files in your pipeline that have these settings embedded.", "Conclusion\nThis post has explained how to leverage Edge Impulse and NVIDIA DeepStream SDK to quickly create HPC vision applications. These applications include vehicle identification, traffic measurement, inspection systems, quality control on production lines, safety and security enhancement through surveillance, smart checkout system implementation, and process measurement. New use cases for AI and IVA are continually emerging.\nTo learn more about DeepStream, see Get Started With the NVIDIA DeepStream SDK. To get started with Edge Impulse and DeepStream, see Using Edge Impulse with NVIDIA DeepStream. This guide includes a link to a repository with a preconfigured DeepStream pipeline that you can use to validate performance or develop your own pipeline. A precompiled custom parser for the Jetson Nano architecture is also included to help you get started."], "document_title": "Fast-Track Computer Vision Deployments with NVIDIA DeepStream and Edge Impulse", "document_url": "https://developer.nvidia.com/blog/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/", "document_date": "2023-12-14T19:57:06", "document_date_modified": "2023-12-14T20:02:20", "document_full_text": "Fast-Track Computer Vision Deployments with NVIDIA DeepStream and Edge Impulse\nAI-based computer vision (CV) applications are increasing, and are particularly important for extracting real-time insights from video feeds. This revolutionary technology empowers you to unlock valuable information that was once impossible to obtain without significant operator intervention, and provides new opportunities for innovation and problem-solving.\nNVIDIA DeepStream SDK targets Intelligent Video Analytics (IVA) use cases that leverage machine learning (ML) to extract insights from video streams. It uses GPU acceleration for ML and accelerated hardware for maximum preprocessing performance when running on NVIDIA hardware.\nThis post explores the potential of combining Edge Impulse for model development with the NVIDIA DeepStream SDK for deployment so that you can rapidly create end-to-end applications. Edge Impulse is a member of the NVIDIA Inception program.\nComputer vision applications\nThe ability to build complex, scalable CV applications rapidly is critical in today\u2019s environment. Typical CV applications include diverse use cases such as vehicle identification, traffic measurement, inspection systems, quality control on production lines, safety and security enhancement through surveillance, smart checkout system implementation, and process measurement.\nIntegrating machine intelligence to analyze multimedia streams in business processes can add immense value. Thanks to unparalleled accuracy and reliability, machine intelligence can help streamline operations, resulting in increased efficiency.\nPrebuilt AI models aren\u2019t always the right solution and often require fine-tuning for a specific problem that prebuilt models don\u2019t account for.\nBuilding AI-based CV applications generally requires expertise in three skill sets: MLOps, CV application development, and deployment (DevOps). Without these specialized skills, the project ROI and delivery timeline could be at risk.\nIn the past, sophisticated CV applications required highly specialized developers. This translated to long learning curves and expensive resources.\nThe combination of Edge Impulse and the NVIDIA DeepStream SDK offers a user-friendly, complementary solution stack that helps developers quickly create IVA solutions. You can easily customize applications for a specific use case, integrating NVIDIA hardware directly into your solution.\nDeepStream is free to use and Edge Impulse offers a free tier that suits many ML model-building use cases.\nEdge Impulse ML tools are at the top of the stack and used for training. The bottom half of the stack is for building models and consists of Python and C/C++ at the top, followed by Deepstream SDK, CUDA-X, and the NVIDIA computing platform as the foundation.\nFigure 1. Edge Impulse and NVIDIA DeepStream SDK solution stack\nBuilding CV applications with NVIDIA DeepStream\nDeepstream SDK is a component of NVIDIA Metropolis, which is designed to support video analytics at scale. You can quickly and easily create production-ready CV pipelines that can be deployed directly on NVIDIA hardware appliances.\nDeepStream apps are built using the following approaches:\nFrom the command line\nVisually using Graph Composer\nWithout code using the DeepStream reference application and config files\nWith C++ or Python code for more customization\nIf you aren\u2019t a developer, you can have a pipeline up and running using one of the first three options together with your trained ML model in less than an hour. If you need more customization, you can build a custom-coded solution from existing templates as a starting point.\nDeploying CV applications\nOnce you have created your pipeline, it can be deployed directly on NVIDIA hardware appliances. These range from edge devices, like the NVIDIA Jetson Nano, to high-performance computing (HPC) and cloud deployments, and a hybrid approach.\nYou can deploy your application to run locally on NVIDIA edge hardware with your video source directly connected for minimal latency. If you need to handle complex pipelines or accommodate multiple video sources that exceed the capability of an NVIDIA edge appliance, you can deploy the same pipeline to an NVIDIA-based cloud instance on your preferred IaaS provider.\nA hybrid approach is also possible, where the pipeline can be deployed to an NVIDIA edge appliance and inference can be performed remotely using NVIDIA Triton Inference Server.\nTriton enables remote execution of models, receiving input frames from the client and serving back the results. Triton leverages NVIDIA GPUs when present and can also perform inference on x86 with support for concurrency and dynamic batching. Triton also has native support for most popular frameworks, including TensorFlow and PyTorch.\nDeepStream supports Triton through an alternative to the Gst-nvinfer inference plugin called Gst-nvinferserver. This plugin enables you to use a Triton instance in a DeepStream application.\nIVA applications are only as good as the ML models they are built around. While many pre built models are available, use cases often require customized models and MLOps workflows. This is where having an easy-to-use MLOps platform enables speedy deployments, especially when combined with DeepStream rapid application development.\nEdge Impulse for machine learning\nEdge Impulse offers a powerful suite of tools to build ML models that can be deployed directly onto NVIDIA targets and dropped into DeepStream applications. Seamlessly integrating with NVIDIA hardware acceleration and the DeepStream SDK, Edge Impulse helps you scale your projects quickly.\nEdge Impulse guides developers at all levels throughout the process. Experienced ML professionals will appreciate the ease and convenience of bringing in data from different sources, as well as the end-to-end model-buildinging process. You can also integrate custom models with the custom learning blocks feature, which takes the heavy lifting out of MLOps.\nFor those new to machine learning, the Edge Impulse process guides you in building basic models as you use the environment. The basic model types you can use with DeepStream are YOLO object detection and classification.\nYou can also repurpose models built for tinyML targets so they work with edge use cases and the more powerful NVIDIA hardware. Many edge AI use cases involve complex applications that demand more powerful compute resources. NVIDIA hardware can help solve challenges associated with the limitations of constrained devices.\nWhile you can create your own models from scratch with Edge Impulse, it also integrates with NVIDIA TAO Toolkit so you can leverage over 100 pretrained models in the Computer Vision Model Zoo. Edge Impulse complements TAO and can be used to adapt these models to custom applications. It is a great starting point for enterprise users.\nDiagram showing models from NVIDIA TAO can be used by Edge Impulse Enterprise users\nFigure 2. NVIDIA TAO combined with Edge Impulse Enterprise\nBuilding models for DeepStream with Edge Impulse\nOnce you are done building your model, deploy it into DeepStream. Export your model files from Edge Impulse and drop them into your DeepStream project. Then follow the configuration steps to ensure your Edge Impulse model works with DeepStream. The process generally involves four steps (Figure 3).\nDiagram of four steps for deploying model files from Edge Impulse into NVIDIA DeepStream. Step 1: Build model in Edge Impulse. Step 2: Export model from Edge Impulse. Step 3: Convert model to DeepStream compatible ONNX. Step 4: Create inference plugin configuration file.\nFigure 3. Four steps for deploying model files from Edge Impulse into NVIDIA DeepStream\nStep 1: Build model in Edge Impulse\nStart by building either a YOLO or Image Classification model in Edge Impulse Studio. The DeepStream inference Gst-nvinfer plugin requires tensors to be in NCHW format for the input layer. Be sure to select Jetson Nano as the target and use FP32 weights.\nStep 2: Export model from Edge Impulse\nEdge Impulse can export models from the Dashboard page in Edge Impulse Studio. YOLOv5 can be exported as an ONNX with an NCHW input layer ready for use with DeepStream.\nScreenshot of Edge Impulse Studio Dashboard, indicating where to export as an ONNX.\nFigure 4. Edge Impulse Studio Dashboard showing how to export as an ONNX model An IVA pipeline in DeepStream typically consists of a primary inference (PGIE) step that performs object detection with the bounding box coordinates. Associated object classes are passed to a secondary inference step (SGIE) that classifies each object. Each is implemented as an instance of the Gst-nvinfer plugin.\nStep 3: Convert model to DeepStream compatible ONNX\nWhen using YOLO with DeepStream, a custom output layer parser is required to extract the bounding boxes and object classes from the output layers that are then passed to the next plugin. For more details about the custom YOLO output parser, see How to Use the Custom YOLO Model.\nEdge Impulse uses YOLOv5, which is a more recent, higher performance model, and has a slightly different output tensor format than YOLOv3. YOLOv3 has three output layers, each responsible for detecting objects at different scales, whereas YOLOv5 has a single output layer that uses anchor boxes to handle objects of various sizes.\nDeepStream is based on GStreamer, which was designed for multimedia use cases. NVIDIA has added features to support deep learning within a GStreamer pipeline, including additional ML-related metadata which is passed down the pipeline with Gst-Buffer and encapsulated in the NvDsBatchMeta structures with Gst-Buffer.\nDeepStream NvDsBatchMeta hierarchy diagram.\nFigure 5. DeepStream metadata hierarchy\nThe output tensor from YOLO is different from the bounding box data required by DeepStream which is held in NvDsObjectMeta. To use YOLO with DeepStream, a custom output parser is needed to transform YOLO output to meet NvDsObjectMeta\u2019s requirements at run-time. NVIDIA provides a sample plugin that works through YOLOv3.\nEdge Impulse uses YOLOv5. The differences between the output layers of YOLOv3 and YOLOv5 make YOLOv3 plugin unsuitable for use with YOLOv5 (Figure 6).\nYOLOv3 and YOLOv5 output layer visualized in Netron.\nFigure 6. A comparison of the YOLOv3 and YOLOv5 output tensor structure\nTo use the YOLOv5 model trained in Edge Impulse, a custom YOLOv5 output parser must be created to process the single output tensor. One implementation that can be used is a third-party output parser that works with the Edge Impulse ONNX exports.\nFor Image Classification models, the default TFLite Float32 provided by Edge Impulse in NHWC format and its input layer need to be converted to NCHW.\nScreenshot of Edge Impulse Studio Dashboard, indicating where to find the TFLight Float32.\nFigure 7. Edge Impulse Studio Dashboard showing where to find the TFLight Float32\nThis is easily achieved using the following ```tf2onnx``` command:\n```\npython -m tf2onnx.convert  --inputs-as-nchw serving_default_x:0  --opset 13 --tflite MODELFILE  --output OUTPUT.ONNX\n\nMODELFILE``` is the input file and ```OUTPUT.ONNX``` is the output file that specifies the input layer name generated by Edge Impulse as ```serving_default_x:0```. As a result, the input layer is transformed to meet DeepStream requirements.\nEdge Impulse\u2019s default input layer is NHWC where as Gst-nvinfer requires NCHW\nFigure 8. Edge Impulse default input layer shape compared to the DeepStream Gst-nvinfer plugin\nStep 4: Create inference plugin configuration file\nDeepStream requires the creation of plain text configuration files for each instance of the Gst-nvinfer plugin to specify runtime requirements. This includes the ONNX model file or generated TRT Engine file and the text file containing the label names. Figure 9 shows the minimum set of parameters required to use the Edge Impulse YOLOv5 and classification models.\nScreenshot of Gst-nvinfer configuration parameter for Edge Impulse.\nFigure 9. Gst-nvinfer plugin configuration parameters for models built using Edge Impulse Note that, although comments are shown inline with parameters for illustrative purposes, all configuration parameters should be separated into new lines.\nThe ```process-mode``` parameter can be used to specify whether the plugin is a primary or secondary stage. Note that the ONNX file is specified and DeepStream uses ```trtexec``` to generate the TensorRT Engine that NVIDIA TensorRT executes on NVIDIA GPUs.\nAfter creating the engine, specify it using the ```model-engine-file``` parameter. The ```model-file``` parameter can be commented out to prevent the engine from being recreated on each run, thereby saving on startup time.\nDepending on the ```model-color-mode``` (whether the model is RGB or grayscale), the parameter must be set to 0 or 2, respectively. This will correspond to the color depth set in Edge Impulse Studio.\nThe preceding example shows how the model used as the primary inference plugin. The model can also be used as the second-stage classifier by setting the ```process-mode``` property as follows:\n```\nprocess-mode=2 #SGIE\n```\nThe example in Figure 9 also shows the minimal configuration files needed for a two-stage pipeline where the YOLO model first detects objects, then individually classifies them in the second stage classifier. For the YOLO model, the default YOLO label file can be edited and the labels replaced with labels from the custom model with each label on a new line, per the YOLO standard format.\nIn the case of the classification model, labels are separated by semicolons. During run time, the models will be indexed accordingly from these files and the text you specify will be displayed.\nDeepStream can be used by referencing the configuration files in your pipeline that have these settings embedded.\nConclusion\nThis post has explained how to leverage Edge Impulse and NVIDIA DeepStream SDK to quickly create HPC vision applications. These applications include vehicle identification, traffic measurement, inspection systems, quality control on production lines, safety and security enhancement through surveillance, smart checkout system implementation, and process measurement. New use cases for AI and IVA are continually emerging.\nTo learn more about DeepStream, see Get Started With the NVIDIA DeepStream SDK. To get started with Edge Impulse and DeepStream, see Using Edge Impulse with NVIDIA DeepStream. This guide includes a link to a repository with a preconfigured DeepStream pipeline that you can use to validate performance or develop your own pipeline. A precompiled custom parser for the Jetson Nano architecture is also included to help you get started."}], "https://developer.nvidia.com/blog/simulate-and-localize-a-husky-robot-with-nvidia-isaac/": [{"text": "The article discusses how to simulate and localize a Husky robot using NVIDIA Isaac ROS. It explains the process of importing the Husky robot into NVIDIA Isaac Sim, equipped with an NVIDIA Jetson Orin Nano and ZED 2 camera. The article details how to set up the demo on a workstation with an NVIDIA RTX GPU and ROS 2 Humble installed, as well as how to run the demo in a Hardware In the Loop configuration with a NVIDIA Jetson Orin Nano. The post provides step-by-step instructions on how to set up and run the demo, including cloning repositories, building ROS 2 packages, and launching Isaac ROS nodes. It also explains how to drive the Husky robot using keyboard, joystick, or Nav2. Overall, the article serves as a guide for researchers and developers interested in simulating and localizing the Husky robot using NVIDIA Isaac ROS.", "text_components": ["Simulate and Localize a Husky Robot with NVIDIA Isaac\nThe Husky robot, developed by Clearpath Robotics, is a versatile four-wheeled platform made for indoor and outdoor research use. It is simple to modify by adding other sensors and changing the high-level board. This post explains how to use the official ROS 2 Husky packages to import the robot into NVIDIA Isaac Sim and create a simulation.\nFor this demo, the Husky robot is equipped with an NVIDIA Jetson Orin Nano and a ZED 2 camera mounted on top. Driving the Husky uses the latest version of Isaac ROS 2, which includes Isaac ROS packages for robot localization ( NVIDIA Isaac ROS VSLAM ), map building ( NVIDIA Isaac ROS NvBlox ), and Apriltag detection ( NVIDIA Isaac ROS AprilTag ).", "How the Isaac ROS demo works\nThe Husky unified robotics description format (URDF) is dynamically loaded from a ROS 2 topic. It is used to visualize the robot in ROS visualization ( ```rviz``` ) and load it into NVIDIA Isaac Sim.\nWhen the robot is loaded, the main Isaac Sim script creates a graph to drive the robot, converting the velocity and steering commands into velocity for each wheel, and publishes the wheel status in a ```tf``` (Figure 1).\nHusky motor control graph screenshot.\nFigure 1. Motor control action graph, automatically generated when Husky is loaded from the ROS 2 topic\nThe script generates a graph for each camera to publish the output in a ROS 2 topic. Each camera has a ROS 2 image message and camera information.\nHusky camera control graph screenshot.\nFigure 2. Camera graph with resolution configuration and publish ROS 2 topic\nAfter loading all graphs and successfully importing the URDF, a new Husky robot appears in Isaac Sim, as shown in Figure 3.\nHusky robot on NVIDIA Isaac Sim screenshot.\nFigure 3. NVIDIA Isaac Sim 2023.1.0 with the Husky and all cameras output\nWhen the Husky has been successfully loaded, different options for ROS 2 will be available, including camera streams, the motor controller, and other auxiliary outputs. You can quickly check what is running in another terminal using the following command:\nros2 topic list\nHusky ROS 2 topics on terminal screenshot.\nFigure 4. List of all Husky ROS 2 topics\nWhen the ROS 2 launcher script starts, all required ROS 2 nodes and Isaac ROS nodes run. The robot will build a connected pipeline where the husky URDF coming from the \u2018Husky description\u2019 will be used for Isaac Sim and the robot itself. Isaac ROS VSLAM and Isaac ROS NvBlox will also be used to localize and build a real-time map.\nWith all the necessary packages now running, the robot can be fully localized and is capable of building a 3D map. You can now drive it using Nav2, a keyboard, or a joystick.\nThe next step is setting up your workstation and Jetson Orin (optional) to test Husky on Isaac Sim 2023 and Isaac ROS 2.\n3D-generated map on rviz2 with Husky robot.\nFigure 5. Husky on the move and generating a 3D map", "Set up your demo\nThere are two ways to run this demo: directly from your workstation or using hardware-in-the-loop (HIL) configuration. This section covers the setup requirements for each of these options.\nFor either option, first install the latest version of Isaac Sim. To see detailed instructions, visit Get Started With NVIDIA Omniverse.\nNote: Do not start Isaac Sim when the download is complete.", "Husky demo workstation version\nTo run the Husky demo, you need a workstation with an NVIDIA RTX GPU and the latest versions of both NVIDIA Isaac Sim and ROS 2 Humble installed. Remember to install the desktop versions.\nsudo apt install ros-humble-desktop", "Install and run\nWhen your system is ready, you can clone the demo and try it on your workstation using the following script:\ngit clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\nAfter running it, use the following command to start Isaac Sim and open a new terminal with Docker built to use Isaac ROS and all packages needed for the demo:\n./husky_demo.sh This command will:\nClone all repositories required\nClone the Isaac ROS Docker image\nBuild a new image\nWhen the build is complete, the terminal will be waiting for a new command (Figure 6). Figure 7 shows Isaac Sim running and the environment fully loaded.\nScreenshot of terminal with Isaac ROS prompt.\nFigure 6. Terminal with a Docker container with Isaac ROS prompt waiting for a command Screenshot of Isaac Sim with empty environment loaded\nFigure 7. Isaac Sim running with the warehouse environment loaded Now you can run this script on your second terminal:\nbash src/husky_isaac_sim/scripts/run_in_docker.sh The script ```run_in_docker.sh``` will execute some steps before running the Isaac ROS launch file. The first stage will update and build all required ROS 2 and Isaac ROS packages. Then it will execute the launch file:\nros2 launch husky_isaac_sim allinone.launch.py This ROS 2 script launches all Isaac ROS packages to localize the robot and start mapping and ```rviz``` to visualize the husky on map.\nThe script will also load a Husky on the environment and automatically set up cameras and controllers.", "Husky demo HIL version\nFor the HIL version, you\u2019ll need a workstation with an NVIDIA RTX GPU and the latest version of NVIDIA Isaac Sim 2023.1.0 installed, as well as a router and an NVIDIA Jetson Orin Nano. (Note that other Jetson Orin models will also do the job.)\nIt\u2019s important to establish a wired connection between your workstation and your Jetson, and using a good router between them is highly recommended. Keep in mind that there will be a lot of data shared between your workstation and the Jetson, so a Wi-Fi connection won\u2019t be sufficient.\nYour NVIDIA Jetson Orin Nano must use the latest Jetpack 5.1.2 and be fully installed. Keep the IP address. If you hostname, you\u2019ll need to connect remotely from your workstation.\nThe required components to run with this configuration are listed below:\nx86/64 machine with Ubuntu 22.04\nNVIDIA RTX GPU\nNVIDIA Jetson Orin Nano Developer Kit with Jetpack 5.1.2\nRouter and cables\nFigure 8 shows how to configure your hardware environment.\nDiagram of hardware setup. From the left is a desktop with an NVIDIA RTX graphic card, router (center), and an NVIDIA Jetson Orin Nano Developer Kit (right).\nFigure 8. Hardware setup: desktop within an NVIDIA RTX graphics card (left), a router (center), and an NVIDIA Jetson Orin Nano Developer Kit (right)", "Install and run\nWhen your system is ready, clone the demo and try it on your workstation using the following command:\ngit clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\nAfter running it, use the following command:\n./husky_demo.sh --HIL\nThis script will automatically:\nClone all repositories required for this demo\nBuild all ROS 2 packages\nStart Isaac Sim\nWhen the build is complete, the terminal will be waiting for a new command. Run the following script:\nros2 launch husky_isaac_sim robot_display.launch.py You will see an image of a Husky on Isaac Sim displayed on your workstation.\nNext, open a new terminal to remotely connect to the NVIDIA Jetson Orin series.\nWhen you are logged in to the Jetsterminal, clone the Husky demo repository:\ngit clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\nAfter running it, use the following command:\n./husky_demo.sh\nThis script will automatically:\nClone all repositories required for this demo\nClone the Isaac ROS docker image\nBuild a new image\nNext, run the following script from the Docker container:\nbash src/husky_isaac_sim/scripts/run_in_docker.sh\nThis script will automatically:\nBuild all ROS 2 packages\nStart Isaac ROS\nYou will then see all Isaac ROS packages running on the terminal (Figure 9).\nRemote terminal to Jetson Orin Nano with Isaac ROS running.\nFigure 9. Isaac ROS running on Jetson Orin Nano and transmitting data", "Drive your Husky\nAssuming that both Isaas Sim and Isaac ROS are up and running\u2014either on your workstation or Jetson, according to your preference\u2014and there are no issues, you should be able to see the robot localized successfully on ```rviz```. Once localized, it should start building a 3D map (Figure 10).\nHusky localized on rviz.\nFigure 10. Husky on ```rviz``` localized and plotting a first slice of a 3D map with NvBlox\nNow, you can drive your Husky using a keyboard, a joystick, or with Nav2. In just a few quick steps, you can control the movement of the Husky within the environment.", "Summary\nThis post has explained how to operate the Husky robot using NVIDIA Isaac ROS, including how to load a URDF derived from a robot description, and how to drive it on ```rviz```. This demo can be run in various ways, either directly from your workstation with an NVIDIA RTX graphics card or in the Hardware In the Loop configuration, where Isaac ROS runs on your NVIDIA Jetson.\nTo learn more about Isaac Sim and Isaac ROS, check out our new Isaac ROS Office Hours on YouTube. And join the conversation on the NVIDIA Developer Robotics Forum."], "document_title": "Simulate and Localize a Husky Robot with NVIDIA Isaac", "document_url": "https://developer.nvidia.com/blog/simulate-and-localize-a-husky-robot-with-nvidia-isaac/", "document_date": "2023-12-14T19:44:18", "document_date_modified": "2023-12-14T19:50:12", "document_full_text": "Simulate and Localize a Husky Robot with NVIDIA Isaac\nThe Husky robot, developed by Clearpath Robotics, is a versatile four-wheeled platform made for indoor and outdoor research use. It is simple to modify by adding other sensors and changing the high-level board. This post explains how to use the official ROS 2 Husky packages to import the robot into NVIDIA Isaac Sim and create a simulation.\nFor this demo, the Husky robot is equipped with an NVIDIA Jetson Orin Nano and a ZED 2 camera mounted on top. Driving the Husky uses the latest version of Isaac ROS 2, which includes Isaac ROS packages for robot localization ( NVIDIA Isaac ROS VSLAM ), map building ( NVIDIA Isaac ROS NvBlox ), and Apriltag detection ( NVIDIA Isaac ROS AprilTag ).\nHow the Isaac ROS demo works\nThe Husky unified robotics description format (URDF) is dynamically loaded from a ROS 2 topic. It is used to visualize the robot in ROS visualization ( ```rviz``` ) and load it into NVIDIA Isaac Sim.\nWhen the robot is loaded, the main Isaac Sim script creates a graph to drive the robot, converting the velocity and steering commands into velocity for each wheel, and publishes the wheel status in a ```tf``` (Figure 1).\nHusky motor control graph screenshot.\nFigure 1. Motor control action graph, automatically generated when Husky is loaded from the ROS 2 topic\nThe script generates a graph for each camera to publish the output in a ROS 2 topic. Each camera has a ROS 2 image message and camera information.\nHusky camera control graph screenshot.\nFigure 2. Camera graph with resolution configuration and publish ROS 2 topic\nAfter loading all graphs and successfully importing the URDF, a new Husky robot appears in Isaac Sim, as shown in Figure 3.\nHusky robot on NVIDIA Isaac Sim screenshot.\nFigure 3. NVIDIA Isaac Sim 2023.1.0 with the Husky and all cameras output\nWhen the Husky has been successfully loaded, different options for ROS 2 will be available, including camera streams, the motor controller, and other auxiliary outputs. You can quickly check what is running in another terminal using the following command:\nros2 topic list\nHusky ROS 2 topics on terminal screenshot.\nFigure 4. List of all Husky ROS 2 topics\nWhen the ROS 2 launcher script starts, all required ROS 2 nodes and Isaac ROS nodes run. The robot will build a connected pipeline where the husky URDF coming from the \u2018Husky description\u2019 will be used for Isaac Sim and the robot itself. Isaac ROS VSLAM and Isaac ROS NvBlox will also be used to localize and build a real-time map.\nWith all the necessary packages now running, the robot can be fully localized and is capable of building a 3D map. You can now drive it using Nav2, a keyboard, or a joystick.\nThe next step is setting up your workstation and Jetson Orin (optional) to test Husky on Isaac Sim 2023 and Isaac ROS 2.\n3D-generated map on rviz2 with Husky robot.\nFigure 5. Husky on the move and generating a 3D map\nSet up your demo\nThere are two ways to run this demo: directly from your workstation or using hardware-in-the-loop (HIL) configuration. This section covers the setup requirements for each of these options.\nFor either option, first install the latest version of Isaac Sim. To see detailed instructions, visit Get Started With NVIDIA Omniverse.\nNote: Do not start Isaac Sim when the download is complete.\nHusky demo workstation version\nTo run the Husky demo, you need a workstation with an NVIDIA RTX GPU and the latest versions of both NVIDIA Isaac Sim and ROS 2 Humble installed. Remember to install the desktop versions.\nsudo apt install ros-humble-desktop\nInstall and run\nWhen your system is ready, you can clone the demo and try it on your workstation using the following script:\ngit clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\nAfter running it, use the following command to start Isaac Sim and open a new terminal with Docker built to use Isaac ROS and all packages needed for the demo:\n./husky_demo.sh This command will:\nClone all repositories required\nClone the Isaac ROS Docker image\nBuild a new image\nWhen the build is complete, the terminal will be waiting for a new command (Figure 6). Figure 7 shows Isaac Sim running and the environment fully loaded.\nScreenshot of terminal with Isaac ROS prompt.\nFigure 6. Terminal with a Docker container with Isaac ROS prompt waiting for a command Screenshot of Isaac Sim with empty environment loaded\nFigure 7. Isaac Sim running with the warehouse environment loaded Now you can run this script on your second terminal:\nbash src/husky_isaac_sim/scripts/run_in_docker.sh The script ```run_in_docker.sh``` will execute some steps before running the Isaac ROS launch file. The first stage will update and build all required ROS 2 and Isaac ROS packages. Then it will execute the launch file:\nros2 launch husky_isaac_sim allinone.launch.py This ROS 2 script launches all Isaac ROS packages to localize the robot and start mapping and ```rviz``` to visualize the husky on map.\nThe script will also load a Husky on the environment and automatically set up cameras and controllers.\nHusky demo HIL version\nFor the HIL version, you\u2019ll need a workstation with an NVIDIA RTX GPU and the latest version of NVIDIA Isaac Sim 2023.1.0 installed, as well as a router and an NVIDIA Jetson Orin Nano. (Note that other Jetson Orin models will also do the job.)\nIt\u2019s important to establish a wired connection between your workstation and your Jetson, and using a good router between them is highly recommended. Keep in mind that there will be a lot of data shared between your workstation and the Jetson, so a Wi-Fi connection won\u2019t be sufficient.\nYour NVIDIA Jetson Orin Nano must use the latest Jetpack 5.1.2 and be fully installed. Keep the IP address. If you hostname, you\u2019ll need to connect remotely from your workstation.\nThe required components to run with this configuration are listed below:\nx86/64 machine with Ubuntu 22.04\nNVIDIA RTX GPU\nNVIDIA Jetson Orin Nano Developer Kit with Jetpack 5.1.2\nRouter and cables\nFigure 8 shows how to configure your hardware environment.\nDiagram of hardware setup. From the left is a desktop with an NVIDIA RTX graphic card, router (center), and an NVIDIA Jetson Orin Nano Developer Kit (right).\nFigure 8. Hardware setup: desktop within an NVIDIA RTX graphics card (left), a router (center), and an NVIDIA Jetson Orin Nano Developer Kit (right)\nInstall and run\nWhen your system is ready, clone the demo and try it on your workstation using the following command:\ngit clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\nAfter running it, use the following command:\n./husky_demo.sh --HIL\nThis script will automatically:\nClone all repositories required for this demo\nBuild all ROS 2 packages\nStart Isaac Sim\nWhen the build is complete, the terminal will be waiting for a new command. Run the following script:\nros2 launch husky_isaac_sim robot_display.launch.py You will see an image of a Husky on Isaac Sim displayed on your workstation.\nNext, open a new terminal to remotely connect to the NVIDIA Jetson Orin series.\nWhen you are logged in to the Jetsterminal, clone the Husky demo repository:\ngit clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\nAfter running it, use the following command:\n./husky_demo.sh\nThis script will automatically:\nClone all repositories required for this demo\nClone the Isaac ROS docker image\nBuild a new image\nNext, run the following script from the Docker container:\nbash src/husky_isaac_sim/scripts/run_in_docker.sh\nThis script will automatically:\nBuild all ROS 2 packages\nStart Isaac ROS\nYou will then see all Isaac ROS packages running on the terminal (Figure 9).\nRemote terminal to Jetson Orin Nano with Isaac ROS running.\nFigure 9. Isaac ROS running on Jetson Orin Nano and transmitting data\nDrive your Husky\nAssuming that both Isaas Sim and Isaac ROS are up and running\u2014either on your workstation or Jetson, according to your preference\u2014and there are no issues, you should be able to see the robot localized successfully on ```rviz```. Once localized, it should start building a 3D map (Figure 10).\nHusky localized on rviz.\nFigure 10. Husky on ```rviz``` localized and plotting a first slice of a 3D map with NvBlox\nNow, you can drive your Husky using a keyboard, a joystick, or with Nav2. In just a few quick steps, you can control the movement of the Husky within the environment.\nSummary\nThis post has explained how to operate the Husky robot using NVIDIA Isaac ROS, including how to load a URDF derived from a robot description, and how to drive it on ```rviz```. This demo can be run in various ways, either directly from your workstation with an NVIDIA RTX graphics card or in the Hardware In the Loop configuration, where Isaac ROS runs on your NVIDIA Jetson.\nTo learn more about Isaac Sim and Isaac ROS, check out our new Isaac ROS Office Hours on YouTube. And join the conversation on the NVIDIA Developer Robotics Forum."}], "https://developer.nvidia.com/blog/next-generation-seismic-monitoring-with-neural-operators/": [{"text": "Virtual seismology has revolutionized earthquake monitoring by automating the manual process of seismic phase picking with the introduction of PhaseNO, a neural network-based algorithm. PhaseNO, powered by NVIDIA GPUs, utilizes deep learning techniques to accurately pick seismic wave arrival times across multiple stations in a seismic network geometry. By training on a dataset from the Northern California Earthquake Data Center, PhaseNO outperformed other state-of-the-art phase-picking methods, detecting more true positives with fewer false negatives and false positives. The innovative architecture of PhaseNO combines Fourier neural operator layers for temporal information and graph neural operator layers for spatial insights, enabling efficient communication between stations in a seismic network. Real-world applications of PhaseNO on the Southern California Seismic Network have shown its ability to detect additional events, improve measurement accuracy, and enhance earthquake catalog construction. The success of PhaseNO signals a new era in seismic monitoring, showcasing the potential of advanced machine learning techniques for refining earthquake monitoring systems.", "text_components": ["Next-Generation Seismic Monitoring with Neural Operators\nVirtual seismology has only been around for a few years, and it has already had a significant impact on earthquake monitoring.\nHistorically, seismic phase picking\u2014the task of annotating seismograms with seismic wave arrival times that underpins earthquake monitoring operations globally\u2014was a manual process. As such, it was labor-intensive, fraught with subjectivity, and prone to errors.\nDeep learning techniques powered by NVIDIA GPUs have addressed these challenges. Our paper, Phase Neural Operator for Multi-Station Picking of Seismic Arrivals, introduces a general-purpose network-wide phase-picking algorithm based on a recently developed machine learning paradigm called neural operator. Our model, called PhaseNO, leverages the spatio-temporal contextual information to pick phases simultaneously for any seismic network geometry.\nAccelerated with NVIDIA DGX GPUs, PhaseNO is rooted in the realm of earthquake seismology and stands as a testament to the transformative potential of neural operators in revolutionizing seismic phase-picking methodologies, opening new chapters in scientific computing.", "Seismic phase detection\nEarthquake detection and phase picking are foundational tasks in earthquake seismology, where the aim is to identify earthquakes in the continuous data and measure the arrival times of seismic waves. These arrival time measurements, or phase picks, are crucial for constructing accurate earthquake catalogs, which are databases of earthquake attributes, including the occurrence time, source location, and magnitude.\nWhen navigating the vast global expanse of continuous seismic data, the imperative to refine earthquake catalogs becomes more pronounced. The enriched catalogs provide profound insights into previously unnoticed seismic events, illuminating fault complexities, earthquake behaviors, and subsurface dynamics.\nState-of-the-art approaches for phase picking use deep neural networks (DNNs) to annotate seismograms at each station independently. Trained with abundant datasets manually labeled by human analysts over the past few decades, DNNs predict the probabilities of P and S arrival times from input seismograms. This enables the determination of arrivals by setting a predetermined probability threshold.\nPhaseNO exemplifies a paradigm shift towards automated and precise phase picking. In this context, deep neural operators are trained to recognize complex patterns in the seismic data and extract useful features for earthquake phase picking without requiring any prior information about the dataset.", "Breaking the single-station barrier\nCurrent single-station deep learning models operate significantly differently from human analysts in analyzing seismic waveforms. Experienced analysts examine waveforms from multiple stations, enabling them to recognize weak but coherent seismic signals and distinguish noisy spikes.\nIn contrast, due to the lack of contextual information from multiple stations, single-station-based models are inherently limited by their design, making them easily fail to detect events buried in a high level of noise or mistakenly detect local noise signals with emergence pulses.\nWith traditional approaches centered around single-station algorithms nearing their performance zenith, PhaseNO introduces a paradigmatic shift. This algorithm employs neural operators to transcend the limitations of single-station methodologies, offering a network-wide perspective. By assimilating data from diverse stations with arbitrary geometries, PhaseNO achieves unprecedented seismic monitoring efficacy.\nAt the neural operator lab at California Institute of Technology (CalTech), directed by Zachary E. Ross, and partnering with NVIDIA Research, we trained PhaseNO on an earthquake dataset from the Northern California Earthquake Data Center spanning the period 1984-2019. We evaluated our approach on real-world seismic datasets and compared its performance with state-of-the-art phase-picking methods PhaseNet, EQTransformer, and EdgePhase.\nWith the highest F1 scores for P- and S-waves being 0.99 and 0.98, respectively, PhaseNO detected more true positives, fewer false negatives, and fewer false positive picks than the other deep learning models.", "The architectural ingenuity of PhaseNO\nThe efficacy of PhaseNO stems from its architectural innovation. It combines two types of neural operators to handle the mathematical structure of seismic network data. Integrating Fourier neural operator (FNO) layers for temporal information and graph neural operators (GNO) for spatial insights, PhaseNO adeptly navigates irregular sensor placements (Figure 2).\nFNO and GNO layers are sequentially connected and repeated several times, enabling sufficient communications and exchange of spatiotemporal information between all stations in a seismic network. This synergistic combination facilitates efficient and accurate phase picking across seismic networks of varying complexities.\nIn addition, effective graph-type data augmentation strategies (by adding virtual stations with only noise and stacking multiple events within a time window) further exploit the potential of PhaseNO and amplify its power dealing with complex real-world datasets.\nA graphic of the PhaseNO model. The model consists of multiple FNO and GNO layers. This model takes input data and predicts the probabilites of arrival times.\nFigure 1. PhaseNO architecture. The model consists of multiple FNO and GNO layers that are sequentially connected and repeat. The model uses seismograms from a seismic network containing multiple stations with an arbitrary geometry as the input and predicts the probabilities of P-phase and S-phase arrival times for all input stations", "Real-world applications\nOver the past year, PhaseNO has undergone rigorous evaluation on real-world seismic datasets, benchmarked against state-of-the-art phase-picking methods. We applied the PhaseNO trained on the Northern California Seismic Network to the Southern California Seismic Network. PhaseNO detects an additional 4,428 events than PhaseNet for the 2019 Ridgecrest earthquake sequence, a challenging dataset due to the overlap of numerous events.\nFigure 2 shows an example of one additional event detected by PhaseNO, which is hidden in the waveforms of the larger event. The results underscore its superior performance in detecting a greater number of earthquakes, picking numerous phase arrivals, and substantially improving measurement accuracy.\nExample of continuous seismic data captured during the 2019 Ridgecrest earthquake sequence. PhaseNO detects an additional event compared with PhaseNet in a 35-s time window.\nFigure 2. Example of continuous data during the 2019 Ridgecrest earthquake sequence. PhaseNO detects an additional event compared with PhaseNet in this 35-second time window. The first arrivals of the newly detected event overlap with the waveforms of the larger event and thus make them challenging for the single-station detector", "The future of seismic monitoring\nPhaseNO has emerged as a beacon of progress in seismic monitoring. The success of the algorithm exemplifies the intersection of advanced machine learning techniques and seismic monitoring, laying the groundwork for refined earthquake monitoring systems. Its nuanced approach, blending temporal and spatial considerations through neural operators, offers a glimpse into the future of earthquake seismology. We look forward to the continued evolution of PhaseNO and its enduring positive impact on seismic monitoring systems.\nTo try the pretrained PhaseNO model, visit PhaseNO on GitHub. And join us for our upcoming presentation at the American Geophysical Union Annual Meeting 2023."], "document_title": "Next-Generation Seismic Monitoring with Neural Operators", "document_url": "https://developer.nvidia.com/blog/next-generation-seismic-monitoring-with-neural-operators/", "document_date": "2023-12-12T18:00:00", "document_date_modified": "2023-12-14T19:27:27", "document_full_text": "Next-Generation Seismic Monitoring with Neural Operators\nVirtual seismology has only been around for a few years, and it has already had a significant impact on earthquake monitoring.\nHistorically, seismic phase picking\u2014the task of annotating seismograms with seismic wave arrival times that underpins earthquake monitoring operations globally\u2014was a manual process. As such, it was labor-intensive, fraught with subjectivity, and prone to errors.\nDeep learning techniques powered by NVIDIA GPUs have addressed these challenges. Our paper, Phase Neural Operator for Multi-Station Picking of Seismic Arrivals, introduces a general-purpose network-wide phase-picking algorithm based on a recently developed machine learning paradigm called neural operator. Our model, called PhaseNO, leverages the spatio-temporal contextual information to pick phases simultaneously for any seismic network geometry.\nAccelerated with NVIDIA DGX GPUs, PhaseNO is rooted in the realm of earthquake seismology and stands as a testament to the transformative potential of neural operators in revolutionizing seismic phase-picking methodologies, opening new chapters in scientific computing.\nSeismic phase detection\nEarthquake detection and phase picking are foundational tasks in earthquake seismology, where the aim is to identify earthquakes in the continuous data and measure the arrival times of seismic waves. These arrival time measurements, or phase picks, are crucial for constructing accurate earthquake catalogs, which are databases of earthquake attributes, including the occurrence time, source location, and magnitude.\nWhen navigating the vast global expanse of continuous seismic data, the imperative to refine earthquake catalogs becomes more pronounced. The enriched catalogs provide profound insights into previously unnoticed seismic events, illuminating fault complexities, earthquake behaviors, and subsurface dynamics.\nState-of-the-art approaches for phase picking use deep neural networks (DNNs) to annotate seismograms at each station independently. Trained with abundant datasets manually labeled by human analysts over the past few decades, DNNs predict the probabilities of P and S arrival times from input seismograms. This enables the determination of arrivals by setting a predetermined probability threshold.\nPhaseNO exemplifies a paradigm shift towards automated and precise phase picking. In this context, deep neural operators are trained to recognize complex patterns in the seismic data and extract useful features for earthquake phase picking without requiring any prior information about the dataset.\nBreaking the single-station barrier\nCurrent single-station deep learning models operate significantly differently from human analysts in analyzing seismic waveforms. Experienced analysts examine waveforms from multiple stations, enabling them to recognize weak but coherent seismic signals and distinguish noisy spikes.\nIn contrast, due to the lack of contextual information from multiple stations, single-station-based models are inherently limited by their design, making them easily fail to detect events buried in a high level of noise or mistakenly detect local noise signals with emergence pulses.\nWith traditional approaches centered around single-station algorithms nearing their performance zenith, PhaseNO introduces a paradigmatic shift. This algorithm employs neural operators to transcend the limitations of single-station methodologies, offering a network-wide perspective. By assimilating data from diverse stations with arbitrary geometries, PhaseNO achieves unprecedented seismic monitoring efficacy.\nAt the neural operator lab at California Institute of Technology (CalTech), directed by Zachary E. Ross, and partnering with NVIDIA Research, we trained PhaseNO on an earthquake dataset from the Northern California Earthquake Data Center spanning the period 1984-2019. We evaluated our approach on real-world seismic datasets and compared its performance with state-of-the-art phase-picking methods PhaseNet, EQTransformer, and EdgePhase.\nWith the highest F1 scores for P- and S-waves being 0.99 and 0.98, respectively, PhaseNO detected more true positives, fewer false negatives, and fewer false positive picks than the other deep learning models.\nThe architectural ingenuity of PhaseNO\nThe efficacy of PhaseNO stems from its architectural innovation. It combines two types of neural operators to handle the mathematical structure of seismic network data. Integrating Fourier neural operator (FNO) layers for temporal information and graph neural operators (GNO) for spatial insights, PhaseNO adeptly navigates irregular sensor placements (Figure 2).\nFNO and GNO layers are sequentially connected and repeated several times, enabling sufficient communications and exchange of spatiotemporal information between all stations in a seismic network. This synergistic combination facilitates efficient and accurate phase picking across seismic networks of varying complexities.\nIn addition, effective graph-type data augmentation strategies (by adding virtual stations with only noise and stacking multiple events within a time window) further exploit the potential of PhaseNO and amplify its power dealing with complex real-world datasets.\nA graphic of the PhaseNO model. The model consists of multiple FNO and GNO layers. This model takes input data and predicts the probabilites of arrival times.\nFigure 1. PhaseNO architecture. The model consists of multiple FNO and GNO layers that are sequentially connected and repeat. The model uses seismograms from a seismic network containing multiple stations with an arbitrary geometry as the input and predicts the probabilities of P-phase and S-phase arrival times for all input stations\nReal-world applications\nOver the past year, PhaseNO has undergone rigorous evaluation on real-world seismic datasets, benchmarked against state-of-the-art phase-picking methods. We applied the PhaseNO trained on the Northern California Seismic Network to the Southern California Seismic Network. PhaseNO detects an additional 4,428 events than PhaseNet for the 2019 Ridgecrest earthquake sequence, a challenging dataset due to the overlap of numerous events.\nFigure 2 shows an example of one additional event detected by PhaseNO, which is hidden in the waveforms of the larger event. The results underscore its superior performance in detecting a greater number of earthquakes, picking numerous phase arrivals, and substantially improving measurement accuracy.\nExample of continuous seismic data captured during the 2019 Ridgecrest earthquake sequence. PhaseNO detects an additional event compared with PhaseNet in a 35-s time window.\nFigure 2. Example of continuous data during the 2019 Ridgecrest earthquake sequence. PhaseNO detects an additional event compared with PhaseNet in this 35-second time window. The first arrivals of the newly detected event overlap with the waveforms of the larger event and thus make them challenging for the single-station detector\nThe future of seismic monitoring\nPhaseNO has emerged as a beacon of progress in seismic monitoring. The success of the algorithm exemplifies the intersection of advanced machine learning techniques and seismic monitoring, laying the groundwork for refined earthquake monitoring systems. Its nuanced approach, blending temporal and spatial considerations through neural operators, offers a glimpse into the future of earthquake seismology. We look forward to the continued evolution of PhaseNO and its enduring positive impact on seismic monitoring systems.\nTo try the pretrained PhaseNO model, visit PhaseNO on GitHub. And join us for our upcoming presentation at the American Geophysical Union Annual Meeting 2023."}], "https://developer.nvidia.com/blog/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/": [{"text": "Quantum computing has the potential to revolutionize various aspects of industry, including the automotive sector. The BMW Group has demonstrated the advantages of using the NVIDIA cuQuantum SDK for accelerating quantum circuit simulations, showcasing a 300x increase in performance for quantum machine learning applications. The group is exploring how quantum computing can enhance algorithms in generative modeling, which can be applied throughout the automotive value chain, such as battery technology development. The QUARK benchmarking framework is designed to evaluate and compare quantum algorithms and hardware implementations, enabling the assessment of efficiency in different quantum computing domains. NVIDIA cuQuantum, with its optimized libraries and tools, allows for significant speedups in quantum circuit simulations, making it possible to train quantum generative models in minutes instead of hours. Overall, this collaboration between BMW Group and NVIDIA demonstrates a valuable framework for benchmarking quantum applications and transitioning theoretical explorations into practical, impactful solutions across industries.", "text_components": ["Benchmarking Quantum Computing Applications with BMW Group and NVIDIA cuQuantum\nQuantum computing has the potential to revolutionize various aspects of industry, ranging from numerical simulations and optimization of complex systems to machine learning (ML). Many computational challenges within the automotive industry are well-suited for quantum computing, including the development of novel materials, efficient design of parts, optimal manufacturing processes, and accurate risk forecasting.\nThe advantages of the NVIDIA cuQuantum SDK for accelerating quantum circuit simulations were showcased by the BMW Group in their recent publication on application-oriented benchmarking of quantum machine learning.\nCurrently, the group is investigating how quantum computing can enhance algorithms in generative modeling. They have observed a 300x increase in simulation performance with cuQuantum, from 8 hours to a few minutes per iteration.", "Benchmarking quantum computing systems\nGenerative AI is a type of ML where algorithms learn to create new data samples that resemble the training data. It\u2019s commonly used for tasks like image generation, text-to-speech, and other applications where producing new content is the goal. Quantum methods in ML can help generate designs that are optimized for specific parameters, enabling faster and more efficient design exploration.\nGenerative AI can also be applied along the entire automotive value chain. For example, battery technology development can benefit from quantum ML by aiding in exploring novel materials for fuel cells, optimizing charging and discharging methods, and predicting battery lifespan.\nHowever, few systematic assessments of the potential of quantum computers for practical applications have been carried out, leaving the precise benefits and applications of quantum computing in industry indistinct. Therefore, the need for benchmarking quantum computing systems, from hardware to simulators, becomes critical to evaluate the scaling performance of quantum algorithms and improve the development of quantum computers.", "QUARK\nQUARK is a benchmarking framework designed to accommodate applications from different domains of quantum computing, such as quantum machine learning, optimization, and numerical simulations. It provides a standardized and extensible platform for evaluating and comparing quantum algorithms and hardware implementations. QUARK assesses the efficiency of the BMW Group\u2019s quantum computing algorithms while also enabling comparison of the performance obtained from both quantum simulators and quantum hardware.", "Simulating quantum circuits\nCurrent quantum processors (QPUs) suffer from noise and are only available on a small scale, hindering the advancement of algorithmic research. Additionally, simulators offer direct access to the information representing the quantum system, unlike real quantum hardware. Therefore, circuit simulation techniques are a critical tool to advance the field of quantum computing.\nEspecially in quantum ML, it\u2019s essential to verify the output of algorithms empirically. While many quantum algorithms have been proposed and implemented for small system sizes, evaluating the robustness of these algorithms as the system size scales up remains a critical challenge.\nThis has created a pressing need for accelerated large-scale quantum simulations. Numerous quantum circuit simulators are available. The QUARK benchmarking framework helps identify the optimal choice of a simulator for a specific quantum workload or research objective.", "Enabling quantum applications across industries\nNVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use cuQuantum to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.\nA plot of the execution time of quantum circuits as a function of qubit count comparing the CPU execution time to GPU execution time and showing the relative speed-up on GPU.\nFigure 1. NVIDIA cuQuantum accelerates the simulation of quantum circuits by 300x, enabling the training of a single iteration of a quantum generative model to be performed in a few minutes instead of 8 hours With only minor changes in their code base, scientists at the BMW Group unlocked the power of the cuQuantum SDK for simulations with up to 30 qubits. By leveraging the power of the cuStateVec, they achieved a 300x speedup in the simulation of quantum circuits for their quantum ML workload using NVIDIA A100 Tensor Core 40 GB GPUs, compared to CPU implementations on dual AMD EPYC 7742.\nAn animated GIF showing the evolution of a quantum model as it is trained.\nFigure 2. During \u200ctraining, the quantum model learns to replicate a target distribution", "Quantum applications for automotive use cases\nThe developments showcased by the BMW Group and accelerated by NVIDIA demonstrate a valuable and high-performance framework for benchmarking quantum applications. With NVIDIA cuQuantum, the BMW Group was able to significantly improve the training time and benchmarking of quantum generative models using QUARK, unblocking a considerable computational bottleneck towards useful quantum ML workloads. Together, this work enables an important step in transitioning quantum applications from theoretical explorations to practical, impactful solutions, enabling broader quantum applications across industries."], "document_title": "Benchmarking Quantum Computing Applications with BMW Group and NVIDIA cuQuantum", "document_url": "https://developer.nvidia.com/blog/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/", "document_date": "2023-12-12T17:00:00", "document_date_modified": "2023-12-14T19:27:27", "document_full_text": "Benchmarking Quantum Computing Applications with BMW Group and NVIDIA cuQuantum\nQuantum computing has the potential to revolutionize various aspects of industry, ranging from numerical simulations and optimization of complex systems to machine learning (ML). Many computational challenges within the automotive industry are well-suited for quantum computing, including the development of novel materials, efficient design of parts, optimal manufacturing processes, and accurate risk forecasting.\nThe advantages of the NVIDIA cuQuantum SDK for accelerating quantum circuit simulations were showcased by the BMW Group in their recent publication on application-oriented benchmarking of quantum machine learning.\nCurrently, the group is investigating how quantum computing can enhance algorithms in generative modeling. They have observed a 300x increase in simulation performance with cuQuantum, from 8 hours to a few minutes per iteration.\nBenchmarking quantum computing systems\nGenerative AI is a type of ML where algorithms learn to create new data samples that resemble the training data. It\u2019s commonly used for tasks like image generation, text-to-speech, and other applications where producing new content is the goal. Quantum methods in ML can help generate designs that are optimized for specific parameters, enabling faster and more efficient design exploration.\nGenerative AI can also be applied along the entire automotive value chain. For example, battery technology development can benefit from quantum ML by aiding in exploring novel materials for fuel cells, optimizing charging and discharging methods, and predicting battery lifespan.\nHowever, few systematic assessments of the potential of quantum computers for practical applications have been carried out, leaving the precise benefits and applications of quantum computing in industry indistinct. Therefore, the need for benchmarking quantum computing systems, from hardware to simulators, becomes critical to evaluate the scaling performance of quantum algorithms and improve the development of quantum computers.\nQUARK\nQUARK is a benchmarking framework designed to accommodate applications from different domains of quantum computing, such as quantum machine learning, optimization, and numerical simulations. It provides a standardized and extensible platform for evaluating and comparing quantum algorithms and hardware implementations. QUARK assesses the efficiency of the BMW Group\u2019s quantum computing algorithms while also enabling comparison of the performance obtained from both quantum simulators and quantum hardware.\nSimulating quantum circuits\nCurrent quantum processors (QPUs) suffer from noise and are only available on a small scale, hindering the advancement of algorithmic research. Additionally, simulators offer direct access to the information representing the quantum system, unlike real quantum hardware. Therefore, circuit simulation techniques are a critical tool to advance the field of quantum computing.\nEspecially in quantum ML, it\u2019s essential to verify the output of algorithms empirically. While many quantum algorithms have been proposed and implemented for small system sizes, evaluating the robustness of these algorithms as the system size scales up remains a critical challenge.\nThis has created a pressing need for accelerated large-scale quantum simulations. Numerous quantum circuit simulators are available. The QUARK benchmarking framework helps identify the optimal choice of a simulator for a specific quantum workload or research objective.\nEnabling quantum applications across industries\nNVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use cuQuantum to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.\nA plot of the execution time of quantum circuits as a function of qubit count comparing the CPU execution time to GPU execution time and showing the relative speed-up on GPU.\nFigure 1. NVIDIA cuQuantum accelerates the simulation of quantum circuits by 300x, enabling the training of a single iteration of a quantum generative model to be performed in a few minutes instead of 8 hours With only minor changes in their code base, scientists at the BMW Group unlocked the power of the cuQuantum SDK for simulations with up to 30 qubits. By leveraging the power of the cuStateVec, they achieved a 300x speedup in the simulation of quantum circuits for their quantum ML workload using NVIDIA A100 Tensor Core 40 GB GPUs, compared to CPU implementations on dual AMD EPYC 7742.\nAn animated GIF showing the evolution of a quantum model as it is trained.\nFigure 2. During \u200ctraining, the quantum model learns to replicate a target distribution\nQuantum applications for automotive use cases\nThe developments showcased by the BMW Group and accelerated by NVIDIA demonstrate a valuable and high-performance framework for benchmarking quantum applications. With NVIDIA cuQuantum, the BMW Group was able to significantly improve the training time and benchmarking of quantum generative models using QUARK, unblocking a considerable computational bottleneck towards useful quantum ML workloads. Together, this work enables an important step in transitioning quantum applications from theoretical explorations to practical, impactful solutions, enabling broader quantum applications across industries."}], "https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-nvue-and-ansible/": [{"text": "The article discusses the automation of data center networks using NVIDIA NVUE and Ansible. NVIDIA NVUE is an object-oriented model of hardware and software systems with a robust API for configuration. Ansible is an open-source IT automation tool that can automate various manual processes in data centers. The NVIDIA NVUE Collection includes modules and roles for interacting with NVIDIA devices managed by NVUE, providing granularity in configuration options. High-level modules and object-specific modules are available for interacting with switches using CLI or REST API commands. Ansible roles in the collection offer examples of fully operationalized data center playbooks for reference topologies. Automation can improve operational efficiency and workload management in enterprises. The NVIDIA NVUE Collection aims to simplify the network automation journey with prebuilt modules and roles. To get started with Ansible and NVUE, refer to the article \"Data Center Network Automation with Ansible\" and try the hands-on-lab on NVIDIA Air.", "text_components": ["Automating Data Center Networks with NVIDIA NVUE and Ansible\nData center automation dates to the early days of the mainframe, with operational efficiency topping the list of its benefits. Over the years, technologies have changed both inside and outside the data center. As a result, tools and approaches have evolved as well.\nThe NVIDIA NVUE Collection and Ansible aim to simplify your network automation journey by providing a comprehensive list of modules and roles for getting started with reference topologies in your environment.", "NVIDIA NVUE\nNVDIA User Experience (NVUE) is an object-oriented, schema-driven model of a complete hardware and software system. It provides a robust API that enables multiple interfaces to both view (show) and configure (set and unset) any element within a system running the NVUE software. NVIDIA Cumulus Linux 5.x includes the NVUE model.\nNVUE is an API-first structured object model that simplifies operations. It provides a declarative command line interface (CLI) and a single configuration file. The CLI and the REST API are equivalent in functionality. You can run all management operations from either the REST API or the CLI.", "Ansible\nAnsible is an open-source agentless IT automation tool that automates provisioning, configuration management, application deployment, orchestration, and many other manual IT processes. It works by connecting to your automation target and pushing programs that execute instructions.\nAnsible modules are included in the NVIDIA NVUE Collection to help you interact with NVIDIA devices managed by NVUE. These modules provide granularity in the configuration options and can be used to build Day 0 and Day 1 through Day N configurations.\nRed Hat Ansible Certified Collections can help jump-start the process, with prebuilt roles ready to download and run. Workflows and templates can further simplify manual steps while making the process repeatable. See a list of certified partners.", "NVIDIA NVUE Collection\nThe various NVIDIA NVUE Collection modules currently available include high-level modules and object-specific modules. Visit /nvue on GitHub to download the modules and read the instructions.", "High-level modules\nThe high-level modules provide a wrapper around the NVUE utilities. You can leverage these to interact with the switches using the CLI or REST API commands.\n```nvidia.nvue.command``` is a wrapper around nv command-line tool with added templating and automated dialog prompting.\n```nvidia.nvue.api``` is a wrapper around the NVUE REST API to send and retrieve NVUE configuration.", "Object-specific modules\nObject-specific modules are designed to work with the individual network objects and support various parameters that enable you to interact with them as required. The various modules supported include acl, bridge, router, interface, evpn, mlag, system, vrf, and VXLAN.\nFor REST API endpoints that aren\u2019t covered by the object-specific modules or for subpaths within the object-specific modules (for example, ```/interface/<id>/qos/roce/counters``` ), you can leverage the ```nvidia.nvue.api``` high-level module and specify the endpoint in the path parameter.\nAll modules other than ```nvidia.nvue.cli``` leverage the REST API to connect with the NVIDIA Cumulus Linux switch, as shown in Figure 1.\nDiagram displaying the communication between modules and the NVIDIA Cumulus Linux switch.\nFigure 1. NVUE modules architecture", "NVUE Collection roles\nIn general, Ansible roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files.\nThe roles in this collection provide several examples of a fully operationalized, automated data center in the form of playbooks. They provide a standard reference topology for various examples, such as MLAG and BGP configurations. See examples of role use.", "Get started\nAutomation enables enterprises to improve operational efficiency, ensure compliance, and effectively manage workloads. The NVIDIA NVUE Collection provides a comprehensive list of modules and roles to simply your network automation journey and work with reference topologies in your environment.\nTo get started with Ansible and the NVIDIA NVUE Collection, see Data Center Network Automation with Ansible. You can also try the hands-on-lab on NVIDIA Air."], "document_title": "Automating Data Center Networks with NVIDIA NVUE and Ansible", "document_url": "https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-nvue-and-ansible/", "document_date": "2023-12-11T18:30:00", "document_date_modified": "2023-12-14T19:27:28", "document_full_text": "Automating Data Center Networks with NVIDIA NVUE and Ansible\nData center automation dates to the early days of the mainframe, with operational efficiency topping the list of its benefits. Over the years, technologies have changed both inside and outside the data center. As a result, tools and approaches have evolved as well.\nThe NVIDIA NVUE Collection and Ansible aim to simplify your network automation journey by providing a comprehensive list of modules and roles for getting started with reference topologies in your environment.\nNVIDIA NVUE\nNVDIA User Experience (NVUE) is an object-oriented, schema-driven model of a complete hardware and software system. It provides a robust API that enables multiple interfaces to both view (show) and configure (set and unset) any element within a system running the NVUE software. NVIDIA Cumulus Linux 5.x includes the NVUE model.\nNVUE is an API-first structured object model that simplifies operations. It provides a declarative command line interface (CLI) and a single configuration file. The CLI and the REST API are equivalent in functionality. You can run all management operations from either the REST API or the CLI.\nAnsible\nAnsible is an open-source agentless IT automation tool that automates provisioning, configuration management, application deployment, orchestration, and many other manual IT processes. It works by connecting to your automation target and pushing programs that execute instructions.\nAnsible modules are included in the NVIDIA NVUE Collection to help you interact with NVIDIA devices managed by NVUE. These modules provide granularity in the configuration options and can be used to build Day 0 and Day 1 through Day N configurations.\nRed Hat Ansible Certified Collections can help jump-start the process, with prebuilt roles ready to download and run. Workflows and templates can further simplify manual steps while making the process repeatable. See a list of certified partners.\nNVIDIA NVUE Collection\nThe various NVIDIA NVUE Collection modules currently available include high-level modules and object-specific modules. Visit /nvue on GitHub to download the modules and read the instructions.\nHigh-level modules\nThe high-level modules provide a wrapper around the NVUE utilities. You can leverage these to interact with the switches using the CLI or REST API commands.\n```nvidia.nvue.command``` is a wrapper around nv command-line tool with added templating and automated dialog prompting.\n```nvidia.nvue.api``` is a wrapper around the NVUE REST API to send and retrieve NVUE configuration.\nObject-specific modules\nObject-specific modules are designed to work with the individual network objects and support various parameters that enable you to interact with them as required. The various modules supported include acl, bridge, router, interface, evpn, mlag, system, vrf, and VXLAN.\nFor REST API endpoints that aren\u2019t covered by the object-specific modules or for subpaths within the object-specific modules (for example, ```/interface/<id>/qos/roce/counters``` ), you can leverage the ```nvidia.nvue.api``` high-level module and specify the endpoint in the path parameter.\nAll modules other than ```nvidia.nvue.cli``` leverage the REST API to connect with the NVIDIA Cumulus Linux switch, as shown in Figure 1.\nDiagram displaying the communication between modules and the NVIDIA Cumulus Linux switch.\nFigure 1. NVUE modules architecture\nNVUE Collection roles\nIn general, Ansible roles provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files.\nThe roles in this collection provide several examples of a fully operationalized, automated data center in the form of playbooks. They provide a standard reference topology for various examples, such as MLAG and BGP configurations. See examples of role use.\nGet started\nAutomation enables enterprises to improve operational efficiency, ensure compliance, and effectively manage workloads. The NVIDIA NVUE Collection provides a comprehensive list of modules and roles to simply your network automation journey and work with reference topologies in your environment.\nTo get started with Ansible and the NVIDIA NVUE Collection, see Data Center Network Automation with Ansible. You can also try the hands-on-lab on NVIDIA Air."}], "https://developer.nvidia.com/blog/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/": [{"text": "At SIGGRAPH Asia Real-Time Live, NVIDIA researchers showcased generative AI integrated into an interactive texture painting workflow, allowing artists to paint complex textures directly on 3D objects. This prototype demonstrates how AI can function as a brush in the hands of an artist, enabling the addition of local details with infinite variations. The project aims to support creativity by developing new workflows with real-time AI inference and direct control. The AI texture painting project takes AI one step further into the interactive loop, allowing artists to directly control the placement, scale, and direction of textures through interactive painting. The prototype achieves fast inference speeds through NVIDIA technologies such as Tensor Cores and Omniverse. The project was a collaborative effort by a team of researchers and developers at NVIDIA. The demo showcases the potential of AI in enhancing creativity and providing artists with more control over their work in interactive environments.", "text_components": ["NVIDIA Research Shows Interactive Texture Painting with Gen AI at SIGGRAPH Asia Real-Time Live\nNVIDIA researchers took the stage at SIGGRAPH Asia Real-Time Live event in Sydney to showcase generative AI integrated into an interactive texture painting workflow, enabling artists to paint complex, non-repeating textures directly on the surface of 3D objects.\nRather than generating complete results with only high-level user guidance, this prototype shows how AI can function as a brush in the hands of an artist. It enables the interactive addition of local details with infinite texture variations and realistic transitions. If you missed the live show, see the prerecorded version of this demo.\nThis is one in a series of NVIDIA research projects seeking to harness the power of AI to support creativity by developing new iterative workflows with real-time AI inference and direct control. The same group showcased Gen AI Materials at SIGGRAPH in August 2023, winning the Real-Time Live show.\nAI texture painting takes AI one step further into the interactive loop. Rather than enabling you to generate and iterate on square tiling physically based rendering (PBR) materials that can then be applied to UV-mapped 3D objects, this project enables you to directly control the placement, scale, and direction of textures by interactive painting. Every patch of the 3D paint stroke is generated by AI in real time.", "Tailoring AI for creativity\nAmong all the aspects of designing tools for creativity, direct iterative control over the outcome is one of the most important. One of the challenges in integrating modern foundational image AI models into interactive workflows, such as painting, is that AI is simply too good at imagining things that may not necessarily be the artist\u2019s intent. In some cases, this can lead to the need for careful prompt engineering and unpredictable results that appear difficult to control.\nNVIDIA Omniverse interface showing a scene with a grass meadow and a house. A windy stone path goes through the meadow and includes swirls with varying stone sizes.\nFigure 1. Adding winding paths with varying brush sizes, using an AI brush seeded with the example rock texture\nIn the case of this interface, researchers opted not to include a text-based interface for either placement or identity of the texture. Following the proverb, \u201cAn image is worth a thousand words,\u201d the AI brush is conditioned on an example image of the target texture.\nInspiration images are a common concept in 3D design. These images typically serve only as a reference and must be heavily processed before they can be integrated into the 3D scene.\nThe AI Material presentation at SIGGRAPH showed how an imperfect inspiration image can be converted to a tileable PBR material, making it much easier to bring inspiration from the real world into 3D workflows. In this new demo, inspirational images of any real-world textures can be turned into AI brushes that artists can use for painting in 3D. You control not just the stroke shape, but brush size and texture direction.\nThe AI in the prototype is designed to ensure that the brushstroke includes variations of the reference, without deviating too much from its identity. The backbone foundational AI model also provides realistic transitions between regions of different textures, without any reference of such transitions. For example, AI can fill in a realistic transition between the original grass texture and the rocky path interactively painted using the AI texture brush.\nNVIDIA Omniverse interface showing a scene with a grass meadow and a pagoda. A windy stone path goes through the meadow and transitions to the pink daisies path seamlessly.\nFigure 2. Painting with pink daisies generated from text queries\nIn this Omniverse screenshot, a stone base is painted for the pagoda with a runes texture.\nFigure 3. Painting with ancient rune textures generated from text queries\nWhat if there is no inspirational image available to seed the brush?\nText-to-image AI can be used to generate several versions. You pick the exact brush you would like to use, opening up a wide array of creative possibilities with direct artist control in the interactive loop.", "Empowered by NVIDIA technologies\nSeveral NVIDIA technologies come together to enable this prototype. One of the requirements of interactive interfaces, as well as the Real-Time Live program, is speed. This prototype achieves an inference speed of 0.23-0.15s per brush stamp, enabled by accelerated inference on Tensor Cores in NVIDIA GPUs.\nThis prototype was developed as an NVIDIA Omniverse extension. Omniverse is a modular development platform of APIs and microservices for building applications and services powered by OpenUSD and NVIDIA RTX, empowering developers to build complex 3D tools incorporating AI.\nIn this case, efficient raycasting from the integrated NVIDIA Warp Library and efficient dynamic texture support allowed AI to deliver fast updates directly to the rendered object.\nUnder the hood, the method relies on the NVIDIA Kaolin Library for 3D deep learning for efficient offscreen rasterization and texture back-projection directly on the GPU.", "Acknowledgments\nThis demo is the result of a cross-team effort by Anita Hu, Nishkrit Desai, Hassan Abu Alhaija, Alexander Zook, Seung Wook Kim, Ashley Goldstein, Carsten Klove, Daniela Hasenbring, Rajeev Rao, and Masha Shugrina. Anita Hu and Alexander Zook delivered the live presentation."], "document_title": "NVIDIA Research Shows Interactive Texture Painting with Gen AI at SIGGRAPH Asia Real-Time Live", "document_url": "https://developer.nvidia.com/blog/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/", "document_date": "2023-12-11T17:00:00", "document_date_modified": "2023-12-14T19:39:03", "document_full_text": "NVIDIA Research Shows Interactive Texture Painting with Gen AI at SIGGRAPH Asia Real-Time Live\nNVIDIA researchers took the stage at SIGGRAPH Asia Real-Time Live event in Sydney to showcase generative AI integrated into an interactive texture painting workflow, enabling artists to paint complex, non-repeating textures directly on the surface of 3D objects.\nRather than generating complete results with only high-level user guidance, this prototype shows how AI can function as a brush in the hands of an artist. It enables the interactive addition of local details with infinite texture variations and realistic transitions. If you missed the live show, see the prerecorded version of this demo.\nThis is one in a series of NVIDIA research projects seeking to harness the power of AI to support creativity by developing new iterative workflows with real-time AI inference and direct control. The same group showcased Gen AI Materials at SIGGRAPH in August 2023, winning the Real-Time Live show.\nAI texture painting takes AI one step further into the interactive loop. Rather than enabling you to generate and iterate on square tiling physically based rendering (PBR) materials that can then be applied to UV-mapped 3D objects, this project enables you to directly control the placement, scale, and direction of textures by interactive painting. Every patch of the 3D paint stroke is generated by AI in real time.\nTailoring AI for creativity\nAmong all the aspects of designing tools for creativity, direct iterative control over the outcome is one of the most important. One of the challenges in integrating modern foundational image AI models into interactive workflows, such as painting, is that AI is simply too good at imagining things that may not necessarily be the artist\u2019s intent. In some cases, this can lead to the need for careful prompt engineering and unpredictable results that appear difficult to control.\nNVIDIA Omniverse interface showing a scene with a grass meadow and a house. A windy stone path goes through the meadow and includes swirls with varying stone sizes.\nFigure 1. Adding winding paths with varying brush sizes, using an AI brush seeded with the example rock texture\nIn the case of this interface, researchers opted not to include a text-based interface for either placement or identity of the texture. Following the proverb, \u201cAn image is worth a thousand words,\u201d the AI brush is conditioned on an example image of the target texture.\nInspiration images are a common concept in 3D design. These images typically serve only as a reference and must be heavily processed before they can be integrated into the 3D scene.\nThe AI Material presentation at SIGGRAPH showed how an imperfect inspiration image can be converted to a tileable PBR material, making it much easier to bring inspiration from the real world into 3D workflows. In this new demo, inspirational images of any real-world textures can be turned into AI brushes that artists can use for painting in 3D. You control not just the stroke shape, but brush size and texture direction.\nThe AI in the prototype is designed to ensure that the brushstroke includes variations of the reference, without deviating too much from its identity. The backbone foundational AI model also provides realistic transitions between regions of different textures, without any reference of such transitions. For example, AI can fill in a realistic transition between the original grass texture and the rocky path interactively painted using the AI texture brush.\nNVIDIA Omniverse interface showing a scene with a grass meadow and a pagoda. A windy stone path goes through the meadow and transitions to the pink daisies path seamlessly.\nFigure 2. Painting with pink daisies generated from text queries\nIn this Omniverse screenshot, a stone base is painted for the pagoda with a runes texture.\nFigure 3. Painting with ancient rune textures generated from text queries\nWhat if there is no inspirational image available to seed the brush?\nText-to-image AI can be used to generate several versions. You pick the exact brush you would like to use, opening up a wide array of creative possibilities with direct artist control in the interactive loop.\nEmpowered by NVIDIA technologies\nSeveral NVIDIA technologies come together to enable this prototype. One of the requirements of interactive interfaces, as well as the Real-Time Live program, is speed. This prototype achieves an inference speed of 0.23-0.15s per brush stamp, enabled by accelerated inference on Tensor Cores in NVIDIA GPUs.\nThis prototype was developed as an NVIDIA Omniverse extension. Omniverse is a modular development platform of APIs and microservices for building applications and services powered by OpenUSD and NVIDIA RTX, empowering developers to build complex 3D tools incorporating AI.\nIn this case, efficient raycasting from the integrated NVIDIA Warp Library and efficient dynamic texture support allowed AI to deliver fast updates directly to the rendered object.\nUnder the hood, the method relies on the NVIDIA Kaolin Library for 3D deep learning for efficient offscreen rasterization and texture back-projection directly on the GPU.\nAcknowledgments\nThis demo is the result of a cross-team effort by Anita Hu, Nishkrit Desai, Hassan Abu Alhaija, Alexander Zook, Seung Wook Kim, Ashley Goldstein, Carsten Klove, Daniela Hasenbring, Rajeev Rao, and Masha Shugrina. Anita Hu and Alexander Zook delivered the live presentation."}], "https://developer.nvidia.com/blog/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/": [{"text": "The NVIDIA TAO Toolkit is a powerful tool for developers to build AI-powered visual perception and computer vision applications. It supports various computer vision modalities and offers over 40 pretrained models on NVIDIA NGC for easy starting. TAO has been downloaded over 100,000 times and is now open source, allowing for more customization and integration with MLOps platforms. Major enterprises like PepsiCo, Pegatron, Siemens, and ExxonMobil are incorporating TAO into their industry-specific AI workflows. TAO can handle the latest AI models, connect with synthetically generated datasets, and provide enterprise-level support. NVIDIA offers the flexibility to deploy advanced AI models on trillions of devices at the far edge through ONNX and TFLite model export. TAO is being leveraged by ARM, STMicroelectronics, Edge Impulse, and Nota to optimize AI runtime and bring edge-optimized solutions to customers. Developers can download the TAO Toolkit and access expert help on the NVIDIA Developer TAO Forum to get started.", "text_components": ["Develop and Optimize Vision AI Models for Trillions of Devices with NVIDIA TAO\nWith NVIDIA TAO Toolkit, developers around the world are building AI-powered visual perception and computer vision applications. Now the process is faster and easier than ever, thanks to significant platform enhancements and strong ecosystem adoption.\nNVIDIA TAO Toolkit supports more than 10 computer vision and vision AI modalities, including image classification, object detection, three types of segmentation, optical character recognition (OCR), action recognition, key point estimation, body pose estimation, embedding models, Siamese networks, and more.\nGetting started with TAO Toolkit is faster than ever, with support for over 40 pretrained models on NVIDIA NGC. Recipes for leveraging TAO continue to expand with workflows for tuning models for various industries. To learn more, see Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models and Customizing AI Models: Train Character Detection and Recognition Models with NVIDIA TAO.", "Massive adoption\nTAO has been downloaded over\u202f100,000 times with nearly 1 million downloads of TAO pretrained models. And now that TAO is open source, solution providers can access more custom integration of TAO into their services and explore the inner workings of the platform with finer granularity.\nLeading MLOps and cloud services are leveraging TAO to improve their services. Integration with MLOps platforms such as Weights & Biases and ClearML helps streamline machine learning (ML) workflows, enabling better experiment tracking for model training. TAO can also work with various cloud ML and Kubernetes services such as Azure ML, GCP Vertex AI, Azure AKS, AWS EKS, and GCP GKE.\nMajor enterprises are also building TAO into their own industry-specific AI development workflows, including PepsiCo in consumer supply chain, Pegatron, Siemens, and others in manufacturing, and ExxonMobil in energy. Large cities and airports around the world also use TAO.", "Built for AI workflows\nEnterprises large and small are building their own vision AI factories and development workflows using TAO.\u202fThese AI workflows are increasingly requiring the following three capabilities now provided by TAO.\nFirst, TAO can handle the latest AI models and algorithms, including foundation model tuning, generative AI, and vision transformers. Second, TAO can seamlessly connect with synthetically generated datasets from simulation approaches such as NVIDIA Omniverse Replicator and Stable Diffusion.\u202fThird, TAO includes enterprise-level support as an essential tool in the development workflow through NVIDIA AI Enterprise.", "Advanced AI on trillions of devices\nWith TAO, NVIDIA offers the flexibility to deploy the latest AI models on trillions of devices\u202fat the far edge through ONNX and TFLite model export, along with strong ecosystem adoption from the world\u2019s leading providers and edge AI software platforms.\nARM is leveraging TAO to optimize AI runtime on Ethos NPU devices. STMicroelectronics uses TAO to run complex vision AI for the first time on STM32 microcontrollers. And partners like Edge Impulse and Nota are integrating TAO into their edge AI platforms to bring edge-optimized solutions to their customers.", "Get started\nDownload NVIDIA TAO Toolkit to get started building AI-powered visual perception and computer vision applications. Access expert help on the NVIDIA Developer TAO Forum.\nJoin the ST Edge AI Summit on December 6 to hear NVIDIA VP and GM of Embedded and Edge Computing Deepu Talla speak about building the next generation of edge AI platforms, and how NVIDIA TAO continues to evolve."], "document_title": "Develop and Optimize Vision AI Models for Trillions of Devices with NVIDIA TAO", "document_url": "https://developer.nvidia.com/blog/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/", "document_date": "2023-12-06T14:00:00", "document_date_modified": "2023-12-14T19:40:12", "document_full_text": "Develop and Optimize Vision AI Models for Trillions of Devices with NVIDIA TAO\nWith NVIDIA TAO Toolkit, developers around the world are building AI-powered visual perception and computer vision applications. Now the process is faster and easier than ever, thanks to significant platform enhancements and strong ecosystem adoption.\nNVIDIA TAO Toolkit supports more than 10 computer vision and vision AI modalities, including image classification, object detection, three types of segmentation, optical character recognition (OCR), action recognition, key point estimation, body pose estimation, embedding models, Siamese networks, and more.\nGetting started with TAO Toolkit is faster than ever, with support for over 40 pretrained models on NVIDIA NGC. Recipes for leveraging TAO continue to expand with workflows for tuning models for various industries. To learn more, see Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models and Customizing AI Models: Train Character Detection and Recognition Models with NVIDIA TAO.\nMassive adoption\nTAO has been downloaded over\u202f100,000 times with nearly 1 million downloads of TAO pretrained models. And now that TAO is open source, solution providers can access more custom integration of TAO into their services and explore the inner workings of the platform with finer granularity.\nLeading MLOps and cloud services are leveraging TAO to improve their services. Integration with MLOps platforms such as Weights & Biases and ClearML helps streamline machine learning (ML) workflows, enabling better experiment tracking for model training. TAO can also work with various cloud ML and Kubernetes services such as Azure ML, GCP Vertex AI, Azure AKS, AWS EKS, and GCP GKE.\nMajor enterprises are also building TAO into their own industry-specific AI development workflows, including PepsiCo in consumer supply chain, Pegatron, Siemens, and others in manufacturing, and ExxonMobil in energy. Large cities and airports around the world also use TAO.\nBuilt for AI workflows\nEnterprises large and small are building their own vision AI factories and development workflows using TAO.\u202fThese AI workflows are increasingly requiring the following three capabilities now provided by TAO.\nFirst, TAO can handle the latest AI models and algorithms, including foundation model tuning, generative AI, and vision transformers. Second, TAO can seamlessly connect with synthetically generated datasets from simulation approaches such as NVIDIA Omniverse Replicator and Stable Diffusion.\u202fThird, TAO includes enterprise-level support as an essential tool in the development workflow through NVIDIA AI Enterprise.\nAdvanced AI on trillions of devices\nWith TAO, NVIDIA offers the flexibility to deploy the latest AI models on trillions of devices\u202fat the far edge through ONNX and TFLite model export, along with strong ecosystem adoption from the world\u2019s leading providers and edge AI software platforms.\nARM is leveraging TAO to optimize AI runtime on Ethos NPU devices. STMicroelectronics uses TAO to run complex vision AI for the first time on STM32 microcontrollers. And partners like Edge Impulse and Nota are integrating TAO into their edge AI platforms to bring edge-optimized solutions to their customers.\nGet started\nDownload NVIDIA TAO Toolkit to get started building AI-powered visual perception and computer vision applications. Access expert help on the NVIDIA Developer TAO Forum.\nJoin the ST Edge AI Summit on December 6 to hear NVIDIA VP and GM of Embedded and Edge Computing Deepu Talla speak about building the next generation of edge AI platforms, and how NVIDIA TAO continues to evolve."}], "https://developer.nvidia.com/blog/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/": [{"text": "NVIDIA has released accelerated GNN framework containers for DGL and PyG to help with data sampling and training challenges in graph neural networks (GNNs). The containers provide GPU acceleration for data sampling, improved training performance, and a GNN Training and Deployment Tool for easier model deployment. The DGL container in the NGC catalog offers multi-arch support for x86 and ARM64 versions, with accelerated performance for midsize datasets using cuGraph-DGL. The PyTorch Geometric (PyG) container is also available with NVIDIA libraries such as cuGraph for faster data loading. Customer success stories include companies like American Express and Astellas using NVIDIA containers for fraud detection and drug discovery, respectively. Overall, the NVIDIA containers aim to accelerate GNN workflows and provide a user-friendly platform for experimentation and deployment.", "text_components": ["Available Now: NVIDIA AI Accelerated DGL and PyG Containers for GNNs\nFrom credit card transactions, social networks, and recommendation systems to transportation networks and protein-protein interactions in biology, graphs are the go-to data structure for modeling and analyzing intricate connections. Graph neural networks (GNNs), with their ability to learn and reason over graph-structured data, have emerged as a game-changer across various domains.\nHowever, uncovering the hidden patterns and valuable insights within these graphs can be challenging, especially in data sampling and end-to-end training of GNNs.\nTo address this gap, NVIDIA has released accelerated GNN framework containers for DGL and PyG with features such as:\nGPU acceleration for data sampling\nGNN Training and Deployment Tool (GNN Tool)\nThis post provides an overview of the benefits of NVIDIA accelerated DGL and PyG containers, showcases how customers are using them in production, and provides metrics on the performance.", "Introducing the DGL container in the NGC catalog\nDeep Graph Library (DGL) is one of the popular open-source libraries available for implementing and training GNNs on top of existing DL frameworks such as PyTorch.\nWe are excited to announce that DGL is now accelerated with other NVIDIA libraries and is publicly available as a container through the NGC Catalog \u2014the hub for GPU-accelerated AI/ML, HPC applications, SDKs, and tools. The catalog provides faster access to performance-optimized software and simplifies building and deploying AI solutions bringing your solutions to market faster. For more information, see 100s of Pretrained Models for AI, Digital Twins, and HPC in the NGC Catalog (video).\nThe 23.09 release of the DGL container improves data sampling and training performance for \u200cDGL users. The following are the top features of this release.", "GPU acceleration for data loader sampling\nRAPIDS cuGraph\u2019s sampler can process hundreds of billions of edges in a matter of seconds, and compute samples for thousands of batches at one time for even the world\u2019s largest GNN datasets. The DGL container comes with cuGraph-DGL, an accelerated extension to DGL, which enables users to take advantage of this incredible performance.\nEven on midsize datasets (~1B edges), cuGraph data loading performance is at least 2\u20133x faster than native DGL, based on benchmarks run with eight V100 GPUs. cuGraph-DGL sampling offers better than linear scaling for up to 100B edges, by distributing the graph across multiple nodes and multiple GPUs, also saving memory in the process.\ncuGraph can sample 100B edges in only 16 seconds! cuGraph-ops, the proprietary NVIDIA library, has accelerated GNN operators and models, such as cuGraphSAGE, cuGraphGAT, and cuGraphRGCN, cutting model forward time in half.", "GNN Training and Deployment Tool\nGNN Tool is a flexible platform for training and deploying GNN models with minimal effort. This tool, built on top of the popular Deep Graph Library (DGL) and PyTorch Geometric (PyG) frameworks, enables you to build end-to-end workflows for rapid GNN experimentation.\nIt provides a fully modular and configurable workflow that enables fast iteration and experimentation for custom GNN use cases. NVIDIA includes example notebooks in our containers for easy experimentation.", "Multi-arch support\nThe DGL containers published in NGC have both x86 and ARM64 versions to support the new NVIDIA Grace Hopper GPU. Both versions use the same container tag. When you pull the container from an Arm-based Linux system, you pull the ARM64 container.", "Example of training GNN on Grace Hopper with an ARM64-based DGL container\nThe Unified Virtual Addressing (UVA) mode in GNN training benefits tremendously from the connection between the NVIDIA Grace CPU and NVIDIA Hopper GPU in Grace Hopper. On Grace Hopper, training the same GraphSAGE model with the ogbn-papers100M dataset takes 1.9 seconds/epoch, which is about 9x faster compared to training on the H100 + Intel CPUs with PCIe connections (Table 1).\nSystem\nGH200\nH100 + Intel CPU\nA100 + AMD\n(DGX A100)\nAMD Genoa\n(CPU only)\n(seconds / epoch)\n1.9\n16.92\n24.8\n107.11\nTable 1. Comparison of training time per epoch for GraphSage model with the ogbn-papers100M dataset on Grace Hopper with UVA mode These numbers take advantage of the huge pages on Grace Hopper for the graph and its features. They were benchmarked on a 512-GB Grace Hopper node. The model is run with batch size 4096 and a (30,30) fanout (looking at up to 30 neighbors of each node in a two-layer GraphSAGE model). This is running on DGL version 1.1 on CUDA 12.1.", "DGL training performance\nOne of the challenges in training GNNs is the data loading process. In some cases, such as node classification using GraphSAGE for the ogb-papers100M dataset, the data loading process takes more than 90% of the end-to-end training time. DGL 0.8v enabled the UVA mode for efficient GPU loading of graph features, which has improved the performance since then.\nConsider the GraphSAGE model with the ogbn-products dataset for a node classification task. It has 2.4M nodes and 61.9M edges. On a DGX-1 V100 GPU, with the UVA mode, it can give up to a 20x speed-up compared to CPU-only training (Figure 1).\nA bar graph compares the training times in sec/epoch for the obgn-products dataset with UVA mode on CPU (46 seconds), one GPU (10.28 seconds), and eight GPUs (2.35 seconds).\nFigure 1. Node classification training time with the UVA mode for the obgn-products dataset\nAs the ogbn-product dataset can be loaded into GPU memory, it can be even faster with an up to 115x speed-up (Figure 2).\nA bar graph compares the training times in sec/epoch for the obgn-products dataset on CPU (46 seconds) and with GPU memory on one GPU (2.96 seconds), and eight GPUs (0.4 seconds).\nFigure 2. Node classification training time using GPU memory for the obgn-products dataset\nFor large datasets, such as ogbn-papers100M, UVA mode must be turned on.\nFigure 3 shows the per epoch training time in seconds for ogbn-papers100M for the node classification task. It has 111M nodes and 3.2B edges.\nA bar graph compares the training times in sec/epoch for obgn-papers dataset with UVA mode on CPU (107 seconds), one GPU (20.29 seconds), and eight GPUs (5.15 seconds).\nFigure 3. Node classification training time with the UVA mode for the obgn-papers dataset\nYou can find the node classification script in the NGC DGL 23.09 container under the ```/workspace/examples/``` multigpu directory.", "PyTorch Geometric container\nPyTorch Geometric (PyG) is another popular open-source library for writing and training GNNs for a wide range of applications. We are launching the PyG container accelerated with NVIDIA libraries such as cuGraph.\nOn midsize datasets (~1B edges), cuGraph data loading performance is at least 4x faster than native PyG, based on benchmarks run with eight A100 GPUs.\nWe are already observing several customers getting benefits from the PyG container, and we plan on leveraging PyG acceleration for use with NVIDIA BioNeMo models as well.", "Customer success stories\nHere\u2019s how different companies have been using the NVIDIA Accelerated DGL and PyG containers to accelerate their workflows.", "GNNs for physics-based ML\nNVIDIA Modulus is an open-source framework for building, training, and fine-tuning physics-based machine learning (ML) models in Python.\nWith the growing interest and application of GNNs across disciplines that include computational fluid dynamics, molecular dynamics simulations, and material science, NVIDIA Modulus started supporting GNNs by leveraging the DGL and cuGraphOps libraries.\nNVIDIA Modulus currently supports GNNs that include the MeshGraphNet, AeroGraphNet, and GraphCast models for mesh-based simulations and global weather forecasting. In addition to the network architectures, Modulus includes training recipes for developing models for weather forecasting, aerodynamic simulation, and vortex shedding.", "GNNs for fraud detection\nDrawing upon decades of experience, American Express has a significant track record of using AI-powered tools and models to monitor and mitigate fraud risks. They also effectively identify individuals engaged in fraudulent activities within the credit card industry.\nAt American Express AI Labs, research is continuously being conducted to gain a deeper understanding of fraudster networks through the implementation of graph-based machine learning solutions.\nThe DGL containers published on NGC enabled AmEx to experiment with a variety of GNN architectures and exploit node and edge information at scale. This results in computational efficiency when dealing with millions of nodes and billions of edges using NVIDIA libraries in a multi-node, multi-GPU environment. Moreover, the user-friendly and adaptable libraries enabled them to easily customize the components such as the loss functions, sampling techniques, and more.", "GNNs for drug discovery\nAstellas, a leading pharmaceutical company known for its forward-thinking drug development approaches, is harnessing the capabilities of GNNs for a range of pivotal tasks in drug discovery. These tasks encompass the application of generative probabilistic deep learning models diffusion-based models for 3D molecular conformation generation, feature extraction, and predictive ML models, particularly in de novo protein design and engineering.\nAs such, Astellas taps into the computational efficiency of the NVIDIA PyG and DGL containers to facilitate and amplify GNN-based AI/ML solutions in drug discovery research activities. The integration of AI and ML-based pipelines powered by PyG and DGL enabled Astellas to boost its internal capabilities, granting researchers and AI practitioners with the expertise needed for developing and implementing AI-powered cutting technologies.\nAs one use case, an Astellas scientist can achieve acceleration rates of at least 50x compared to traditional simulation-based methods for 3D molecular conformations.\nGenentech, a biotechnology company and member of the Roche Group, is using GNNs with the NVIDIA PyG container to accelerate their small molecule prediction training. The PyG container provided a reliable base for the PyG framework, which enabled the development team to focus more on development rather than setting up the dev environments.", "Next steps\nWith NVIDIA accelerated DGL and PyG containers, you can also significantly improve the data sampling and training performance of GNNs.\nTo get started, download the following resources:\nDGL container: The overview section on the catalog page provides detailed steps for pulling and running the DGL container.\nPyG container"], "document_title": "Available Now: NVIDIA AI Accelerated DGL and PyG Containers for GNNs", "document_url": "https://developer.nvidia.com/blog/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/", "document_date": "2023-12-08T22:07:12", "document_date_modified": "2023-12-14T19:27:28", "document_full_text": "Available Now: NVIDIA AI Accelerated DGL and PyG Containers for GNNs\nFrom credit card transactions, social networks, and recommendation systems to transportation networks and protein-protein interactions in biology, graphs are the go-to data structure for modeling and analyzing intricate connections. Graph neural networks (GNNs), with their ability to learn and reason over graph-structured data, have emerged as a game-changer across various domains.\nHowever, uncovering the hidden patterns and valuable insights within these graphs can be challenging, especially in data sampling and end-to-end training of GNNs.\nTo address this gap, NVIDIA has released accelerated GNN framework containers for DGL and PyG with features such as:\nGPU acceleration for data sampling\nGNN Training and Deployment Tool (GNN Tool)\nThis post provides an overview of the benefits of NVIDIA accelerated DGL and PyG containers, showcases how customers are using them in production, and provides metrics on the performance.\nIntroducing the DGL container in the NGC catalog\nDeep Graph Library (DGL) is one of the popular open-source libraries available for implementing and training GNNs on top of existing DL frameworks such as PyTorch.\nWe are excited to announce that DGL is now accelerated with other NVIDIA libraries and is publicly available as a container through the NGC Catalog \u2014the hub for GPU-accelerated AI/ML, HPC applications, SDKs, and tools. The catalog provides faster access to performance-optimized software and simplifies building and deploying AI solutions bringing your solutions to market faster. For more information, see 100s of Pretrained Models for AI, Digital Twins, and HPC in the NGC Catalog (video).\nThe 23.09 release of the DGL container improves data sampling and training performance for \u200cDGL users. The following are the top features of this release.\nGPU acceleration for data loader sampling\nRAPIDS cuGraph\u2019s sampler can process hundreds of billions of edges in a matter of seconds, and compute samples for thousands of batches at one time for even the world\u2019s largest GNN datasets. The DGL container comes with cuGraph-DGL, an accelerated extension to DGL, which enables users to take advantage of this incredible performance.\nEven on midsize datasets (~1B edges), cuGraph data loading performance is at least 2\u20133x faster than native DGL, based on benchmarks run with eight V100 GPUs. cuGraph-DGL sampling offers better than linear scaling for up to 100B edges, by distributing the graph across multiple nodes and multiple GPUs, also saving memory in the process.\ncuGraph can sample 100B edges in only 16 seconds! cuGraph-ops, the proprietary NVIDIA library, has accelerated GNN operators and models, such as cuGraphSAGE, cuGraphGAT, and cuGraphRGCN, cutting model forward time in half.\nGNN Training and Deployment Tool\nGNN Tool is a flexible platform for training and deploying GNN models with minimal effort. This tool, built on top of the popular Deep Graph Library (DGL) and PyTorch Geometric (PyG) frameworks, enables you to build end-to-end workflows for rapid GNN experimentation.\nIt provides a fully modular and configurable workflow that enables fast iteration and experimentation for custom GNN use cases. NVIDIA includes example notebooks in our containers for easy experimentation.\nMulti-arch support\nThe DGL containers published in NGC have both x86 and ARM64 versions to support the new NVIDIA Grace Hopper GPU. Both versions use the same container tag. When you pull the container from an Arm-based Linux system, you pull the ARM64 container.\nExample of training GNN on Grace Hopper with an ARM64-based DGL container\nThe Unified Virtual Addressing (UVA) mode in GNN training benefits tremendously from the connection between the NVIDIA Grace CPU and NVIDIA Hopper GPU in Grace Hopper. On Grace Hopper, training the same GraphSAGE model with the ogbn-papers100M dataset takes 1.9 seconds/epoch, which is about 9x faster compared to training on the H100 + Intel CPUs with PCIe connections (Table 1).\nSystem\nGH200\nH100 + Intel CPU\nA100 + AMD\n(DGX A100)\nAMD Genoa\n(CPU only)\n(seconds / epoch)\n1.9\n16.92\n24.8\n107.11\nTable 1. Comparison of training time per epoch for GraphSage model with the ogbn-papers100M dataset on Grace Hopper with UVA mode These numbers take advantage of the huge pages on Grace Hopper for the graph and its features. They were benchmarked on a 512-GB Grace Hopper node. The model is run with batch size 4096 and a (30,30) fanout (looking at up to 30 neighbors of each node in a two-layer GraphSAGE model). This is running on DGL version 1.1 on CUDA 12.1.\nDGL training performance\nOne of the challenges in training GNNs is the data loading process. In some cases, such as node classification using GraphSAGE for the ogb-papers100M dataset, the data loading process takes more than 90% of the end-to-end training time. DGL 0.8v enabled the UVA mode for efficient GPU loading of graph features, which has improved the performance since then.\nConsider the GraphSAGE model with the ogbn-products dataset for a node classification task. It has 2.4M nodes and 61.9M edges. On a DGX-1 V100 GPU, with the UVA mode, it can give up to a 20x speed-up compared to CPU-only training (Figure 1).\nA bar graph compares the training times in sec/epoch for the obgn-products dataset with UVA mode on CPU (46 seconds), one GPU (10.28 seconds), and eight GPUs (2.35 seconds).\nFigure 1. Node classification training time with the UVA mode for the obgn-products dataset\nAs the ogbn-product dataset can be loaded into GPU memory, it can be even faster with an up to 115x speed-up (Figure 2).\nA bar graph compares the training times in sec/epoch for the obgn-products dataset on CPU (46 seconds) and with GPU memory on one GPU (2.96 seconds), and eight GPUs (0.4 seconds).\nFigure 2. Node classification training time using GPU memory for the obgn-products dataset\nFor large datasets, such as ogbn-papers100M, UVA mode must be turned on.\nFigure 3 shows the per epoch training time in seconds for ogbn-papers100M for the node classification task. It has 111M nodes and 3.2B edges.\nA bar graph compares the training times in sec/epoch for obgn-papers dataset with UVA mode on CPU (107 seconds), one GPU (20.29 seconds), and eight GPUs (5.15 seconds).\nFigure 3. Node classification training time with the UVA mode for the obgn-papers dataset\nYou can find the node classification script in the NGC DGL 23.09 container under the ```/workspace/examples/``` multigpu directory.\nPyTorch Geometric container\nPyTorch Geometric (PyG) is another popular open-source library for writing and training GNNs for a wide range of applications. We are launching the PyG container accelerated with NVIDIA libraries such as cuGraph.\nOn midsize datasets (~1B edges), cuGraph data loading performance is at least 4x faster than native PyG, based on benchmarks run with eight A100 GPUs.\nWe are already observing several customers getting benefits from the PyG container, and we plan on leveraging PyG acceleration for use with NVIDIA BioNeMo models as well.\nCustomer success stories\nHere\u2019s how different companies have been using the NVIDIA Accelerated DGL and PyG containers to accelerate their workflows.\nGNNs for physics-based ML\nNVIDIA Modulus is an open-source framework for building, training, and fine-tuning physics-based machine learning (ML) models in Python.\nWith the growing interest and application of GNNs across disciplines that include computational fluid dynamics, molecular dynamics simulations, and material science, NVIDIA Modulus started supporting GNNs by leveraging the DGL and cuGraphOps libraries.\nNVIDIA Modulus currently supports GNNs that include the MeshGraphNet, AeroGraphNet, and GraphCast models for mesh-based simulations and global weather forecasting. In addition to the network architectures, Modulus includes training recipes for developing models for weather forecasting, aerodynamic simulation, and vortex shedding.\nGNNs for fraud detection\nDrawing upon decades of experience, American Express has a significant track record of using AI-powered tools and models to monitor and mitigate fraud risks. They also effectively identify individuals engaged in fraudulent activities within the credit card industry.\nAt American Express AI Labs, research is continuously being conducted to gain a deeper understanding of fraudster networks through the implementation of graph-based machine learning solutions.\nThe DGL containers published on NGC enabled AmEx to experiment with a variety of GNN architectures and exploit node and edge information at scale. This results in computational efficiency when dealing with millions of nodes and billions of edges using NVIDIA libraries in a multi-node, multi-GPU environment. Moreover, the user-friendly and adaptable libraries enabled them to easily customize the components such as the loss functions, sampling techniques, and more.\nGNNs for drug discovery\nAstellas, a leading pharmaceutical company known for its forward-thinking drug development approaches, is harnessing the capabilities of GNNs for a range of pivotal tasks in drug discovery. These tasks encompass the application of generative probabilistic deep learning models diffusion-based models for 3D molecular conformation generation, feature extraction, and predictive ML models, particularly in de novo protein design and engineering.\nAs such, Astellas taps into the computational efficiency of the NVIDIA PyG and DGL containers to facilitate and amplify GNN-based AI/ML solutions in drug discovery research activities. The integration of AI and ML-based pipelines powered by PyG and DGL enabled Astellas to boost its internal capabilities, granting researchers and AI practitioners with the expertise needed for developing and implementing AI-powered cutting technologies.\nAs one use case, an Astellas scientist can achieve acceleration rates of at least 50x compared to traditional simulation-based methods for 3D molecular conformations.\nGenentech, a biotechnology company and member of the Roche Group, is using GNNs with the NVIDIA PyG container to accelerate their small molecule prediction training. The PyG container provided a reliable base for the PyG framework, which enabled the development team to focus more on development rather than setting up the dev environments.\nNext steps\nWith NVIDIA accelerated DGL and PyG containers, you can also significantly improve the data sampling and training performance of GNNs.\nTo get started, download the following resources:\nDGL container: The overview section on the catalog page provides detailed steps for pulling and running the DGL container.\nPyG container"}], "https://developer.nvidia.com/blog/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/": [{"text": "The article discusses the challenges in training autonomous vehicles (AVs) due to the lack of diverse and complex driving scenarios in real-world data. It introduces EmerNeRF, a new neural radiance field-based method that uses self-supervised learning to generate dynamic driving scenes accurately. EmerNeRF decomposes scenes into static, dynamic, and flow fields, eliminating the need for human-labeled ground truth annotations and external models. By combining these elements, EmerNeRF improves scene reconstruction accuracy for both dynamic and static scenes. The article evaluates EmerNeRF's performance against other NeRF-based methods, showing significant improvements in scene reconstruction and novel view synthesis. Additionally, EmerNeRF incorporates semantic understanding using foundation models to enhance scene reconstruction and autolabeling tasks. The article concludes that EmerNeRF is a promising solution for accurately reconstructing complex driving scenarios, unlocking new capabilities in AV simulation and training.", "text_components": ["Reconstructing Dynamic Driving Scenarios Using Self-Supervised Learning\nFrom monotonous highways to routine neighborhood trips, driving is often uneventful. As a result, much of the training data for autonomous vehicle (AV) development collected in the real world is heavily skewed toward simple scenarios.\nThis poses a challenge to deploying robust perception models. AVs must be thoroughly trained, tested, and validated to handle complex situations, which requires an immense amount of data covering such scenarios.\nSimulation offers an alternative to finding and collecting such data in the real world\u2014which would be incredibly time- and cost-intensive. And yet generating complicated, dynamic scenarios at scale is still a significant hurdle.\nIn a recently released paper, NVIDIA Research shows how a new neural radiance field (NeRF)-based method, known as EmerNeRF, uses self-supervised learning to accurately generate dynamic scenarios. By training through self-supervision, EmerNeRF not only outperforms other NeRF-based methods for dynamic objects, but also for static scenes. For more details, see EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision.\nExample of EmerNeRF reconstructing dynamic driving scene: nighttime.\nExample of EmerNeRF reconstructing dynamic driving scene: busy intersection with cars.\nExample of EmerNeRF reconstructing dynamic driving scene: busy intersection with pedestrians.\nFigure 1. Examples of EmerNeRF reconstructing dynamic driving scenes\nWhen running EmerNeRF alongside similar NeRFs, it increases dynamic scene reconstruction accuracy by 15% and static scene by 11%, additionally achieving a 12% improvement for novel view synthesis.", "Addressing limitations in NeRF-based methods\nNeRFs take a set of static images and reconstruct them into a realistic 3D scene. They make it possible to create high-fidelity simulations from driving logs for closed-loop deep neural network (DNN) training, testing, and validation.\nHowever, current NeRF-based reconstruction methods struggle with dynamic objects and have proven difficult to scale. For example, while some approaches can generate both static and dynamic scenes, they require ground truth (GT) labels to do so. This means that each object in the driving logs must be accurately outlined and defined using autolabeling techniques or human annotators.\nOther NeRF methods rely on additional models to achieve complete information about a scene, such as optical flow.\nTo address these limitations, EmerNeRF uses self-supervised learning to decompose a scene into static, dynamic, and flow fields. The model learns associations and structure from raw data rather than relying on human-labeled GT annotations. It then renders both the temporal and spatial aspects of a scene simultaneously, eliminating the need for an external model to fill in the gaps while improving accuracy.\nA decomposed version of the nighttime driving scene, broken into rendered depth, decomposed dynamic RGB, decomposed dynamic depth, emerged forward flow, decomposed static RGB, and decomposed static depth.\nFigure 2. EmerNeRF breaks down the scene shown in the first video in Figure 1 into dynamic, static, and flow fields\nAs a result, while other models tend to produce over-smoothed renderings and dynamic objects with lower accuracy, EmerNeRF reconstructs high-fidelity background scenery as well as dynamic objects, all while preserving the fine details of a scene.\nDynamic-32 Split\nScene Reconstruction\nNovel View Synthesis\nMethods\nFull Image\nDynamic Only\nFull Image\nDynamic Only\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\nDPSNR\u2191\nSSIM\u2191\nD 2 NeRF\n24.35\n0.645\n21.78\n0.504\n24.17\n0.642\n21.44\n0.494\nHyperNeRF\n25.17\n0.688\n22.93\n0.569\n24.71\n0.682\n22.43\n0.554\nEmerNeRF\n28.87\n0.814\n26.19\n0.736\n27.62\n0.792\n24.18\n0.67\nTable 1. Evaluation results comparing EmerNeRF with other NeRF-based reconstruction methods for dynamic scenes, categorized into performance for scene reconstruction and novel view synthesis Static-32 Split\nMethods\nStatic Scene Reconstruction\nPSNR\u2191\nSSIM\u2191\niNGP\n24.46\n0.694\nStreetSurf\n26.15\n0.753\nEmerNeRF\n29.08\n0.803\nTable 2. Evaluation results comparing EmerNeRF with other NeRF-based reconstruction for static scenes", "The EmerNeRF approach\nUsing self-supervised learning, rather than human annotation or external models, enables EmerNeRF to bypass challenges previous methods have encountered.\nA diagram showing how EmerNeRF breaks a scene into static, dynamic, and flow fields, then reconstructs the final scene rendering simultaneously, rather than use an external model.\nFigure 3. EmerNeRF decomposition and reconstruction pipeline EmerNeRF is designed to break down a scene into dynamic and static elements. As it decomposes a scene, EmerNeRF also estimates a flow field from dynamic objects, such as cars and pedestrians, and uses this field to further improve reconstruction quality by aggregating features across time. Other approaches use external models to provide such optical flow data, which can often lead to inaccuracies.\nBy combining the static, dynamic, and flow fields all at once, EmerNeRF can represent highly dynamic scenes self-sufficiently, which improves accuracy and enables scaling to general data sources.", "Adding semantic understanding with foundation models\nEmerNeRF\u2019s semantic understanding of a scene is further strengthened using foundation models for additional supervision. Foundation models have a broad knowledge of objects (specific types of vehicles or animals, for example). EmerNeRF leverages vision transformer (ViT) models such as DINO and DINOv2 to incorporate semantic features into its scene reconstruction.\nThis enables EmerNeRF to better predict objects in a scene, as well as perform downstream tasks such as autolabeling.\nFour views of the same driving scene, clockwise: the original ground truth recording, the EmerNeRF reconstruction, the DINO semantic rendering and the DINOv2 semantic rendering.\nFigure 4. EmerNeRF uses foundation models such as DINO and DINOv2 to strengthen its semantic understanding of a scene However, transformer-based foundation models pose a new challenge: semantic features can exhibit position-dependent noise, which can significantly limit downstream task performance.\nFour views of the same driving scene, clockwise: the original ground truth recording, the EmerNeRF reconstruction, the decomposed noise-free DINO semantic rendering and the decomposed noise-free DINOv2 semantic rendering.\nFigure 5. EmerNeRF uses positional embedding to eliminate noise caused by transformer-based foundation models To solve the noise issue, EmerNeRF uses positional embedding decomposition to recover a noise-free feature map. This unlocks the full, accurate representation of foundation model semantic features, as shown in Figure 5.", "Evaluating EmerNeRF\nAs detailed in EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision, we evaluated the performance of EmerNeRF by curating a dataset of 120 unique scenarios, divided into 32 static, 32 dynamic, and 56 diverse scenes across challenging conditions such as high-speed and low-light conditions.\nEach NeRF model was then evaluated on its ability to reconstruct scenes and synthesize novel views based on different subsets of the dataset.\nAccordingly, we found EmerNeRF consistently and significantly outperformed other methods in both scene reconstruction and novel view synthesis, as shown in Table 1.\nEmerNeRF also outperformed methods specifically designed for static scenes, suggesting that self-supervised decomposing of a scene into static and dynamic elements improves static reconstruction as well as dynamic.", "Conclusion\nAV simulation is only effective if it can accurately reproduce the real world. The need for fidelity increases\u2014and becomes more challenging to achieve\u2014as scenarios become more dynamic and complex.\nEmerNeRF represents and reconstructs dynamic scenarios more accurately than previous methods, without requiring human supervision or external models. This enables reconstructing and modifying complicated driving data at scale, addressing current imbalances in AV training datasets.\nWe\u2019re eager to investigate new capabilities that EmerNeRF unlocks, including end-to-end driving, autolabeling, and simulation.\nTo learn more, visit the EmerNeRF project page and read the paper, EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision."], "document_title": "Reconstructing Dynamic Driving Scenarios Using Self-Supervised Learning", "document_url": "https://developer.nvidia.com/blog/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/", "document_date": "2023-12-05T18:30:00", "document_date_modified": "2024-01-10T18:05:00", "document_full_text": "Reconstructing Dynamic Driving Scenarios Using Self-Supervised Learning\nFrom monotonous highways to routine neighborhood trips, driving is often uneventful. As a result, much of the training data for autonomous vehicle (AV) development collected in the real world is heavily skewed toward simple scenarios.\nThis poses a challenge to deploying robust perception models. AVs must be thoroughly trained, tested, and validated to handle complex situations, which requires an immense amount of data covering such scenarios.\nSimulation offers an alternative to finding and collecting such data in the real world\u2014which would be incredibly time- and cost-intensive. And yet generating complicated, dynamic scenarios at scale is still a significant hurdle.\nIn a recently released paper, NVIDIA Research shows how a new neural radiance field (NeRF)-based method, known as EmerNeRF, uses self-supervised learning to accurately generate dynamic scenarios. By training through self-supervision, EmerNeRF not only outperforms other NeRF-based methods for dynamic objects, but also for static scenes. For more details, see EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision.\nExample of EmerNeRF reconstructing dynamic driving scene: nighttime.\nExample of EmerNeRF reconstructing dynamic driving scene: busy intersection with cars.\nExample of EmerNeRF reconstructing dynamic driving scene: busy intersection with pedestrians.\nFigure 1. Examples of EmerNeRF reconstructing dynamic driving scenes\nWhen running EmerNeRF alongside similar NeRFs, it increases dynamic scene reconstruction accuracy by 15% and static scene by 11%, additionally achieving a 12% improvement for novel view synthesis.\nAddressing limitations in NeRF-based methods\nNeRFs take a set of static images and reconstruct them into a realistic 3D scene. They make it possible to create high-fidelity simulations from driving logs for closed-loop deep neural network (DNN) training, testing, and validation.\nHowever, current NeRF-based reconstruction methods struggle with dynamic objects and have proven difficult to scale. For example, while some approaches can generate both static and dynamic scenes, they require ground truth (GT) labels to do so. This means that each object in the driving logs must be accurately outlined and defined using autolabeling techniques or human annotators.\nOther NeRF methods rely on additional models to achieve complete information about a scene, such as optical flow.\nTo address these limitations, EmerNeRF uses self-supervised learning to decompose a scene into static, dynamic, and flow fields. The model learns associations and structure from raw data rather than relying on human-labeled GT annotations. It then renders both the temporal and spatial aspects of a scene simultaneously, eliminating the need for an external model to fill in the gaps while improving accuracy.\nA decomposed version of the nighttime driving scene, broken into rendered depth, decomposed dynamic RGB, decomposed dynamic depth, emerged forward flow, decomposed static RGB, and decomposed static depth.\nFigure 2. EmerNeRF breaks down the scene shown in the first video in Figure 1 into dynamic, static, and flow fields\nAs a result, while other models tend to produce over-smoothed renderings and dynamic objects with lower accuracy, EmerNeRF reconstructs high-fidelity background scenery as well as dynamic objects, all while preserving the fine details of a scene.\nDynamic-32 Split\nScene Reconstruction\nNovel View Synthesis\nMethods\nFull Image\nDynamic Only\nFull Image\nDynamic Only\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\nDPSNR\u2191\nSSIM\u2191\nD 2 NeRF\n24.35\n0.645\n21.78\n0.504\n24.17\n0.642\n21.44\n0.494\nHyperNeRF\n25.17\n0.688\n22.93\n0.569\n24.71\n0.682\n22.43\n0.554\nEmerNeRF\n28.87\n0.814\n26.19\n0.736\n27.62\n0.792\n24.18\n0.67\nTable 1. Evaluation results comparing EmerNeRF with other NeRF-based reconstruction methods for dynamic scenes, categorized into performance for scene reconstruction and novel view synthesis Static-32 Split\nMethods\nStatic Scene Reconstruction\nPSNR\u2191\nSSIM\u2191\niNGP\n24.46\n0.694\nStreetSurf\n26.15\n0.753\nEmerNeRF\n29.08\n0.803\nTable 2. Evaluation results comparing EmerNeRF with other NeRF-based reconstruction for static scenes\nThe EmerNeRF approach\nUsing self-supervised learning, rather than human annotation or external models, enables EmerNeRF to bypass challenges previous methods have encountered.\nA diagram showing how EmerNeRF breaks a scene into static, dynamic, and flow fields, then reconstructs the final scene rendering simultaneously, rather than use an external model.\nFigure 3. EmerNeRF decomposition and reconstruction pipeline EmerNeRF is designed to break down a scene into dynamic and static elements. As it decomposes a scene, EmerNeRF also estimates a flow field from dynamic objects, such as cars and pedestrians, and uses this field to further improve reconstruction quality by aggregating features across time. Other approaches use external models to provide such optical flow data, which can often lead to inaccuracies.\nBy combining the static, dynamic, and flow fields all at once, EmerNeRF can represent highly dynamic scenes self-sufficiently, which improves accuracy and enables scaling to general data sources.\nAdding semantic understanding with foundation models\nEmerNeRF\u2019s semantic understanding of a scene is further strengthened using foundation models for additional supervision. Foundation models have a broad knowledge of objects (specific types of vehicles or animals, for example). EmerNeRF leverages vision transformer (ViT) models such as DINO and DINOv2 to incorporate semantic features into its scene reconstruction.\nThis enables EmerNeRF to better predict objects in a scene, as well as perform downstream tasks such as autolabeling.\nFour views of the same driving scene, clockwise: the original ground truth recording, the EmerNeRF reconstruction, the DINO semantic rendering and the DINOv2 semantic rendering.\nFigure 4. EmerNeRF uses foundation models such as DINO and DINOv2 to strengthen its semantic understanding of a scene However, transformer-based foundation models pose a new challenge: semantic features can exhibit position-dependent noise, which can significantly limit downstream task performance.\nFour views of the same driving scene, clockwise: the original ground truth recording, the EmerNeRF reconstruction, the decomposed noise-free DINO semantic rendering and the decomposed noise-free DINOv2 semantic rendering.\nFigure 5. EmerNeRF uses positional embedding to eliminate noise caused by transformer-based foundation models To solve the noise issue, EmerNeRF uses positional embedding decomposition to recover a noise-free feature map. This unlocks the full, accurate representation of foundation model semantic features, as shown in Figure 5.\nEvaluating EmerNeRF\nAs detailed in EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision, we evaluated the performance of EmerNeRF by curating a dataset of 120 unique scenarios, divided into 32 static, 32 dynamic, and 56 diverse scenes across challenging conditions such as high-speed and low-light conditions.\nEach NeRF model was then evaluated on its ability to reconstruct scenes and synthesize novel views based on different subsets of the dataset.\nAccordingly, we found EmerNeRF consistently and significantly outperformed other methods in both scene reconstruction and novel view synthesis, as shown in Table 1.\nEmerNeRF also outperformed methods specifically designed for static scenes, suggesting that self-supervised decomposing of a scene into static and dynamic elements improves static reconstruction as well as dynamic.\nConclusion\nAV simulation is only effective if it can accurately reproduce the real world. The need for fidelity increases\u2014and becomes more challenging to achieve\u2014as scenarios become more dynamic and complex.\nEmerNeRF represents and reconstructs dynamic scenarios more accurately than previous methods, without requiring human supervision or external models. This enables reconstructing and modifying complicated driving data at scale, addressing current imbalances in AV training datasets.\nWe\u2019re eager to investigate new capabilities that EmerNeRF unlocks, including end-to-end driving, autolabeling, and simulation.\nTo learn more, visit the EmerNeRF project page and read the paper, EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision."}], "https://developer.nvidia.com/blog/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/": [{"text": "NVIDIA has upgraded its Avatar Cloud Engine (ACE) with new AI animation and speech features, making it easier for developers to create lifelike avatars and digital humans. The new capabilities include improved emotional expressions, automatic speech recognition, text-to-speech, neural machine translation, and Audio2Face technology. These advancements allow for more natural conversations and enhanced realism in digital human interactions. The ACE suite also includes tools for AI-powered animations, enhanced speech capabilities in multiple languages, and new tooling and frameworks for developers. With these upgrades, creators can quickly build and deploy advanced avatar experiences using popular rendering tools like Unreal Engine 5. The enhancements in ACE technology raise the quality of digital human experiences and provide developers with simplified configurations to build next-generation applications. Developers interested in exploring these cutting-edge technologies can apply for early access to the ACE suite.", "text_components": ["Create Lifelike Avatars with AI Animation and Speech Features in NVIDIA ACE\nNVIDIA today unveiled major upgrades to the NVIDIA Avatar Cloud Engine (ACE) suite of technologies, bringing enhanced realism and accessibility to AI-powered avatars and digital humans. These latest animation and speech capabilities enable more natural conversations and emotional expressions.\nDevelopers can now easily implement and scale intelligent avatars across applications using new cloud APIs for automatic speech recognition (ASR), text-to-speech (TTS), neural machine translation (NMT), and Audio2Face (A2F).\nWith these advanced features, available through the early access program, creators can leverage NVIDIA technologies to rapidly build next-generation avatar experiences. It is now easier than ever to build and deploy digital humans anywhere and at scale, using some of the most popular rendering tools such as Unreal Engine 5.", "AI-powered animations with emotion\nBuild more expressive digital humans with the latest ACE AI animation features and microservices, including newly added A2F emotional support. An Animation Graph microservice for body, head, and eye movements is also now available.\nFor developers handling rendering production through the cloud or looking to do real-time inference, there is now an easy-to-use microservice. And A2F quality improvements include lip sync, bringing even more realism to digital humans.\nDiagram of an NVIDIA ACE end-to-end avatar development pipeline.\nFigure 1. NVIDIA ACE end-to-end development suite", "Enhanced AI speech capabilities\nLanguages supported now include Italian, EU Spanish, German, and Mandarin. The overall accuracy of the ASR technology has also been improved. And cloud APIs for ASR, TTS, and NMT simplify access to the latest Speech AI features.\nA new Voice Font microservice enables you to customize TTS output, whether you want to apply a custom voice to an intelligent NPC using your own voice or randomize a user\u2019s voice in a video conferencing call. This technology converts a speaker\u2019s distinct pitch and volume and converts it into a reference audio while maintaining the same patterns of rhythm and sound.", "New tooling and frameworks\nACE Agent is a streamlined dialog management and system integrator that provides a more seamless end-to-end experience, efficiently orchestrating connections between microservices. Developers also have more control over accurate, adjustable responses through integrations with NVIDIA NeMo Guardrails, NVIDIA SteerLM, and LangChain.\nIt is now easier to get these tools up and running in your renderer or coding environment of choice. New features include:\nSupport for blendshapes within the Avatar configurator to easily integrate popular renderers, including Unreal Engine.\nA new A2F application for Python users.\nA reference application for developers interested in building virtual assistants for customer service.", "Summary\nThese newly introduced NVIDIA ACE features raise the quality bar for digital human experiences. With enhancements that make building and deployment easier, developers now have simplified configurations necessary to build next-generation digital human applications.\nInterested in exploring cutting-edge digital human technologies? Apply for early access."], "document_title": "Create Lifelike Avatars with AI Animation and Speech Features in NVIDIA ACE", "document_url": "https://developer.nvidia.com/blog/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/", "document_date": "2023-12-04T22:00:00", "document_date_modified": "2023-12-14T19:41:01", "document_full_text": "Create Lifelike Avatars with AI Animation and Speech Features in NVIDIA ACE\nNVIDIA today unveiled major upgrades to the NVIDIA Avatar Cloud Engine (ACE) suite of technologies, bringing enhanced realism and accessibility to AI-powered avatars and digital humans. These latest animation and speech capabilities enable more natural conversations and emotional expressions.\nDevelopers can now easily implement and scale intelligent avatars across applications using new cloud APIs for automatic speech recognition (ASR), text-to-speech (TTS), neural machine translation (NMT), and Audio2Face (A2F).\nWith these advanced features, available through the early access program, creators can leverage NVIDIA technologies to rapidly build next-generation avatar experiences. It is now easier than ever to build and deploy digital humans anywhere and at scale, using some of the most popular rendering tools such as Unreal Engine 5.\nAI-powered animations with emotion\nBuild more expressive digital humans with the latest ACE AI animation features and microservices, including newly added A2F emotional support. An Animation Graph microservice for body, head, and eye movements is also now available.\nFor developers handling rendering production through the cloud or looking to do real-time inference, there is now an easy-to-use microservice. And A2F quality improvements include lip sync, bringing even more realism to digital humans.\nDiagram of an NVIDIA ACE end-to-end avatar development pipeline.\nFigure 1. NVIDIA ACE end-to-end development suite\nEnhanced AI speech capabilities\nLanguages supported now include Italian, EU Spanish, German, and Mandarin. The overall accuracy of the ASR technology has also been improved. And cloud APIs for ASR, TTS, and NMT simplify access to the latest Speech AI features.\nA new Voice Font microservice enables you to customize TTS output, whether you want to apply a custom voice to an intelligent NPC using your own voice or randomize a user\u2019s voice in a video conferencing call. This technology converts a speaker\u2019s distinct pitch and volume and converts it into a reference audio while maintaining the same patterns of rhythm and sound.\nNew tooling and frameworks\nACE Agent is a streamlined dialog management and system integrator that provides a more seamless end-to-end experience, efficiently orchestrating connections between microservices. Developers also have more control over accurate, adjustable responses through integrations with NVIDIA NeMo Guardrails, NVIDIA SteerLM, and LangChain.\nIt is now easier to get these tools up and running in your renderer or coding environment of choice. New features include:\nSupport for blendshapes within the Avatar configurator to easily integrate popular renderers, including Unreal Engine.\nA new A2F application for Python users.\nA reference application for developers interested in building virtual assistants for customer service.\nSummary\nThese newly introduced NVIDIA ACE features raise the quality bar for digital human experiences. With enhancements that make building and deployment easier, developers now have simplified configurations necessary to build next-generation digital human applications.\nInterested in exploring cutting-edge digital human technologies? Apply for early access."}], "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/": [{"text": "NVIDIA has made significant enhancements to its TensorRT-LLM software, resulting in massive speedups for large language models (LLMs) on the NVIDIA H200 GPU. These improvements include optimizations for both compute throughput and memory usage, allowing for significant improvements in LLM inference performance. The latest enhancements have led to a 6.7x speedup on the Llama 2 70B model and the ability to run the Falcon-180B model on a single GPU. These advancements are achieved through techniques like Grouped Query Attention (GQA) and custom INT4 AWQ, which reduce memory usage while maintaining high accuracy. The improvements in TensorRT-LLM software have resulted in a 2.4x increase in performance compared to previous versions. These enhancements will be included in upcoming releases of TensorRT-LLM and are aimed at optimizing GPU compute resources and reducing operational costs for deploying large language models.", "text_components": ["NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\nLarge language models (LLMs) have seen dramatic growth over the last year, and the challenge of delivering great user experiences depends on both high-compute throughput as well as large amounts of high-bandwidth memory. NVIDIA TensorRT-LLM provides optimizations for both peak throughput and memory optimization, delivering massive improvements in LLM inference performance.\nThe latest TensorRT-LLM enhancements on NVIDIA H200 GPUs deliver a 6.7x speedup on the Llama 2 70B LLM, and enable huge models, like Falcon-180B, to run on a single GPU. Llama 2 70B acceleration stems from optimizing a technique called Grouped Query Attention (GQA)\u2014an extension of multi-head attention techniques\u2014which is the key layer in Llama 2 70B.\nFalcon-180B is one of the largest and most accurate open-source large language models available, and previously required a minimum of eight NVIDIA A100 Tensor Core GPUs to run it.\nTensorRT-LLM advancements in a custom INT4 AWQ make it possible to run entirely on a single H200 Tensor Core GPU, featuring 141 GB of the latest HBM3e memory with nearly 5 TB/s of memory bandwidth.\nIn this post, we share the latest TensorRT-LLM innovations and the performance they\u2019re bringing to two popular LLMs, Llama 2 70B and Falcon-180B.", "Llama 2 70B on H200 delivers a 6.7x performance boost\nThe latest version of TensorRT-LLM features improved group query attention (GQA) kernels in the generation phase, providing up to a 6.7x performance boost with H200 compared to the same network running on an NVIDIA A100 GPU.\nUsed in Llama 2 70B, GQA is a variant of multi-head attention (MHA) that groups key-value (KV) heads together, resulting in fewer KV heads than query (Q) heads. TensorRT-LLM has a custom implementation of MHA that supports GQA, multi-query attention (MQA), and standard MHA.\nIt leverages NVIDIA Tensor Cores, in the generation and context phases, and delivers great performance on NVIDIA GPUs.\nA chart showing the Llama 2 70B inference throughputs and speedups baselined to A100 across various input/output sequence lengths; H200 with the latest release of TensorRT-LLM achieves up to 6.7x more throughput compared to A100 using the same TensorRT-LLM version.\nFigure 1. Llama 2 70B inference throughput on H200 across various input/output sequence length sizes\nH200 Llama 2 70B Inference Performance\nInput Sequence Length\nOutput Sequence Length\nThroughput (Tokens/s per GPU)\n128\n128\n3,803\n128\n2048\n3,163\n128\n4096\n2,263\nTable 1. Llama 2 70B H200 inference throughput per GPU at different input sequence lengths When evaluating LLM performance, it\u2019s important to consider different input and output sequence lengths, which vary depending on the specific application where the LLM is being deployed. As we increase the output sequence length, raw throughput decreases as expected, however, the performance speedup compared to A100 increases significantly.\nImprovements in TensorRT-LLM software alone are bringing a 2.4x improvement compared to the previous version running on H200.", "Falcon-180B performance examined\nLLMs place both significant compute and memory demands on data center systems, and with the ongoing growth of these models, this problem will persist for some time to come. There are many techniques that developers are evolving to help address this challenge.\nOne of these is INT4 Activation-aware Weight Quantization (AWQ) ( Lin et al., 2023 ). This quantization technique compresses the weights of an LLM down to just four bits based on their relative importance and then performs the computation in FP16.\nThis approach enables AWQ to maintain higher accuracy than other 4-bit methods while also reducing memory usage. To achieve this, special kernels capable of handling the change in precision at high performance are required.\nThe latest release of TensorRT-LLM implements custom kernels for AWQ. It takes the technique a step further, performing the computations in FP8 precision on NVIDIA Hopper GPUs instead of FP16, using the latest Hopper Tensor Core technology.\nA chart showing the achieved inference throughput in tokens/second when running Falcon-180B on a single H200 GPU for two different batch size/sequence length combinations. The first shows throughput of 798 tokens/second using a batch size of 256 and input/output sequence lengths of 128. The second shows throughput of 664 tokens/second using a batch size of 128, input sequence length of 128 and output length of 2048.\nFigure 2. Falcon-180B inference throughput on a single H200 GPU\nThese are the results seen running INT4 AWQ with FP8 on a single H200. In addition to being able to fit the entire Falcon-180B model, H200 also runs the model with excellent inference throughput of up to 800 tokens/second.", "Stay on target\nQuantization can often hurt model accuracy. However, TensorRT-LLM AWQ achieves a nearly 4x reduction in memory footprint and excellent inference throughput all while maintaining exceptional accuracy.\nA chart showing the accuracy scores on Falcon-180B at FP16, FP8, and INT4 AWQ, across the following accuracy metrics: Rogue1, Rogue2, RogueL, RogueLsum, and MMLU.\nFigure 3. Accuracy scores at FP16, FP8, and INT4 AWQ\nTensorRT-LLM v0.7a | Falcon-180B | 1xH200 TP1 | INT4 AWQ\nAccuracy stays at or above 95% compared to running at higher precision, while delivering higher performance, and making the best use of GPU compute resources by fitting the entire model onto a single GPU. Making efficient use of the GPUs on deployed applications makes optimal use of compute resources, and helps reduce operational costs as well.", "Ongoing work\nThese improvements will be available soon in TensorRT-LLM, and will be included in the v0.7 and v0.8 releases. Similar examples running Llama 2 70B in TensorRT-LLM are available on the TensorRT-LLM GitHub page.\nFor more information, visit the NVIDIA H200 Tensor Core GPU product page.\nThis blog post has been adapted from a technical post on the TensorRT-LLM GitHub: Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100"], "document_title": "NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200", "document_url": "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/", "document_date": "2023-12-05T01:11:43", "document_date_modified": "2023-12-14T19:27:30", "document_full_text": "NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\nLarge language models (LLMs) have seen dramatic growth over the last year, and the challenge of delivering great user experiences depends on both high-compute throughput as well as large amounts of high-bandwidth memory. NVIDIA TensorRT-LLM provides optimizations for both peak throughput and memory optimization, delivering massive improvements in LLM inference performance.\nThe latest TensorRT-LLM enhancements on NVIDIA H200 GPUs deliver a 6.7x speedup on the Llama 2 70B LLM, and enable huge models, like Falcon-180B, to run on a single GPU. Llama 2 70B acceleration stems from optimizing a technique called Grouped Query Attention (GQA)\u2014an extension of multi-head attention techniques\u2014which is the key layer in Llama 2 70B.\nFalcon-180B is one of the largest and most accurate open-source large language models available, and previously required a minimum of eight NVIDIA A100 Tensor Core GPUs to run it.\nTensorRT-LLM advancements in a custom INT4 AWQ make it possible to run entirely on a single H200 Tensor Core GPU, featuring 141 GB of the latest HBM3e memory with nearly 5 TB/s of memory bandwidth.\nIn this post, we share the latest TensorRT-LLM innovations and the performance they\u2019re bringing to two popular LLMs, Llama 2 70B and Falcon-180B.\nLlama 2 70B on H200 delivers a 6.7x performance boost\nThe latest version of TensorRT-LLM features improved group query attention (GQA) kernels in the generation phase, providing up to a 6.7x performance boost with H200 compared to the same network running on an NVIDIA A100 GPU.\nUsed in Llama 2 70B, GQA is a variant of multi-head attention (MHA) that groups key-value (KV) heads together, resulting in fewer KV heads than query (Q) heads. TensorRT-LLM has a custom implementation of MHA that supports GQA, multi-query attention (MQA), and standard MHA.\nIt leverages NVIDIA Tensor Cores, in the generation and context phases, and delivers great performance on NVIDIA GPUs.\nA chart showing the Llama 2 70B inference throughputs and speedups baselined to A100 across various input/output sequence lengths; H200 with the latest release of TensorRT-LLM achieves up to 6.7x more throughput compared to A100 using the same TensorRT-LLM version.\nFigure 1. Llama 2 70B inference throughput on H200 across various input/output sequence length sizes\nH200 Llama 2 70B Inference Performance\nInput Sequence Length\nOutput Sequence Length\nThroughput (Tokens/s per GPU)\n128\n128\n3,803\n128\n2048\n3,163\n128\n4096\n2,263\nTable 1. Llama 2 70B H200 inference throughput per GPU at different input sequence lengths When evaluating LLM performance, it\u2019s important to consider different input and output sequence lengths, which vary depending on the specific application where the LLM is being deployed. As we increase the output sequence length, raw throughput decreases as expected, however, the performance speedup compared to A100 increases significantly.\nImprovements in TensorRT-LLM software alone are bringing a 2.4x improvement compared to the previous version running on H200.\nFalcon-180B performance examined\nLLMs place both significant compute and memory demands on data center systems, and with the ongoing growth of these models, this problem will persist for some time to come. There are many techniques that developers are evolving to help address this challenge.\nOne of these is INT4 Activation-aware Weight Quantization (AWQ) ( Lin et al., 2023 ). This quantization technique compresses the weights of an LLM down to just four bits based on their relative importance and then performs the computation in FP16.\nThis approach enables AWQ to maintain higher accuracy than other 4-bit methods while also reducing memory usage. To achieve this, special kernels capable of handling the change in precision at high performance are required.\nThe latest release of TensorRT-LLM implements custom kernels for AWQ. It takes the technique a step further, performing the computations in FP8 precision on NVIDIA Hopper GPUs instead of FP16, using the latest Hopper Tensor Core technology.\nA chart showing the achieved inference throughput in tokens/second when running Falcon-180B on a single H200 GPU for two different batch size/sequence length combinations. The first shows throughput of 798 tokens/second using a batch size of 256 and input/output sequence lengths of 128. The second shows throughput of 664 tokens/second using a batch size of 128, input sequence length of 128 and output length of 2048.\nFigure 2. Falcon-180B inference throughput on a single H200 GPU\nThese are the results seen running INT4 AWQ with FP8 on a single H200. In addition to being able to fit the entire Falcon-180B model, H200 also runs the model with excellent inference throughput of up to 800 tokens/second.\nStay on target\nQuantization can often hurt model accuracy. However, TensorRT-LLM AWQ achieves a nearly 4x reduction in memory footprint and excellent inference throughput all while maintaining exceptional accuracy.\nA chart showing the accuracy scores on Falcon-180B at FP16, FP8, and INT4 AWQ, across the following accuracy metrics: Rogue1, Rogue2, RogueL, RogueLsum, and MMLU.\nFigure 3. Accuracy scores at FP16, FP8, and INT4 AWQ\nTensorRT-LLM v0.7a | Falcon-180B | 1xH200 TP1 | INT4 AWQ\nAccuracy stays at or above 95% compared to running at higher precision, while delivering higher performance, and making the best use of GPU compute resources by fitting the entire model onto a single GPU. Making efficient use of the GPUs on deployed applications makes optimal use of compute resources, and helps reduce operational costs as well.\nOngoing work\nThese improvements will be available soon in TensorRT-LLM, and will be included in the v0.7 and v0.8 releases. Similar examples running Llama 2 70B in TensorRT-LLM are available on the TensorRT-LLM GitHub page.\nFor more information, visit the NVIDIA H200 Tensor Core GPU product page.\nThis blog post has been adapted from a technical post on the TensorRT-LLM GitHub: Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100"}], "https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/": [{"text": "The article discusses the new features of the NVIDIA NeMo framework and the performance improvements achieved with the NVIDIA H200 GPUs in training large language models (LLMs). The NeMo framework is a cloud-native platform for building and deploying generative AI models, incorporating advanced parallelism techniques for efficient training at scale. The latest release of NeMo includes optimizations and new features that significantly improve performance on NVIDIA AI Foundation Models and expand model architecture support. The introduction of Fully Sharded Data Parallelism (FSDP) and Mixture of Experts (MoE) architectures further enhance the training capabilities of NeMo. Additionally, the article highlights the use of reinforcement learning from human feedback (RLHF) with TensorRT-LLM for improved inference performance. The NVIDIA AI platform continues to push the boundaries of generative AI with top performance and versatility, making it the platform of choice for developing and deploying advanced AI applications. The NeMo framework is available as an open-source library on GitHub and as part of the NVIDIA AI Enterprise software platform.", "text_components": ["New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility\nThe rapid growth in the size, complexity, and diversity of large language models (LLMs) continues to drive an insatiable need for AI training performance. Delivering top performance requires the ability to train models at the scale of an entire data center efficiently. This is achieved through exceptional craftsmanship at every layer of the technology stack, spanning chips, systems, and software.\nThe NVIDIA NeMo framework is an end-to-end, cloud-native framework for building, customizing, and deploying generative AI models. It incorporates a full array of advanced parallelism techniques to enable efficient training of LLMs at scale.\nIn fact, NeMo powered the exceptional GPT-3 175B performance submissions by NVIDIA in the latest MLPerf Training industry-standard benchmarks, achieving up to 797 TFLOPS per H100 GPU. And, in the largest scale submitted by NVIDIA, record performance and near-linear performance scaling were achieved using an unprecedented 10,752 H100 Tensor Core GPUs.\nToday, NVIDIA is announcing that the upcoming January release of the NeMo framework incorporates a host of optimizations and new features. These dramatically improve performance on NVIDIA AI Foundation Models, including Llama 2, Nemotron-3, and other LLMs, and expand NeMo model architecture support. It also provides a much-requested parallelism technique, making it even easier to train various models on the NVIDIA AI platform.", "Up to 4.2x faster Llama 2 70B pre-training and supervised fine-tuning\nLlama 2 is a popular, open-source large language model originally developed by Meta. The latest release of NeMo includes many improvements that increase Llama 2 performance. Compared to the prior NeMo release running on A100 GPUs, the latest NeMo release running on H200 GPUs delivers up to 4.2x faster Llama 2 pre-training and supervised fine-tuning performance.\nThe first improvement is the addition of mixed-precision implementations of the model optimizer\u2019s state. This reduces model capacity requirements and improves the effective memory bandwidth for operations that interact with the model state by 1.8x.\nThe performance of rotary positional embedding (RoPE) operations\u2014state-of-the-art algorithms employed by many recent LLM architectures\u2014has also increased. Additionally, the performance of Swish-Gated Linear Unit (SwiGLU) activation functions is optimized, which commonly substitutes Gaussian Error Linear Unit (GELU) in modern LLMs.\nFinally, the communication efficiency for tensor parallelism has been greatly improved, and communication chunk sizes for pipeline parallelism have been tuned.\nCollectively, these improvements dramatically increase Tensor Core usage on GPUs based on the NVIDIA Hopper architecture, achieving up to 836 TFLOPS per H200 GPU for Llama 2 70B pre-training and supervised fine-tuning.\nA chart showing that H200 with the latest release of NeMo delivers 779 TFLOPS, 822 TFLOPS, and 836 TFLOPs on Llama 2 7B, 13B, and 70B, respectively. This compares to 211 TFLOPS, 206 TFLOPS, and 201 TFLOPS on these models, respectively, on A100 and the prior NeMo release.\nFigure 1. Training performance, in model TFLOPS per GPU, on the Llama 2 family of models (7B, 13B, and 70B) on H200 using the latest NeMo release compared to performance on A100 using the prior NeMo release\nMeasured performance per GPU. Global Batch Size = 128.\nLlama 2 7B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 13B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 70B: Sequence Length 4096 | A100 32x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nH200 GPUs, coupled with the latest version of NeMo, can achieve exceptional Llama 2 training throughput, delivering up to a 4.2x uplift compared to A100 GPUs running the prior NeMo release.\nTraining Tokens/Sec/GPU\nLlama 2 7B\nLlama 2 13B\nLlama 2 70B\nH200 (Latest NeMo)\n16,913\n9,432\n1,880\nA100 (Prior NeMo)\n4,583\n2,357\n451\nSpeedup\n3.7X\n4.0X\n4.2X\nTable 1. Training performance, in tokens per second per GPU\nMeasured performance per GPU. Global Batch Size = 128.\nLlama 2 7B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 13B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 70B: Sequence Length 4096 | A100 32x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha Putting this performance into context, a single system based on the eight-way NVIDIA HGX H200 can fine-tune Llama 2 with 70B parameters on sequences of length 4096 at a rate of over 15,000 tokens/second. This means that it can complete a supervised fine-tuning task consisting of 1B tokens in just over 18 hours.", "Fully Sharded Data Parallelism\nFully Sharded Data Parallelism (FSDP) is a well-known and popular feature within the deep learning community. It is used by deep learning practitioners across major frameworks, including PyTorch, DeepSpeed, and JAX, and is applied to a wide variety of models.\nFSDP can be applied to a wide variety of models, and is particularly useful for LLMs, as the compute and memory requirements of modern LLMs are well beyond the scope of even a single, advanced GPU.\nPipelining a model is an effective performance optimization. But it requires that a model has a very regular structure (for example, the same layer repeated 128 times) because different layers are distributed to different GPUs and data flows between them in a pipelined manner.\nFSDP provides developers with improved usability and minimal performance loss across various situations. This is because the data and memory of a model are distributed on a per-layer basis, which makes it easier to manage regular or irregular neural network structures.\nA natural extension of data parallelism, FSDP can often be used through simple model wrappers, without needing to consider how a model is partitioned (as is the case in pipeline parallelism). This also makes it easier to extend FSDP to new and emerging model architectures, such as multi-modal LLMs.\nFSDP can also achieve performance competitive with traditional combinations of tensor parallelism and pipeline parallelism methods when there is sufficient parallelism at scales smaller than the global batch size.\nGPT-20B Training Performance (BF16)\n# of H100 GPUs\n8\n16\n32\n64\n128\nFSDP\n0.78x\n0.88x\n0.89x\n0.86x\n0.96x\n3D Parallelism\n1.0x\n1.0x\n1.0x\n1.0x\n1.0x\nTable 2: Measured relative training performance on H100 GPUs when using FSDP compared to using 3D Parallelism Measured performance. Global Batch Size = 256, Sequence Length = 2048", "Mixture of Experts\nOne proven method of improving the information absorption and generalization capabilities of generative AI models is to increase the number of parameters in the model. However, a challenge that emerges with larger models is that as their capacity increases, the compute required to perform inference also grows, increasing the cost to run the models in production.\nRecently, a mechanism called Mixture of Experts (MoE) has gained significant attention. It enables model capacity to be increased without a proportional increase in both the training and inference compute requirements. MoE architectures achieve this through a conditional computation approach where each input token is routed to only one or a few expert neural network layers instead of being routed through all of them. This decouples model capacity from required compute.\nThe latest release of NeMo introduces official support for MoE-based LLM architectures with expert parallelism. This implementation uses an architecture similar to that of Balanced Assignment of Experts (BASE) where each MoE layer routes each token to exactly one expert and uses algorithmic load balancing. NeMo uses Sinkhorn-based routing to balance the token load across the various experts.\nMoE models based on NeMo support expert parallelism, which can be used in combination with data parallelism to distribute MoE experts across data parallel ranks. NeMo also provides the ability to configure expert parallelism arbitrarily. Users can map experts to different GPUs in various ways without restricting the number of experts on a single device (all devices, however, must contain the same number of experts). NeMo also supports cases where the expert parallel size is less than the data parallel size.\nDevelopers can use the NeMo expert parallelism method in combination with the many other parallelism dimensions offered by NeMo including tensor, pipeline, and sequence parallelism. This facilitates efficient training of models with more than a trillion parameters on clusters with many NVIDIA GPUs.", "RLHF with TensorRT-LLM\nNeMo support for reinforcement learning from human feedback (RLHF) has now been enhanced with the ability to use TensorRT-LLM for inference inside of the RLHF loop.\nTensorRT-LLM accelerates the inference stage of the actor model, which currently takes most of the end-to-end compute time. The actor model is the model of interest that is being aligned and will be the ultimate output of the RLHF process.\nThe upcoming NeMo release enables pipeline parallelism for RLHF through TensorRT-LLM, enabling it to achieve better performance with fewer nodes, all while also supporting larger models.\nIn fact, for the Llama 2 70B parameter model, using TensorRT-LLM in the RLHF loop with H100 GPUs enables up to a 5.6x performance increase compared to RLHF without TensorRT-LLM in the loop on the same H100 GPUs.\nA chart showing the relative performance of H100 using the latest NeMo release for RLHF across four different models\u2013GPT-2B, Llama 2 7B, Llama 2 13B, and Llama 2 70B\u2013with respective H100 GPU counts of 16, 16, 32, and 128 compared to the prior NeMo release. Performance increases are 5x, 2.2x, 3.1x, and 5.6x, respectively.\nFigure 2. The latest version of NeMo brings dramatic improvements compared to the previous version, accelerating the inference part of the RLHF training operation\nMeasured performance. Global Batch Size = 64, Rollout Size = 512, Maximum Generation Length = 1024.\nHalf of the nodes for each result run the actor and the other half run the critic in the RLHF algorithm implemented.", "Pushing the boundaries of generative AI\nAI training requires a full-stack approach. NVIDIA NeMo is regularly updated to provide optimal performance for training advanced generative AI models. It incorporates the most recent training methods to improve performance and provide more flexibility for NVIDIA platform users.\nThe NVIDIA platform is also incredibly versatile and accelerates the entire AI workflow end-to-end, from data prep to model training to deploying inference. Following the introduction of TensorRT-LLM in October, NVIDIA recently demonstrated the ability to run the latest Falcon-180B model on a single H200 GPU, leveraging TensorRT-LLM\u2019s advanced 4-bit quantization feature, while maintaining 99% accuracy. Read more about this implementation in the latest post about TensorRT-LLM.\nThe NVIDIA AI platform continues to advance performance, versatility, and features at the speed of light. That\u2019s why it is the platform of choice for developing and deploying today\u2019s generative AI applications and inventing the models and techniques that are powering what comes next.", "Get started with NeMo framework\nThe NVIDIA NeMo framework is available as an open-source library on GitHub, a container on NGC, and as part of NVIDIA AI Enterprise, an enterprise-grade AI software platform with security, stability, manageability, and support."], "document_title": "New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility", "document_url": "https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/", "document_date": "2023-12-04T18:00:00", "document_date_modified": "2023-12-14T19:27:31", "document_full_text": "New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility\nThe rapid growth in the size, complexity, and diversity of large language models (LLMs) continues to drive an insatiable need for AI training performance. Delivering top performance requires the ability to train models at the scale of an entire data center efficiently. This is achieved through exceptional craftsmanship at every layer of the technology stack, spanning chips, systems, and software.\nThe NVIDIA NeMo framework is an end-to-end, cloud-native framework for building, customizing, and deploying generative AI models. It incorporates a full array of advanced parallelism techniques to enable efficient training of LLMs at scale.\nIn fact, NeMo powered the exceptional GPT-3 175B performance submissions by NVIDIA in the latest MLPerf Training industry-standard benchmarks, achieving up to 797 TFLOPS per H100 GPU. And, in the largest scale submitted by NVIDIA, record performance and near-linear performance scaling were achieved using an unprecedented 10,752 H100 Tensor Core GPUs.\nToday, NVIDIA is announcing that the upcoming January release of the NeMo framework incorporates a host of optimizations and new features. These dramatically improve performance on NVIDIA AI Foundation Models, including Llama 2, Nemotron-3, and other LLMs, and expand NeMo model architecture support. It also provides a much-requested parallelism technique, making it even easier to train various models on the NVIDIA AI platform.\nUp to 4.2x faster Llama 2 70B pre-training and supervised fine-tuning\nLlama 2 is a popular, open-source large language model originally developed by Meta. The latest release of NeMo includes many improvements that increase Llama 2 performance. Compared to the prior NeMo release running on A100 GPUs, the latest NeMo release running on H200 GPUs delivers up to 4.2x faster Llama 2 pre-training and supervised fine-tuning performance.\nThe first improvement is the addition of mixed-precision implementations of the model optimizer\u2019s state. This reduces model capacity requirements and improves the effective memory bandwidth for operations that interact with the model state by 1.8x.\nThe performance of rotary positional embedding (RoPE) operations\u2014state-of-the-art algorithms employed by many recent LLM architectures\u2014has also increased. Additionally, the performance of Swish-Gated Linear Unit (SwiGLU) activation functions is optimized, which commonly substitutes Gaussian Error Linear Unit (GELU) in modern LLMs.\nFinally, the communication efficiency for tensor parallelism has been greatly improved, and communication chunk sizes for pipeline parallelism have been tuned.\nCollectively, these improvements dramatically increase Tensor Core usage on GPUs based on the NVIDIA Hopper architecture, achieving up to 836 TFLOPS per H200 GPU for Llama 2 70B pre-training and supervised fine-tuning.\nA chart showing that H200 with the latest release of NeMo delivers 779 TFLOPS, 822 TFLOPS, and 836 TFLOPs on Llama 2 7B, 13B, and 70B, respectively. This compares to 211 TFLOPS, 206 TFLOPS, and 201 TFLOPS on these models, respectively, on A100 and the prior NeMo release.\nFigure 1. Training performance, in model TFLOPS per GPU, on the Llama 2 family of models (7B, 13B, and 70B) on H200 using the latest NeMo release compared to performance on A100 using the prior NeMo release\nMeasured performance per GPU. Global Batch Size = 128.\nLlama 2 7B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 13B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 70B: Sequence Length 4096 | A100 32x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nH200 GPUs, coupled with the latest version of NeMo, can achieve exceptional Llama 2 training throughput, delivering up to a 4.2x uplift compared to A100 GPUs running the prior NeMo release.\nTraining Tokens/Sec/GPU\nLlama 2 7B\nLlama 2 13B\nLlama 2 70B\nH200 (Latest NeMo)\n16,913\n9,432\n1,880\nA100 (Prior NeMo)\n4,583\n2,357\n451\nSpeedup\n3.7X\n4.0X\n4.2X\nTable 1. Training performance, in tokens per second per GPU\nMeasured performance per GPU. Global Batch Size = 128.\nLlama 2 7B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 13B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha\nLlama 2 70B: Sequence Length 4096 | A100 32x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha Putting this performance into context, a single system based on the eight-way NVIDIA HGX H200 can fine-tune Llama 2 with 70B parameters on sequences of length 4096 at a rate of over 15,000 tokens/second. This means that it can complete a supervised fine-tuning task consisting of 1B tokens in just over 18 hours.\nFully Sharded Data Parallelism\nFully Sharded Data Parallelism (FSDP) is a well-known and popular feature within the deep learning community. It is used by deep learning practitioners across major frameworks, including PyTorch, DeepSpeed, and JAX, and is applied to a wide variety of models.\nFSDP can be applied to a wide variety of models, and is particularly useful for LLMs, as the compute and memory requirements of modern LLMs are well beyond the scope of even a single, advanced GPU.\nPipelining a model is an effective performance optimization. But it requires that a model has a very regular structure (for example, the same layer repeated 128 times) because different layers are distributed to different GPUs and data flows between them in a pipelined manner.\nFSDP provides developers with improved usability and minimal performance loss across various situations. This is because the data and memory of a model are distributed on a per-layer basis, which makes it easier to manage regular or irregular neural network structures.\nA natural extension of data parallelism, FSDP can often be used through simple model wrappers, without needing to consider how a model is partitioned (as is the case in pipeline parallelism). This also makes it easier to extend FSDP to new and emerging model architectures, such as multi-modal LLMs.\nFSDP can also achieve performance competitive with traditional combinations of tensor parallelism and pipeline parallelism methods when there is sufficient parallelism at scales smaller than the global batch size.\nGPT-20B Training Performance (BF16)\n# of H100 GPUs\n8\n16\n32\n64\n128\nFSDP\n0.78x\n0.88x\n0.89x\n0.86x\n0.96x\n3D Parallelism\n1.0x\n1.0x\n1.0x\n1.0x\n1.0x\nTable 2: Measured relative training performance on H100 GPUs when using FSDP compared to using 3D Parallelism Measured performance. Global Batch Size = 256, Sequence Length = 2048\nMixture of Experts\nOne proven method of improving the information absorption and generalization capabilities of generative AI models is to increase the number of parameters in the model. However, a challenge that emerges with larger models is that as their capacity increases, the compute required to perform inference also grows, increasing the cost to run the models in production.\nRecently, a mechanism called Mixture of Experts (MoE) has gained significant attention. It enables model capacity to be increased without a proportional increase in both the training and inference compute requirements. MoE architectures achieve this through a conditional computation approach where each input token is routed to only one or a few expert neural network layers instead of being routed through all of them. This decouples model capacity from required compute.\nThe latest release of NeMo introduces official support for MoE-based LLM architectures with expert parallelism. This implementation uses an architecture similar to that of Balanced Assignment of Experts (BASE) where each MoE layer routes each token to exactly one expert and uses algorithmic load balancing. NeMo uses Sinkhorn-based routing to balance the token load across the various experts.\nMoE models based on NeMo support expert parallelism, which can be used in combination with data parallelism to distribute MoE experts across data parallel ranks. NeMo also provides the ability to configure expert parallelism arbitrarily. Users can map experts to different GPUs in various ways without restricting the number of experts on a single device (all devices, however, must contain the same number of experts). NeMo also supports cases where the expert parallel size is less than the data parallel size.\nDevelopers can use the NeMo expert parallelism method in combination with the many other parallelism dimensions offered by NeMo including tensor, pipeline, and sequence parallelism. This facilitates efficient training of models with more than a trillion parameters on clusters with many NVIDIA GPUs.\nRLHF with TensorRT-LLM\nNeMo support for reinforcement learning from human feedback (RLHF) has now been enhanced with the ability to use TensorRT-LLM for inference inside of the RLHF loop.\nTensorRT-LLM accelerates the inference stage of the actor model, which currently takes most of the end-to-end compute time. The actor model is the model of interest that is being aligned and will be the ultimate output of the RLHF process.\nThe upcoming NeMo release enables pipeline parallelism for RLHF through TensorRT-LLM, enabling it to achieve better performance with fewer nodes, all while also supporting larger models.\nIn fact, for the Llama 2 70B parameter model, using TensorRT-LLM in the RLHF loop with H100 GPUs enables up to a 5.6x performance increase compared to RLHF without TensorRT-LLM in the loop on the same H100 GPUs.\nA chart showing the relative performance of H100 using the latest NeMo release for RLHF across four different models\u2013GPT-2B, Llama 2 7B, Llama 2 13B, and Llama 2 70B\u2013with respective H100 GPU counts of 16, 16, 32, and 128 compared to the prior NeMo release. Performance increases are 5x, 2.2x, 3.1x, and 5.6x, respectively.\nFigure 2. The latest version of NeMo brings dramatic improvements compared to the previous version, accelerating the inference part of the RLHF training operation\nMeasured performance. Global Batch Size = 64, Rollout Size = 512, Maximum Generation Length = 1024.\nHalf of the nodes for each result run the actor and the other half run the critic in the RLHF algorithm implemented.\nPushing the boundaries of generative AI\nAI training requires a full-stack approach. NVIDIA NeMo is regularly updated to provide optimal performance for training advanced generative AI models. It incorporates the most recent training methods to improve performance and provide more flexibility for NVIDIA platform users.\nThe NVIDIA platform is also incredibly versatile and accelerates the entire AI workflow end-to-end, from data prep to model training to deploying inference. Following the introduction of TensorRT-LLM in October, NVIDIA recently demonstrated the ability to run the latest Falcon-180B model on a single H200 GPU, leveraging TensorRT-LLM\u2019s advanced 4-bit quantization feature, while maintaining 99% accuracy. Read more about this implementation in the latest post about TensorRT-LLM.\nThe NVIDIA AI platform continues to advance performance, versatility, and features at the speed of light. That\u2019s why it is the platform of choice for developing and deploying today\u2019s generative AI applications and inventing the models and techniques that are powering what comes next.\nGet started with NeMo framework\nThe NVIDIA NeMo framework is available as an open-source library on GitHub, a container on NGC, and as part of NVIDIA AI Enterprise, an enterprise-grade AI software platform with security, stability, manageability, and support."}], "https://developer.nvidia.com/blog/building-your-first-llm-agent-application/": [{"text": "When building a large language model (LLM) agent application, you need four key components: an agent core, a memory module, agent tools, and a planning module. There are various implementation frameworks available, such as LangChain, LLaMaIndex, and Haystack, depending on the type of agent you are building. For beginners, it is recommended to familiarize yourself with the developer ecosystem and explore available frameworks.\n\nFor those developing an LLM agent for the first time, a beginner-level tutorial is provided for building a question-answering agent. This tutorial covers the tools needed, planning module for question decomposition, memory module for tracking questions, and evaluating the mental model of the agent. The agent core is essential for defining how the agent executes its flow, with options like a linear solver, single-thread recursive solver, or multi-thread recursive solver.\n\nAfter building a complex agent, the next step is to adapt the principles to your specific problems and explore further resources like the NVIDIA Deep Learning Institute page and the NVIDIA NeMo Framework for customizing and deploying LLM agents for your use case.", "text_components": ["Building Your First LLM Agent Application\nWhen building a large language model (LLM) agent application, there are four key components you need: an agent core, a memory module, agent tools, and a planning module. Whether you are designing a question-answering agent, multi-modal agent, or swarm of agents, you can consider many implementation frameworks\u2014from open-source to production-ready. For more information, see Introduction to LLM Agents.\nFor those experimenting with developing an LLM agent for the first time, this post provides the following:\nAn overview of the developer ecosystem, including available frameworks and recommended readings to get up-to-speed on LLM agents\nA beginner-level tutorial for building your first LLM-powered agent", "Developer ecosystem overview for agents\nMost of you have probably read articles about LangChain or LLaMa-Index agents. Here are a few of the implementation frameworks available today:\nLangChain Agents\nLLaMaIndex Agents\nHayStack Agents\nAutoGen\nAgentVerse\nChatDev\nGenerative Agents\nSo, which one do I recommend? The answer is, \u201cIt depends.\u201d", "Single-agent frameworks\nThere are several frameworks built by the community to further the LLM application development ecosystem, offering you an easy path to develop agents. Some examples of popular frameworks include LangChain, LlamaIndex, and Haystack. These frameworks provide a generic agent class, connectors, and features for memory modules, access to third-party tools, as well as data retrieval and ingestion mechanisms.\nA choice of which framework to choose largely comes down to the specifics of your pipeline and your requirements. In cases where you must build complex agents that have a directed acyclic graph (DAG), like logical flow, or which have unique properties, these frameworks offer a good reference point for prompts and general architecture for your own custom implementation.", "Multi-agent frameworks\nYou might ask, \u201cWhat\u2019s different in a multi-agent framework?\u201d The short answer is a \u201cworld\u201d class. To manage multiple agents, you must architect the world, or rather the environment in which they interact with each other, the user, and the tools in the environment.\nThe challenge here is that for every application, the world will be different. What you need is a toolkit custom-made to build simulation environments and one that can manage world states and has generic classes for agents. You also need a communication protocol established for managing traffic amongst the agents. The choice of OSS frameworks depends on the type of application that you are building and the level of customization required.", "Recommended reading list for building agents\nThere are plenty of resources and materials that you can use to stimulate your thinking around what is possible with agents, but the following resources are an excellent starting point to cover the overall ethos of agents:\nAutoGPT: This GitHub project was one of the first true agents that was built to showcase the capabilities that agents can provide. Looking at the general architecture and the prompting techniques used in the project can be quite helpful.\nVoyager: This project from NVIDIA Research touches upon the concept of self-improving agents that learn to use new tools or build tools without any external intervention.\nOlaGPT: Conceptual frameworks for agents, like OlaGPT, are a great starting point to stimulate ideas on how to go beyond simple agents, which have the basic four modules.\nMRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning: This paper first suggested the core mechanism for using tools with language models to execute complex tasks.\nGenerative Agents: Interactive Simulacra of Human Behavior: This was one of the first projects to build a true swarm of agents: a solution made up of multiple agents interacting with each other in a decentralized manner.\nIf you are looking for more reading material, I find the Awesome LLM-Powered Agent list to be useful. If you have specific queries, drop a comment on this post.", "Tutorial: Build a question-answering agent\nFor this tutorial, you build a question-answering (QA) agent that can help you talk to your data.\nTo show that a fairly simple agent can tackle fairly hard challenges, you build an agent that can mine information from earnings calls. You can view the earnings call transcripts. Figure 1 shows the general structure of the earnings call so that you can understand the files used for this tutorial.\nThe earnings call transcript is largely divided into three distinct sections: metadata, attendees, introductory remarks; overall remarks and revenue, general trends, and remarks for the next quarter; and a Q&A session. Not every section contains complete context.\nFigure 1. Conceptual breakdown of an earnings call\nBy the end of this post, the agent you build will answer complex and layered questions like the following:\nHow much did revenue grow between Q1 of 2024 and Q2 of 2024?\nWhat were the key takeaways from Q2 of FY24?\nScreenshot of an agent called Clone providing answers to a complex question related to earnings.\nFigure 2. Example question and answer for the agent you are building\nAs described in part 1 of this series, there are four agent components:\nTools\nPlanning module\nMemory\nAgent core", "Tools\nTo build an LLM agent, you need the following tools:\nRetrieval-augmented generation (RAG) pipeline: You can\u2019t solve the talk-to-your-data problem without RAG. So, one of the tools that you need is a RAG pipeline. For more information about how to build a production-grade RAG pipeline, refer to the GitHub repo.\nMathematical tool: You also require a mathematical tool for performing any type of analysis. To keep it simple for this post, I use an LLM to answer math questions, but tools like WolframAlpha are the ones that I recommend for production applications.", "Planning module\nWith this LLM agent, you will be able to answer questions such as: \u201cHow much did the revenue grow between Q1 of 2024 and Q2 of 2024?\u201d Fundamentally, these are three questions rolled into one:\nWhat was the revenue in Q1?\nWhat was the revenue in Q2?\nAnd, what\u2019s the difference between the two?\nThe answer is that you must build a question decomposition module:\n```\ndecomp_template = \"\"\"GENERAL INSTRUCTIONS\nYou are a domain expert. Your task is to break down a complex question into simpler sub-parts.\n\nUSER QUESTION\n{{user_question}}\n\nANSWER FORMAT\n{\"sub-questions\":[\"<FILL>\"]}\"\"\n```\nAs you can see, the decomposition module is prompting the LLM to break the question down into less complex parts. Figure 3 shows what an answer looks like.\nScreenshot of an LLM response following the prompt of breaking a question into subparts: What was the revenue in Q1, the revenue in Q2, and the difference between the two?\nFigure 3. Planning module and prototyping decomposition", "Memory\nNext, you must build a memory module to keep track of all the questions being asked or just to keep a list of all the sub-questions and the answers for said questions.\n```\nclass Ledger:\n    def __init__(self):\n        self.question_trace = []\n        self.answer_trace = []\n```\nYou do this with a simple ledger made up of two lists: one to keep track of all the questions and one to keep track of all the answers. This helps the agent remember the questions it has answered and has yet to answer.", "Evaluate the mental model\nBefore you build an agent core, evaluate what you have right now:\nTools to search and do mathematical calculations\nA planner to break down the question\nA memory module to keep track of questions asked.\nAt this point, you can tie these together to see if it works as a mental model (Figure 4).\n```\ntemplate = \"\"\"GENERAL INSTRUCTIONS\nYour task is to answer questions. If you cannot answer the question, request a helper or use a tool. Fill with Nil where no tool or helper is required.\n\nAVAILABLE TOOLS\n- Search Tool\n- Math Tool\n\nAVAILABLE HELPERS\n- Decomposition: Breaks Complex Questions down into simpler subparts\n\nCONTEXTUAL INFORMATION\n<No previous questions asked>\n\nQUESTION\nHow much did the revenue grow between Q1 of 2024 and Q2 of 2024?\n\nANSWER FORMAT\n{\"Tool_Request\": \"<Fill>\", \"Helper_Request \"<Fill>\"}\"\"\"\n```\nFigure 4 shows the answer received for the LLM.\nScreenshot shows that the LLM lists a search tool as a tool request and that the helper request is nil.\nFigure 4. Putting all the modules together\nYou can see that the LLM requested the use of a search tool, which is a logical step as the answer may well be in the corpus. That said, you know that none of the transcripts contain the answer. In the next step (Figure 5), you provide the input from the RAG pipeline that the answer wasn\u2019t available, so the agent then decides to decompose the question into simpler sub-parts.\nScreenshot shows that, after adding the sub-answer \"The tool cannot answer this question,\" the tool request is now nil but that the helper request is Decomposition.\nFigure 5. Adding an answer to the sub-contextual question\nWith this exercise, you validated that the core mechanism of logic is sound. The LLM is selecting tools and helpers as and when required.\nNow, all that is left is to neatly wrap this in a Python function, which would look something like the following code example:\n```\ndef agent_core(question):\n    answer_dict = prompt_core_llm(question, memory)\n    update_memory()\n    if answer_dict[tools]:\n        execute_tool()\n        update_memory()\n    if answer_dict[planner]:\n        questions = execute_planner()\n        update_memory()\n    if no_new_questions and no tool request:\n        return generate_final_answer(memory)\n```", "Agent core\nYou just saw the example of an agent core, so what\u2019s left? Well, there is a bit more to an agent core than just stitching all the pieces together. You must define the mechanism by which the agent is supposed to execute its flow. There are essentially three major choices:\nLinear solver\nSingle-thread recursive solver\nMulti-thread recursive solver", "Linear solver\nThis is the type of execution that I discussed earlier. There is a single linear chain of solutions where the agent can use tools and do one level of planning. While this is a simple setup, true complex and nuanced questions often require layered thinking.", "Single-thread recursive solver\nYou can also build a recursive solver that constructs a tree of questions and answers till the original question is answered. This tree is solved in a depth-first traversal. The following code example shows the logic:\n```\ndef Agent_Core(Question, Context):\n    Action = LLM(Context + Question)\n\n    if Action == \"Decomposition\":\n        Sub Questions = LLM(Question)\n        Agent_Core(Sub Question, Context)\n\n    if Action == \"Search Tool\":\n        Answer = RAG_Pipeline(Question)\n        Context = Context + Answer\n        Agent_Core(Question, Context)\n\n    if Action == \"Gen Final Answer\u201d:\n        return LLM(Context)\n\n    if Action == \"<Another Tool>\":\n        <Execute Another Tool>\n```", "Multi-thread recursive solver\nInstead of iteratively solving the tree, you can spin off parallel execution threads for each node on the tree. This method adds execution complexity but yields massive latency benefits as the LLM calls can be processed in parallel.", "What\u2019s next?\nCongratulations! You are now armed with the knowledge you for building fairly complex agents! One next step is adapting the principles discussed earlier to your problems.\nTo develop more tools for LLM agents, see the NVIDIA Deep Learning Institute page. To build, customize, and deploy an LLM for your use case, see the NVIDIA NeMo Framework."], "document_title": "Building Your First LLM Agent Application", "document_url": "https://developer.nvidia.com/blog/building-your-first-llm-agent-application/", "document_date": "2023-11-30T19:12:44", "document_date_modified": "2023-12-30T00:47:31", "document_full_text": "Building Your First LLM Agent Application\nWhen building a large language model (LLM) agent application, there are four key components you need: an agent core, a memory module, agent tools, and a planning module. Whether you are designing a question-answering agent, multi-modal agent, or swarm of agents, you can consider many implementation frameworks\u2014from open-source to production-ready. For more information, see Introduction to LLM Agents.\nFor those experimenting with developing an LLM agent for the first time, this post provides the following:\nAn overview of the developer ecosystem, including available frameworks and recommended readings to get up-to-speed on LLM agents\nA beginner-level tutorial for building your first LLM-powered agent\nDeveloper ecosystem overview for agents\nMost of you have probably read articles about LangChain or LLaMa-Index agents. Here are a few of the implementation frameworks available today:\nLangChain Agents\nLLaMaIndex Agents\nHayStack Agents\nAutoGen\nAgentVerse\nChatDev\nGenerative Agents\nSo, which one do I recommend? The answer is, \u201cIt depends.\u201d\nSingle-agent frameworks\nThere are several frameworks built by the community to further the LLM application development ecosystem, offering you an easy path to develop agents. Some examples of popular frameworks include LangChain, LlamaIndex, and Haystack. These frameworks provide a generic agent class, connectors, and features for memory modules, access to third-party tools, as well as data retrieval and ingestion mechanisms.\nA choice of which framework to choose largely comes down to the specifics of your pipeline and your requirements. In cases where you must build complex agents that have a directed acyclic graph (DAG), like logical flow, or which have unique properties, these frameworks offer a good reference point for prompts and general architecture for your own custom implementation.\nMulti-agent frameworks\nYou might ask, \u201cWhat\u2019s different in a multi-agent framework?\u201d The short answer is a \u201cworld\u201d class. To manage multiple agents, you must architect the world, or rather the environment in which they interact with each other, the user, and the tools in the environment.\nThe challenge here is that for every application, the world will be different. What you need is a toolkit custom-made to build simulation environments and one that can manage world states and has generic classes for agents. You also need a communication protocol established for managing traffic amongst the agents. The choice of OSS frameworks depends on the type of application that you are building and the level of customization required.\nRecommended reading list for building agents\nThere are plenty of resources and materials that you can use to stimulate your thinking around what is possible with agents, but the following resources are an excellent starting point to cover the overall ethos of agents:\nAutoGPT: This GitHub project was one of the first true agents that was built to showcase the capabilities that agents can provide. Looking at the general architecture and the prompting techniques used in the project can be quite helpful.\nVoyager: This project from NVIDIA Research touches upon the concept of self-improving agents that learn to use new tools or build tools without any external intervention.\nOlaGPT: Conceptual frameworks for agents, like OlaGPT, are a great starting point to stimulate ideas on how to go beyond simple agents, which have the basic four modules.\nMRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning: This paper first suggested the core mechanism for using tools with language models to execute complex tasks.\nGenerative Agents: Interactive Simulacra of Human Behavior: This was one of the first projects to build a true swarm of agents: a solution made up of multiple agents interacting with each other in a decentralized manner.\nIf you are looking for more reading material, I find the Awesome LLM-Powered Agent list to be useful. If you have specific queries, drop a comment on this post.\nTutorial: Build a question-answering agent\nFor this tutorial, you build a question-answering (QA) agent that can help you talk to your data.\nTo show that a fairly simple agent can tackle fairly hard challenges, you build an agent that can mine information from earnings calls. You can view the earnings call transcripts. Figure 1 shows the general structure of the earnings call so that you can understand the files used for this tutorial.\nThe earnings call transcript is largely divided into three distinct sections: metadata, attendees, introductory remarks; overall remarks and revenue, general trends, and remarks for the next quarter; and a Q&A session. Not every section contains complete context.\nFigure 1. Conceptual breakdown of an earnings call\nBy the end of this post, the agent you build will answer complex and layered questions like the following:\nHow much did revenue grow between Q1 of 2024 and Q2 of 2024?\nWhat were the key takeaways from Q2 of FY24?\nScreenshot of an agent called Clone providing answers to a complex question related to earnings.\nFigure 2. Example question and answer for the agent you are building\nAs described in part 1 of this series, there are four agent components:\nTools\nPlanning module\nMemory\nAgent core\nTools\nTo build an LLM agent, you need the following tools:\nRetrieval-augmented generation (RAG) pipeline: You can\u2019t solve the talk-to-your-data problem without RAG. So, one of the tools that you need is a RAG pipeline. For more information about how to build a production-grade RAG pipeline, refer to the GitHub repo.\nMathematical tool: You also require a mathematical tool for performing any type of analysis. To keep it simple for this post, I use an LLM to answer math questions, but tools like WolframAlpha are the ones that I recommend for production applications.\nPlanning module\nWith this LLM agent, you will be able to answer questions such as: \u201cHow much did the revenue grow between Q1 of 2024 and Q2 of 2024?\u201d Fundamentally, these are three questions rolled into one:\nWhat was the revenue in Q1?\nWhat was the revenue in Q2?\nAnd, what\u2019s the difference between the two?\nThe answer is that you must build a question decomposition module:\n```\ndecomp_template = \"\"\"GENERAL INSTRUCTIONS\nYou are a domain expert. Your task is to break down a complex question into simpler sub-parts.\n\nUSER QUESTION\n{{user_question}}\n\nANSWER FORMAT\n{\"sub-questions\":[\"<FILL>\"]}\"\"\n```\nAs you can see, the decomposition module is prompting the LLM to break the question down into less complex parts. Figure 3 shows what an answer looks like.\nScreenshot of an LLM response following the prompt of breaking a question into subparts: What was the revenue in Q1, the revenue in Q2, and the difference between the two?\nFigure 3. Planning module and prototyping decomposition\nMemory\nNext, you must build a memory module to keep track of all the questions being asked or just to keep a list of all the sub-questions and the answers for said questions.\n```\nclass Ledger:\n    def __init__(self):\n        self.question_trace = []\n        self.answer_trace = []\n```\nYou do this with a simple ledger made up of two lists: one to keep track of all the questions and one to keep track of all the answers. This helps the agent remember the questions it has answered and has yet to answer.\nEvaluate the mental model\nBefore you build an agent core, evaluate what you have right now:\nTools to search and do mathematical calculations\nA planner to break down the question\nA memory module to keep track of questions asked.\nAt this point, you can tie these together to see if it works as a mental model (Figure 4).\n```\ntemplate = \"\"\"GENERAL INSTRUCTIONS\nYour task is to answer questions. If you cannot answer the question, request a helper or use a tool. Fill with Nil where no tool or helper is required.\n\nAVAILABLE TOOLS\n- Search Tool\n- Math Tool\n\nAVAILABLE HELPERS\n- Decomposition: Breaks Complex Questions down into simpler subparts\n\nCONTEXTUAL INFORMATION\n<No previous questions asked>\n\nQUESTION\nHow much did the revenue grow between Q1 of 2024 and Q2 of 2024?\n\nANSWER FORMAT\n{\"Tool_Request\": \"<Fill>\", \"Helper_Request \"<Fill>\"}\"\"\"\n```\nFigure 4 shows the answer received for the LLM.\nScreenshot shows that the LLM lists a search tool as a tool request and that the helper request is nil.\nFigure 4. Putting all the modules together\nYou can see that the LLM requested the use of a search tool, which is a logical step as the answer may well be in the corpus. That said, you know that none of the transcripts contain the answer. In the next step (Figure 5), you provide the input from the RAG pipeline that the answer wasn\u2019t available, so the agent then decides to decompose the question into simpler sub-parts.\nScreenshot shows that, after adding the sub-answer \"The tool cannot answer this question,\" the tool request is now nil but that the helper request is Decomposition.\nFigure 5. Adding an answer to the sub-contextual question\nWith this exercise, you validated that the core mechanism of logic is sound. The LLM is selecting tools and helpers as and when required.\nNow, all that is left is to neatly wrap this in a Python function, which would look something like the following code example:\n```\ndef agent_core(question):\n    answer_dict = prompt_core_llm(question, memory)\n    update_memory()\n    if answer_dict[tools]:\n        execute_tool()\n        update_memory()\n    if answer_dict[planner]:\n        questions = execute_planner()\n        update_memory()\n    if no_new_questions and no tool request:\n        return generate_final_answer(memory)\n```\nAgent core\nYou just saw the example of an agent core, so what\u2019s left? Well, there is a bit more to an agent core than just stitching all the pieces together. You must define the mechanism by which the agent is supposed to execute its flow. There are essentially three major choices:\nLinear solver\nSingle-thread recursive solver\nMulti-thread recursive solver\nLinear solver\nThis is the type of execution that I discussed earlier. There is a single linear chain of solutions where the agent can use tools and do one level of planning. While this is a simple setup, true complex and nuanced questions often require layered thinking.\nSingle-thread recursive solver\nYou can also build a recursive solver that constructs a tree of questions and answers till the original question is answered. This tree is solved in a depth-first traversal. The following code example shows the logic:\n```\ndef Agent_Core(Question, Context):\n    Action = LLM(Context + Question)\n\n    if Action == \"Decomposition\":\n        Sub Questions = LLM(Question)\n        Agent_Core(Sub Question, Context)\n\n    if Action == \"Search Tool\":\n        Answer = RAG_Pipeline(Question)\n        Context = Context + Answer\n        Agent_Core(Question, Context)\n\n    if Action == \"Gen Final Answer\u201d:\n        return LLM(Context)\n\n    if Action == \"<Another Tool>\":\n        <Execute Another Tool>\n```\nMulti-thread recursive solver\nInstead of iteratively solving the tree, you can spin off parallel execution threads for each node on the tree. This method adds execution complexity but yields massive latency benefits as the LLM calls can be processed in parallel.\nWhat\u2019s next?\nCongratulations! You are now armed with the knowledge you for building fairly complex agents! One next step is adapting the principles discussed earlier to your problems.\nTo develop more tools for LLM agents, see the NVIDIA Deep Learning Institute page. To build, customize, and deploy an LLM for your use case, see the NVIDIA NeMo Framework."}], "https://developer.nvidia.com/blog/introduction-to-llm-agents/": [{"text": "LLM Agents are advanced language model applications that can reason through complex problems, create plans to solve them, and execute tasks with the help of various tools. These agents consist of key components such as the agent core, memory module, tools, and planning module. The agent core manages the logic and behavior of the agent, setting goals, providing execution tools, and guiding planning. The memory module stores internal logs and interactions with users, while tools are executable workflows that help agents perform tasks. The planning module helps agents decompose complex questions and refine their execution plans.\n\nLLM-powered agents have various enterprise applications, including \"Talk to your data\" agents, swarms of agents working together, recommendation and experience design agents, customized AI author agents, and multi-modal agents that can process different types of inputs. These agents can revolutionize industries by providing personalized experiences, assisting with tasks like email writing, and analyzing multi-modal data. Overall, LLM-powered agents offer a new level of sophistication in AI applications and have the potential to transform how businesses operate in the future.", "text_components": ["Introduction to LLM Agents\nConsider a large language model (LLM) application that is designed to help financial analysts answer questions about the performance of a company. With a well-designed retrieval augmented generation (RAG) pipeline, analysts can answer questions like, \u201cWhat was X corporation\u2019s total revenue for FY 2022?\u201d This information can be easily extracted from financial statements by a seasoned analyst.\nNow consider a question like, \u201cWhat were the three takeaways from the Q2 earnings call from FY 23? Focus on the technological moats that the company is building\u201d. This is the type of question a financial analyst would want answered to include in their reports but would need to invest time to answer.\nHow do we develop a solution to answer a question like above? It is immediately apparent that this information requires more than a simple lookup from an earnings call. This inquiry requires planning, tailored focus, memory, using different tools, and breaking down a complex question into simpler sub-parts.. These concepts assembled together are essentially what we have come to refer to as an LLM Agent.\nIn this post, I introduce LLM-powered agents and discuss what an agent is and some use cases for enterprise applications. For more information, see Building Your First Agent Application. In that post, I offer an ecosystem walkthrough, covering the available frameworks for building AI agents and a getting started guide for anyone experimenting with question-and-answer (Q&A) agents.", "What is an AI agent?\nWhile there isn\u2019t a widely accepted definition for LLM-powered agents, they can be described as a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools.\nIn short, agents are a system with complex reasoning capabilities, memory, and the means to execute tasks.\nThis capability was first observed in projects like AutoGPT or BabyAGI, where complex problems were solved without much intervention. To describe agents a bit more, here\u2019s the general architecture of an LLM-powered agent application (Figure 1).\nArchitecture diagram of an LLM-powered agent. The agent core is at the center with bi-directional arrow connecting user requests, memory module, planning module and tools.\nFigure 1. General components of an agent\nAn agent is made up of the following key components (more details on these shortly):\nAgent core\nMemory module\nTools\nPlanning module", "Agent core\nThe agent core is the central coordination module that manages the core logic and behavioral characteristics of an Agent. Think of it as the \u201ckey decision making module\u201d of the agent. It is also where we define:\nGeneral goals of the agent: Contains overall goals and objectives for the agent.\nTools for execution: Essentially a short list or a \u201cuser manual\u201d for all the tools to which the agent has access\nExplanation for how to make use of different planning modules: Details about the utility of different planning modules and which to use in what situation.\nRelevant Memory: This is a dynamic section which fills the most relevant memory items from past conversations with the user at inference time. The \u201crelevance\u201d is determined using the question user asks.\nPersona of the Agent (optional): This persona description is typically used to either bias the model to prefer using certain types of tools or to imbue typical idiosyncrasies in the agent\u2019s final response.\nThe Agent core has access to Goals, Tools, planning helpers and a general format for the answer.\nFigure 2. Basic template of how the different modules of an agent are assembled in its core.", "Memory module\nMemory modules play a critical role in AI agents. A memory module can essentially be thought of as a store of the agent\u2019s internal logs as well as interactions with a user.\nThere are two types of memory modules:\nShort-term memory: A ledger of actions and thoughts that an agent goes through to attempt to answer a single question from a user: the agent\u2019s \u201ctrain of thought.\u201d\nLong-term memory: A ledger of actions and thoughts about events that happen between the user and agent. It is a log book that contains a conversation history stretching across weeks or months.\nMemory requires more than semantic similarity-based retrieval. Typically, a composite score is made up of semantic similarity, importance, recency, and other application-specific metrics. It is used for retrieving specific information.", "Tools\nTools are well-defined executable workflows that agents can use to execute tasks. Oftentimes, they can be thought of as specialized third-party APIs.\nFor instance, agents can use a RAG pipeline to generate context aware answers, a code interpreter to solve complex programmatically tasks, an API to search information over the internet, or even any simple API service like a weather API or an API for an Instant messaging application.", "Planning module\nComplex problems, such as analyzing a set of financial reports to answer a layered business question, often require nuanced approaches. With an LLM\u2013powered agent, this complexity can be dealt with by using a combination of two techniques:\nTask and question decomposition\nReflection or critic", "Task and question decomposition\nCompound questions or inferred information require some form of decomposition. Take, for instance, the question, \u201cWhat were the three takeaways from NVIDIA\u2019s last earnings call?\u201d\nThe information required to answer this question is not directly extractable from the transcript of an hour long meeting. However, the problem can be broken down into multiple question topics:\n\u201cWhich technological shifts were discussed the most?\u201d\n\u201cAre there any business headwinds?\u201d\n\u201cWhat were the financial results?\u201d\nEach of these questions can be further broken into subparts. That said, a specialized AI agent must guide this decomposition.", "Reflection or critic\nTechniques like ReAct, Reflexion, Chain of Thought, and Graph of thought have served as critic\u2013 or evidence-based prompting frameworks. They have been widely used to improve the reasoning capabilities and responses of LLMs. These techniques can also be used to refine the execution plan generated by the agent.", "Agents for enterprise applications\nWhile the applications of agents are practically boundless, the following are a few interesting cases that may have an outsized impact for many businesses:\n\u201cTalk to your data\u201d agent\nSwarm of agents\nRecommendation and experience design agents\nCustomized AI author agents\nMulti-modal agents", "\u201cTalk to your data\u201d agent\n\u201cTalk to your data\u201d isn\u2019t a simple problem. There are a lot of challenges that a straightforward RAG pipeline can\u2019t solve:\nSemantic similarity of source documents\nComplex data structures, like tables\nLack of apparent context (not every chunk contains markers for its source)\nThe complexity of the questions that users ask\n\u2026and more\nFor instance, go back to the prior earning\u2019s call transcript example ( Q3, 2023 | Q1 2024 ). How do you answer the question, \u201cHow much did the data center revenue increase between Q3 of 2023 and Q1 of 2024?\u201d To answer this question, you essentially must answer three questions individually (i.e., we need a planning module):\nWhat was the data center revenue in Q3 of 2023?\nWhat was the data center revenue in Q1 of 2024?\nWhat was the difference between the two?\nIn this case, you would need an agent that has access to a Planning Module that does question-decomposition (generates sub-questions and searches for answers till the larger problem is solved), a RAG pipeline (used as a tool) to retrieve specific information, and memory modules to accurately handle the subquestions. In the LLM-Powered Agents: Building Your First Agent Application post, I go over this type of case in detail.", "Swarm of agents\nA swarm of agents can be understood as a collection of agents working together towards co-existing in a single environment which can collaborate with each other to solve problems. A decentralized ecosystem of agents is very much akin to multiple \u201csmart\u201d microservices used in tandem to solve problems.\nMulti-agent environments like Generative Agents and ChatDev have been extremely popular with the community (Figure 3). Why? Frameworks like ChatDev enable you to build a team of engineers, designers, product management, CEO, and agents to build basic software at low costs. Popular games like Brick Breaker or Flappy Bird can be prototyped for as low as 50 cents!\nWith a swarm of agents, you can populate a digital company, neighborhood, or even a whole town for applications like behavioral simulations for economic studies, enterprise marketing campaigns, UX elements of physical infrastructure, and more.\nGraphic showing a swarm of comic digital agents for application development: designing, coding, testing, and documenting.\nFigure 3. Example of multiple agents in a ChatDev environment\nThese applications are currently not possible to simulate without LLMs and are extremely expensive to run in the real world.", "Agents for recommendation and experience design\nThe internet works off recommendations. Conversational recommendation systems powered by agents can be used to craft personalized experiences.\nFor example, consider an AI agent on an e-commerce website that helps you compare products and provides recommendations based on your general requests and selections. A full concierge-like experience can also be built, with multiple agents assisting an end user to navigate a digital store. Experiences like selecting which movie to watch or which hotel room to book can be crafted as conversations\u2014and not just a series of decision-tree-style conversations!", "Customized AI author agents\nAnother powerful tool is having a personal AI author that can help you with tasks such as co-authoring emails or preparing you for time-sensitive meetings and presentations. The problem with regular authoring tools is that different types of material must be tailored according to various audiences. For instance, an investor pitch must be worded differently than a team presentation.\nAgents can harness your previous work. Then, you have the agent mold an agent-generated pitch to your personal style and customize the work according to your specific use case and needs. This process is often too nuanced for general LLM fine-tuning.", "Multi-modal agents\nWith only text as an input, you cannot really \u201ctalk to your data.\u201d All the mentioned use cases can be augmented by building multi-modal agents that can digest a variety of inputs, such as images and audio files.\nScreenshot shows a Schrodinger query and an answer that is based on information in files with both text and bar chart.\nFigure 4. Example multi-modal agent answering questions based on graphs\nThat was just a few examples of directions that can be pursued to solve enterprise challenges. Agents for data curation, social graphs, and domain expertise are all active areas being pursued by the development community for enterprise applications.", "What\u2019s next?\nLLM-powered agents differ from typical chatbot applications in that they have complex reasoning skills. Made up of an agent core, memory module, set of tools, and planning module, agents can generate highly personalized answers and content in a variety of enterprise settings\u2014from data curation to advanced e-commerce recommendation systems.\nFor an overview of the technical ecosystem around agents, such as implementation frameworks, must-read papers, posts, and related topics, see Building Your First Agent Application. The walkthrough of a no-framework implementation of a Q&A agent helps you better talk to your data."], "document_title": "Introduction to LLM Agents", "document_url": "https://developer.nvidia.com/blog/introduction-to-llm-agents/", "document_date": "2023-11-30T17:00:00", "document_date_modified": "2023-12-20T17:34:56", "document_full_text": "Introduction to LLM Agents\nConsider a large language model (LLM) application that is designed to help financial analysts answer questions about the performance of a company. With a well-designed retrieval augmented generation (RAG) pipeline, analysts can answer questions like, \u201cWhat was X corporation\u2019s total revenue for FY 2022?\u201d This information can be easily extracted from financial statements by a seasoned analyst.\nNow consider a question like, \u201cWhat were the three takeaways from the Q2 earnings call from FY 23? Focus on the technological moats that the company is building\u201d. This is the type of question a financial analyst would want answered to include in their reports but would need to invest time to answer.\nHow do we develop a solution to answer a question like above? It is immediately apparent that this information requires more than a simple lookup from an earnings call. This inquiry requires planning, tailored focus, memory, using different tools, and breaking down a complex question into simpler sub-parts.. These concepts assembled together are essentially what we have come to refer to as an LLM Agent.\nIn this post, I introduce LLM-powered agents and discuss what an agent is and some use cases for enterprise applications. For more information, see Building Your First Agent Application. In that post, I offer an ecosystem walkthrough, covering the available frameworks for building AI agents and a getting started guide for anyone experimenting with question-and-answer (Q&A) agents.\nWhat is an AI agent?\nWhile there isn\u2019t a widely accepted definition for LLM-powered agents, they can be described as a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools.\nIn short, agents are a system with complex reasoning capabilities, memory, and the means to execute tasks.\nThis capability was first observed in projects like AutoGPT or BabyAGI, where complex problems were solved without much intervention. To describe agents a bit more, here\u2019s the general architecture of an LLM-powered agent application (Figure 1).\nArchitecture diagram of an LLM-powered agent. The agent core is at the center with bi-directional arrow connecting user requests, memory module, planning module and tools.\nFigure 1. General components of an agent\nAn agent is made up of the following key components (more details on these shortly):\nAgent core\nMemory module\nTools\nPlanning module\nAgent core\nThe agent core is the central coordination module that manages the core logic and behavioral characteristics of an Agent. Think of it as the \u201ckey decision making module\u201d of the agent. It is also where we define:\nGeneral goals of the agent: Contains overall goals and objectives for the agent.\nTools for execution: Essentially a short list or a \u201cuser manual\u201d for all the tools to which the agent has access\nExplanation for how to make use of different planning modules: Details about the utility of different planning modules and which to use in what situation.\nRelevant Memory: This is a dynamic section which fills the most relevant memory items from past conversations with the user at inference time. The \u201crelevance\u201d is determined using the question user asks.\nPersona of the Agent (optional): This persona description is typically used to either bias the model to prefer using certain types of tools or to imbue typical idiosyncrasies in the agent\u2019s final response.\nThe Agent core has access to Goals, Tools, planning helpers and a general format for the answer.\nFigure 2. Basic template of how the different modules of an agent are assembled in its core.\nMemory module\nMemory modules play a critical role in AI agents. A memory module can essentially be thought of as a store of the agent\u2019s internal logs as well as interactions with a user.\nThere are two types of memory modules:\nShort-term memory: A ledger of actions and thoughts that an agent goes through to attempt to answer a single question from a user: the agent\u2019s \u201ctrain of thought.\u201d\nLong-term memory: A ledger of actions and thoughts about events that happen between the user and agent. It is a log book that contains a conversation history stretching across weeks or months.\nMemory requires more than semantic similarity-based retrieval. Typically, a composite score is made up of semantic similarity, importance, recency, and other application-specific metrics. It is used for retrieving specific information.\nTools\nTools are well-defined executable workflows that agents can use to execute tasks. Oftentimes, they can be thought of as specialized third-party APIs.\nFor instance, agents can use a RAG pipeline to generate context aware answers, a code interpreter to solve complex programmatically tasks, an API to search information over the internet, or even any simple API service like a weather API or an API for an Instant messaging application.\nPlanning module\nComplex problems, such as analyzing a set of financial reports to answer a layered business question, often require nuanced approaches. With an LLM\u2013powered agent, this complexity can be dealt with by using a combination of two techniques:\nTask and question decomposition\nReflection or critic\nTask and question decomposition\nCompound questions or inferred information require some form of decomposition. Take, for instance, the question, \u201cWhat were the three takeaways from NVIDIA\u2019s last earnings call?\u201d\nThe information required to answer this question is not directly extractable from the transcript of an hour long meeting. However, the problem can be broken down into multiple question topics:\n\u201cWhich technological shifts were discussed the most?\u201d\n\u201cAre there any business headwinds?\u201d\n\u201cWhat were the financial results?\u201d\nEach of these questions can be further broken into subparts. That said, a specialized AI agent must guide this decomposition.\nReflection or critic\nTechniques like ReAct, Reflexion, Chain of Thought, and Graph of thought have served as critic\u2013 or evidence-based prompting frameworks. They have been widely used to improve the reasoning capabilities and responses of LLMs. These techniques can also be used to refine the execution plan generated by the agent.\nAgents for enterprise applications\nWhile the applications of agents are practically boundless, the following are a few interesting cases that may have an outsized impact for many businesses:\n\u201cTalk to your data\u201d agent\nSwarm of agents\nRecommendation and experience design agents\nCustomized AI author agents\nMulti-modal agents\n\u201cTalk to your data\u201d agent\n\u201cTalk to your data\u201d isn\u2019t a simple problem. There are a lot of challenges that a straightforward RAG pipeline can\u2019t solve:\nSemantic similarity of source documents\nComplex data structures, like tables\nLack of apparent context (not every chunk contains markers for its source)\nThe complexity of the questions that users ask\n\u2026and more\nFor instance, go back to the prior earning\u2019s call transcript example ( Q3, 2023 | Q1 2024 ). How do you answer the question, \u201cHow much did the data center revenue increase between Q3 of 2023 and Q1 of 2024?\u201d To answer this question, you essentially must answer three questions individually (i.e., we need a planning module):\nWhat was the data center revenue in Q3 of 2023?\nWhat was the data center revenue in Q1 of 2024?\nWhat was the difference between the two?\nIn this case, you would need an agent that has access to a Planning Module that does question-decomposition (generates sub-questions and searches for answers till the larger problem is solved), a RAG pipeline (used as a tool) to retrieve specific information, and memory modules to accurately handle the subquestions. In the LLM-Powered Agents: Building Your First Agent Application post, I go over this type of case in detail.\nSwarm of agents\nA swarm of agents can be understood as a collection of agents working together towards co-existing in a single environment which can collaborate with each other to solve problems. A decentralized ecosystem of agents is very much akin to multiple \u201csmart\u201d microservices used in tandem to solve problems.\nMulti-agent environments like Generative Agents and ChatDev have been extremely popular with the community (Figure 3). Why? Frameworks like ChatDev enable you to build a team of engineers, designers, product management, CEO, and agents to build basic software at low costs. Popular games like Brick Breaker or Flappy Bird can be prototyped for as low as 50 cents!\nWith a swarm of agents, you can populate a digital company, neighborhood, or even a whole town for applications like behavioral simulations for economic studies, enterprise marketing campaigns, UX elements of physical infrastructure, and more.\nGraphic showing a swarm of comic digital agents for application development: designing, coding, testing, and documenting.\nFigure 3. Example of multiple agents in a ChatDev environment\nThese applications are currently not possible to simulate without LLMs and are extremely expensive to run in the real world.\nAgents for recommendation and experience design\nThe internet works off recommendations. Conversational recommendation systems powered by agents can be used to craft personalized experiences.\nFor example, consider an AI agent on an e-commerce website that helps you compare products and provides recommendations based on your general requests and selections. A full concierge-like experience can also be built, with multiple agents assisting an end user to navigate a digital store. Experiences like selecting which movie to watch or which hotel room to book can be crafted as conversations\u2014and not just a series of decision-tree-style conversations!\nCustomized AI author agents\nAnother powerful tool is having a personal AI author that can help you with tasks such as co-authoring emails or preparing you for time-sensitive meetings and presentations. The problem with regular authoring tools is that different types of material must be tailored according to various audiences. For instance, an investor pitch must be worded differently than a team presentation.\nAgents can harness your previous work. Then, you have the agent mold an agent-generated pitch to your personal style and customize the work according to your specific use case and needs. This process is often too nuanced for general LLM fine-tuning.\nMulti-modal agents\nWith only text as an input, you cannot really \u201ctalk to your data.\u201d All the mentioned use cases can be augmented by building multi-modal agents that can digest a variety of inputs, such as images and audio files.\nScreenshot shows a Schrodinger query and an answer that is based on information in files with both text and bar chart.\nFigure 4. Example multi-modal agent answering questions based on graphs\nThat was just a few examples of directions that can be pursued to solve enterprise challenges. Agents for data curation, social graphs, and domain expertise are all active areas being pursued by the development community for enterprise applications.\nWhat\u2019s next?\nLLM-powered agents differ from typical chatbot applications in that they have complex reasoning skills. Made up of an agent core, memory module, set of tools, and planning module, agents can generate highly personalized answers and content in a variety of enterprise settings\u2014from data curation to advanced e-commerce recommendation systems.\nFor an overview of the technical ecosystem around agents, such as implementation frameworks, must-read papers, posts, and related topics, see Building Your First Agent Application. The walkthrough of a no-framework implementation of a Q&A agent helps you better talk to your data."}], "https://developer.nvidia.com/blog/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/": [{"text": "Meetings are crucial for organizations, but leading effective meetings can be challenging. Adam.ai offers a cloud-native architecture for intelligent note-taking and meeting management. This platform leverages AI technology, including Google Cloud Dataflow, NVIDIA Riva speech-to-text models, and Large Language Models (LLMs) for efficient summarization. Automatic note-taking eliminates the need for manual transcription and ensures accurate and detailed meeting minutes. The architecture is scalable, low-latency, and cost-effective, making it a valuable tool for managing meeting data. The AI-driven system streamlines the process of capturing meeting information and extracting insights, facilitating collaboration and decision-making. Adam.ai's user-friendly interface provides easy access to meeting notes and action items. The architecture is designed for Google Cloud but can be adapted for other platforms. Overall, adam.ai enhances meeting productivity by automating note-taking and summarization processes, freeing participants to focus on active participation and collaboration.", "text_components": ["Boost Meeting Productivity with AI-Powered Note-Taking and Summarization\nMeetings are the lifeblood of an organization. They foster collaboration and informed decision-making. They eliminate silos through brainstorming and problem-solving. And they further strategic goals and planning.\nYet, leading meetings that accomplish these goals\u2014especially those involving cross-functional teams and external participants\u2014can be challenging. A unique blend of people management skills and adept documentation strategies are required to seamlessly facilitate decision-making and ensure effective post-meeting task execution.\nThis post introduces the cloud-native microservice-based architecture for intelligent note-taking from adam.ai. Part of the NVIDIA Inception program, adam.ai is a comprehensive meeting management platform designed to empower organizations, teams, and professionals throughout their entire meeting lifecycle. The architecture offers high scalability, low latency, and cost-effective provisioning of automatic note-taking services in online meetings. Specifically, adam.ai leverages:\nGoogle Cloud Dataflow for automated provisioning of processing resources\nNVIDIA Riva speech-to-text (STT) models for low-latency transcription\nLarge language models (LLMs) for efficient summarization", "AI-driven automatic note-taking\nManual note-taking requires real-time decisions about what information to record and what to omit. Moreover, balancing active participation with meticulous note-taking presents challenges even for those most adept. The endurance required to focus, especially during lengthy or complex discussions, remains a constant hurdle.\nAdvancements in automatic speech recognition (ASR) and LLMs pave the way for novel approaches to managing and organizing meeting information. Automatic note-taking harnesses the power of transcription to ensure accuracy and depth in capturing nuances.\nTranscription models transform spoken words into accurate text in real time, empowering teams, corporate executives, and professionals to create comprehensive meeting minutes, leaving no critical details overlooked. LLMs leverage a capacity for understanding, reasoning, and knowledge representation to analyze meeting data and extract invaluable insights.\nWith the user-friendly adam.ai interface, essential agenda items become accessible, and decisions and action items are meticulously tracked (Figure 1). This intuitive approach facilitates meeting management, fosters seamless collaboration, and supports superior meeting outcomes.\nScreenshot of adam.ai user interface of meeting AI Summary displaying insights and next steps.\nFigure 1. adam.ai provides insightful notes and next steps to facilitate meeting management", "Transcription and summarization architecture\nThe AI Engineering team at adam.ai developed a microservice architecture specifically designed for Google Cloud (Figure 2). This architecture, which includes the note-taking system, can seamlessly translate to other cloud platforms such as AWS and Azure.\nThe adam.ai architecture diagram for meeting transcription and automatic note-taking service. User data flows through Google Cloud for preprocessing, NVIDIA Riva state-of-the-art speech-to-text models for low-latency transcription, and LLMs for efficient summarization.\nFigure 2. The adam.ai automatic note-taking architecture The architecture leverages Google Cloud components, such as Storage, Dataflow, and the Pub/Sub system for storing users\u2019 data, managing data-processing resources, and facilitating communication between the different components.\nMeeting transcription is powered by NVIDIA Riva models, offering unmatched accuracy and low latency while efficiently handling real-time audio processing tasks at scale. What sets Riva apart is its full customization capabilities. Riva can be fine-tuned for specialized industries such as legal and medical, providing precise transcription even in niche vocabularies and language usage. Additionally, for variable demand, deploying Riva models using Helm charts enables scalable resource management, providing a cost-effective solution.", "Note-taking data flow\nThe adam.ai note-taking data flow is orchestrated through four key steps:", "Step 1: Initiate a note-taking job\nWhen a new meeting recording is uploaded, an event message is generated and transmitted through the Google Cloud Pub/Sub messaging service. This event-driven, distributed mechanism establishes a loosely coupled architecture, simplifying communication between the platform and the note-taking service, especially when processing lengthy meetings that require significant analysis and summarization time.", "Step 2: Start the data processing pipeline\nEvent messages, which encapsulate the location of the audio and video recordings, undergo processing through customized data processing pipelines to derive meeting insights. These pipelines are executed through Google Cloud Dataflow, enabling automated provisioning of computing resources tailored to dynamic user workload, thereby ensuring optimal performance and cost efficiency of processing tasks.", "Step 3: Generate meeting transcriptions\nThe data processing pipeline begins by downloading audio and video recordings from \u200ccloud storage. Downloaded files are then meticulously transcribed by NVIDIA Riva. Producing more than a simple conversion of speech to text, Riva enhances transcription quality using contextual understanding. Punctuation and capitalization are refined to provide robust and accurate summarization and insight generation.", "Step 4: Generate summary and actionable insights\nThe meticulously transcribed text is then passed to an LLM to summarize the meeting content. Through refined prompt engineering, the LLM summarizes the meeting and generates valuable, actionable insights. The meeting summary and insights are then returned to the platform for user display.", "Benefits of adam.ai architecture\nThis architecture ensures \u200cefficient, scalable, and cost-effective meeting transcription and summarization. Specific benefits include:", "Dynamically scalable and fault-tolerant system\nUsing Google Cloud Pub/Sub, the architecture embraces a loosely coupled, event-driven microservices approach, prompting a scalable, fault-tolerant system. This not only simplifies communication but also provides independent functionality of components. Additionally, Google Cloud Dataflow automatic resource provisioning dynamically scales computing power, resulting in cost-effective data processing.", "Real-time accurate meeting transcriptions\nThe Riva ASR model supports streaming audio and provides real-time accurate transcriptions. Its ability to refine punctuation and capitalization elevates transcript quality, enabling accurate summarization and extraction of valuable insights.", "Intelligible and well-structured summarizations\nLLM integration provides intelligible and well-structured summaries, fostering the extraction of valuable and actionable insights from the meeting transcript.", "Intuitive user experience\nThe entire process, from transcription to summarization, is seamlessly integrated into the platform. Requests and results flow efficiently through the Pub/Sub system, providing a smooth and intuitive user experience and easy access to meeting insights.", "Summary\nTransform your meetings into more productive, dynamic collaborations with adam.ai. Working together, ASR and LLMs seamlessly capture every word spoken, extract key insights, and generate detailed notes. This frees participants from the burden of note-taking so they can fully engage in the meeting.\nTo ensure scalable, low-latency, and cost-effective processing of meetings\u2019 audio data, the adam.ai meeting management platform employs a cloud-native microservice-based architecture. This architecture enables real-time accurate transcriptions and enhanced punctuation and capitalization powered by NVIDIA Riva, providing you with a comprehensive and polished record of your meetings.\nTo explore how adam.ai can help elevate your meetings, sign up for a free trial. To learn more about LLM enterprise applications, see Getting Started with Large Language Models for Enterprise Solutions. And join the conversation on Speech AI in the NVIDIA Riva forum."], "document_title": "Boost Meeting Productivity with AI-Powered Note-Taking and Summarization", "document_url": "https://developer.nvidia.com/blog/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/", "document_date": "2023-11-29T21:00:00", "document_date_modified": "2023-12-14T19:27:34", "document_full_text": "Boost Meeting Productivity with AI-Powered Note-Taking and Summarization\nMeetings are the lifeblood of an organization. They foster collaboration and informed decision-making. They eliminate silos through brainstorming and problem-solving. And they further strategic goals and planning.\nYet, leading meetings that accomplish these goals\u2014especially those involving cross-functional teams and external participants\u2014can be challenging. A unique blend of people management skills and adept documentation strategies are required to seamlessly facilitate decision-making and ensure effective post-meeting task execution.\nThis post introduces the cloud-native microservice-based architecture for intelligent note-taking from adam.ai. Part of the NVIDIA Inception program, adam.ai is a comprehensive meeting management platform designed to empower organizations, teams, and professionals throughout their entire meeting lifecycle. The architecture offers high scalability, low latency, and cost-effective provisioning of automatic note-taking services in online meetings. Specifically, adam.ai leverages:\nGoogle Cloud Dataflow for automated provisioning of processing resources\nNVIDIA Riva speech-to-text (STT) models for low-latency transcription\nLarge language models (LLMs) for efficient summarization\nAI-driven automatic note-taking\nManual note-taking requires real-time decisions about what information to record and what to omit. Moreover, balancing active participation with meticulous note-taking presents challenges even for those most adept. The endurance required to focus, especially during lengthy or complex discussions, remains a constant hurdle.\nAdvancements in automatic speech recognition (ASR) and LLMs pave the way for novel approaches to managing and organizing meeting information. Automatic note-taking harnesses the power of transcription to ensure accuracy and depth in capturing nuances.\nTranscription models transform spoken words into accurate text in real time, empowering teams, corporate executives, and professionals to create comprehensive meeting minutes, leaving no critical details overlooked. LLMs leverage a capacity for understanding, reasoning, and knowledge representation to analyze meeting data and extract invaluable insights.\nWith the user-friendly adam.ai interface, essential agenda items become accessible, and decisions and action items are meticulously tracked (Figure 1). This intuitive approach facilitates meeting management, fosters seamless collaboration, and supports superior meeting outcomes.\nScreenshot of adam.ai user interface of meeting AI Summary displaying insights and next steps.\nFigure 1. adam.ai provides insightful notes and next steps to facilitate meeting management\nTranscription and summarization architecture\nThe AI Engineering team at adam.ai developed a microservice architecture specifically designed for Google Cloud (Figure 2). This architecture, which includes the note-taking system, can seamlessly translate to other cloud platforms such as AWS and Azure.\nThe adam.ai architecture diagram for meeting transcription and automatic note-taking service. User data flows through Google Cloud for preprocessing, NVIDIA Riva state-of-the-art speech-to-text models for low-latency transcription, and LLMs for efficient summarization.\nFigure 2. The adam.ai automatic note-taking architecture The architecture leverages Google Cloud components, such as Storage, Dataflow, and the Pub/Sub system for storing users\u2019 data, managing data-processing resources, and facilitating communication between the different components.\nMeeting transcription is powered by NVIDIA Riva models, offering unmatched accuracy and low latency while efficiently handling real-time audio processing tasks at scale. What sets Riva apart is its full customization capabilities. Riva can be fine-tuned for specialized industries such as legal and medical, providing precise transcription even in niche vocabularies and language usage. Additionally, for variable demand, deploying Riva models using Helm charts enables scalable resource management, providing a cost-effective solution.\nNote-taking data flow\nThe adam.ai note-taking data flow is orchestrated through four key steps:\nStep 1: Initiate a note-taking job\nWhen a new meeting recording is uploaded, an event message is generated and transmitted through the Google Cloud Pub/Sub messaging service. This event-driven, distributed mechanism establishes a loosely coupled architecture, simplifying communication between the platform and the note-taking service, especially when processing lengthy meetings that require significant analysis and summarization time.\nStep 2: Start the data processing pipeline\nEvent messages, which encapsulate the location of the audio and video recordings, undergo processing through customized data processing pipelines to derive meeting insights. These pipelines are executed through Google Cloud Dataflow, enabling automated provisioning of computing resources tailored to dynamic user workload, thereby ensuring optimal performance and cost efficiency of processing tasks.\nStep 3: Generate meeting transcriptions\nThe data processing pipeline begins by downloading audio and video recordings from \u200ccloud storage. Downloaded files are then meticulously transcribed by NVIDIA Riva. Producing more than a simple conversion of speech to text, Riva enhances transcription quality using contextual understanding. Punctuation and capitalization are refined to provide robust and accurate summarization and insight generation.\nStep 4: Generate summary and actionable insights\nThe meticulously transcribed text is then passed to an LLM to summarize the meeting content. Through refined prompt engineering, the LLM summarizes the meeting and generates valuable, actionable insights. The meeting summary and insights are then returned to the platform for user display.\nBenefits of adam.ai architecture\nThis architecture ensures \u200cefficient, scalable, and cost-effective meeting transcription and summarization. Specific benefits include:\nDynamically scalable and fault-tolerant system\nUsing Google Cloud Pub/Sub, the architecture embraces a loosely coupled, event-driven microservices approach, prompting a scalable, fault-tolerant system. This not only simplifies communication but also provides independent functionality of components. Additionally, Google Cloud Dataflow automatic resource provisioning dynamically scales computing power, resulting in cost-effective data processing.\nReal-time accurate meeting transcriptions\nThe Riva ASR model supports streaming audio and provides real-time accurate transcriptions. Its ability to refine punctuation and capitalization elevates transcript quality, enabling accurate summarization and extraction of valuable insights.\nIntelligible and well-structured summarizations\nLLM integration provides intelligible and well-structured summaries, fostering the extraction of valuable and actionable insights from the meeting transcript.\nIntuitive user experience\nThe entire process, from transcription to summarization, is seamlessly integrated into the platform. Requests and results flow efficiently through the Pub/Sub system, providing a smooth and intuitive user experience and easy access to meeting insights.\nSummary\nTransform your meetings into more productive, dynamic collaborations with adam.ai. Working together, ASR and LLMs seamlessly capture every word spoken, extract key insights, and generate detailed notes. This frees participants from the burden of note-taking so they can fully engage in the meeting.\nTo ensure scalable, low-latency, and cost-effective processing of meetings\u2019 audio data, the adam.ai meeting management platform employs a cloud-native microservice-based architecture. This architecture enables real-time accurate transcriptions and enhanced punctuation and capitalization powered by NVIDIA Riva, providing you with a comprehensive and polished record of your meetings.\nTo explore how adam.ai can help elevate your meetings, sign up for a free trial. To learn more about LLM enterprise applications, see Getting Started with Large Language Models for Enterprise Solutions. And join the conversation on Speech AI in the NVIDIA Riva forum."}], "https://developer.nvidia.com/blog/train-generative-ai-models-for-drug-discovery-with-bionemo-framework/": [{"text": "NVIDIA has released the BioNeMo Framework, available for download on NGC, to facilitate the training of generative AI models, large language models, and foundation models for drug discovery applications. The platform offers managed services, API endpoints, and training frameworks to simplify and scale generative AI for drug discovery. BioNeMo supports pre-training or fine-tuning of state-of-the-art models with end-to-end acceleration at scale and can be deployed on cloud platforms or on-premises infrastructure. The framework includes optimized model architectures and tooling for training protein and small molecule models, such as ESM, MegaMolBART, and ProtT5. BioNeMo enables high throughput and scalability through techniques like parallelism and precision optimization, allowing for the training of large models in shorter time frames. By leveraging BioNeMo Framework, researchers and developers in drug discovery can easily build and integrate AI applications across the entire drug discovery pipeline.", "text_components": ["Train Generative AI Models for Drug Discovery with NVIDIA BioNeMo Framework\nNVIDIA BioNeMo Framework has been released and is now generally available to download on NGC, enabling researchers to build and deploy generative AI, large language models (LLMs), and foundation models in drug discovery applications.\nThe BioNeMo platform includes managed services, API endpoints, and training frameworks to simplify, accelerate, and scale generative AI for drug discovery. BioNeMo provides the capability to pre-train or fine-tune state-of-the-art models with end-to-end acceleration at scale. It is available as a fully managed service on NVIDIA DGX Cloud with NVIDIA Base Command platform and also as a downloadable framework for deployment with on-premises infrastructure and a variety of cloud platforms.\nThis provides drug discovery researchers and developers with a fast and easy way to build and integrate state-of-the-art AI applications across the entire drug discovery pipeline, from target identification to lead optimization.", "BioNeMo Framework v1.0 features\nEasy data loading with automatic downloaders, pre-processed data, and support for common biomolecular data formats.\nSOTA domain-specific models, including out-of-the-box architectures and validated checkpoints for training on protein and small molecule data.\nOptimized scaling recipes for seamless accelerated training on 1,000s of GPUs, optimized to maximize throughput and reduce cost.\nFlexible training workflows to enable easy large-scale pre-training from scratch, fine-tuning from reliable checkpoints, and downstream task training at speed.\nValidation-in-the-loop, with periodic supervised task training to measure the quality of embeddings as the model trains. Fully automated and integrated with Weights and Biases.", "Optimized training for protein and small molecule models\nNVIDIA BioNeMo provides optimizations for generative AI models across multiple domains. BioNeMo Framework v1.0 delivers optimized model architectures and tooling for training protein and small molecule LLMs:\nBioNeMo ESM1 and ESM2\nBioNeMo MegaMolBART\nBioNeMo ProtT5", "BioNeMo ESM1 and ESM2\nThe ESM model family is a collection of transformer-based, protein language models built on the BERT architecture and produced by the Meta Fundamental AI Research Protein Team (FAIR).\nThe general-purpose ESM-like architectures are optimized and available now in BioNeMo Framework and can be leveraged for custom training of protein LLMs. These models are trained on massive datasets of protein sequences to learn the underlying patterns and relationships between amino acids that govern protein structure and function.\nImportantly, trained ESM models can be harnessed for a variety of downstream tasks through transfer learning. For instance, you can use the embeddings from its encoder to train a smaller model with a supervised learning objective to infer the properties of proteins. This has been shown to produce highly accurate models for a variety of tasks such as 3D structure prediction, variant effect prediction, or designing de novo proteins.\nBioNeMo Framework includes validated training checkpoints for ESM-2 650 million\u2013 and 3B-parameter models, enabling a zero-shot start to create custom, domain-specific applications. A number of example downstream tasks, including secondary structure prediction, subcellular localization prediction, and thermal stability prediction are also provided.", "BioNeMo MegaMolBART\nThe MegaMolBART model is a generative chemistry model built using the seq2seq transformer BART architecture, and inspired by the Chemformer model developed by AstraZeneca. MegaMolBART was trained on the ZINC-15 database of small molecule SMILES strings, using 1.5B molecules for training in total.\nThe embeddings from its encoder can be used for downstream predictive models, much in the same way as ESM or the encoder and decoder can be used together for novel molecule generation by sampling the embedding space. This means MegaMolBART can be used for a variety of cheminformatics drug discovery tasks, such as reaction prediction, molecular optimization, and de novo molecular generation.\nMegaMolBART was developed using BioNeMo Framework, which includes a trained and validated checkpoint for a 45M parameter model. Downstream task workflows are also provided for the prediction of retrosynthetic reactions and physicochemical properties, such as lipophilicity, aqueous solubility (ESOL), and hydration-free energy (FreeSolv).", "BioNeMo ProtT5\nProtT5 is a protein language model built on an encoder/decoder LLM, developed by the Rost Lab using the T5 architecture. Like the ESM models, ProtT5 can produce embeddings from its encoder for representation learning but it can also use the entire encoder/decoder architecture for sequence translation tasks.\nAs with others, the base model can be extended for downstream tasks such as generating protein sequences. A recent example of this was the extension of the model by startup Evozyne to create two proteins. The proteins have significant potential in healthcare (aiming to cure a congenital disease) and also in clean energy (designed to consume carbon dioxide to reduce global warming).\nOptimized as part of BioNeMo Framework, the ProtT5 model includes a trained and validated checkpoint for a 192M parameter model and a sample downstream task workflow for secondary structure prediction.", "Speed and scale with BioNeMo Framework\nBioNeMo Framework uses a variety of techniques to achieve higher throughput and improved scalability, including parallelism:\nModel pipeline parallelism: The layers of a model are distributed for parallel training.\nModel tensor parallelism: The layers are themselves sliced and distributed.\nSpecifying optimizations like precision can also confer huge performance benefits, often with little to no effect on model accuracy.\nBioNeMo Framework includes best practices for selecting and tuning hyperparameters of the models, with the ability to easily configure many of these options for maximum performance. One example would be applying techniques such as model tensor parallelism to models over 1B parameters in size and model pipeline parallelism for models over 5B parameters.", "Scaling ESM2 training across H100 GPUs with BioNeMo Framework\nThe graph shows scaling of million tokens per second as the number of GPUs increases.\nFigure 1. Example of scaling training for a 3B parameter ESM2 model on DGX H100\nFigure 1 shows scaling from a single DGX node (eight H100 GPUs) to 32 DGX nodes (256 H100 GPUs), and the resultant increase in throughput (tokens per second).\nThe full-stack optimizations afforded by BioNeMo Framework and the latest NVIDIA GPUs enable training state-of-the-art models at a much-improved speed and efficiency.\nAs an example, ESM2 was trained as part of its original publication in 8 days for a 650M parameter model, and 30 days for a 3B parameter model, on 512 V100 GPUs. Training these same models with BioNeMo Framework and on 512 H100 GPUs (trained on 1T tokens, or 1.19B protein sequences) can now be achieved in just 1.2 days and 3.5 days respectively.\nThis provides the opportunity to train even larger models in shorter time frames. For example, an ESM2 model of 20B parameters can be trained on 1T tokens in just 18.6 days with BioNeMo Framework and 512 H100 GPUs.", "Training larger ESM2 models in less time\nThe graph shows that training time is up to 16x faster with H100 GPUs and BioNeMo.\nFigure 2. Example training times of ESM2 for different GPUs with BioNeMo Framework\nThe original published model training time (512 V100s) is shown for reference in gray in the first column. Models trained with BioNeMo were trained on 1T tokens, equivalent to 1.19B protein sequences.", "BioNeMo workflow\nDiagram shows the steps for access and resources: bringing your own data, getting DGX Cloud Service, selecting a BioNeMo model and training with pretrained models, data loaders, and training scripts, and a central UI for launching multi-node training.\nFigure 3. Resources for BioNeMo Framework", "Getting started with BioNeMo Framework\nBioNeMo Framework v1.0 is available now on NGC. For more information about access, the latest technical posts, and talks on AI for drug discovery, see the BioNeMo Get Started and Resources pages.\nBioNeMo Framework is best deployed on NVIDIA DGX Cloud, which provides on-demand DGX infrastructure for optimal throughput performance. This provides a full-stack AI-training-as-a-service solution for enterprise-grade AI computing in the cloud and direct access to NVIDIA AI experts. For more information, see the DGX Cloud page."], "document_title": "Train Generative AI Models for Drug Discovery with NVIDIA BioNeMo Framework", "document_url": "https://developer.nvidia.com/blog/train-generative-ai-models-for-drug-discovery-with-bionemo-framework/", "document_date": "2023-11-29T19:11:15", "document_date_modified": "2023-12-14T19:27:34", "document_full_text": "Train Generative AI Models for Drug Discovery with NVIDIA BioNeMo Framework\nNVIDIA BioNeMo Framework has been released and is now generally available to download on NGC, enabling researchers to build and deploy generative AI, large language models (LLMs), and foundation models in drug discovery applications.\nThe BioNeMo platform includes managed services, API endpoints, and training frameworks to simplify, accelerate, and scale generative AI for drug discovery. BioNeMo provides the capability to pre-train or fine-tune state-of-the-art models with end-to-end acceleration at scale. It is available as a fully managed service on NVIDIA DGX Cloud with NVIDIA Base Command platform and also as a downloadable framework for deployment with on-premises infrastructure and a variety of cloud platforms.\nThis provides drug discovery researchers and developers with a fast and easy way to build and integrate state-of-the-art AI applications across the entire drug discovery pipeline, from target identification to lead optimization.\nBioNeMo Framework v1.0 features\nEasy data loading with automatic downloaders, pre-processed data, and support for common biomolecular data formats.\nSOTA domain-specific models, including out-of-the-box architectures and validated checkpoints for training on protein and small molecule data.\nOptimized scaling recipes for seamless accelerated training on 1,000s of GPUs, optimized to maximize throughput and reduce cost.\nFlexible training workflows to enable easy large-scale pre-training from scratch, fine-tuning from reliable checkpoints, and downstream task training at speed.\nValidation-in-the-loop, with periodic supervised task training to measure the quality of embeddings as the model trains. Fully automated and integrated with Weights and Biases.\nOptimized training for protein and small molecule models\nNVIDIA BioNeMo provides optimizations for generative AI models across multiple domains. BioNeMo Framework v1.0 delivers optimized model architectures and tooling for training protein and small molecule LLMs:\nBioNeMo ESM1 and ESM2\nBioNeMo MegaMolBART\nBioNeMo ProtT5\nBioNeMo ESM1 and ESM2\nThe ESM model family is a collection of transformer-based, protein language models built on the BERT architecture and produced by the Meta Fundamental AI Research Protein Team (FAIR).\nThe general-purpose ESM-like architectures are optimized and available now in BioNeMo Framework and can be leveraged for custom training of protein LLMs. These models are trained on massive datasets of protein sequences to learn the underlying patterns and relationships between amino acids that govern protein structure and function.\nImportantly, trained ESM models can be harnessed for a variety of downstream tasks through transfer learning. For instance, you can use the embeddings from its encoder to train a smaller model with a supervised learning objective to infer the properties of proteins. This has been shown to produce highly accurate models for a variety of tasks such as 3D structure prediction, variant effect prediction, or designing de novo proteins.\nBioNeMo Framework includes validated training checkpoints for ESM-2 650 million\u2013 and 3B-parameter models, enabling a zero-shot start to create custom, domain-specific applications. A number of example downstream tasks, including secondary structure prediction, subcellular localization prediction, and thermal stability prediction are also provided.\nBioNeMo MegaMolBART\nThe MegaMolBART model is a generative chemistry model built using the seq2seq transformer BART architecture, and inspired by the Chemformer model developed by AstraZeneca. MegaMolBART was trained on the ZINC-15 database of small molecule SMILES strings, using 1.5B molecules for training in total.\nThe embeddings from its encoder can be used for downstream predictive models, much in the same way as ESM or the encoder and decoder can be used together for novel molecule generation by sampling the embedding space. This means MegaMolBART can be used for a variety of cheminformatics drug discovery tasks, such as reaction prediction, molecular optimization, and de novo molecular generation.\nMegaMolBART was developed using BioNeMo Framework, which includes a trained and validated checkpoint for a 45M parameter model. Downstream task workflows are also provided for the prediction of retrosynthetic reactions and physicochemical properties, such as lipophilicity, aqueous solubility (ESOL), and hydration-free energy (FreeSolv).\nBioNeMo ProtT5\nProtT5 is a protein language model built on an encoder/decoder LLM, developed by the Rost Lab using the T5 architecture. Like the ESM models, ProtT5 can produce embeddings from its encoder for representation learning but it can also use the entire encoder/decoder architecture for sequence translation tasks.\nAs with others, the base model can be extended for downstream tasks such as generating protein sequences. A recent example of this was the extension of the model by startup Evozyne to create two proteins. The proteins have significant potential in healthcare (aiming to cure a congenital disease) and also in clean energy (designed to consume carbon dioxide to reduce global warming).\nOptimized as part of BioNeMo Framework, the ProtT5 model includes a trained and validated checkpoint for a 192M parameter model and a sample downstream task workflow for secondary structure prediction.\nSpeed and scale with BioNeMo Framework\nBioNeMo Framework uses a variety of techniques to achieve higher throughput and improved scalability, including parallelism:\nModel pipeline parallelism: The layers of a model are distributed for parallel training.\nModel tensor parallelism: The layers are themselves sliced and distributed.\nSpecifying optimizations like precision can also confer huge performance benefits, often with little to no effect on model accuracy.\nBioNeMo Framework includes best practices for selecting and tuning hyperparameters of the models, with the ability to easily configure many of these options for maximum performance. One example would be applying techniques such as model tensor parallelism to models over 1B parameters in size and model pipeline parallelism for models over 5B parameters.\nScaling ESM2 training across H100 GPUs with BioNeMo Framework\nThe graph shows scaling of million tokens per second as the number of GPUs increases.\nFigure 1. Example of scaling training for a 3B parameter ESM2 model on DGX H100\nFigure 1 shows scaling from a single DGX node (eight H100 GPUs) to 32 DGX nodes (256 H100 GPUs), and the resultant increase in throughput (tokens per second).\nThe full-stack optimizations afforded by BioNeMo Framework and the latest NVIDIA GPUs enable training state-of-the-art models at a much-improved speed and efficiency.\nAs an example, ESM2 was trained as part of its original publication in 8 days for a 650M parameter model, and 30 days for a 3B parameter model, on 512 V100 GPUs. Training these same models with BioNeMo Framework and on 512 H100 GPUs (trained on 1T tokens, or 1.19B protein sequences) can now be achieved in just 1.2 days and 3.5 days respectively.\nThis provides the opportunity to train even larger models in shorter time frames. For example, an ESM2 model of 20B parameters can be trained on 1T tokens in just 18.6 days with BioNeMo Framework and 512 H100 GPUs.\nTraining larger ESM2 models in less time\nThe graph shows that training time is up to 16x faster with H100 GPUs and BioNeMo.\nFigure 2. Example training times of ESM2 for different GPUs with BioNeMo Framework\nThe original published model training time (512 V100s) is shown for reference in gray in the first column. Models trained with BioNeMo were trained on 1T tokens, equivalent to 1.19B protein sequences.\nBioNeMo workflow\nDiagram shows the steps for access and resources: bringing your own data, getting DGX Cloud Service, selecting a BioNeMo model and training with pretrained models, data loaders, and training scripts, and a central UI for launching multi-node training.\nFigure 3. Resources for BioNeMo Framework\nGetting started with BioNeMo Framework\nBioNeMo Framework v1.0 is available now on NGC. For more information about access, the latest technical posts, and talks on AI for drug discovery, see the BioNeMo Get Started and Resources pages.\nBioNeMo Framework is best deployed on NVIDIA DGX Cloud, which provides on-demand DGX infrastructure for optimal throughput performance. This provides a full-stack AI-training-as-a-service solution for enterprise-grade AI computing in the cloud and direct access to NVIDIA AI experts. For more information, see the DGX Cloud page."}], "https://developer.nvidia.com/blog/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/": [{"text": "The article discusses the NVIDIA Base Command Platform software service, which aims to streamline AI development workflows. It highlights several new features, including one-click environment deployments with Quick Start CPU nodes, efficient data ingestion with Data Mover, and secure data and collaborative workflows with Secrets Management Service. The Quick Start feature allows teams to easily define and launch frequent configurations with a single click, while CPU instances can now be added to the Accelerated Compute Environment for tasks such as preprocessing and data wrangling. Data Mover simplifies data import and export, allowing teams to bring external datasets into Base Command Platform storage. Additionally, Secrets Management Service enables the secure injection of hidden environment variables for collaborative workflows. Overall, NVIDIA Base Command Platform aims to simplify high-performance workloads on NVIDIA GPUs, offering features to accelerate job initialization, manage data workflows, support diverse workloads, and secure sensitive information. Organizations can enable these features through the Web UI Setup page and NGC CLI.", "text_components": ["Streamline Job Initialization and CPU-Based Tasks with NVIDIA Base Command Platform\nNVIDIA Base Command Platform software service offers increasingly streamlined workflows for accelerating AI development. This post explains several recently added features, including:\nOne-click environment deployments with Quick Start\nCPU nodes for compute-light tasks\nEfficient data ingestion with Data Mover\nSecure data and collaborative workflows with Secrets Management Service (SMS)", "Effortless environment initialization\nLaunching an exploration environment on a supercomputer has never been easier.\nWith the Quick Start feature, teams can programmatically define their frequent configurations\u2014from GPU and CPU instance types, memory, and storage needs to adding datasets, workspaces, swapping containers, and even setting default commands. These are defined as a template that can be launched with a single click from the Base Command Dashboard.\nIn addition to building templates yourself, you can leverage a catalog of NVIDIA-built Quick Starts. Default Quick Starts for JupyterLab and for Dask+RAPIDS are now available. As that catalog grows, GPU-accelerated data science will become more accessible across the wide variety of toolsets data scientists use today.\nDeep integration of software and hardware is a core principle of Base Command Platform, and Quick Starts are no different. The NVIDIA custom scheduler ensures that these user-defined environment needs are routed appropriately in the cluster to ensure high cluster utilization across all jobs. Base Command Platform makes hard things simple.", "Support for diverse workloads\nCPU instances are now available in Base Command Platform. Adding both CPU and GPU instances to your Accelerated Compute Environment (ACE) brings flexibility, optimization, and cost-efficiency to your AI workflows.\nCPU instances are well suited for performing preprocessing and data wrangling tasks that don\u2019t require the computational intensity of a GPU. GPU instances are then free for compute-heavy jobs like training.\nUse CPU nodes in Base Command Platform for a wide range of tasks, from downloading datasets and running notebooks to editing code, browsing files inside datasets, and even running monitoring tools like TensorBoard.\nYou also get a seamless experience across CPU and GPU nodes in Base Command Platform. CPU instances can leverage existing constructs associated with GPU workloads, like interaction with shared resources (datasets, for example). Like the existing GPU experience, you\u2019ll receive default CPU quota, run time limits, and be able to experience CPU telemetry in the user interface.\nTo request CPU nodes in Base Command Platform for your organization, contact your account team.", "Streamline data import and export\nManaging your data workflow in Base Command Platform is simpler than ever with the Data Mover feature.\nData Mover enables the import of external object datasets directly into Base Command Platform storage using NVIDIA NGC CLI. While Base Command Platform supports connecting to external object dataset during a training job, teams often choose to bring datasets into Base Command Platform to take advantage of our performance optimizations for the compute-adjacent storage inside your ACE.\nData Mover has strong ties to two things we all care deeply about: utilization and security. Data Mover enables teams to perform their data processing jobs on CPU nodes, keeping GPU nodes free to participate in more compute-heavy workloads. A wget should never hold GPU cycles hostage. And on the security front, Data Mover requires the setup of NGC Secrets with designated keys.\nThe following functionalities are now available: Dataset Import, Dataset Export, and Workspace Export. The ability to customize the default Dask+NVIDIA RAPIDS Quick Start and define repeated data movement jobs will be a strong combination.", "Secure collaborative workflows\nHandling sensitive data like API keys or tokens within an application is a critical concern. You can make security best practices the default while still delivering interesting collaborative functionality.\nIntegrated with Base Command Platform jobs, Secrets Management Service (SMS) enables the direct injection of hidden environment variables. Information will remain private unless you decide to make it visible.\nAt this time, organizations must enable Secrets Management to utilize SMS. After enablement, access to SMS is available through the Web UI Setup page and the NGC CLI from version 3.21.1.", "Summary\nNVIDIA Base Command Platform enables teams to simplify high-performance workloads on NVIDIA GPUs. With recently added features, you can rapidly initialize jobs, better manage data workflows, support diverse workloads, and secure sensitive information.\nGet started with Base Command Platform and NVIDIA DGX Cloud.\nRead Simplifying AI Development with NVIDIA Base Command Platform to learn more about NVIDIA Base Command Platform.\nCheck out the NVIDIA Base Command Platform documentation and release notes.\nWatch a demo video of NVIDIA Base Command Platform.\nJoin the conversation in the NVIDIA Developer Forums."], "document_title": "Streamline Job Initialization and CPU-Based Tasks with NVIDIA Base Command Platform", "document_url": "https://developer.nvidia.com/blog/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/", "document_date": "2023-11-29T19:00:00", "document_date_modified": "2023-12-14T19:27:35", "document_full_text": "Streamline Job Initialization and CPU-Based Tasks with NVIDIA Base Command Platform\nNVIDIA Base Command Platform software service offers increasingly streamlined workflows for accelerating AI development. This post explains several recently added features, including:\nOne-click environment deployments with Quick Start\nCPU nodes for compute-light tasks\nEfficient data ingestion with Data Mover\nSecure data and collaborative workflows with Secrets Management Service (SMS)\nEffortless environment initialization\nLaunching an exploration environment on a supercomputer has never been easier.\nWith the Quick Start feature, teams can programmatically define their frequent configurations\u2014from GPU and CPU instance types, memory, and storage needs to adding datasets, workspaces, swapping containers, and even setting default commands. These are defined as a template that can be launched with a single click from the Base Command Dashboard.\nIn addition to building templates yourself, you can leverage a catalog of NVIDIA-built Quick Starts. Default Quick Starts for JupyterLab and for Dask+RAPIDS are now available. As that catalog grows, GPU-accelerated data science will become more accessible across the wide variety of toolsets data scientists use today.\nDeep integration of software and hardware is a core principle of Base Command Platform, and Quick Starts are no different. The NVIDIA custom scheduler ensures that these user-defined environment needs are routed appropriately in the cluster to ensure high cluster utilization across all jobs. Base Command Platform makes hard things simple.\nSupport for diverse workloads\nCPU instances are now available in Base Command Platform. Adding both CPU and GPU instances to your Accelerated Compute Environment (ACE) brings flexibility, optimization, and cost-efficiency to your AI workflows.\nCPU instances are well suited for performing preprocessing and data wrangling tasks that don\u2019t require the computational intensity of a GPU. GPU instances are then free for compute-heavy jobs like training.\nUse CPU nodes in Base Command Platform for a wide range of tasks, from downloading datasets and running notebooks to editing code, browsing files inside datasets, and even running monitoring tools like TensorBoard.\nYou also get a seamless experience across CPU and GPU nodes in Base Command Platform. CPU instances can leverage existing constructs associated with GPU workloads, like interaction with shared resources (datasets, for example). Like the existing GPU experience, you\u2019ll receive default CPU quota, run time limits, and be able to experience CPU telemetry in the user interface.\nTo request CPU nodes in Base Command Platform for your organization, contact your account team.\nStreamline data import and export\nManaging your data workflow in Base Command Platform is simpler than ever with the Data Mover feature.\nData Mover enables the import of external object datasets directly into Base Command Platform storage using NVIDIA NGC CLI. While Base Command Platform supports connecting to external object dataset during a training job, teams often choose to bring datasets into Base Command Platform to take advantage of our performance optimizations for the compute-adjacent storage inside your ACE.\nData Mover has strong ties to two things we all care deeply about: utilization and security. Data Mover enables teams to perform their data processing jobs on CPU nodes, keeping GPU nodes free to participate in more compute-heavy workloads. A wget should never hold GPU cycles hostage. And on the security front, Data Mover requires the setup of NGC Secrets with designated keys.\nThe following functionalities are now available: Dataset Import, Dataset Export, and Workspace Export. The ability to customize the default Dask+NVIDIA RAPIDS Quick Start and define repeated data movement jobs will be a strong combination.\nSecure collaborative workflows\nHandling sensitive data like API keys or tokens within an application is a critical concern. You can make security best practices the default while still delivering interesting collaborative functionality.\nIntegrated with Base Command Platform jobs, Secrets Management Service (SMS) enables the direct injection of hidden environment variables. Information will remain private unless you decide to make it visible.\nAt this time, organizations must enable Secrets Management to utilize SMS. After enablement, access to SMS is available through the Web UI Setup page and the NGC CLI from version 3.21.1.\nSummary\nNVIDIA Base Command Platform enables teams to simplify high-performance workloads on NVIDIA GPUs. With recently added features, you can rapidly initialize jobs, better manage data workflows, support diverse workloads, and secure sensitive information.\nGet started with Base Command Platform and NVIDIA DGX Cloud.\nRead Simplifying AI Development with NVIDIA Base Command Platform to learn more about NVIDIA Base Command Platform.\nCheck out the NVIDIA Base Command Platform documentation and release notes.\nWatch a demo video of NVIDIA Base Command Platform.\nJoin the conversation in the NVIDIA Developer Forums."}], "https://developer.nvidia.com/blog/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/": [{"text": "CUDA Quantum 0.5 is a platform for building quantum-classical computing applications that accelerates workflows such as quantum simulation and quantum chemistry using GPUs. The latest release introduces more QPU backends, simulators, and improvements in control flow support. New features include adaptive quantum kernels, Fermionic and Givens rotation kernels for quantum chemistry simulations, and support for exponentials of Pauli matrices. CUDA Quantum now supports IQM and Oxford Quantum Circuits QPU backends, as well as tensor network and matrix product state simulators for large-scale quantum circuit simulation. The Getting Started guide provides setup steps for Python and C++ examples to help users explore CUDA Quantum capabilities. Researchers can also access the open-source repository to report issues and make feature suggestions. Overall, CUDA Quantum 0.5 offers enhanced support for quantum-classical programming and a variety of quantum technologies.", "text_components": ["CUDA Quantum 0.5 Delivers New Features for Quantum-Classical Computing\nCUDA Quantum is a platform for building quantum-classical computing applications. It is an open-source programming model for heterogeneous computing such as quantum processor units (QPUs), GPUs, and CPUs.\nCUDA Quantum accelerates workflows such as quantum simulation, quantum machine learning, quantum chemistry, and more. It optimizes these workflows as part of its compiler toolchain and uses the power of GPUs to accelerate them. CUDA Quantum offers kernel-based programming and can be used with Python or C++.", "What\u2019s new in CUDA Quantum 0.5?\nThe latest release, CUDA Quantum 0.5, introduces more QPUs backends, more simulators, and other improvements. For more information, see the CUDA Quantum 0.5 release notes.", "Core features\nQuantum error correction and other forms of hybrid quantum-classical computation often require nontrivial control flow and tightly interwoven primitives. CUDA Quantum now supports running adaptive quantum kernels: a specification from the QIR alliance and a key step towards truly integrated quantum-classical programming.\nFermionic and Givens rotation and fermionic SWAP kernels are used in quantum chemistry simulations to perform operations on fermionic systems. The Givens rotation kernel is used to perform rotations on qubits, while the fermionic SWAP kernel is used to swap the states of two qubits. The addition of these kernels to the CUDA Quantum makes it easier for researchers to perform quantum chemistry simulations and develop new quantum algorithms for chemistry applications.\nThe Pauli matrices are a set of matrices that are commonly used in quantum mechanics to represent quantum states and operators. The added support of exponentials of Pauli matrices in CUDA Quantum is useful for researchers performing quantum simulations of physical systems, such as molecules, and for developing quantum algorithms for optimization problems.\nCUDA Quantum now has improved support for ```std::vector``` and (C style) arrays, as well as support for execution of for\u2013 and while-loops of known lengths on quantum hardware backends. These features are useful for developing quantum algorithms that require complex data structures and control flow.", "IQM and Oxford Quantum Circuits QPU backends\nA QPU backend is a hardware computing device that acts as a quantum processing unit and can run quantum workloads. CUDA Quantum is integrated with several quantum hardware providers\u2019 QPUs.\nIQM and Oxford Quantum Circuits (OQC) quantum computers are now supported as QPU backends in CUDA Quantum. This is a great addition to the already supported quantum computers from Quantinuum and IonQ, which enable you to run CUDA Quantum code on a variety of different quantum technologies available today.\nFor more information about how to use the backends in either Python or C++, see the IQM or OQC documentation.", "Tensor network and matrix product state simulators\nTensor network-based simulators are suitable for large-scale simulation of certain classes of quantum circuits involving many qubits, beyond the memory limit of state vector-based simulators. Tensor network simulation is improved with this release and is accelerated with the cuQuantum library. For more information, see Tensor Network Simulators.\nA matrix product state (MPS) simulator has been added to CUDA Quantum in this release. MPS representation takes advantage of tensor network sparsity by using tensor decomposition techniques such as QR and SVD. This is an approximate simulator in nature and therefore can handle a large number of qubits and more gate depth for certain classes of quantum circuits on a relatively small memory footprint. For more information, see Matrix Product State Simulator.", "Getting started with CUDA Quantum\nThe CUDA Quantum Getting Started guide walks you through the setup steps so you can get started with Python and C++ examples that provide a quick learning path for CUDA Quantum capabilities.\nFor more information about advanced use cases for quantum\u2013classical applications, see the tutorials gallery. Finally, explore the code in the CUDA Quantum open-source repository. This is where you can report issues and also make feature suggestions."], "document_title": "CUDA Quantum 0.5 Delivers New Features for Quantum-Classical Computing", "document_url": "https://developer.nvidia.com/blog/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/", "document_date": "2023-11-29T17:00:00", "document_date_modified": "2023-12-14T19:27:35", "document_full_text": "CUDA Quantum 0.5 Delivers New Features for Quantum-Classical Computing\nCUDA Quantum is a platform for building quantum-classical computing applications. It is an open-source programming model for heterogeneous computing such as quantum processor units (QPUs), GPUs, and CPUs.\nCUDA Quantum accelerates workflows such as quantum simulation, quantum machine learning, quantum chemistry, and more. It optimizes these workflows as part of its compiler toolchain and uses the power of GPUs to accelerate them. CUDA Quantum offers kernel-based programming and can be used with Python or C++.\nWhat\u2019s new in CUDA Quantum 0.5?\nThe latest release, CUDA Quantum 0.5, introduces more QPUs backends, more simulators, and other improvements. For more information, see the CUDA Quantum 0.5 release notes.\nCore features\nQuantum error correction and other forms of hybrid quantum-classical computation often require nontrivial control flow and tightly interwoven primitives. CUDA Quantum now supports running adaptive quantum kernels: a specification from the QIR alliance and a key step towards truly integrated quantum-classical programming.\nFermionic and Givens rotation and fermionic SWAP kernels are used in quantum chemistry simulations to perform operations on fermionic systems. The Givens rotation kernel is used to perform rotations on qubits, while the fermionic SWAP kernel is used to swap the states of two qubits. The addition of these kernels to the CUDA Quantum makes it easier for researchers to perform quantum chemistry simulations and develop new quantum algorithms for chemistry applications.\nThe Pauli matrices are a set of matrices that are commonly used in quantum mechanics to represent quantum states and operators. The added support of exponentials of Pauli matrices in CUDA Quantum is useful for researchers performing quantum simulations of physical systems, such as molecules, and for developing quantum algorithms for optimization problems.\nCUDA Quantum now has improved support for ```std::vector``` and (C style) arrays, as well as support for execution of for\u2013 and while-loops of known lengths on quantum hardware backends. These features are useful for developing quantum algorithms that require complex data structures and control flow.\nIQM and Oxford Quantum Circuits QPU backends\nA QPU backend is a hardware computing device that acts as a quantum processing unit and can run quantum workloads. CUDA Quantum is integrated with several quantum hardware providers\u2019 QPUs.\nIQM and Oxford Quantum Circuits (OQC) quantum computers are now supported as QPU backends in CUDA Quantum. This is a great addition to the already supported quantum computers from Quantinuum and IonQ, which enable you to run CUDA Quantum code on a variety of different quantum technologies available today.\nFor more information about how to use the backends in either Python or C++, see the IQM or OQC documentation.\nTensor network and matrix product state simulators\nTensor network-based simulators are suitable for large-scale simulation of certain classes of quantum circuits involving many qubits, beyond the memory limit of state vector-based simulators. Tensor network simulation is improved with this release and is accelerated with the cuQuantum library. For more information, see Tensor Network Simulators.\nA matrix product state (MPS) simulator has been added to CUDA Quantum in this release. MPS representation takes advantage of tensor network sparsity by using tensor decomposition techniques such as QR and SVD. This is an approximate simulator in nature and therefore can handle a large number of qubits and more gate depth for certain classes of quantum circuits on a relatively small memory footprint. For more information, see Matrix Product State Simulator.\nGetting started with CUDA Quantum\nThe CUDA Quantum Getting Started guide walks you through the setup steps so you can get started with Python and C++ examples that provide a quick learning path for CUDA Quantum capabilities.\nFor more information about advanced use cases for quantum\u2013classical applications, see the tutorials gallery. Finally, explore the code in the CUDA Quantum open-source repository. This is where you can report issues and also make feature suggestions."}], "https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/": [{"text": "AWS and NVIDIA have partnered to introduce the NVIDIA GH200 NVL32 superchip in DGX Cloud on AWS, offering a 32-GPU NVLink domain and 19.5 TB of memory. This technology significantly improves performance in tasks like GPT-3 training and LLM inference. The CPU-GPU memory interconnect is 7x faster than PCIe Gen 5, providing more memory for applications. The GH200 NVL32 is a scalable design for hyperscale data centers, supported by NVIDIA software and libraries for thousands of GPU applications. This superchip is ideal for tasks such as LLM training, recommender systems, and GNNs, providing up to 7.9x faster training performance for models with massive embedding tables. The GH200 NVL32 can also increase GNN training performance by up to 5.8x compared to previous models. This technology is a game-changer for cloud computing, offering improved performance and efficiency for a wide range of AI and computing applications.", "text_components": ["One Giant Superchip for LLMs, Recommenders, and GNNs: Introducing NVIDIA GH200 NVL32\nAt AWS re:Invent 2023, AWS and NVIDIA announced that AWS will be the first cloud provider to offer NVIDIA GH200 Grace Hopper Superchips interconnected with NVIDIA NVLink technology through NVIDIA DGX Cloud and running on Amazon Elastic Compute Cloud (Amazon EC2). This is a game-changing technology for cloud computing.\nThe NVIDIA GH200 NVL32, a rack-scale solution within NVIDIA DGX Cloud or an Amazon instance, boasts a 32-GPU NVIDIA NVLink domain and a massive 19.5 TB of unified memory. Breaking through the memory constraints of a single system, it is 1.7x faster for GPT-3 training and 2x faster for large language model (LLM) inference compared to NVIDIA HGX H100.\nNVIDIA GH200 Grace Hopper Superchip-powered instances in AWS will feature 4.5 TB of HBM3e memory, a 7.2x increase compared to current-generation NVIDIA H100-powered EC2 P5 instances. This enables developers to run larger models, while improving training performance.\nAdditionally, the CPU to GPU memory interconnect is 900 GB/s, which is 7x faster than PCIe Gen 5. GPUs access CPU memory in a cache-coherent way, extending the total memory available for applications. This is the first use of the NVIDIA GH200 NVL32 scale-out design, a modular reference design for supercomputing, data centers, and cloud infrastructure. It provides a common architecture for GH200 and successor processor configurations.\nThis post explains the reference design that makes this possible and includes some representative application performance results.", "NVIDIA GH200 NVL32\nNVIDIA GH200 NVL32 is a rack-scale reference design for NVIDIA GH200 Grace Hopper Superchips connected through NVLink targeted for hyperscale data centers. NVIDIA GH200 NVL32 supports 16 dual NVIDIA Grace Hopper server nodes compatible with the NVIDIA MGX chassis design and can be liquid cooled to maximize compute density and efficiency.\nOn the left is the front view of the GH200 NVL32 rack with three power shelves at the bottom, eight GH200 compute trays, nine NVLink switch trays, eight GH200 compute trays, and then three power shelves on top. There are seven open shelves at the top. On the right is a back view of the GH200 NVL32 rack showing the liquid cooling manifold and the NVLink Interconnect cable cartridges that fully interconnect the GH200 nodes with the NVLink Switches.\nFigure 1. NVIDIA GH200 NVL32 is a rack-scale solution delivering a 32-GPU NVLink domain and 19.5 TB of unified memory\nThe NVIDIA GH200 Grace Hopper Superchip with a coherent NVLink-C2C creates an NVLink addressable memory address space to simplify model programming. It combines high-bandwidth and low-power system memory, LPDDR5X, and HBM3e to take full advantage of NVIDIA GPU acceleration and high-performance Arm cores in a well-balanced system.\nGH200 server nodes are connected with an NVLink passive copper cable cartridge to enable each Hopper GPU to access the memory of any other Grace Hopper Superchip in the network, providing 32 x 624 GB, or 19.5 TB of NVLink addressable memory (Figure 1).\nThis update to the NVLink Switch System uses the NVLink copper interconnect to connect 32 GH200 GPUs together using nine NVLink switches incorporating third-generation NVSwitch chips. The NVLink Switch System implements a fully connected fat-tree network for all the GPUs in the cluster. For larger scale requirements, scaling with 400 Gb/s InfiniBand or Ethernet delivers incredible performance and an energy-efficient AI supercomputing solution.\nNVIDIA GH200 NVL32 is supported by the NVIDIA HPC SDK and the full suite of CUDA, NVIDIA CUDA-X, and NVIDIA Magnum IO libraries, accelerating over 3,000 GPU applications.", "Use cases and performance results\nNVIDIA GH200 NVL32 is ideal for LLM training and inference, recommender systems, graph neural networks (GNNs), vector databases, and retrieval-augmented generation (RAG) models, as detailed below.", "AI training and inference\nGenerative AI has taken the world by storm, exemplified by the groundbreaking capabilities of services like ChatGPT. LLMs such as GPT-3 and GPT-4 are enabling the integration of AI capabilities into every product in every industry, and their adoption rate is astounding.\nChatGPT became the fastest application to reach 100 million users, achieving that milestone in just 2 months. The demand for generative AI applications is immense and growing exponentially.\nBar chart comparison shows a relative performance of 1.7x for NVIDIA GH200 NVL32 on the left and 1x for the H100 NVL8 on the right. The comparison is for GPT-3 training performance for an Ethernet data center using a batch size of 4 million tokens.\nFigure 2. An Ethernet data center with 16K GPUs using NVIDIA GH200 NVL32 will deliver 1.7x the performance of one composed of H100 NVL8, which is an NVIDIA HGX H100 server with eight NVLink-connected H100 GPUs. (Preliminary performance estimates subject to change.) LLMs require large-scale, multi-GPU training. The memory requirements for GPT-175B would be 700 GB, as each parameter needs four bytes (FP32). A combination of model parallelism and fast communications is used to avoid running out of memory with smaller memory GPUs.\nNVIDIA GH200 NVL32 is built for inference and for training the next generation of LLMs. Breaking through memory, communications, and computational bottlenecks with 32 NVLink-connected GH200 Grace Hopper Superchips, the system can train a trillion-parameter model over 1.7x faster than NVIDIA HGX H100.\nA graph with the vertical axis showing relative max throughput from 0 to 3x. The first column represents four servers with 8-way NVIDIA HGX H100 GPUs interconnected by NVLink and Ethernet connecting the four servers as the baseline at 1x. The second column shows GH200 NVL32 with 32 GPUs interconnected by NVLink and 2x relative max throughput.\nFigure 3. NVIDIA GH200 NVL32 shows 2x faster GPT-3 530B model inference performance compared to H100 NVL8 with 80 GB GPU memory. (Preliminary performance estimates subject to change.)\nFigure 3 shows that the NVIDIA GH200 NVL32 system outperforms four H100 NVL8 systems by 2x on a GPT-530B inference model. The large memory space of NVIDIA GH200 NVL32 also improves operational efficiency, with the ability to store multiple models on the same node and quickly swap models in to maximize utilization.", "Recommender systems\nRecommender systems are the engine of the personalized internet. They\u2019re used across e-commerce and retail, media and social media, digital ads, and more to personalize content. This drives revenue and business value. Recommenders use embeddings that represent users, products, categories, and context, and can range up to tens of terabytes in size.\nA highly accurate recommender system will provide a more engaging user experience, but also requires a larger embedding and more precise recommender. Embeddings have unique characteristics for AI models, requiring large amounts of memory at high bandwidth and lightning-fast networking.\nNVIDIA GH200 NVL32 with Grace Hopper provides 7x the amount of fast-access memory compared to four HGX H100 and delivers 7x the bandwidth compared to the PCIe Gen5 connections to the GPU in conventional x86-based designs. It enables 7x more detailed embeddings compared to H100 with x86.\nNVIDIA GH200 NVL32 can also deliver up to 7.9x the training performance for models with massive embedding tables. Figure 4 shows a comparison of one GH200 NVL32 system with 144 GB HBM3e memory and 32-way NVLink interconnect compared to four HGX H100 servers with 80 GB HBM3 memory connected with 8-way NVLink interconnect using a DLRM model. The comparisons were made between GH200 and H100 systems using 10 TB embedding tables and using 2 TB embedding tables.\nBar chart; left vertical axis scale is the geomean normalized to H100 time to train. The comparison for GH200 NVL32 to H100 NVL8 on the left shows GH200 is 2.5x faster to train a model with 2-TB embedding tables. The comparison for GH200 NVL32 to H100 NVL8 on the right shows GH200 is 7.9X faster to train a model with 10 TB embedding tables.\nFigure 4. A comparison of one NVIDIA GH200 NVL32 system to four HGX H100 servers on recommender training. (Preliminary performance estimates subject to change.)", "Graph neural networks\nGNNs apply the predictive power of deep learning to rich data structures that depict objects and their relationships as points connected by lines in a graph. Many branches of science and industry already store valuable data in graph databases.\nDeep learning is used to train predictive models that unearth fresh insights from graphs. An expanding list of organizations are applying GNNs to improve drug discovery, fraud detection, computer graphics, cybersecurity, genomics, materials science, and recommendation systems. Today\u2019s most complex graphs processed by GNNs have billions of nodes, trillions of edges, and features spread across nodes and edges.\nNVIDIA GH200 NVL32 provides massive CPU-GPU memory to store these complex data structures for accelerated computing. Furthermore, graph algorithms often require random accesses over these large datasets storing vertex properties.\nThese accesses are typically bottlenecked by internode communication bandwidth. The GPU-to-GPU NVLink connectivity of NVIDIA GH200 NVL32 provides massive speedups to such random accesses. GH200 NVL32 can increase GNN training performance by up to 5.8x compared to NVIDIA H100.\nFigure 5 shows a comparison of one GH200 NVL32 system with 144 GB HBM3e memory and 32-way NVLink interconnect compared to four HGX H100 servers with 80 GB HBM3 memory connected with 8-way NVLink interconnect using GraphSAGE. GraphSAGE is a general inductive framework to efficiently generate node embeddings for previously unseen data.\nBar chart with a vertical axis scale from 0 to 6x to show speed up normalized to H100. The comparison shows GH200 NVL32 at 5.8x compared to H100 NVL8 at 1x.\nFigure 5. A comparison of one NVIDIA GH200 NVL32 system to four HGX H100 servers on graph training. (Preliminary performance estimates subject to change.)", "Summary\nAmazon and NVIDIA have announced that NVIDIA DGX Cloud is coming to AWS. AWS will be the first cloud service provider to offer NVIDIA GH200 NVL32 in DGX Cloud and as an EC2 instance. The NVIDIA GH200 NVL32 solution boasts a 32-GPU NVLink domain and a massive 19.5 TB of unified memory. This setup significantly outperforms previous models in GPT-3 training and LLM inference.\nThe CPU-GPU memory interconnect of the NVIDIA GH200 NVL32 is remarkably fast, enhancing memory availability for applications. This technology is part of a scalable design for hyperscale data centers, supported by a comprehensive suite of NVIDIA software and libraries, accelerating thousands of GPU applications. NVIDIA GH200 NVL32 is ideal for tasks like LLM training and inference, recommender systems, GNNs, and more, offering significant performance improvements to AI and computing applications.\nTo learn more, check out the AWS re:Invent Keynote and the NVIDIA GH200 Grace Hopper Superchip Architecture Whitepaper. You can also watch the NVIDIA SC23 Special Address."], "document_title": "One Giant Superchip for LLMs, Recommenders, and GNNs: Introducing NVIDIA GH200 NVL32", "document_url": "https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/", "document_date": "2023-11-28T18:19:07", "document_date_modified": "2023-12-14T19:27:37", "document_full_text": "One Giant Superchip for LLMs, Recommenders, and GNNs: Introducing NVIDIA GH200 NVL32\nAt AWS re:Invent 2023, AWS and NVIDIA announced that AWS will be the first cloud provider to offer NVIDIA GH200 Grace Hopper Superchips interconnected with NVIDIA NVLink technology through NVIDIA DGX Cloud and running on Amazon Elastic Compute Cloud (Amazon EC2). This is a game-changing technology for cloud computing.\nThe NVIDIA GH200 NVL32, a rack-scale solution within NVIDIA DGX Cloud or an Amazon instance, boasts a 32-GPU NVIDIA NVLink domain and a massive 19.5 TB of unified memory. Breaking through the memory constraints of a single system, it is 1.7x faster for GPT-3 training and 2x faster for large language model (LLM) inference compared to NVIDIA HGX H100.\nNVIDIA GH200 Grace Hopper Superchip-powered instances in AWS will feature 4.5 TB of HBM3e memory, a 7.2x increase compared to current-generation NVIDIA H100-powered EC2 P5 instances. This enables developers to run larger models, while improving training performance.\nAdditionally, the CPU to GPU memory interconnect is 900 GB/s, which is 7x faster than PCIe Gen 5. GPUs access CPU memory in a cache-coherent way, extending the total memory available for applications. This is the first use of the NVIDIA GH200 NVL32 scale-out design, a modular reference design for supercomputing, data centers, and cloud infrastructure. It provides a common architecture for GH200 and successor processor configurations.\nThis post explains the reference design that makes this possible and includes some representative application performance results.\nNVIDIA GH200 NVL32\nNVIDIA GH200 NVL32 is a rack-scale reference design for NVIDIA GH200 Grace Hopper Superchips connected through NVLink targeted for hyperscale data centers. NVIDIA GH200 NVL32 supports 16 dual NVIDIA Grace Hopper server nodes compatible with the NVIDIA MGX chassis design and can be liquid cooled to maximize compute density and efficiency.\nOn the left is the front view of the GH200 NVL32 rack with three power shelves at the bottom, eight GH200 compute trays, nine NVLink switch trays, eight GH200 compute trays, and then three power shelves on top. There are seven open shelves at the top. On the right is a back view of the GH200 NVL32 rack showing the liquid cooling manifold and the NVLink Interconnect cable cartridges that fully interconnect the GH200 nodes with the NVLink Switches.\nFigure 1. NVIDIA GH200 NVL32 is a rack-scale solution delivering a 32-GPU NVLink domain and 19.5 TB of unified memory\nThe NVIDIA GH200 Grace Hopper Superchip with a coherent NVLink-C2C creates an NVLink addressable memory address space to simplify model programming. It combines high-bandwidth and low-power system memory, LPDDR5X, and HBM3e to take full advantage of NVIDIA GPU acceleration and high-performance Arm cores in a well-balanced system.\nGH200 server nodes are connected with an NVLink passive copper cable cartridge to enable each Hopper GPU to access the memory of any other Grace Hopper Superchip in the network, providing 32 x 624 GB, or 19.5 TB of NVLink addressable memory (Figure 1).\nThis update to the NVLink Switch System uses the NVLink copper interconnect to connect 32 GH200 GPUs together using nine NVLink switches incorporating third-generation NVSwitch chips. The NVLink Switch System implements a fully connected fat-tree network for all the GPUs in the cluster. For larger scale requirements, scaling with 400 Gb/s InfiniBand or Ethernet delivers incredible performance and an energy-efficient AI supercomputing solution.\nNVIDIA GH200 NVL32 is supported by the NVIDIA HPC SDK and the full suite of CUDA, NVIDIA CUDA-X, and NVIDIA Magnum IO libraries, accelerating over 3,000 GPU applications.\nUse cases and performance results\nNVIDIA GH200 NVL32 is ideal for LLM training and inference, recommender systems, graph neural networks (GNNs), vector databases, and retrieval-augmented generation (RAG) models, as detailed below.\nAI training and inference\nGenerative AI has taken the world by storm, exemplified by the groundbreaking capabilities of services like ChatGPT. LLMs such as GPT-3 and GPT-4 are enabling the integration of AI capabilities into every product in every industry, and their adoption rate is astounding.\nChatGPT became the fastest application to reach 100 million users, achieving that milestone in just 2 months. The demand for generative AI applications is immense and growing exponentially.\nBar chart comparison shows a relative performance of 1.7x for NVIDIA GH200 NVL32 on the left and 1x for the H100 NVL8 on the right. The comparison is for GPT-3 training performance for an Ethernet data center using a batch size of 4 million tokens.\nFigure 2. An Ethernet data center with 16K GPUs using NVIDIA GH200 NVL32 will deliver 1.7x the performance of one composed of H100 NVL8, which is an NVIDIA HGX H100 server with eight NVLink-connected H100 GPUs. (Preliminary performance estimates subject to change.) LLMs require large-scale, multi-GPU training. The memory requirements for GPT-175B would be 700 GB, as each parameter needs four bytes (FP32). A combination of model parallelism and fast communications is used to avoid running out of memory with smaller memory GPUs.\nNVIDIA GH200 NVL32 is built for inference and for training the next generation of LLMs. Breaking through memory, communications, and computational bottlenecks with 32 NVLink-connected GH200 Grace Hopper Superchips, the system can train a trillion-parameter model over 1.7x faster than NVIDIA HGX H100.\nA graph with the vertical axis showing relative max throughput from 0 to 3x. The first column represents four servers with 8-way NVIDIA HGX H100 GPUs interconnected by NVLink and Ethernet connecting the four servers as the baseline at 1x. The second column shows GH200 NVL32 with 32 GPUs interconnected by NVLink and 2x relative max throughput.\nFigure 3. NVIDIA GH200 NVL32 shows 2x faster GPT-3 530B model inference performance compared to H100 NVL8 with 80 GB GPU memory. (Preliminary performance estimates subject to change.)\nFigure 3 shows that the NVIDIA GH200 NVL32 system outperforms four H100 NVL8 systems by 2x on a GPT-530B inference model. The large memory space of NVIDIA GH200 NVL32 also improves operational efficiency, with the ability to store multiple models on the same node and quickly swap models in to maximize utilization.\nRecommender systems\nRecommender systems are the engine of the personalized internet. They\u2019re used across e-commerce and retail, media and social media, digital ads, and more to personalize content. This drives revenue and business value. Recommenders use embeddings that represent users, products, categories, and context, and can range up to tens of terabytes in size.\nA highly accurate recommender system will provide a more engaging user experience, but also requires a larger embedding and more precise recommender. Embeddings have unique characteristics for AI models, requiring large amounts of memory at high bandwidth and lightning-fast networking.\nNVIDIA GH200 NVL32 with Grace Hopper provides 7x the amount of fast-access memory compared to four HGX H100 and delivers 7x the bandwidth compared to the PCIe Gen5 connections to the GPU in conventional x86-based designs. It enables 7x more detailed embeddings compared to H100 with x86.\nNVIDIA GH200 NVL32 can also deliver up to 7.9x the training performance for models with massive embedding tables. Figure 4 shows a comparison of one GH200 NVL32 system with 144 GB HBM3e memory and 32-way NVLink interconnect compared to four HGX H100 servers with 80 GB HBM3 memory connected with 8-way NVLink interconnect using a DLRM model. The comparisons were made between GH200 and H100 systems using 10 TB embedding tables and using 2 TB embedding tables.\nBar chart; left vertical axis scale is the geomean normalized to H100 time to train. The comparison for GH200 NVL32 to H100 NVL8 on the left shows GH200 is 2.5x faster to train a model with 2-TB embedding tables. The comparison for GH200 NVL32 to H100 NVL8 on the right shows GH200 is 7.9X faster to train a model with 10 TB embedding tables.\nFigure 4. A comparison of one NVIDIA GH200 NVL32 system to four HGX H100 servers on recommender training. (Preliminary performance estimates subject to change.)\nGraph neural networks\nGNNs apply the predictive power of deep learning to rich data structures that depict objects and their relationships as points connected by lines in a graph. Many branches of science and industry already store valuable data in graph databases.\nDeep learning is used to train predictive models that unearth fresh insights from graphs. An expanding list of organizations are applying GNNs to improve drug discovery, fraud detection, computer graphics, cybersecurity, genomics, materials science, and recommendation systems. Today\u2019s most complex graphs processed by GNNs have billions of nodes, trillions of edges, and features spread across nodes and edges.\nNVIDIA GH200 NVL32 provides massive CPU-GPU memory to store these complex data structures for accelerated computing. Furthermore, graph algorithms often require random accesses over these large datasets storing vertex properties.\nThese accesses are typically bottlenecked by internode communication bandwidth. The GPU-to-GPU NVLink connectivity of NVIDIA GH200 NVL32 provides massive speedups to such random accesses. GH200 NVL32 can increase GNN training performance by up to 5.8x compared to NVIDIA H100.\nFigure 5 shows a comparison of one GH200 NVL32 system with 144 GB HBM3e memory and 32-way NVLink interconnect compared to four HGX H100 servers with 80 GB HBM3 memory connected with 8-way NVLink interconnect using GraphSAGE. GraphSAGE is a general inductive framework to efficiently generate node embeddings for previously unseen data.\nBar chart with a vertical axis scale from 0 to 6x to show speed up normalized to H100. The comparison shows GH200 NVL32 at 5.8x compared to H100 NVL8 at 1x.\nFigure 5. A comparison of one NVIDIA GH200 NVL32 system to four HGX H100 servers on graph training. (Preliminary performance estimates subject to change.)\nSummary\nAmazon and NVIDIA have announced that NVIDIA DGX Cloud is coming to AWS. AWS will be the first cloud service provider to offer NVIDIA GH200 NVL32 in DGX Cloud and as an EC2 instance. The NVIDIA GH200 NVL32 solution boasts a 32-GPU NVLink domain and a massive 19.5 TB of unified memory. This setup significantly outperforms previous models in GPT-3 training and LLM inference.\nThe CPU-GPU memory interconnect of the NVIDIA GH200 NVL32 is remarkably fast, enhancing memory availability for applications. This technology is part of a scalable design for hyperscale data centers, supported by a comprehensive suite of NVIDIA software and libraries, accelerating thousands of GPU applications. NVIDIA GH200 NVL32 is ideal for tasks like LLM training and inference, recommender systems, GNNs, and more, offering significant performance improvements to AI and computing applications.\nTo learn more, check out the AWS re:Invent Keynote and the NVIDIA GH200 Grace Hopper Superchip Architecture Whitepaper. You can also watch the NVIDIA SC23 Special Address."}], "https://developer.nvidia.com/blog/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/": [{"text": "The article discusses the challenges faced by autonomous vehicle developers in simulating realistic traffic behavior for different operational design domains and countries. The Bi-Level Imitation for Traffic Simulation (BITS) model, developed by the NVIDIA Research team, aims to address these challenges by decoupling the traffic modeling process into high-level intent prediction and low-level controller branches. BITS outperforms previous methods by improving coverage, diversity, and reducing failure rates in traffic simulation. The hierarchical structure of BITS, trained on real-world traffic logs, allows for more accurate and diverse traffic pattern generation. The model includes a prediction-and-planning module to stabilize behavior in new environments. Evaluation of BITS focuses on rollout metrics, statistical differences compared to the real world, and resemblance to human drivers. The article concludes by highlighting the importance of realistic traffic simulation for developing robust AV technology and invites the industry to use and contribute to the open-sourced BITS model on GitHub.", "text_components": ["Simulating Realistic Traffic Behavior with a Bi-Level Imitation Learning AI Model\nFrom last-minute cut-ins to impromptu U-turns, human drivers can be incredibly unpredictable. This unpredictability stems from the complex nature of human decision-making, which is influenced by multiple factors and varies across different operational design domains (ODD) and countries, making it difficult to emulate in simulation.\nYet, autonomous vehicle (AV) developers need to confidently develop and deploy systems that can operate in multiple ODDs with varying traffic behaviors. In the recently published paper, BITS: Bi-Level Imitation for Traffic Simulation, the NVIDIA Research team outlines a novel approach to simulating real-world traffic behavior that enables developers to do just that.\nBi-Level Imitation for Traffic Simulation (BITS) is a traffic model that captures the complexity of the real world with incredible fidelity while also outperforming previous methods. In a trial detailed in the paper, BITS improved coverage and diversity over the next best-performing model by 64% and 118%, respectively, and lowered failure rates by 36%.\nSide-by-side views of the BITS model planning a traffic route - one showing prediction and the other showing the controller.\nFigure 1. By decoupling the traffic modeling process, BITS enables more realistic traffic simulation", "Traffic modeling challenges\nMost simulators model traffic behavior by either replaying recorded data or using a predefined rule-based system to govern vehicle motion.\nWhile replaying data enables accurate review and testing of specific scenarios encountered in real-world driving, it is difficult to simulate behaviors outside of those already recorded. On the other hand, rule-based controllers are limited to simple behaviors, preventing accurate simulation of more complex situations.\nThere are also learning-based approaches, which are trained on real-world driving logs to predict realistic future trajectories. While these models have proven effective in creating accurate and dynamic driving paths, they struggle to produce diverse trajectories that respect road boundaries and the presence of other agents.\nBITS decouples the AI model into a high-level intent prediction and a low-level controller that achieves the overarching intent. By doing so, the model can synthesize a broad spectrum of traffic patterns that closely resemble real-world behavior, while also generating specific scenarios.\nWhen BITS is run alongside other AI-powered traffic models, it consistently displays variety in traffic patterns while maintaining low failure rates (Figure 2).\nThree bar charts comparing BITS model performance with three other learning-based models in coverage, diversity, and failure. BITS shows the highest levels of coverage and diversity and the lowest in failure rates.\nFigure 2. BITS shows the highest levels of coverage and diversity and the lowest in failure rates", "The BITS approach\nBITS achieves such high levels of fidelity and diversity due to its hierarchical structure.\nBoth branches of the model are trained on real-world traffic logs. The high-level network is trained to identify possible goals for the vehicle, and the low-level network is trained to determine a policy that achieves the predicted goal. By splitting up these tasks, we can move the burden of modeling different trajectories to the high-level goal predictor, so the low-level goal-oriented policy can operate more efficiently.\nBITS also includes a prediction-and-planning module to help stabilize the model in new environments and over longer time horizons. It achieves this by reviewing the model\u2019s possible trajectories and selecting those that follow the rules of plausible driving behavior. This reduces the risk of diverging away from reasonable behaviors.", "Evaluating BITS quality\nDetermining whether the behavior of a traffic model is realistic, as well as its ability to generate accurate and unseen scenarios, is incredibly difficult. This is because there is no ground truth for direct comparison. Thus, evaluating the BITS traffic model presents its own challenge.\nAs detailed in BITS: Bi-Level Imitation for Traffic Simulation, we divide our evaluation into three domains: rollout metrics (coverage, diversity, and failure rates), statistical differences compared to the real world, and resemblance to human drivers.\nThe first domain directly measures the low-level network in terms of its coverage area, the diversity of each run, and the frequency of collisions or off-road driving incidents. The second domain compares the speed and jerk differences of the simulated cars to real-world data. The third domain measures human-like behavior by comparing it to a prediction model that forecasts the agent\u2019s future position at a given timestamp.\n2D sketches of car trajectories, organized by four traffic models over five trials. The TPP and TrafficSim models show little variety in repeated trials, while the BITS model shows different trajectories across all five trials.\nFigure 3. Comparison of trajectories planned by various learning-based traffic models As shown in Figures 2 and 3, while other models exhibit tradeoffs between generating diverse trajectories and falling into repeated behaviors, BITS charts a new scenario each time with lower failure rates.", "Conclusion\nThe ability to model realistic traffic behavior in simulation is critical to developing robust AV technology. By optimizing fidelity and diversity, BITS brings AI-generated traffic simulation even closer to the complexity of the real world. We aim to further develop and refine BITS, and ultimately integrate it into the production NVIDIA DRIVE Sim pipeline.\nWe invite the industry to use and contribute to this developing work in simulation, which is open-sourced at NVlabs/traffic-behavior-simulation on GitHub. We are also building and open-sourcing trajdata, a software tool that unifies data formats from different AV datasets and transforms scenes from existing datasets into interactive simulation environments."], "document_title": "Simulating Realistic Traffic Behavior with a Bi-Level Imitation Learning AI Model", "document_url": "https://developer.nvidia.com/blog/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/", "document_date": "2023-11-28T17:00:00", "document_date_modified": "2023-12-14T19:27:38", "document_full_text": "Simulating Realistic Traffic Behavior with a Bi-Level Imitation Learning AI Model\nFrom last-minute cut-ins to impromptu U-turns, human drivers can be incredibly unpredictable. This unpredictability stems from the complex nature of human decision-making, which is influenced by multiple factors and varies across different operational design domains (ODD) and countries, making it difficult to emulate in simulation.\nYet, autonomous vehicle (AV) developers need to confidently develop and deploy systems that can operate in multiple ODDs with varying traffic behaviors. In the recently published paper, BITS: Bi-Level Imitation for Traffic Simulation, the NVIDIA Research team outlines a novel approach to simulating real-world traffic behavior that enables developers to do just that.\nBi-Level Imitation for Traffic Simulation (BITS) is a traffic model that captures the complexity of the real world with incredible fidelity while also outperforming previous methods. In a trial detailed in the paper, BITS improved coverage and diversity over the next best-performing model by 64% and 118%, respectively, and lowered failure rates by 36%.\nSide-by-side views of the BITS model planning a traffic route - one showing prediction and the other showing the controller.\nFigure 1. By decoupling the traffic modeling process, BITS enables more realistic traffic simulation\nTraffic modeling challenges\nMost simulators model traffic behavior by either replaying recorded data or using a predefined rule-based system to govern vehicle motion.\nWhile replaying data enables accurate review and testing of specific scenarios encountered in real-world driving, it is difficult to simulate behaviors outside of those already recorded. On the other hand, rule-based controllers are limited to simple behaviors, preventing accurate simulation of more complex situations.\nThere are also learning-based approaches, which are trained on real-world driving logs to predict realistic future trajectories. While these models have proven effective in creating accurate and dynamic driving paths, they struggle to produce diverse trajectories that respect road boundaries and the presence of other agents.\nBITS decouples the AI model into a high-level intent prediction and a low-level controller that achieves the overarching intent. By doing so, the model can synthesize a broad spectrum of traffic patterns that closely resemble real-world behavior, while also generating specific scenarios.\nWhen BITS is run alongside other AI-powered traffic models, it consistently displays variety in traffic patterns while maintaining low failure rates (Figure 2).\nThree bar charts comparing BITS model performance with three other learning-based models in coverage, diversity, and failure. BITS shows the highest levels of coverage and diversity and the lowest in failure rates.\nFigure 2. BITS shows the highest levels of coverage and diversity and the lowest in failure rates\nThe BITS approach\nBITS achieves such high levels of fidelity and diversity due to its hierarchical structure.\nBoth branches of the model are trained on real-world traffic logs. The high-level network is trained to identify possible goals for the vehicle, and the low-level network is trained to determine a policy that achieves the predicted goal. By splitting up these tasks, we can move the burden of modeling different trajectories to the high-level goal predictor, so the low-level goal-oriented policy can operate more efficiently.\nBITS also includes a prediction-and-planning module to help stabilize the model in new environments and over longer time horizons. It achieves this by reviewing the model\u2019s possible trajectories and selecting those that follow the rules of plausible driving behavior. This reduces the risk of diverging away from reasonable behaviors.\nEvaluating BITS quality\nDetermining whether the behavior of a traffic model is realistic, as well as its ability to generate accurate and unseen scenarios, is incredibly difficult. This is because there is no ground truth for direct comparison. Thus, evaluating the BITS traffic model presents its own challenge.\nAs detailed in BITS: Bi-Level Imitation for Traffic Simulation, we divide our evaluation into three domains: rollout metrics (coverage, diversity, and failure rates), statistical differences compared to the real world, and resemblance to human drivers.\nThe first domain directly measures the low-level network in terms of its coverage area, the diversity of each run, and the frequency of collisions or off-road driving incidents. The second domain compares the speed and jerk differences of the simulated cars to real-world data. The third domain measures human-like behavior by comparing it to a prediction model that forecasts the agent\u2019s future position at a given timestamp.\n2D sketches of car trajectories, organized by four traffic models over five trials. The TPP and TrafficSim models show little variety in repeated trials, while the BITS model shows different trajectories across all five trials.\nFigure 3. Comparison of trajectories planned by various learning-based traffic models As shown in Figures 2 and 3, while other models exhibit tradeoffs between generating diverse trajectories and falling into repeated behaviors, BITS charts a new scenario each time with lower failure rates.\nConclusion\nThe ability to model realistic traffic behavior in simulation is critical to developing robust AV technology. By optimizing fidelity and diversity, BITS brings AI-generated traffic simulation even closer to the complexity of the real world. We aim to further develop and refine BITS, and ultimately integrate it into the production NVIDIA DRIVE Sim pipeline.\nWe invite the industry to use and contribute to this developing work in simulation, which is open-sourced at NVlabs/traffic-behavior-simulation on GitHub. We are also building and open-sourcing trajdata, a software tool that unifies data formats from different AV datasets and transforms scenes from existing datasets into interactive simulation environments."}], "https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/": [{"text": "The article discusses the importance of large language models (LLMs) in enhancing enterprise productivity applications and introduces the concept of retrieval-augmented generation (RAG) to address limitations of traditional LLMs. The NVIDIA NeMo Retriever is highlighted as a tool that optimizes the embedding and retrieval components of RAG, providing higher accuracy and more efficient responses. The article explains the process of a canonical RAG pipeline, including encoding the knowledge base offline and deployment online, outlining the challenges and complexities involved in building RAG pipelines for enterprise applications. The article introduces the NVIDIA Retrieval QA Embedding Model, a transformer encoder fine-tuned for text question-answering retrieval, trained on a mixture of public and proprietary datasets. Evaluation results show that the model outperforms other baselines in terms of information retrieval accuracy. The article concludes by offering access to the NVIDIA Retrieval QA Embedding Model through an early access program and provides information on how to use the embedding model through the NVIDIA Retrieval QA Embedding Playground API.", "text_components": ["Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model\nLarge language models (LLMs) are transforming the AI landscape with their profound grasp of human and programming languages. Essential for next-generation enterprise productivity applications, they enhance user efficiency across tasks like programming, copy editing, brainstorming, and answering questions on a wide range of topics.\nHowever, these models often struggle with real-time events and specific knowledge domains, leading to inaccuracies. Fine-tuning these models can enhance their knowledge, but it\u2019s costly and requires regular updates.\nRetrieval-augmented generation (RAG) offers a solution by combining information retrieval with LLMs for open-domain question-answering applications. RAG provides LLMs with vast, updatable knowledge, effectively addressing these limitations (Figure 1). NVIDIA NeMo Retriever, the latest service in the NVIDIA NeMo framework, optimizes the embedding and retrieval part of RAG to deliver higher accuracy and more efficient responses.\nHigh-level overview of RAG architecture.\nFigure 1. A high-level retrieval augmented generation architecture\nThis post provides an overview of how RAG pipeline components work and the enterprise challenges associated with creating RAG-enabled AI applications, such as commercial viability. You will learn about NeMo Retriever, which includes production-ready components for enterprise RAG pipelines, and the model we are sharing today.", "A canonical RAG pipeline\nRAG applications typically have multiple stages, from embedding to retrieval and response. Let\u2019s look at the canonical RAG pipeline to understand how the NeMo Retriever can help.", "Encoding the knowledge base (offline)\nIn this phase, the knowledge base, typically consisting of documents in text, PDF, HTML, or other formats, is fragmented into chunks. These chunks are then fed to an embedding deep learning model, which produces a dense vector representation for each chunk.\nThe resulting embeddings, along with their corresponding documents and other metadata, are stored in a vector database (Figure 2). The chunking strategy depends on the type and content of documents, the use of metadata (such as document details), and the method of generating synthetic data if applicable. It must be carefully considered when developing a retrieval system.\nDepiction of the offline part of a RAG pipeline chunking, embedding, and storing a knowledge base into a vector database.\nFigure 2. The process of chunking knowledge-base documents, embedding them, and storing them in a vector database\nThe embeddings can be used for semantic search, by calculating the similarity (for example, dot product) between embeddings from a user\u2019s query and those of the documents stored in the database. Vector databases are specialized in storing vast amounts of vectorized data and can perform fast approximate nearest-neighbor searches.", "Deployment (online)\nThis stage focuses on deployment when the vector database is connected to the LLM application so it can answer questions in real time. It has two phases\u2014retrieval from the vector database and generating a response.", "Phase 1. Retrieval from vector database based on the user\u2019s query\u200b\nThe user\u2019s query is first embedded as a dense vector. Typically, a special prefix is added to the query, so that the embedding model used by the retriever can understand that it is a question. This enables asymmetric semantic search, where a short query can be used to find a longer paragraph that answers the query.\nNext, the query embedding is used to search a vector database that retrieves a small number of the most relevant document chunks to the user\u2019s query (Figure 3).\nSearching with query embeddings over the vector database returns relevant chunks.\nFigure 3. The query embedding is used to search the vector database, which returns the Top k most relevant chunks\nThe vector database achieves this by employing a similarity/distance measure, such as cosine similarity, with an approximate search algorithm. This guarantees scalability and low latency.", "Phase 2. Use an LLM to generate a response leveraging the context\nIn this phase, the most relevant chunks are combined to form a context, which is then combined with the user\u2019s query as the final input for the LLM. The prompt usually contains extra instructions to guide the LLM to generate the response based on the context only. Figure 4 illustrates this response generation process.\nThe LLM returns a response with a prompt made of a user query, top retrieved chunks, and a prompt.\nFigure 4. The LLM takes in the user query along with the top-k retrieved chunks, and the prompt to return a response", "Challenges of building a RAG pipeline for enterprise applications\nWhile RAG offers significant advantages over an LLM by itself, it\u2019s important to note that its benefits come with several challenges that must be addressed. One major issue is finding a commercially viable retriever, often constrained by licensing restrictions in training datasets like MSMARCO. Real-world queries further complicate matters with their ambiguity; users tend to enter incomplete or vague queries, making retrieval difficult.\nIn multi-turn conversations, this complexity increases as user queries often reference earlier parts of the conversation, necessitating contextual understanding for effective retrieval. Additionally, some queries require synthesizing information from multiple sources, demanding advanced integration capabilities.\nFor LLMs, handling long-context inputs is a challenge. These models, despite continuous improvements, often struggle with forgetting details in lengthy inputs and require substantial computational resources, which become more pronounced in multi-turn scenarios.\nDeployment of these systems involves complex RAG pipelines, which include various microservices like embedding, vector databases, and LLMs. Setting up and managing these services in a secure, efficient manner is a significant task.\nFor more information about how to build a production-grade RAG pipeline, refer to the NVIDIA/GenerativeAIExamples GitHub repo.\nLet\u2019s look at how NeMo Retriever\u2014which brings an optimized, commercially viable set of tools\u2014streamlines the retrieval process in complex scenarios.", "NVIDIA NeMo Retriever for retrieval-augmented generation\nWe announced the latest addition to the NVIDIA NeMo framework, NVIDIA NeMo Retriever, an information retrieval service that can be deployed on-premises or in the cloud. It provides a secure and simplified path for enterprises to integrate enterprise-grade RAG capabilities into their customized production AI applications.\nNeMo Retriever aims to provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It also features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pretrained models available as starting points, developers can also quickly customize them for their domain-specific use cases, such as IT or HR help assistants, and R&D research assistants.\nToday, we are sharing our embedding model, optimized for text question-answering retrieval. We are in the process of developing reranking and retrieval microservices, which will be available soon.", "NVIDIA Retrieval QA Embedding Model\nAn embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are usually transformer encoders that process tokens of input text (for example, question, passage) to output an embedding.\nThe figure depicts the Bi-encoder retrieval architecture. Query and passage texts are embedded through encoders. Cosine similarity is used to determine how close they are.\nFigure 5. Bi-encoder retrieval architecture\nEmbedding models for text retrieval are typically trained using a bi-encoder architecture, such as the one depicted in Figure 5. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question.\nThe NVIDIA Retrieval QA Embedding Model is a transformer encoder\u2014a fine-tuned version of E5-Large-Unsupervised, with 24 layers and an embedding size of 1024, trained on private and public datasets. It supports a maximum input of 512 tokens. Furthermore, we are committed to investigating cutting-edge model architectures and datasets to enable state-of-the-art (SOTA) retrieval models with NeMo Retriever.", "Training dataset\nThe development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MSMARCO restricts \u200ccommercial licensing, limiting the use of these models in commercial settings. To address this, we created our own internal open-domain QA dataset to train a commercially viable embedding model.\nWe searched weblogs for passages related to NVIDIA proprietary data collection and chose a set of passages that were relevant to customer use cases. These passages were annotated by the NVIDIA internal data annotation team.\nTo minimize the redundancy in our data collection process, we selected samples that maximized relevancy distance scores and increased diversity in the data. The pretrained embedding model was fine-tuned using a mixture of English language datasets, which includes our proprietary dataset, along with selected samples from public datasets that are available for commercial use.\nOur main objective was to refine information retrieval capabilities, specifically tailoring the embedding model for the common enterprise LLM use case of text-based question-and-answering over knowledge bases.", "Evaluation results\nThe NVIDIA Retrieval QA Embedding Model is focused on question-answering applications. This is an asymmetric semantic search problem, as the questions and passages typically have different distributions and patterns\u2014the questions generally being shorter than paragraphs that contain the answer.\nWe evaluated our embedding model with real internal customer datasets from telco, IT, consulting, and energy industries. The metric was Recall@5, to emulate a RAG scenario where we provided the top five most relevant passages as context in the prompt for the LLM model to respond to the question. We compared our model\u2019s information retrieval accuracy to a number of well-known embedding models made available by the AI community, including ones trained on non-commercial datasets, which are marked with a *. Recall@5 is a measure of how often the relevant item is present in the top five retrieved items.\nYou can see the results of the benchmark in Figure 6. Notice that our retriever model achieves the best performance among those baselines.\nBar Chart comparing Average Recall@5 scores of various community models in comparison to NVIDIA Retrieval QA Embedding Model, on internal customer datasets.\nFigure 6. Average Recall@5 on customer datasets in telco, IT, consulting, and energy industries\nAs shown in Figure 7, we compared the NVIDIA Retrieval QA Embedding Model to popular open source and commercial retriever models on academic benchmarks NQ, HotpotQA, FiQA from BeIR benchmark, and the TechQA dataset. In this benchmark, the metric used is Normalized Discounted Cumulative Gain @10 (NDCG@10).\nA bar chart comparing open-source and commercial retrieval models in comparison with NVIDIA retrieval QA embedding model.\nFigure 7. Average NDCG@10 comparing various open-source and commercial retrieval models on NQ, HotpotQA, FiQA, and TechQA datasets. The NVIDIA Retrieval QA model outperforms the others in terms of average NDCG@1 0\nNote that the techQA dataset, consisting of questions and answers curated from the IBM technical forum together with 800k technotes as the knowledge base, wasn\u2019t used in a retrieval benchmark setting before. We provide a notebook to convert this dataset to a BEIR-compliant format for benchmarking.", "Getting started\nThe NVIDIA Retrieval QA Embedding Model will be available soon as part of a microservices container in early access (EA). Apply to be a part of the EA program.\nYou can also gain free-trial access to the NVIDIA Retrieval QA embedding API in the NGC catalog.", "NVIDIA Retrieval QA Embedding Playground API\nThe NVIDIA Retriever QA Embedding Model is a fine-tuned version of E5-Large-Unsupervised and a similar input format requirement applies. When making a request, you must indicate if it is a \u201cquery\u201d or \u201cpassage\u201d in the payload. This is necessary for asymmetric tasks such as passage retrieval in open QA.\nThe API accepts a simple payload format, with the \u201cinput\u201d being the chunk of text to produce embedding. In the following example API call, we embed two longer texts as \u201cpassages\u201d and one smaller \u201cquery\u201d text and then compute the similarity between the passages and the query using the dot product.\nNote that you must log in the NGC AI Playground and obtain an API key, which is used as \u201cAPI_KEY\u201d in the following snippet.\n```\nimport requests    \nimport numpy as np\n\n# Make sure to fill this. You will need to obtain the API_KEY from NGC playground\nAPI_KEY=\"<YOUR_NGC_PLAYGROUND_API_KEY>\"\n\ninvoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/091a03bb-7364-4087-8090-bd71e9277520\"\nfetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\nheaders = {\n    \"Authorization\": \"Bearer {}\".format(API_KEY),\n    \"Accept\": \"application/json\",\n}\n\n# To re-use connections\nsession = requests.Session()\n\n# Note the \"model\": \"passage\" field in the payload.\npassage_payload = {\n  \"input\": [\"Pablo Ruiz Picasso was a Spanish painter, sculptor, printmaker, ceramicist and theater designer who spent most of his adult life in France.\",\n            \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\"],\n    \"model\" : \"passage\",\n    \"encoding_format\": \"float\"\n}\n\npassage_response = session.post(invoke_url, headers=headers, json=passage_payload)\nwhile passage_response.status_code == 202:\n    request_id = passage_response.headers.get(\"NVCF-REQID\")\n    fetch_url = fetch_url_format + request_id\n    passage_response = session.get(fetch_url, headers=headers)\n    \npassage_response.raise_for_status()\npassage_embeddings = np.asarray([item['embedding'] for item in passage_response.json()['data']])\n```\n2. Embed the query\n```\n# Note the \"model\": \"query\" field in the payload\nquery_payload = {\n  \"input\": \"Who is a great particle physicist?\",\n    \"model\" : \"query\",\n    \"encoding_format\": \"float\"\n}\nquery_response = session.post(invoke_url, headers=headers, json=query_payload)\nwhile query_response.status_code == 202:\n    request_id = query_response.headers.get(\"NVCF-REQID\")\n    fetch_url = fetch_url_format + request_id\n    query_response = session.get(fetch_url, headers=headers)\n    \nquery_response.raise_for_status()\nquery_embedding = np.asarray(query_response.json()['data'][0]['embedding'])\n```\n3. Calculate the similarity between the passage and query embeddings\n```\n# A simple dot product\nnp.dot(passage_embeddings, query_embedding)\n```\nOutput:\n```\narray([0.33193235, 0.52141018])\n```\nIn this example, the query shares more similarities with the second paragraph, both related to the physics domain.", "Conclusion\nNVIDIA NeMo Retriever provides \u200can embedding service tailored for question-answering applications. The embedding model and service are provided under a commercial use license. While this model has shown promising results on several public and internal benchmarks, we\u2019re working on continually improving the model\u2019s quality.\nBe sure to apply for early access to NeMo Retriever microservices, and expect more to come in future releases.\nTo get exclusive access to over 600 SDKs and AI models, free training, and network with our community of technical experts, join the free NVIDIA Developer Program. For a limited time, new members will get a free self-paced course from the NVIDIA Deep Learning Institute upon joining."], "document_title": "Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model", "document_url": "https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/", "document_date": "2023-11-28T18:10:50", "document_date_modified": "2024-01-22T21:25:24", "document_full_text": "Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model\nLarge language models (LLMs) are transforming the AI landscape with their profound grasp of human and programming languages. Essential for next-generation enterprise productivity applications, they enhance user efficiency across tasks like programming, copy editing, brainstorming, and answering questions on a wide range of topics.\nHowever, these models often struggle with real-time events and specific knowledge domains, leading to inaccuracies. Fine-tuning these models can enhance their knowledge, but it\u2019s costly and requires regular updates.\nRetrieval-augmented generation (RAG) offers a solution by combining information retrieval with LLMs for open-domain question-answering applications. RAG provides LLMs with vast, updatable knowledge, effectively addressing these limitations (Figure 1). NVIDIA NeMo Retriever, the latest service in the NVIDIA NeMo framework, optimizes the embedding and retrieval part of RAG to deliver higher accuracy and more efficient responses.\nHigh-level overview of RAG architecture.\nFigure 1. A high-level retrieval augmented generation architecture\nThis post provides an overview of how RAG pipeline components work and the enterprise challenges associated with creating RAG-enabled AI applications, such as commercial viability. You will learn about NeMo Retriever, which includes production-ready components for enterprise RAG pipelines, and the model we are sharing today.\nA canonical RAG pipeline\nRAG applications typically have multiple stages, from embedding to retrieval and response. Let\u2019s look at the canonical RAG pipeline to understand how the NeMo Retriever can help.\nEncoding the knowledge base (offline)\nIn this phase, the knowledge base, typically consisting of documents in text, PDF, HTML, or other formats, is fragmented into chunks. These chunks are then fed to an embedding deep learning model, which produces a dense vector representation for each chunk.\nThe resulting embeddings, along with their corresponding documents and other metadata, are stored in a vector database (Figure 2). The chunking strategy depends on the type and content of documents, the use of metadata (such as document details), and the method of generating synthetic data if applicable. It must be carefully considered when developing a retrieval system.\nDepiction of the offline part of a RAG pipeline chunking, embedding, and storing a knowledge base into a vector database.\nFigure 2. The process of chunking knowledge-base documents, embedding them, and storing them in a vector database\nThe embeddings can be used for semantic search, by calculating the similarity (for example, dot product) between embeddings from a user\u2019s query and those of the documents stored in the database. Vector databases are specialized in storing vast amounts of vectorized data and can perform fast approximate nearest-neighbor searches.\nDeployment (online)\nThis stage focuses on deployment when the vector database is connected to the LLM application so it can answer questions in real time. It has two phases\u2014retrieval from the vector database and generating a response.\nPhase 1. Retrieval from vector database based on the user\u2019s query\u200b\nThe user\u2019s query is first embedded as a dense vector. Typically, a special prefix is added to the query, so that the embedding model used by the retriever can understand that it is a question. This enables asymmetric semantic search, where a short query can be used to find a longer paragraph that answers the query.\nNext, the query embedding is used to search a vector database that retrieves a small number of the most relevant document chunks to the user\u2019s query (Figure 3).\nSearching with query embeddings over the vector database returns relevant chunks.\nFigure 3. The query embedding is used to search the vector database, which returns the Top k most relevant chunks\nThe vector database achieves this by employing a similarity/distance measure, such as cosine similarity, with an approximate search algorithm. This guarantees scalability and low latency.\nPhase 2. Use an LLM to generate a response leveraging the context\nIn this phase, the most relevant chunks are combined to form a context, which is then combined with the user\u2019s query as the final input for the LLM. The prompt usually contains extra instructions to guide the LLM to generate the response based on the context only. Figure 4 illustrates this response generation process.\nThe LLM returns a response with a prompt made of a user query, top retrieved chunks, and a prompt.\nFigure 4. The LLM takes in the user query along with the top-k retrieved chunks, and the prompt to return a response\nChallenges of building a RAG pipeline for enterprise applications\nWhile RAG offers significant advantages over an LLM by itself, it\u2019s important to note that its benefits come with several challenges that must be addressed. One major issue is finding a commercially viable retriever, often constrained by licensing restrictions in training datasets like MSMARCO. Real-world queries further complicate matters with their ambiguity; users tend to enter incomplete or vague queries, making retrieval difficult.\nIn multi-turn conversations, this complexity increases as user queries often reference earlier parts of the conversation, necessitating contextual understanding for effective retrieval. Additionally, some queries require synthesizing information from multiple sources, demanding advanced integration capabilities.\nFor LLMs, handling long-context inputs is a challenge. These models, despite continuous improvements, often struggle with forgetting details in lengthy inputs and require substantial computational resources, which become more pronounced in multi-turn scenarios.\nDeployment of these systems involves complex RAG pipelines, which include various microservices like embedding, vector databases, and LLMs. Setting up and managing these services in a secure, efficient manner is a significant task.\nFor more information about how to build a production-grade RAG pipeline, refer to the NVIDIA/GenerativeAIExamples GitHub repo.\nLet\u2019s look at how NeMo Retriever\u2014which brings an optimized, commercially viable set of tools\u2014streamlines the retrieval process in complex scenarios.\nNVIDIA NeMo Retriever for retrieval-augmented generation\nWe announced the latest addition to the NVIDIA NeMo framework, NVIDIA NeMo Retriever, an information retrieval service that can be deployed on-premises or in the cloud. It provides a secure and simplified path for enterprises to integrate enterprise-grade RAG capabilities into their customized production AI applications.\nNeMo Retriever aims to provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It also features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pretrained models available as starting points, developers can also quickly customize them for their domain-specific use cases, such as IT or HR help assistants, and R&D research assistants.\nToday, we are sharing our embedding model, optimized for text question-answering retrieval. We are in the process of developing reranking and retrieval microservices, which will be available soon.\nNVIDIA Retrieval QA Embedding Model\nAn embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are usually transformer encoders that process tokens of input text (for example, question, passage) to output an embedding.\nThe figure depicts the Bi-encoder retrieval architecture. Query and passage texts are embedded through encoders. Cosine similarity is used to determine how close they are.\nFigure 5. Bi-encoder retrieval architecture\nEmbedding models for text retrieval are typically trained using a bi-encoder architecture, such as the one depicted in Figure 5. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question.\nThe NVIDIA Retrieval QA Embedding Model is a transformer encoder\u2014a fine-tuned version of E5-Large-Unsupervised, with 24 layers and an embedding size of 1024, trained on private and public datasets. It supports a maximum input of 512 tokens. Furthermore, we are committed to investigating cutting-edge model architectures and datasets to enable state-of-the-art (SOTA) retrieval models with NeMo Retriever.\nTraining dataset\nThe development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MSMARCO restricts \u200ccommercial licensing, limiting the use of these models in commercial settings. To address this, we created our own internal open-domain QA dataset to train a commercially viable embedding model.\nWe searched weblogs for passages related to NVIDIA proprietary data collection and chose a set of passages that were relevant to customer use cases. These passages were annotated by the NVIDIA internal data annotation team.\nTo minimize the redundancy in our data collection process, we selected samples that maximized relevancy distance scores and increased diversity in the data. The pretrained embedding model was fine-tuned using a mixture of English language datasets, which includes our proprietary dataset, along with selected samples from public datasets that are available for commercial use.\nOur main objective was to refine information retrieval capabilities, specifically tailoring the embedding model for the common enterprise LLM use case of text-based question-and-answering over knowledge bases.\nEvaluation results\nThe NVIDIA Retrieval QA Embedding Model is focused on question-answering applications. This is an asymmetric semantic search problem, as the questions and passages typically have different distributions and patterns\u2014the questions generally being shorter than paragraphs that contain the answer.\nWe evaluated our embedding model with real internal customer datasets from telco, IT, consulting, and energy industries. The metric was Recall@5, to emulate a RAG scenario where we provided the top five most relevant passages as context in the prompt for the LLM model to respond to the question. We compared our model\u2019s information retrieval accuracy to a number of well-known embedding models made available by the AI community, including ones trained on non-commercial datasets, which are marked with a *. Recall@5 is a measure of how often the relevant item is present in the top five retrieved items.\nYou can see the results of the benchmark in Figure 6. Notice that our retriever model achieves the best performance among those baselines.\nBar Chart comparing Average Recall@5 scores of various community models in comparison to NVIDIA Retrieval QA Embedding Model, on internal customer datasets.\nFigure 6. Average Recall@5 on customer datasets in telco, IT, consulting, and energy industries\nAs shown in Figure 7, we compared the NVIDIA Retrieval QA Embedding Model to popular open source and commercial retriever models on academic benchmarks NQ, HotpotQA, FiQA from BeIR benchmark, and the TechQA dataset. In this benchmark, the metric used is Normalized Discounted Cumulative Gain @10 (NDCG@10).\nA bar chart comparing open-source and commercial retrieval models in comparison with NVIDIA retrieval QA embedding model.\nFigure 7. Average NDCG@10 comparing various open-source and commercial retrieval models on NQ, HotpotQA, FiQA, and TechQA datasets. The NVIDIA Retrieval QA model outperforms the others in terms of average NDCG@1 0\nNote that the techQA dataset, consisting of questions and answers curated from the IBM technical forum together with 800k technotes as the knowledge base, wasn\u2019t used in a retrieval benchmark setting before. We provide a notebook to convert this dataset to a BEIR-compliant format for benchmarking.\nGetting started\nThe NVIDIA Retrieval QA Embedding Model will be available soon as part of a microservices container in early access (EA). Apply to be a part of the EA program.\nYou can also gain free-trial access to the NVIDIA Retrieval QA embedding API in the NGC catalog.\nNVIDIA Retrieval QA Embedding Playground API\nThe NVIDIA Retriever QA Embedding Model is a fine-tuned version of E5-Large-Unsupervised and a similar input format requirement applies. When making a request, you must indicate if it is a \u201cquery\u201d or \u201cpassage\u201d in the payload. This is necessary for asymmetric tasks such as passage retrieval in open QA.\nThe API accepts a simple payload format, with the \u201cinput\u201d being the chunk of text to produce embedding. In the following example API call, we embed two longer texts as \u201cpassages\u201d and one smaller \u201cquery\u201d text and then compute the similarity between the passages and the query using the dot product.\nNote that you must log in the NGC AI Playground and obtain an API key, which is used as \u201cAPI_KEY\u201d in the following snippet.\n```\nimport requests    \nimport numpy as np\n\n# Make sure to fill this. You will need to obtain the API_KEY from NGC playground\nAPI_KEY=\"<YOUR_NGC_PLAYGROUND_API_KEY>\"\n\ninvoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/091a03bb-7364-4087-8090-bd71e9277520\"\nfetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\nheaders = {\n    \"Authorization\": \"Bearer {}\".format(API_KEY),\n    \"Accept\": \"application/json\",\n}\n\n# To re-use connections\nsession = requests.Session()\n\n# Note the \"model\": \"passage\" field in the payload.\npassage_payload = {\n  \"input\": [\"Pablo Ruiz Picasso was a Spanish painter, sculptor, printmaker, ceramicist and theater designer who spent most of his adult life in France.\",\n            \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\"],\n    \"model\" : \"passage\",\n    \"encoding_format\": \"float\"\n}\n\npassage_response = session.post(invoke_url, headers=headers, json=passage_payload)\nwhile passage_response.status_code == 202:\n    request_id = passage_response.headers.get(\"NVCF-REQID\")\n    fetch_url = fetch_url_format + request_id\n    passage_response = session.get(fetch_url, headers=headers)\n    \npassage_response.raise_for_status()\npassage_embeddings = np.asarray([item['embedding'] for item in passage_response.json()['data']])\n```\n2. Embed the query\n```\n# Note the \"model\": \"query\" field in the payload\nquery_payload = {\n  \"input\": \"Who is a great particle physicist?\",\n    \"model\" : \"query\",\n    \"encoding_format\": \"float\"\n}\nquery_response = session.post(invoke_url, headers=headers, json=query_payload)\nwhile query_response.status_code == 202:\n    request_id = query_response.headers.get(\"NVCF-REQID\")\n    fetch_url = fetch_url_format + request_id\n    query_response = session.get(fetch_url, headers=headers)\n    \nquery_response.raise_for_status()\nquery_embedding = np.asarray(query_response.json()['data'][0]['embedding'])\n```\n3. Calculate the similarity between the passage and query embeddings\n```\n# A simple dot product\nnp.dot(passage_embeddings, query_embedding)\n```\nOutput:\n```\narray([0.33193235, 0.52141018])\n```\nIn this example, the query shares more similarities with the second paragraph, both related to the physics domain.\nConclusion\nNVIDIA NeMo Retriever provides \u200can embedding service tailored for question-answering applications. The embedding model and service are provided under a commercial use license. While this model has shown promising results on several public and internal benchmarks, we\u2019re working on continually improving the model\u2019s quality.\nBe sure to apply for early access to NeMo Retriever microservices, and expect more to come in future releases.\nTo get exclusive access to over 600 SDKs and AI models, free training, and network with our community of technical experts, join the free NVIDIA Developer Program. For a limited time, new members will get a free self-paced course from the NVIDIA Deep Learning Institute upon joining."}], "https://developer.nvidia.com/blog/new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai/": [{"text": "Financial services are utilizing Dell Technologies and NVIDIA H100 systems for HPC and AI to accelerate generative AI workloads, data analytics, and quantitative financial applications. These systems have set records in financial risk calculations and have been audited by the STAC. The systems offer speed, efficiency, and cost savings, making them ideal for HPC quantitative financial applications and AI deep learning workloads. The systems use NVIDIA GPUs and software components like cuBLAS and cuRAND for efficient performance. The convergence of HPC and AI in financial services is leading to solutions that combine quantitative finance, data engineering, analytics, ML, and AI neural net algorithms. The use of RL algorithms and generative AI models is becoming prominent in financial areas, allowing for more diverse HPC workloads. Organizations can use tools provided by NVIDIA to train RLHF-incorporated, GPT-based models for improved decision-making and insights from unstructured data sources. The combination of HPC and AI workloads enables financial firms to maximize ROI and reduce TCO by catering to all types of workloads, including the most demanding AI and HPC initiatives.", "text_components": ["New Risk Calculation Record in Financial Services with Dell Technologies and NVIDIA H100 System for HPC and AI\nEnd clients are working on converged HPC quant finance and AI business solutions. Dell Technologies, along with NVIDIA, is uniquely positioned to accelerate generative AI workloads and data analytics as well as high performance computing (HPC) quantitative financial applications where converged HPC quantitative finance plus AI workloads are the need of the hour for clients.\nDell and NVIDIA initially covered quantitative applications setting new records on NVIDIA Certified Dell PowerEdge XE9680 servers with NVIDIA GPUs. The system was independently audited by the Strategic Technology Analysis Center (STAC) on financial quantitative HPC workloads. For more information, see NVIDIA H100 System for HPC and Generative AI Sets Record for Financial Risk Calculations.\nNVIDIA H00 Tensor Core GPUs were featured in a stack that set several records in a recent STAC-A2 audit with eight NVIDIA H100 SXM5 80 GiB GPUs, offering incredible speed with great efficiency and cost savings. Such systems are ideal for both HPC quantitative financial applications and AI deep learning neural net based workloads.\nDesigned by quants and technologists from some of the world\u2019s largest banks, STAC-A2 is a technology benchmark standard based on financial market risk analysis. The benchmark is a Monte Carlo estimation of Heston-based Greeks for path-dependent, multi-asset options with early exercise.\nSTAC recently performed STAC-A2 Benchmark tests performance, scaling, quality, and resource efficiency. The stack under test (SUT) was a Dell PowerEdge XE9680 server with eight NVIDIA H100 SXM5 80 GiB GPUs. Compared to all publicly reported solutions to date, this system set numerous performance and efficiency records:\nThe highest throughput (561 options / second) ( STAC-A2.\u03b22.HPORTFOLIO.SPEED )\nThe fastest warm time (7.40 ms) in the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD] )\nThe fastest warm (160 ms) and cold (598 ms) times in the large Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD] )\nThe most correlated assets (440) and Monte Carlo paths (316,000,000) simulated in 10 minutes ( STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS] )\nThe best energy efficiency (364,945 options / kWh) ( STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF )\nCompared to a liquid-cooled solution using four GPUs ( INTC230927 ), this NVIDIA 8-GPU solution set the following records:\n16% more energy-efficient ( STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF )\n2.5x / 1.8x the speed in the warm / cold runs of the large Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD] )\n1.2x / 7.5 x the speed in the warm / cold runs of the baseline Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD] )\nSimulated 2.4x the correlated assets and 316x the Monte Carlo paths in 10 minutes ( STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS] )\n2.0x the throughput ( STAC-A2.\u03b22.HPORTFOLIO.SPEED )\nCompared to a solution using eight NVIDIA H100 PCIe GPUs, as well as previous versions of the NVIDIA STAC Pack and CUDA, this solution using NVIDIA H100 SXM5 GPUs set the following records:\n1.59x the throughput ( STAC-A2.\u03b22.HPORTFOLIO.SPEED )\n1.17x the speed in the warm runs of the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD] )\n3.1x / 3.0x the speed in the warm / cold runs of the large Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD] )\nSimulated 10% more correlated assets in 10 minutes ( STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS] )\n17% more energy-efficient ( STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF )\nIn addition to the hardware, NVIDIA provides all the key software component layers with the NVIDIA HPC SDK. This offers multiple options to developers, including NVIDIA CUDA SDK for CUDA/C++ and enabling other languages and directive-based solutions such as OpenMP, OpenACC, accelerations with C++ 17 standard parallelism, and Fortran parallel constructs.\nThis particular implementation was developed on CUDA 12.2 using the highly optimized libraries delivered with CUDA:\ncuBLAS: The GPU-enabled implementation of the linear algebra package BLAS.\ncuRAND: A parallel and efficient GPU implementation of random-number generators.\nThe implementation was supported by tools:\nNVIDIA Nsight Systems for timeline profiling\nNVIDIA Nsight Compute for kernel profiling\nNVIDIA Compute Sanitizer and CUDA-GDB for debugging\nThe different operations of the benchmark are implemented using building blocks from a set of quantitative finance reference implementations of state-of-the-art algorithms, including asset diffusion and Longstaff-Schwartz pricing. These are exposed in a modular and maintainable framework using object-oriented programming in CUDA/C++.", "HPC plus AI extensions\nEnd customers perform other price-discovery, market-risk calculations in their real-world workflow calculations:\nSensitivity Greeks\nProfit and loss (P&L) calculations\nValue at risk (VaR)\nMargin and counterparty credit risk (CCR) calculations, such as credit valuation adjustment (CVA)\nPricing/risk calculation, algorithmic trading model development, and backtesting need a robust scalable environment. In areas such as CVA, such scaled setups on dense server nodes with eight NVIDIA GPUs have been shown to reduce the number of required nodes from 100 to 4 in simulation\u2013 and compute-intensive calculations (separately from STAC benchmarking). This reduces the total cost of ownership (TCO).\nWe are increasingly seeing a convergence of extended HPC quantitative finance and AI requirements from end users where end financial solutions use a combination of quantitative finance, data engineering and analytics, ML, and AI neural net algorithms.\nThe current solutions work with traditional quantitative models, extended with data engineering and analytics with accelerated machine learning (ML), such as XGBOOST, with tools such as NVIDIA RAPIDS, and AI deep learning. Deep learning involves long short-term memory (LSTM), recurrent neural networks (RNNs), and other advanced areas such as large language models (LLMs).\nAlso in today\u2019s AI world, we have gone on to new topic areas such as generative AI. Generative AI refers to AI algorithms that enable computers to use existing or past content like text, audio, and video files, images, and even code to generate new content. Examples of these AI generative models include the range of generative pretrained transformer (GPT) models to diffusion models used in image generation.\nMore diverse HPC workloads are becoming prominent in financial areas using reinforcement learning (RL) and applied in financial areas such as limit order book price prediction.\nAgent-based models are used to reproduce the interactions between economic agents, such as market participants in financial transactions. This is a trial and error process akin to a child learning the real world. The agent operates in an environment that has an end task with states (observable or not observed) and potential actions with positive and negative rewards to those other states with simulation. As a result, the states have a value function that is also iteratively updated based on the trail and error process. The agent must solve for the optimal policy behavior equivalent to strategy.\nIn most real-world problems, the optimal behavior is hard to determine. With a learnable policy, the optimal policy with its associated action can be evaluated as good or bad where the agent performs an action or set of actions with rewards and RL is the technique to maximize this reward.\nSuch RL algorithms have been extended to RL from human feedback (RLHF), which makes use of LLMs trained to optimize policy rewards, such as ChatGPT. NVIDIA provides tools to train similar RLHF-incorporated, GPT-based models.\nOrganizations can use such foundational LLMs on unstructured sources of information such as financial news along with customization techniques such as training, parameter-efficient fine-tuning, and fine-tuning to improve the LLMs for understanding the financial domain better.\nEnd users can interface with such customized models combining it with techniques such as r etrieval augmented Generation (RAG) to gain an information edge by querying and obtaining insights from unstructured data sources that are untapped for decision-making insights.\nFor example, these techniques can be used on news, financial documents, financial 10K or 10Q filings, and federal reserve commentary beyond traditional sources of tabular market data, to generate insights and signals, referred to as alternative data (alt data). These alt data information signals can be used to support systematic algorithmic trading models as well as discretionary traders.", "Summary\nThe convergence of HPC and AI is happening as financial firms work on big-picture solutions. Stakeholders include both sell-side (global market banks and broker dealers) and buy-side (insurers, hedge funds, market-makers, high frequency traders, and asset managers). Solutions typically involve combining various modeling techniques, such as HPC quantitative finance, ML, RL, and NLP LLM generative AI models.\nDue to the ability to cater to multiple HPC plus AI workloads, end clients are able to gain the maximum ROI and lower TCO by catering to all workloads including the largest and most demanding AI, ML, DL training, HPC modeling, and simulation initiatives."], "document_title": "New Risk Calculation Record in Financial Services with Dell Technologies and NVIDIA H100 System for HPC and AI", "document_url": "https://developer.nvidia.com/blog/new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai/", "document_date": "2023-11-27T22:02:15", "document_date_modified": "2023-12-14T19:27:39", "document_full_text": "New Risk Calculation Record in Financial Services with Dell Technologies and NVIDIA H100 System for HPC and AI\nEnd clients are working on converged HPC quant finance and AI business solutions. Dell Technologies, along with NVIDIA, is uniquely positioned to accelerate generative AI workloads and data analytics as well as high performance computing (HPC) quantitative financial applications where converged HPC quantitative finance plus AI workloads are the need of the hour for clients.\nDell and NVIDIA initially covered quantitative applications setting new records on NVIDIA Certified Dell PowerEdge XE9680 servers with NVIDIA GPUs. The system was independently audited by the Strategic Technology Analysis Center (STAC) on financial quantitative HPC workloads. For more information, see NVIDIA H100 System for HPC and Generative AI Sets Record for Financial Risk Calculations.\nNVIDIA H00 Tensor Core GPUs were featured in a stack that set several records in a recent STAC-A2 audit with eight NVIDIA H100 SXM5 80 GiB GPUs, offering incredible speed with great efficiency and cost savings. Such systems are ideal for both HPC quantitative financial applications and AI deep learning neural net based workloads.\nDesigned by quants and technologists from some of the world\u2019s largest banks, STAC-A2 is a technology benchmark standard based on financial market risk analysis. The benchmark is a Monte Carlo estimation of Heston-based Greeks for path-dependent, multi-asset options with early exercise.\nSTAC recently performed STAC-A2 Benchmark tests performance, scaling, quality, and resource efficiency. The stack under test (SUT) was a Dell PowerEdge XE9680 server with eight NVIDIA H100 SXM5 80 GiB GPUs. Compared to all publicly reported solutions to date, this system set numerous performance and efficiency records:\nThe highest throughput (561 options / second) ( STAC-A2.\u03b22.HPORTFOLIO.SPEED )\nThe fastest warm time (7.40 ms) in the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD] )\nThe fastest warm (160 ms) and cold (598 ms) times in the large Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD] )\nThe most correlated assets (440) and Monte Carlo paths (316,000,000) simulated in 10 minutes ( STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS] )\nThe best energy efficiency (364,945 options / kWh) ( STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF )\nCompared to a liquid-cooled solution using four GPUs ( INTC230927 ), this NVIDIA 8-GPU solution set the following records:\n16% more energy-efficient ( STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF )\n2.5x / 1.8x the speed in the warm / cold runs of the large Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD] )\n1.2x / 7.5 x the speed in the warm / cold runs of the baseline Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD] )\nSimulated 2.4x the correlated assets and 316x the Monte Carlo paths in 10 minutes ( STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS] )\n2.0x the throughput ( STAC-A2.\u03b22.HPORTFOLIO.SPEED )\nCompared to a solution using eight NVIDIA H100 PCIe GPUs, as well as previous versions of the NVIDIA STAC Pack and CUDA, this solution using NVIDIA H100 SXM5 GPUs set the following records:\n1.59x the throughput ( STAC-A2.\u03b22.HPORTFOLIO.SPEED )\n1.17x the speed in the warm runs of the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD] )\n3.1x / 3.0x the speed in the warm / cold runs of the large Greeks benchmarks ( STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD] )\nSimulated 10% more correlated assets in 10 minutes ( STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS] )\n17% more energy-efficient ( STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF )\nIn addition to the hardware, NVIDIA provides all the key software component layers with the NVIDIA HPC SDK. This offers multiple options to developers, including NVIDIA CUDA SDK for CUDA/C++ and enabling other languages and directive-based solutions such as OpenMP, OpenACC, accelerations with C++ 17 standard parallelism, and Fortran parallel constructs.\nThis particular implementation was developed on CUDA 12.2 using the highly optimized libraries delivered with CUDA:\ncuBLAS: The GPU-enabled implementation of the linear algebra package BLAS.\ncuRAND: A parallel and efficient GPU implementation of random-number generators.\nThe implementation was supported by tools:\nNVIDIA Nsight Systems for timeline profiling\nNVIDIA Nsight Compute for kernel profiling\nNVIDIA Compute Sanitizer and CUDA-GDB for debugging\nThe different operations of the benchmark are implemented using building blocks from a set of quantitative finance reference implementations of state-of-the-art algorithms, including asset diffusion and Longstaff-Schwartz pricing. These are exposed in a modular and maintainable framework using object-oriented programming in CUDA/C++.\nHPC plus AI extensions\nEnd customers perform other price-discovery, market-risk calculations in their real-world workflow calculations:\nSensitivity Greeks\nProfit and loss (P&L) calculations\nValue at risk (VaR)\nMargin and counterparty credit risk (CCR) calculations, such as credit valuation adjustment (CVA)\nPricing/risk calculation, algorithmic trading model development, and backtesting need a robust scalable environment. In areas such as CVA, such scaled setups on dense server nodes with eight NVIDIA GPUs have been shown to reduce the number of required nodes from 100 to 4 in simulation\u2013 and compute-intensive calculations (separately from STAC benchmarking). This reduces the total cost of ownership (TCO).\nWe are increasingly seeing a convergence of extended HPC quantitative finance and AI requirements from end users where end financial solutions use a combination of quantitative finance, data engineering and analytics, ML, and AI neural net algorithms.\nThe current solutions work with traditional quantitative models, extended with data engineering and analytics with accelerated machine learning (ML), such as XGBOOST, with tools such as NVIDIA RAPIDS, and AI deep learning. Deep learning involves long short-term memory (LSTM), recurrent neural networks (RNNs), and other advanced areas such as large language models (LLMs).\nAlso in today\u2019s AI world, we have gone on to new topic areas such as generative AI. Generative AI refers to AI algorithms that enable computers to use existing or past content like text, audio, and video files, images, and even code to generate new content. Examples of these AI generative models include the range of generative pretrained transformer (GPT) models to diffusion models used in image generation.\nMore diverse HPC workloads are becoming prominent in financial areas using reinforcement learning (RL) and applied in financial areas such as limit order book price prediction.\nAgent-based models are used to reproduce the interactions between economic agents, such as market participants in financial transactions. This is a trial and error process akin to a child learning the real world. The agent operates in an environment that has an end task with states (observable or not observed) and potential actions with positive and negative rewards to those other states with simulation. As a result, the states have a value function that is also iteratively updated based on the trail and error process. The agent must solve for the optimal policy behavior equivalent to strategy.\nIn most real-world problems, the optimal behavior is hard to determine. With a learnable policy, the optimal policy with its associated action can be evaluated as good or bad where the agent performs an action or set of actions with rewards and RL is the technique to maximize this reward.\nSuch RL algorithms have been extended to RL from human feedback (RLHF), which makes use of LLMs trained to optimize policy rewards, such as ChatGPT. NVIDIA provides tools to train similar RLHF-incorporated, GPT-based models.\nOrganizations can use such foundational LLMs on unstructured sources of information such as financial news along with customization techniques such as training, parameter-efficient fine-tuning, and fine-tuning to improve the LLMs for understanding the financial domain better.\nEnd users can interface with such customized models combining it with techniques such as r etrieval augmented Generation (RAG) to gain an information edge by querying and obtaining insights from unstructured data sources that are untapped for decision-making insights.\nFor example, these techniques can be used on news, financial documents, financial 10K or 10Q filings, and federal reserve commentary beyond traditional sources of tabular market data, to generate insights and signals, referred to as alternative data (alt data). These alt data information signals can be used to support systematic algorithmic trading models as well as discretionary traders.\nSummary\nThe convergence of HPC and AI is happening as financial firms work on big-picture solutions. Stakeholders include both sell-side (global market banks and broker dealers) and buy-side (insurers, hedge funds, market-makers, high frequency traders, and asset managers). Solutions typically involve combining various modeling techniques, such as HPC quantitative finance, ML, RL, and NLP LLM generative AI models.\nDue to the ability to cater to multiple HPC plus AI workloads, end clients are able to gain the maximum ROI and lower TCO by catering to all workloads including the largest and most demanding AI, ML, DL training, HPC modeling, and simulation initiatives."}], "https://developer.nvidia.com/blog/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/": [{"text": "NVIDIA has introduced the HelpSteer dataset, a multi-attribute resource that enables developers to control large language model responses using the SteerLM technique. This dataset, created in collaboration with Scale AI, improves factuality and coherence in LLM responses by annotating attributes like helpfulness, correctness, coherence, complexity, and verbosity. LLMs trained on HelpSteer are more accurate and coherent, making them useful in industries like customer service and education. The Llama2-70B model trained with HelpSteer and SteerLM outperforms the Llama 2-70B chat model on benchmarks like MT Bench and TruthfulQA MC2, indicating higher helpfulness and truthfulness. The SteerLM model allows for adjusting complexity and verbosity levels at inference time, making it versatile for different use cases. Developers can access the dataset, train custom LLMs with NeMo framework and SteerLM, and try the Llama2-70B-SteerLM-Chat model on NVIDIA NGC Catalog. The article provides examples of low and high complexity responses generated by the model and invites readers to explore further resources for training and utilizing custom LLMs.", "text_components": ["Announcing HelpSteer: An Open-Source Dataset for Building Helpful LLMs\nNVIDIA recently announced the NVIDIA NeMo SteerLM technique as part of the NVIDIA NeMo framework. This technique enables users to control large language model (LLM) responses during inference. The developer community has shown great interest in using the approach for building custom LLMs.\nThe NVIDIA NeMo team is now open-sourcing a multi-attribute dataset called Helpfulness SteerLM dataset (HelpSteer). This new resource enables developers to get started with using the SteerLM technique quickly and build state-of-the-art custom models.\nHelpSteer is a collaborative effort between our team and Scale AI. Coupled with the SteerLM technique, it improves the factuality and coherence of responses. Developers can now guide LLM responses on additional attributes like complexity and verbosity and enhance the overall controllability of the responses for end-users.\nBy using this new dataset and the SteerLM technique, NVIDIA trained a Llama 2 70B foundational model that outperforms the Llama 2-70B chat model on MT Bench and TruthfulQA MC2 benchmarks.\nLet\u2019s dive deep into how we built this dataset.", "Collecting high-quality human feedback\nWe collected around 10K high-quality prompts and generated four responses for each using an in-house LLM. The Scale AI team then engaged their human experts to evaluate each response on a scale of 0\u20134. Scale\u2019s Generative AI Data Engine combines automation and human intelligence to generate training data tailored to specific AI goals and data needs rapidly.\nAll the responses in the dataset were annotated on five attributes, namely:\nHelpfulness: how well the response overall satisfies what the prompt asks for.\nCorrectness: whether the response contains factually accurate and relevant information without hallucinations or false information.\nCoherence: how clear, logical, and self-consistent the response is.\nComplexity: the sophistication of vocabulary and sentence structure from basic to expert level.\nVerbosity: how concise or verbose the response is relative to what the prompt asks for.\nWe selected these attributes to complement the attributes in OpenAssistant, the dataset used to train the original SteerLM model.\nA sample of the dataset has the following structure:\n### Prompt\nWhat is the spiciest chili in the world? Is it a type of chili or made from another ingredient?\n### Response 1\nThe spiciest chili in the world is the Carolina Reaper. It is a type of chili that is made from the chili pepper variety.\n### Label 1\nhelpfulness:1,correctness:1,coherence:3,complexity:1,verbosity:1\n### Response 2\nThe spiciest chili in the world is the Carolina Reaper, which is a type of chili pepper. It was created by a man named Ed Currie and is a cross between a Ghost Pepper and a Red Habanero. It is known for its extreme heat, which is measured at over one million Scoville units. The Carolina Reaper is made from a chili pepper that has been selectively bred for its high levels of capsaicin, the compound that gives chili peppers their spicy heat. While it is not made from any other ingredient, some people may add it to other ingredients to make a spicy sauce or salsa.\n### Label 2\nhelpfulness:4,correctness:4,coherence:4,complexity:2,verbosity:2 LLMs trained on this dataset are more factually correct, and coherent in their responses, making them more helpful in several industries such as customer service, education, and more. Users can also adjust the preferred level of complexity and verbosity of the language model at inference time.\nFor example, developers can build LLMs that can explain complex subjects such as American History to a middle school student or to someone whose first language isn\u2019t English, all with just one customization run.", "Llama2-70B-SteerLM-Chat evaluation results\nThe Llama 2-70B foundational model trained using the HelpSteer dataset along with the SteerLM technique outperformed the Llama 2-70B chat model on several benchmarks.\nThe following table compares the performance metrics for both the models on two popular benchmarks: MT Bench and TruthfulQA MC2. A higher score is better, meaning that the SteerLM model is both more helpful and truthful compared to the Llama 2-70B chat model.\nBenchmark\nLlama 2-70B chat\nLlama2-70B-SteerLM-Chat\nMT Bench\n6.86\n7.54\nTruthfulQA MC2\n0.528\n0.561\nTable 1. Comparison of Llama 2-70B chat and Llama2-70B-SteerLM-Chat models on MT Bench and TruthfulQA MC2 benchmarks In Figure 1, we observe that the Llama2-70B-SteerLM-Chat model outperforms the Llama 2-70B chat model in most categories within MT Bench.\nwe observe that the Llama2-70B-SteerLM-Chat model outperforms the Llama 2-70B chat model in most categories within MT Bench.\nFigure 1. Score in each category of MT Bench for Llama2-70B-SteerLM-Chat compared to Llama 2 70B chat", "Llama2-70B-SteerLM-Chat model responses\nLet\u2019s look at some examples from this SteerLM model. For the same prompt, the model provided different responses based on different levels of complexity. Developers can use the same model and tailor it for different use cases at the time of inference.\nPrompt\nCan we travel at the speed of light?\nLow-Complexity Response\n'No, it is not possible to travel at the speed of light. The speed of light is a fundamental constant of nature and is the maximum speed at which information can be transmitted in a vacuum. According to our current understanding of physics, it is not possible for any matter or energy to reach or exceed the speed of light.'\nHigh-Complexity Response\nAccording to our current understanding of physics, it is not possible to travel at the speed of light. This is because as an object approaches the speed of light, its mass would increase infinitely, making it impossible to reach the speed of light. This effect is described by Einstein's theory of relativity and is known as the \"mass-energy equivalence. Additionally, as an object approaches the speed of light, its length would contract and time would slow down, making it impossible to reach the speed of light from an observer's perspective. Therefore, it is currently believed that it is not possible for any material object to travel at the speed of light, and the speed of light is considered to be the upper limit for the speed of any matter or energy in the universe.", "Learn more\nTo learn more about HelpSteer dataset and how to train custom LLMs with NeMo framework and SteerLM technique,\nExperience the Llama2-70B-SteerLM-Chat model directly from your browser on NVIDIA NGC Catalog\nLearn how to train SteerLM model with HelpSteer dataset\nRead the technical paper submitted on ArXiv\nDownload the dataset from HuggingFace\nRead the SteerLM announcement blog post"], "document_title": "Announcing HelpSteer: An Open-Source Dataset for Building Helpful LLMs", "document_url": "https://developer.nvidia.com/blog/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/", "document_date": "2023-11-27T17:00:00", "document_date_modified": "2024-01-03T23:48:02", "document_full_text": "Announcing HelpSteer: An Open-Source Dataset for Building Helpful LLMs\nNVIDIA recently announced the NVIDIA NeMo SteerLM technique as part of the NVIDIA NeMo framework. This technique enables users to control large language model (LLM) responses during inference. The developer community has shown great interest in using the approach for building custom LLMs.\nThe NVIDIA NeMo team is now open-sourcing a multi-attribute dataset called Helpfulness SteerLM dataset (HelpSteer). This new resource enables developers to get started with using the SteerLM technique quickly and build state-of-the-art custom models.\nHelpSteer is a collaborative effort between our team and Scale AI. Coupled with the SteerLM technique, it improves the factuality and coherence of responses. Developers can now guide LLM responses on additional attributes like complexity and verbosity and enhance the overall controllability of the responses for end-users.\nBy using this new dataset and the SteerLM technique, NVIDIA trained a Llama 2 70B foundational model that outperforms the Llama 2-70B chat model on MT Bench and TruthfulQA MC2 benchmarks.\nLet\u2019s dive deep into how we built this dataset.\nCollecting high-quality human feedback\nWe collected around 10K high-quality prompts and generated four responses for each using an in-house LLM. The Scale AI team then engaged their human experts to evaluate each response on a scale of 0\u20134. Scale\u2019s Generative AI Data Engine combines automation and human intelligence to generate training data tailored to specific AI goals and data needs rapidly.\nAll the responses in the dataset were annotated on five attributes, namely:\nHelpfulness: how well the response overall satisfies what the prompt asks for.\nCorrectness: whether the response contains factually accurate and relevant information without hallucinations or false information.\nCoherence: how clear, logical, and self-consistent the response is.\nComplexity: the sophistication of vocabulary and sentence structure from basic to expert level.\nVerbosity: how concise or verbose the response is relative to what the prompt asks for.\nWe selected these attributes to complement the attributes in OpenAssistant, the dataset used to train the original SteerLM model.\nA sample of the dataset has the following structure:\n### Prompt\nWhat is the spiciest chili in the world? Is it a type of chili or made from another ingredient?\n### Response 1\nThe spiciest chili in the world is the Carolina Reaper. It is a type of chili that is made from the chili pepper variety.\n### Label 1\nhelpfulness:1,correctness:1,coherence:3,complexity:1,verbosity:1\n### Response 2\nThe spiciest chili in the world is the Carolina Reaper, which is a type of chili pepper. It was created by a man named Ed Currie and is a cross between a Ghost Pepper and a Red Habanero. It is known for its extreme heat, which is measured at over one million Scoville units. The Carolina Reaper is made from a chili pepper that has been selectively bred for its high levels of capsaicin, the compound that gives chili peppers their spicy heat. While it is not made from any other ingredient, some people may add it to other ingredients to make a spicy sauce or salsa.\n### Label 2\nhelpfulness:4,correctness:4,coherence:4,complexity:2,verbosity:2 LLMs trained on this dataset are more factually correct, and coherent in their responses, making them more helpful in several industries such as customer service, education, and more. Users can also adjust the preferred level of complexity and verbosity of the language model at inference time.\nFor example, developers can build LLMs that can explain complex subjects such as American History to a middle school student or to someone whose first language isn\u2019t English, all with just one customization run.\nLlama2-70B-SteerLM-Chat evaluation results\nThe Llama 2-70B foundational model trained using the HelpSteer dataset along with the SteerLM technique outperformed the Llama 2-70B chat model on several benchmarks.\nThe following table compares the performance metrics for both the models on two popular benchmarks: MT Bench and TruthfulQA MC2. A higher score is better, meaning that the SteerLM model is both more helpful and truthful compared to the Llama 2-70B chat model.\nBenchmark\nLlama 2-70B chat\nLlama2-70B-SteerLM-Chat\nMT Bench\n6.86\n7.54\nTruthfulQA MC2\n0.528\n0.561\nTable 1. Comparison of Llama 2-70B chat and Llama2-70B-SteerLM-Chat models on MT Bench and TruthfulQA MC2 benchmarks In Figure 1, we observe that the Llama2-70B-SteerLM-Chat model outperforms the Llama 2-70B chat model in most categories within MT Bench.\nwe observe that the Llama2-70B-SteerLM-Chat model outperforms the Llama 2-70B chat model in most categories within MT Bench.\nFigure 1. Score in each category of MT Bench for Llama2-70B-SteerLM-Chat compared to Llama 2 70B chat\nLlama2-70B-SteerLM-Chat model responses\nLet\u2019s look at some examples from this SteerLM model. For the same prompt, the model provided different responses based on different levels of complexity. Developers can use the same model and tailor it for different use cases at the time of inference.\nPrompt\nCan we travel at the speed of light?\nLow-Complexity Response\n'No, it is not possible to travel at the speed of light. The speed of light is a fundamental constant of nature and is the maximum speed at which information can be transmitted in a vacuum. According to our current understanding of physics, it is not possible for any matter or energy to reach or exceed the speed of light.'\nHigh-Complexity Response\nAccording to our current understanding of physics, it is not possible to travel at the speed of light. This is because as an object approaches the speed of light, its mass would increase infinitely, making it impossible to reach the speed of light. This effect is described by Einstein's theory of relativity and is known as the \"mass-energy equivalence. Additionally, as an object approaches the speed of light, its length would contract and time would slow down, making it impossible to reach the speed of light from an observer's perspective. Therefore, it is currently believed that it is not possible for any material object to travel at the speed of light, and the speed of light is considered to be the upper limit for the speed of any matter or energy in the universe.\nLearn more\nTo learn more about HelpSteer dataset and how to train custom LLMs with NeMo framework and SteerLM technique,\nExperience the Llama2-70B-SteerLM-Chat model directly from your browser on NVIDIA NGC Catalog\nLearn how to train SteerLM model with HelpSteer dataset\nRead the technical paper submitted on ArXiv\nDownload the dataset from HuggingFace\nRead the SteerLM announcement blog post"}], "https://developer.nvidia.com/blog/bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security/": [{"text": "The article discusses the increasing threat of identity-based attacks, particularly phishing, and the challenges faced by IT teams in securing enterprise applications and managing connected devices. It highlights the importance of using AI, particularly generative AI and large language models, in transforming digital security by improving threat detection, training data generation, and vulnerability defense. The article also introduces the concept of security copilots to enhance the efficiency of security teams and the significance of synthetic data generation in improving spear phishing email detection. It emphasizes the need for a comprehensive approach to enterprise security, utilizing AI at different levels to address cybersecurity challenges. The article showcases the capabilities of NVIDIA AI platform in enhancing cybersecurity and provides examples of how generative AI and LLMs can be applied in cybersecurity. It also mentions the availability of tools such as NVIDIA Morpheus and NeMo for accelerated AI in cybersecurity.", "text_components": ["Bolstering Cybersecurity: How Large Language Models and Generative AI are Transforming Digital Security\nIdentity-based attacks are on the rise, with phishing remaining the most common and second-most expensive attack vector. Some attackers are using AI to craft more convincing phishing messages and deploying bots to get around automated defenses designed to spot suspicious behavior.\nAt the same time, a continued increase in enterprise applications introduces challenges for IT teams who must support, secure, and manage these applications, often with no increase in staffing.\nThe number of connected devices continues to grow, introducing security risks due to an increase in the attack surface. This is compounded by potential vulnerabilities associated with each device.\nWhile there are many security tools and applications available to help enterprises defend against attacks, integrating and managing a large number of tools introduces more cost, complexity, and risk.\n\u200b\u200bCybersecurity is among the top three challenges for CEOs, second to environmental sustainability and just ahead of tech modernization. Generative AI can be transformational for cybersecurity. It can help security analysts find the information they need to do their jobs faster, generate synthetic data to train AI models to identify risks accurately, and run what-if scenarios to better prepare for potential threats.", "Using AI to keep pace with an expanding threat landscape\nCybersecurity is a data problem, and the vast amount of data available is too large for manual screening and threat detection. This means human analysts can no longer effectively defend against the most sophisticated attacks because the speed and complexity of attacks and defenses exceed human capacity. With AI, organizations can achieve 100 percent visibility of their data and quickly discover anomalies, enabling them to detect threats faster.\nAlthough the exponentially increasing quantity of data poses a challenge for threat detection, AI-based approaches to cyber defense require access to training data. In some cases, this isn\u2019t readily available, because organizations don\u2019t typically share sensitive data. With generative AI, synthetic data can help \u200caddress the data gap and improve cybersecurity AI defense.\nOne of the most effective ways of synthesizing and contextualizing data is through natural language. The advancements of large language models (LLMs) are expanding threat detection and data generation techniques that improve cybersecurity.\nThis post explores three use cases showing how generative AI and LLMs improve cybersecurity and provides three examples of how AI foundation models for cybersecurity can be applied.", "Copilots boost the efficiency and capabilities of security teams\nStaffing shortages for cybersecurity professionals persist. Security copilots with retrieval-augmented generation (RAG) enable organizations to tap into existing knowledge bases and extend the capabilities of human analysts, making them more efficient and effective.\nCopilots learn from the behaviors of security analysts, adapt to their needs, and provide relevant insights that guide them in their daily work, all in a natural interface. Organizations are quickly discovering the value of RAG chatbots.\nBy 2025, two-thirds of businesses will leverage a combination of generative AI and RAG to power domain-specific, self-service knowledge discovery, improving decision efficacy by 50% 1.\nIn addition to not having enough cybersecurity personnel, organizations are challenged in training new and existing employees. With copilots, cybersecurity professionals can get near real-time responses and guidance on complex deployment scenarios without the need for additional training or research.\nWhile security copilots can bring transformational benefits to an organization, they\u2019re only useful when they can provide fast, accurate, and up-to-date information. The NVIDIA AI Chatbot with Retrieval-Augmented Generation workflow provides a great starting point. It demonstrates how to build agents and chatbots that can retrieve the most up-to-date information in real-time and provide accurate responses in natural language.", "Generative AI can dramatically improve common vulnerability defense\nPatching software security issues are becoming increasingly challenging as the number of reported security flaws in the common vulnerabilities and exposures (CVEs) database hit a record high in 2022. With over 200,000 cumulative vulnerabilities reported as of the third quarter of 2023, it\u2019s clear that a traditional approach to scanning and patching has become unmanageable.\nOrganizations that deploy risk-based analysis experience less costly breaches compared to those that rely solely on CVE scoring to prioritize vulnerabilities. Using generative AI, it\u2019s possible to improve vulnerability defense while decreasing the load on security teams.\nUsing the NVIDIA Morpheus LLM engine integration, NVIDIA built a pipeline to address CVE risk analysis with RAG. Security analysts can determine whether a software container includes vulnerable and exploitable components using LLMs and RAG.\nThis method enabled analysts to investigate individual CVEs 4X faster, on average, and identify vulnerabilities with high accuracy so patches could be prioritized and addressed accordingly.\nA diagram of NVIDIA Morpheus LLM engine for CVE Exploitability using retrieval augmented generation.\nFigure 1. CVE exploitability using Morpheus LLM engine supporting model-generated RAG tasks and multiple loops", "Foundation models for cybersecurity\nWhile pretrained models are useful for many applications, there are times when it\u2019s beneficial to train a custom model from scratch. This is helpful when there\u2019s a specific domain with a unique vocabulary or the content has properties that do not conform to traditional language paradigms and structures.\nIn cybersecurity, this is observed with certain types of raw logs. Think about a book and how words form sentences, sentences form paragraphs, and paragraphs form chapters. There\u2019s an inherent structure that is part of the language model. Contrast that to data contained in a format like JSON-lines or CEF. Proximity of the data keys and values doesn\u2019t have the same meaning.\nUsing custom foundation models presents multiple opportunities.\nAddressing the data gap: while making better use of the influx of data can lead to improved cybersecurity, the quality of the data matters. When there is a lack of available training data, the accuracy of detecting threats is compromised. Generative AI can help \u200caddress the data gap with synthetic data generation, or by using large models to generate data to train smaller models.\nPerforming \u201cwhat if\u201d scenarios: novel threats are challenging to defend against without data sets to build the defenses. Generative AI can be used for attack simulations and to perform \u201cwhat if\u201d scenarios\u2014to test against attack patterns that haven\u2019t yet been experienced. This dynamic model training, based on evolving threats and changing patterns in data can help to improve overall security.\nFeed downstream anomaly detectors: use large models to generate data that train downstream, lightweight models used for threat detection, which can reduce infrastructure costs while keeping the same level of accuracy.\nNVIDIA performed many experiments and trained several cybersecurity-specific foundation models, including one based on GPT-2 style models referenced as CyberGPT. One of those is a model that is trained on identity data (including application logs like Azure AD). With this model, one can generate highly realistic synthetic data that addresses a data gap and can perform \u201cwhat if\u201d scenarios.\nFigure 2 shows the Rogue2 F1 scores for CyberGPT models of various sizes, with each instance achieving around 80% accuracy. This means that 8 out of 10 logs generated are virtually indistinguishable from logs generated by real network users.\nA bar chart showing 80% accuracy for Rogue2 F1 scores of CyberGPT models generated compared with authentic logs.\nFigure 2. Accuracy and realism scores of logs generated by CyberGPT models\nAs for training times, a supercomputer isn\u2019t necessary to realize quality results. In testing, training times were as low as 12 GPU hours for a GPT-2-small model with character-level tokenization. This model is trained on 2.3M rows of over 100 user logs with 1,000 iterations. This model was trained on multiple types of data, including Azure, SharePoint, Confluence, and Jira.\nExperiments were also run with tokenizers\u2013primarily character-level tokenizers, off-the-shelf byte pair encoding (BPE) tokenizers, and custom-trained tokenizers. While there are benefits and drawbacks to each, the best performance comes as a result of training custom tokenizers. This not only enables more efficient use of resources due to the custom vocabulary, but it results in reduced tokenization errors and can handle log-specific syntax.\nWhile these results reflect experiments with language models, the same tests with LLMs achieve similar results.", "Synthetic data generation provides 100% detection of spear phishing e-mails\nSpear phishing e-mails are highly targeted, and therefore, very convincing. The only real difference between a spear phishing (and, in general, any effective phishing campaign) and a benign e-mail is the intent of the sender. This makes spear phishing challenging to defend against with AI because there is a lack of available training data.\nTo explore the potential of synthetic data generation in enhancing spear phishing e-mail detection, a pipeline was constructed using NVIDIA Morpheus.\nWith off-the-shelf models, the spear phishing detection pipeline missed 16% (about 600) of malicious e-mails. The uncaught malicious e-mails were then used to create a new synthetic dataset. A new intent model was learned from the synthetically generated e-mails, and integrated into our spear phishing detection pipeline. The addition of this new intent model feature in the detection pipeline resulted in 100% detection of spear phishing e-mails trained solely on synthetic e-mails.\nThe NVIDIA spear phishing detection AI workflow provides an example of how to build this solution using NVIDIA Morpheus.\nA diagram showing NVIDIA Morpheus spear-phishing detection AI pipeline using generative AI.\nFigure 3. Spear phishing detection pipeline built using synthetically generated spear phishing e-mails that correspond to specific behavioral intents", "A comprehensive approach to enterprise security\nThe NVIDIA AI platform is uniquely positioned to help address these challenges\u2013building in security at multiple levels. At the hardware infrastructure level, and beyond the data center perimeter to the edge of every server, while also providing tools that help to secure your data with AI.", "Learn more\nWatch the session from Bartley Richardson, head of cybersecurity engineering at NVIDIA, to see demonstrations of the use cases illustrated in this post. Learn about integrating language models and cybersecurity featured at NVIDIA LLM Developer Day.\nCheck out the November 2023 release of NVIDIA Morpheus to access the new LLM engine integration feature, and get started with accelerated AI for cybersecurity.\nFind out how NVIDIA NeMo provides an easy way to get started with building, customizing, and deploying generative AI models.\nNVIDIA Morpheus and NeMo are included with NVIDIA AI Enterprise, the enterprise-grade software that powers the NVIDIA AI platform.\nIDC FutureScape: Worldwide Artificial Intelligence and Automation 2024 Predictions, #AP50341323, October 2023 \u21a9\ufe0e"], "document_title": "Bolstering Cybersecurity: How Large Language Models and Generative AI are Transforming Digital Security", "document_url": "https://developer.nvidia.com/blog/bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security/", "document_date": "2023-11-27T09:00:00", "document_date_modified": "2023-12-14T19:27:39", "document_full_text": "Bolstering Cybersecurity: How Large Language Models and Generative AI are Transforming Digital Security\nIdentity-based attacks are on the rise, with phishing remaining the most common and second-most expensive attack vector. Some attackers are using AI to craft more convincing phishing messages and deploying bots to get around automated defenses designed to spot suspicious behavior.\nAt the same time, a continued increase in enterprise applications introduces challenges for IT teams who must support, secure, and manage these applications, often with no increase in staffing.\nThe number of connected devices continues to grow, introducing security risks due to an increase in the attack surface. This is compounded by potential vulnerabilities associated with each device.\nWhile there are many security tools and applications available to help enterprises defend against attacks, integrating and managing a large number of tools introduces more cost, complexity, and risk.\n\u200b\u200bCybersecurity is among the top three challenges for CEOs, second to environmental sustainability and just ahead of tech modernization. Generative AI can be transformational for cybersecurity. It can help security analysts find the information they need to do their jobs faster, generate synthetic data to train AI models to identify risks accurately, and run what-if scenarios to better prepare for potential threats.\nUsing AI to keep pace with an expanding threat landscape\nCybersecurity is a data problem, and the vast amount of data available is too large for manual screening and threat detection. This means human analysts can no longer effectively defend against the most sophisticated attacks because the speed and complexity of attacks and defenses exceed human capacity. With AI, organizations can achieve 100 percent visibility of their data and quickly discover anomalies, enabling them to detect threats faster.\nAlthough the exponentially increasing quantity of data poses a challenge for threat detection, AI-based approaches to cyber defense require access to training data. In some cases, this isn\u2019t readily available, because organizations don\u2019t typically share sensitive data. With generative AI, synthetic data can help \u200caddress the data gap and improve cybersecurity AI defense.\nOne of the most effective ways of synthesizing and contextualizing data is through natural language. The advancements of large language models (LLMs) are expanding threat detection and data generation techniques that improve cybersecurity.\nThis post explores three use cases showing how generative AI and LLMs improve cybersecurity and provides three examples of how AI foundation models for cybersecurity can be applied.\nCopilots boost the efficiency and capabilities of security teams\nStaffing shortages for cybersecurity professionals persist. Security copilots with retrieval-augmented generation (RAG) enable organizations to tap into existing knowledge bases and extend the capabilities of human analysts, making them more efficient and effective.\nCopilots learn from the behaviors of security analysts, adapt to their needs, and provide relevant insights that guide them in their daily work, all in a natural interface. Organizations are quickly discovering the value of RAG chatbots.\nBy 2025, two-thirds of businesses will leverage a combination of generative AI and RAG to power domain-specific, self-service knowledge discovery, improving decision efficacy by 50% 1.\nIn addition to not having enough cybersecurity personnel, organizations are challenged in training new and existing employees. With copilots, cybersecurity professionals can get near real-time responses and guidance on complex deployment scenarios without the need for additional training or research.\nWhile security copilots can bring transformational benefits to an organization, they\u2019re only useful when they can provide fast, accurate, and up-to-date information. The NVIDIA AI Chatbot with Retrieval-Augmented Generation workflow provides a great starting point. It demonstrates how to build agents and chatbots that can retrieve the most up-to-date information in real-time and provide accurate responses in natural language.\nGenerative AI can dramatically improve common vulnerability defense\nPatching software security issues are becoming increasingly challenging as the number of reported security flaws in the common vulnerabilities and exposures (CVEs) database hit a record high in 2022. With over 200,000 cumulative vulnerabilities reported as of the third quarter of 2023, it\u2019s clear that a traditional approach to scanning and patching has become unmanageable.\nOrganizations that deploy risk-based analysis experience less costly breaches compared to those that rely solely on CVE scoring to prioritize vulnerabilities. Using generative AI, it\u2019s possible to improve vulnerability defense while decreasing the load on security teams.\nUsing the NVIDIA Morpheus LLM engine integration, NVIDIA built a pipeline to address CVE risk analysis with RAG. Security analysts can determine whether a software container includes vulnerable and exploitable components using LLMs and RAG.\nThis method enabled analysts to investigate individual CVEs 4X faster, on average, and identify vulnerabilities with high accuracy so patches could be prioritized and addressed accordingly.\nA diagram of NVIDIA Morpheus LLM engine for CVE Exploitability using retrieval augmented generation.\nFigure 1. CVE exploitability using Morpheus LLM engine supporting model-generated RAG tasks and multiple loops\nFoundation models for cybersecurity\nWhile pretrained models are useful for many applications, there are times when it\u2019s beneficial to train a custom model from scratch. This is helpful when there\u2019s a specific domain with a unique vocabulary or the content has properties that do not conform to traditional language paradigms and structures.\nIn cybersecurity, this is observed with certain types of raw logs. Think about a book and how words form sentences, sentences form paragraphs, and paragraphs form chapters. There\u2019s an inherent structure that is part of the language model. Contrast that to data contained in a format like JSON-lines or CEF. Proximity of the data keys and values doesn\u2019t have the same meaning.\nUsing custom foundation models presents multiple opportunities.\nAddressing the data gap: while making better use of the influx of data can lead to improved cybersecurity, the quality of the data matters. When there is a lack of available training data, the accuracy of detecting threats is compromised. Generative AI can help \u200caddress the data gap with synthetic data generation, or by using large models to generate data to train smaller models.\nPerforming \u201cwhat if\u201d scenarios: novel threats are challenging to defend against without data sets to build the defenses. Generative AI can be used for attack simulations and to perform \u201cwhat if\u201d scenarios\u2014to test against attack patterns that haven\u2019t yet been experienced. This dynamic model training, based on evolving threats and changing patterns in data can help to improve overall security.\nFeed downstream anomaly detectors: use large models to generate data that train downstream, lightweight models used for threat detection, which can reduce infrastructure costs while keeping the same level of accuracy.\nNVIDIA performed many experiments and trained several cybersecurity-specific foundation models, including one based on GPT-2 style models referenced as CyberGPT. One of those is a model that is trained on identity data (including application logs like Azure AD). With this model, one can generate highly realistic synthetic data that addresses a data gap and can perform \u201cwhat if\u201d scenarios.\nFigure 2 shows the Rogue2 F1 scores for CyberGPT models of various sizes, with each instance achieving around 80% accuracy. This means that 8 out of 10 logs generated are virtually indistinguishable from logs generated by real network users.\nA bar chart showing 80% accuracy for Rogue2 F1 scores of CyberGPT models generated compared with authentic logs.\nFigure 2. Accuracy and realism scores of logs generated by CyberGPT models\nAs for training times, a supercomputer isn\u2019t necessary to realize quality results. In testing, training times were as low as 12 GPU hours for a GPT-2-small model with character-level tokenization. This model is trained on 2.3M rows of over 100 user logs with 1,000 iterations. This model was trained on multiple types of data, including Azure, SharePoint, Confluence, and Jira.\nExperiments were also run with tokenizers\u2013primarily character-level tokenizers, off-the-shelf byte pair encoding (BPE) tokenizers, and custom-trained tokenizers. While there are benefits and drawbacks to each, the best performance comes as a result of training custom tokenizers. This not only enables more efficient use of resources due to the custom vocabulary, but it results in reduced tokenization errors and can handle log-specific syntax.\nWhile these results reflect experiments with language models, the same tests with LLMs achieve similar results.\nSynthetic data generation provides 100% detection of spear phishing e-mails\nSpear phishing e-mails are highly targeted, and therefore, very convincing. The only real difference between a spear phishing (and, in general, any effective phishing campaign) and a benign e-mail is the intent of the sender. This makes spear phishing challenging to defend against with AI because there is a lack of available training data.\nTo explore the potential of synthetic data generation in enhancing spear phishing e-mail detection, a pipeline was constructed using NVIDIA Morpheus.\nWith off-the-shelf models, the spear phishing detection pipeline missed 16% (about 600) of malicious e-mails. The uncaught malicious e-mails were then used to create a new synthetic dataset. A new intent model was learned from the synthetically generated e-mails, and integrated into our spear phishing detection pipeline. The addition of this new intent model feature in the detection pipeline resulted in 100% detection of spear phishing e-mails trained solely on synthetic e-mails.\nThe NVIDIA spear phishing detection AI workflow provides an example of how to build this solution using NVIDIA Morpheus.\nA diagram showing NVIDIA Morpheus spear-phishing detection AI pipeline using generative AI.\nFigure 3. Spear phishing detection pipeline built using synthetically generated spear phishing e-mails that correspond to specific behavioral intents\nA comprehensive approach to enterprise security\nThe NVIDIA AI platform is uniquely positioned to help address these challenges\u2013building in security at multiple levels. At the hardware infrastructure level, and beyond the data center perimeter to the edge of every server, while also providing tools that help to secure your data with AI.\nLearn more\nWatch the session from Bartley Richardson, head of cybersecurity engineering at NVIDIA, to see demonstrations of the use cases illustrated in this post. Learn about integrating language models and cybersecurity featured at NVIDIA LLM Developer Day.\nCheck out the November 2023 release of NVIDIA Morpheus to access the new LLM engine integration feature, and get started with accelerated AI for cybersecurity.\nFind out how NVIDIA NeMo provides an easy way to get started with building, customizing, and deploying generative AI models.\nNVIDIA Morpheus and NeMo are included with NVIDIA AI Enterprise, the enterprise-grade software that powers the NVIDIA AI platform.\nIDC FutureScape: Worldwide Artificial Intelligence and Automation 2024 Predictions, #AP50341323, October 2023 \u21a9\ufe0e"}], "https://developer.nvidia.com/blog/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/": [{"text": "The article discusses how AI is being used to improve medical imaging, specifically focusing on 3D medical imaging for health screenings and risk assessments. NVIDIA MONAI Cloud APIs are introduced as a solution to the challenges of achieving consistent and reliable results in medical image segmentation. The article explains the use of VISTA-3D and Auto3DSeg in conjunction with the NVIDIA MONAI Cloud APIs to achieve adaptable 3D medical imaging. VISTA-3D offers interactive annotation for accurate segmentation analysis across anatomies and modalities, reducing costs and expediting workflows. Auto3DSeg provides developers with tools for top-tier 3D medical image segmentation, with features like data-driven model selection and streamlined development cycles. The article emphasizes the flexibility and efficiency of these tools in training AI models for medical imaging, leading to improved performance and reduced complexity. Overall, platform integrators can benefit from utilizing NVIDIA MONAI Cloud APIs to accelerate the development of AI models for medical imaging.", "text_components": ["Accelerate AI Workflows for 3D Medical Imaging with NVIDIA MONAI Cloud APIs\nAI is increasingly being used to improve medical imaging for health screenings and risk assessments. Medical image segmentation, for example, provides vital data for tumor detection and treatment planning. And yet the unique and varied nature of medical images makes achieving consistent and reliable results challenging.\nNVIDIA MONAI Cloud APIs help solve these challenges, simplifying the journey of AI capabilities and infrastructure setup for platform integrators. This post introduces NVIDIA MONAI Cloud APIs, VISTA-3D, and Auto3DSeg and explains how to use them together to achieve adaptable 3D medical imaging.\nNVIDIA MONAI Cloud APIs offer low-latency, interactive, and cost-effective AI-assisted annotation workflows. With the continual learning mechanism, the model adapts to new real-world data, enabling it to maintain its relevance and robustness over time.", "Interactive annotation with VISTA-3D\nTrained on vast datasets, VISTA-3D is a specialized interactive foundational model for 3D medical imaging. Powering NVIDIA MONAI Cloud API interactive annotation, VISTA-3D provides accurate and adaptable segmentation analysis across anatomies and modalities. It handles a variety of tasks and adapts to different conditions and anatomical areas. This versatility reduces costs and expedites AI imaging workflows without the intricate complexities of model selection and adaptation.\nIn the realm of medical imaging annotation, VISTA-3D represents a transformative approach. By blending semantic segmentation and interactivity, it bridges the gap between AI and human experts (bioimaging scientists and radiologists, for example). VISTA-3D offers these professionals an evolving AI model that ensures both high accuracy and ease of use.\nCore to VISTA-3D are three adaptable, user-friendly workflows:\nSegment everything: Entire image exploration useful for understanding diseases impacting multiple organs or for holistic treatment planning.\nSegment using class: Detailed section views selectable by specific classes for targeted analysis of certain diseases or organs; valuable for mapping tumors in organs.\nSegment point prompts: User feedback-guided image segmentation, through click-based selection of interest areas, for improved accuracy and faster creation of ground-truth data.\nVISTA-3D architecture boasts an impressive mean dice score of about 0.91, thanks to its blend of interactive and automatic systems. This foundational flexibility enables users to quickly tailor the model for their tasks.\nDiagram showing the architecture of VISTA-3D featuring the two prompt heads that help with automatic or interactive segmentation\nFigure 1. VISTA-3D architecture blends automatic and interactive systems for model tailoring", "AI model training with Auto3DSeg\nBuilt on the foundations of MONAI and powered by cutting-edge GPUs, Auto3DSeg provides developers with the tools for achieving top-tier 3D medical image segmentation. The computational efficiency of Auto3DSeg is optimized to ensure rapid training while extracting the most from GPU computational capabilities.\nAuto3DSeg uses MONAI components to achieve state-of-the-art segmentation performance. A MONAI Bundle offers a more customized solution. A MONAI Bundle defines a packaged network or model that includes the information necessary for users and programs to understand how the model is used and for what purpose. Bring your own MONAI Bundle to training and take full advantage of the robust, scalable training infrastructure.\nOnce you have your annotated data, use either custom model training or Auto3DSeg for an optimized model development experience.\nTraining is all about flexibility\u2014bring your unique MONAI Bundle, and watch as your training scales effortlessly on the NVIDIA MONAI Cloud API platform.\nIf automation is your goal, Auto3DSeg is your answer. Designed with developers in mind, Auto3DSeg stands out with its innovative features:\nData-driven model selection: Auto3DSeg delivers intelligent automation. By analyzing annotated imaging datasets, Auto3DSeg can automatically select and scale the most appropriate model architectures. This ensures optimized performance tailored specifically to your data.\nStreamlined development cycle: The development cycle is significantly streamlined with Auto3DSeg. It reduces complexity by incorporating automated parallel training and hyperparameter optimization. This expedites the entire process, from dataset analysis to creating deployment-ready models, saving time and resources.\nProven state-of-the-art segmentation performance: Auto3DSeg has secured multiple top positions at MICCAI in various segmentation challenges, notably the BraTS 2023, KiTS 2023, SEG.A. 2023 and MVSEG 2023 competitions.\nDiagram showing the flow of Auto3DSeg starting with labeled data on the left, the Auto3DSeg sequence in the middle, and resulting trained model(s) on the right.\nFigure 2. Auto3DSeg workflow", "Summary\nPlatform integrators striving for innovation in medical imaging can fully harness the capabilities of NVIDIA MONAI Cloud APIs, expediting the development of their AI models for medical imaging. This approach reduces operational overhead and grants immediate access to accelerated computing and AI capabilities in a streamlined manner.\nTo explore VISTA-3D, custom model training, and Auto3DSeg, sign up for the early access program. To try VISTA-3D, visit NVIDIA AI Foundation Models starting November 27."], "document_title": "Accelerate AI Workflows for 3D Medical Imaging with NVIDIA MONAI Cloud APIs", "document_url": "https://developer.nvidia.com/blog/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/", "document_date": "2023-11-26T14:00:00", "document_date_modified": "2023-12-01T19:30:20", "document_full_text": "Accelerate AI Workflows for 3D Medical Imaging with NVIDIA MONAI Cloud APIs\nAI is increasingly being used to improve medical imaging for health screenings and risk assessments. Medical image segmentation, for example, provides vital data for tumor detection and treatment planning. And yet the unique and varied nature of medical images makes achieving consistent and reliable results challenging.\nNVIDIA MONAI Cloud APIs help solve these challenges, simplifying the journey of AI capabilities and infrastructure setup for platform integrators. This post introduces NVIDIA MONAI Cloud APIs, VISTA-3D, and Auto3DSeg and explains how to use them together to achieve adaptable 3D medical imaging.\nNVIDIA MONAI Cloud APIs offer low-latency, interactive, and cost-effective AI-assisted annotation workflows. With the continual learning mechanism, the model adapts to new real-world data, enabling it to maintain its relevance and robustness over time.\nInteractive annotation with VISTA-3D\nTrained on vast datasets, VISTA-3D is a specialized interactive foundational model for 3D medical imaging. Powering NVIDIA MONAI Cloud API interactive annotation, VISTA-3D provides accurate and adaptable segmentation analysis across anatomies and modalities. It handles a variety of tasks and adapts to different conditions and anatomical areas. This versatility reduces costs and expedites AI imaging workflows without the intricate complexities of model selection and adaptation.\nIn the realm of medical imaging annotation, VISTA-3D represents a transformative approach. By blending semantic segmentation and interactivity, it bridges the gap between AI and human experts (bioimaging scientists and radiologists, for example). VISTA-3D offers these professionals an evolving AI model that ensures both high accuracy and ease of use.\nCore to VISTA-3D are three adaptable, user-friendly workflows:\nSegment everything: Entire image exploration useful for understanding diseases impacting multiple organs or for holistic treatment planning.\nSegment using class: Detailed section views selectable by specific classes for targeted analysis of certain diseases or organs; valuable for mapping tumors in organs.\nSegment point prompts: User feedback-guided image segmentation, through click-based selection of interest areas, for improved accuracy and faster creation of ground-truth data.\nVISTA-3D architecture boasts an impressive mean dice score of about 0.91, thanks to its blend of interactive and automatic systems. This foundational flexibility enables users to quickly tailor the model for their tasks.\nDiagram showing the architecture of VISTA-3D featuring the two prompt heads that help with automatic or interactive segmentation\nFigure 1. VISTA-3D architecture blends automatic and interactive systems for model tailoring\nAI model training with Auto3DSeg\nBuilt on the foundations of MONAI and powered by cutting-edge GPUs, Auto3DSeg provides developers with the tools for achieving top-tier 3D medical image segmentation. The computational efficiency of Auto3DSeg is optimized to ensure rapid training while extracting the most from GPU computational capabilities.\nAuto3DSeg uses MONAI components to achieve state-of-the-art segmentation performance. A MONAI Bundle offers a more customized solution. A MONAI Bundle defines a packaged network or model that includes the information necessary for users and programs to understand how the model is used and for what purpose. Bring your own MONAI Bundle to training and take full advantage of the robust, scalable training infrastructure.\nOnce you have your annotated data, use either custom model training or Auto3DSeg for an optimized model development experience.\nTraining is all about flexibility\u2014bring your unique MONAI Bundle, and watch as your training scales effortlessly on the NVIDIA MONAI Cloud API platform.\nIf automation is your goal, Auto3DSeg is your answer. Designed with developers in mind, Auto3DSeg stands out with its innovative features:\nData-driven model selection: Auto3DSeg delivers intelligent automation. By analyzing annotated imaging datasets, Auto3DSeg can automatically select and scale the most appropriate model architectures. This ensures optimized performance tailored specifically to your data.\nStreamlined development cycle: The development cycle is significantly streamlined with Auto3DSeg. It reduces complexity by incorporating automated parallel training and hyperparameter optimization. This expedites the entire process, from dataset analysis to creating deployment-ready models, saving time and resources.\nProven state-of-the-art segmentation performance: Auto3DSeg has secured multiple top positions at MICCAI in various segmentation challenges, notably the BraTS 2023, KiTS 2023, SEG.A. 2023 and MVSEG 2023 competitions.\nDiagram showing the flow of Auto3DSeg starting with labeled data on the left, the Auto3DSeg sequence in the middle, and resulting trained model(s) on the right.\nFigure 2. Auto3DSeg workflow\nSummary\nPlatform integrators striving for innovation in medical imaging can fully harness the capabilities of NVIDIA MONAI Cloud APIs, expediting the development of their AI models for medical imaging. This approach reduces operational overhead and grants immediate access to accelerated computing and AI capabilities in a streamlined manner.\nTo explore VISTA-3D, custom model training, and Auto3DSeg, sign up for the early access program. To try VISTA-3D, visit NVIDIA AI Foundation Models starting November 27."}], "https://developer.nvidia.com/blog/advanced-api-performance-intrinsics/": [{"text": "The article discusses the use of intrinsics in API performance, which are higher-level abstractions of hardware instructions that allow for direct access to low-level operations or hardware-specific features, resulting in increased performance. Wave intrinsics can speed up shaders by reducing shared memory usage and memory barriers, providing a noticeable performance boost. Different types of shuffles and ballots can be useful in this context. It is recommended to use wave instructions with GroupSize or WorkGroup values larger than the warp or subgroup size (32 threads) to minimize memory barriers and shared memory accesses. When vendor-specific extensions are not applicable or hard to implement, native HLSL code can be used. Overall, leveraging intrinsics and optimizing shader code can significantly improve API performance.", "text_components": ["Advanced API Performance: Intrinsics\nIntrinsics can be thought of as higher-level abstractions of specific hardware instructions. They offer direct access to low-level operations or hardware-specific features, enabling increased performance. In this way, operations can be performed across threads within a warp, also known as a wavefront.", "Recommended\nWave intrinsics can noticeably speed up your shaders.\nMany sorting or reduction algorithms can use much less or no shared memory with fewer memory barriers, providing a noticeable performance boost.\nDifferent types of shuffles and ballots can be useful.\nUse wave instructions with ```GroupSize``` or ```WorkGroup``` values larger than the warp or subgroup size (32 threads) wave instructions. There are fewer memory barriers and shared memory accesses that are needed.\nFor more information, see Reading Between The Threads: Shader Intrinsics and Unlocking GPU Intrinsics in HLSL.\nUse ```GroupSize``` and ```WorkGroup``` as a multiplier of warp size ( ```32 * N``` ), 64 is usually a sweet spot.\nWith intrinsic ```GroupSize``` and ```WorkGroup``` size equal, 32 could be a better choice to avoid shared memory usage.\nUse native HLSL code when vendor-specific extensions are not applicable or are hard to implement.\nSome instructions can be implemented with recent shader model versions.\nThe following code example is an example with SM6:\n```\nfloat(4) NvShflXor (float(4) input, uint LaneMask)\n{\nfloat(4) output = WaveReadLaneAt(input, WaveGetLaneIndex() ^ LaneMask);\nreturn output;\n}\n```"], "document_title": "Advanced API Performance: Intrinsics", "document_url": "https://developer.nvidia.com/blog/advanced-api-performance-intrinsics/", "document_date": "2023-11-21T18:37:48", "document_date_modified": "2023-12-30T00:44:05", "document_full_text": "Advanced API Performance: Intrinsics\nIntrinsics can be thought of as higher-level abstractions of specific hardware instructions. They offer direct access to low-level operations or hardware-specific features, enabling increased performance. In this way, operations can be performed across threads within a warp, also known as a wavefront.\nRecommended\nWave intrinsics can noticeably speed up your shaders.\nMany sorting or reduction algorithms can use much less or no shared memory with fewer memory barriers, providing a noticeable performance boost.\nDifferent types of shuffles and ballots can be useful.\nUse wave instructions with ```GroupSize``` or ```WorkGroup``` values larger than the warp or subgroup size (32 threads) wave instructions. There are fewer memory barriers and shared memory accesses that are needed.\nFor more information, see Reading Between The Threads: Shader Intrinsics and Unlocking GPU Intrinsics in HLSL.\nUse ```GroupSize``` and ```WorkGroup``` as a multiplier of warp size ( ```32 * N``` ), 64 is usually a sweet spot.\nWith intrinsic ```GroupSize``` and ```WorkGroup``` size equal, 32 could be a better choice to avoid shared memory usage.\nUse native HLSL code when vendor-specific extensions are not applicable or are hard to implement.\nSome instructions can be implemented with recent shader model versions.\nThe following code example is an example with SM6:\n```\nfloat(4) NvShflXor (float(4) input, uint LaneMask)\n{\nfloat(4) output = WaveReadLaneAt(input, WaveGetLaneIndex() ^ LaneMask);\nreturn output;\n}\n```"}], "https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/": [{"text": "The article discusses the use of GPU intrinsics in HLSL, specifically focusing on NVIDIA GPU instruction set. These intrinsics provide additional functionality not available in standard graphics APIs like DirectX and OpenGL. The article explains how to implement intrinsics in DirectX, using special sequences of regular HLSL instructions that need to pass through the compiler without optimizations. It also discusses the creation of extended shaders in DirectX 11 and extended pipeline state objects in DirectX 12. Additionally, it mentions the availability of new intrinsics and cross-vendor APIs in DirectX 12 and Vulkan, eliminating the need for NVAPI. The article provides examples of how to use intrinsics in shaders and emphasizes the importance of compiling shaders with the correct options. Overall, the article serves as a guide for developers looking to unlock the full potential of GPU intrinsics in HLSL for enhanced graphics programming.", "text_components": ["Unlocking GPU Intrinsics in HLSL\nThere are some useful intrinsic functions in the NVIDIA GPU instruction set that are not included in standard graphics APIs.\nUpdated from the original 2016 post to add information about new intrinsics and cross-vendor APIs in DirectX and Vulkan.\nFor example, a shader can use warp shuffle instructions to exchange data between threads in a warp without going through shared memory, which is especially valuable in pixel shaders where there is no shared memory. Or a shader can perform atomic additions on half-precision floating-point numbers in global memory.\nThe Reading Between The Threads: Shader Intrinsics post showed you how the intrinsic instructions worked. Now, I take you into the machinery to make them work in DirectX.\nNone of the intrinsics are possible in standard DirectX or OpenGL. [ 2023: This is no longer true. More information is shared later in this post. ] But they have been supported and well-documented in CUDA for years. A mechanism to support them in DirectX has been available for a while but not widely documented. I happen to have an old NVAPI version 343 on my system from October 2014 and the intrinsics are supported in DirectX by that version and probably earlier versions. This post explains the mechanism for using them in DirectX.\nUnlike OpenGL or Vulkan, DirectX unfortunately doesn\u2019t have a native mechanism for vendor-specific extensions. However, there is still a way to make all this functionality available in DirectX 11 or 12 through custom intrinsics. That mechanism is implemented in the graphics driver and accessible through the NVAPI library.", "Extending HLSL shaders\nTo use the intrinsics, they have to be encoded as special sequences of regular HLSL instructions that the driver can recognize and turn into the intended operations. These special sequences are provided in one of the header files that comes with the NVAPI SDK: ```nvHLSLExtns.h```.\nOne important thing about these instruction sequences is that they have to pass through the HLSL compiler without optimizations because the compiler does not understand their true meaning and therefore could modify them beyond recognition, change their order, or even completely remove them.\nTo prevent the compiler from doing that, the sequences use atomic operations on a UAV buffer. The HLSL compiler cannot optimize away these instructions because it is unaware of possible dependencies, even though there are none. That UAV buffer is basically a fake and it is not used by the actual shader after it\u2019s passed through the NVIDIA GPU driver. But the applications still have to allocate a UAV slot for it and tell the driver which slot that is.\nFor example, the ```NvShfl``` function that implements warp shuffle looks like the following code example, as defined in ```nvHLSLExtns.h```:\n```int NvShfl(int val, uint srcLane, int width = NV_WARP_SIZE)\n{\n     uint index = g_NvidiaExt.IncrementCounter();\n     g_NvidiaExt[index].src0u.x  =  val;          // variable to be shuffled\n     g_NvidiaExt[index].src0u.y  =  srcLane;      // source lane\n     g_NvidiaExt[index].src0u.z  =  __NvGetShflMaskFromWidth(width);\n     g_NvidiaExt[index].opcode   =  NV_EXTN_OP_SHFL;\n\t    \n// result is returned as the return value of IncrementCounter on fake UAV slot\n     return g_NvidiaExt.IncrementCounter();\n}``` A shader that uses this function would look something like the following code example:\n```// Declare that the driver should use UAV 0 to encode the instruction sequences.\n// It's a pixel shader with one output, so u0 is taken by the render target - use u1.\n#define NV_SHADER_EXTN_SLOT u1\n\n// On DirectX12 and Shader Model 5.1, you can also define the register space for that UAV.\n#define NV_SHADER_EXTN_REGISTER_SPACE space0\n\n// Include the header - note that the UAV slot has to be declared before including it.\n#include \"nvHLSLExtns.h\"\n\nTexture2D tex : register(t0);\nSamplerState samp : register(s0);\n\nfloat4 main(in float2 texCoord : UV) : SV_Target\n{\n     float4 color = tex.Sample(samp, texCoord);\n\n     // Use NvShfl to distribute the color from lane 0 to all other lanes in the warp.\n     // The NvShfl function accepts and returns uint data, so use asuint/asfloat to pass float values.\n\tcolor.r = asfloat(NvShfl(asuint(color.r), 0));\n\tcolor.g = asfloat(NvShfl(asuint(color.g), 0));\n\tcolor.b = asfloat(NvShfl(asuint(color.b), 0));\n\tcolor.a = asfloat(NvShfl(asuint(color.a), 0));\n\n\treturn color;\n}``` This example may look like it\u2019s doing something meaningless, and it is. Realistic use cases of the intrinsics in graphics applications are usually complicated. For example, warp shuffle can be used to optimize memory access in algorithms like light culling. Floating-point atomics are used in VXGI to accumulate emittance during voxelization. However, those applications require a significant amount of shader and host code to work. This example, on the other hand, can be plugged into virtually any pixel shader, and the effect is obvious.\nWhen you compile this shader, each call to ```NvShfl``` is expanded into this sequence, give or take the register names:\n```imm_atomic_alloc r1.x, u1\nmov r3.yz, l(0,0,31,0)\nmov r3.x, r2.z\nstore_structured u1.xyz, r1.x, l(76), r3.xyzx\nstore_structured u1.x, r1.x, l(0), l(1)\nimm_atomic_alloc r0.y, u1``` And when this shader passes through the driver\u2019s JIT compiler, each ```NvShfl``` function maps to just one GPU instruction:\n```SHFL.IDX        PT, R3, R3, RZ, 0x1f;```", "Creating extended shaders in DirectX 11\nTo actually use this shader, its runtime object has to be created in a special way. A regular call to ```ID3D11Device::CreatePixelShader``` does not suffice because the driver must know that the shader intends to use intrinsics. It also has to know which UAV slot is used.\nIf you\u2019re working with DirectX 11, use the ```NvAPI_D3D11_SetNvShaderExtnSlot``` function before and after calling ```CreatePixelShader```:\n```// Do this one time during app initialization.\nNvAPI_Initialize();\n\nID3D11PixelShader* pShader = nullptr;\nHRESULT D3DResult = E_FAIL;\n\n// First, enable compilation of intrinsics. \n// The second parameter is the UAV slot index that is used in the shader: u1.\nNvAPI_Status NvapiStatus = NvAPI_D3D11_SetNvShaderExtnSlot(pDevice, 1);\nif(NvapiStatus == NVAPI_OK)\n{\n     // Then create the shader as usual...\n     D3DResult = pDevice->CreatePixelShader(pBytecode, BytecodeLength, nullptr, &pShader);\n\n     // And disable again by telling the driver to use an invalid UAV slot.\n     NvAPI_D3D11_SetNvShaderExtnSlot(pDevice, ~0u);\n}\n\nif(FAILED(D3DResult))\n{\n     // ...Handle the error...\n}``` This method works with any shader that can reference a UAV. So, in DirectX 11.0 it works with pixel and compute shaders. In DirectX 11.1 and later, it should work with all kinds of shaders.", "Creating extended pipeline state objects in DirectX 12\nIf you\u2019re working with DirectX 12, there are no individual shader objects. Instead, complete pipeline states (PSOs) are created.\nThere are various other NVIDIA-specific pipeline state extensions that can be accessed through NVAPI, so to avoid a combinatorial explosion of functions that create PSOs with various sets of extensions, NVIDIA made just two functions, one for graphics and one for compute, that accept a list of extensions to use:\n```NvAPI_D3D12_CreateGraphicsPipelineState```\n```NvAPI_D3D12_CreateComputePipelineState```\nThe HLSL extension is described by the ```NVAPI_D3D12_PSO_SET_SHADER_EXTENSION_SLOT_DESC``` structure. There\u2019s only one for the whole pipeline state though, so if two or more shaders in the pipeline use intrinsics, they must use the same UAV slot for it.\n```// Do this one time during app initialization.\nNvAPI_Initialize();\n\n// Fill the PSO description structure\nD3D12_GRAPHICS_PIPELINE_STATE_DESC PsoDesc;\nPsoDesc.VS = { pVSBytecode, VSBytecodeLength };\n// ...And so on, as usual...\n\n// Also fill the extension structure. \n// Use the same UAV slot index and register space that are declared in the shader.\nNVAPI_D3D12_PSO_SET_SHADER_EXTENSION_SLOT_DESC ExtensionDesc;       \nExtensionDesc.baseVersion = NV_PSO_EXTENSION_DESC_VER;\nExtensionDesc.psoExtension = NV_PSO_SET_SHADER_EXTNENSION_SLOT_AND_SPACE;\nExtensionDesc.version = NV_SET_SHADER_EXTENSION_SLOT_DESC_VER;\nExtensionDesc.uavSlot = 1;\nExtensionDesc.registerSpace = 0;\n\n// Put the pointer to the extension into an array. There can be multiple extensions enabled at one time.\n// Other supported extensions are: \n       //     - Extended rasterizer state\n       //  - Pass-through geometry shader, implicit or explicit\n       //  - Depth bound test\n       const NVAPI_D3D12_PSO_EXTENSION_DESC* pExtensions[] = { &ExtensionDesc };\n\n// Now create the PSO.\nID3D12PipelineState* pPSO = nullptr;\nNvAPI_Status NvapiStatus = NvAPI_D3D12_CreateGraphicsPipelineState(pDevice, &PsoDesc, ARRAYSIZE(pExtensions), pExtensions, &pPSO);\n\nif(NvapiStatus != NVAPI_OK)\n     {\n        // ...Handle the error...\n     }\n}```", "Querying GPU feature support\nFinally, before trying to use the intrinsics, you\u2019ll probably want to know whether the device that the app\u2019s working with actually supports those intrinsics. There are two NVAPI functions that can tell you just that:\n```NvAPI_D3D11_IsNvShaderExtnOpCodeSupported```\n```NvAPI_D3D12_IsNvShaderExtnOpCodeSupported```\nThe ```opCode``` parameter identifies the specific operation that you\u2019re interested in. Operation codes are defined in the ```nvShaderExtnEnums.h``` file supplied with NVAPI SDK. For example, to test whether a DirectX 11 device supports warp shuffle, use the following code example:\n```#include \"nvShaderExtnEnums.h\"\n\nbool bSupported = false;\nNvAPI_Status NvapiStatus = NvAPI_D3D11_IsNvShaderExtnOpCodeSupported(pDevice, NV_EXTN_OP_SHFL, &bSupported);\n\nif(NvapiStatus == NVAPI_OK && bSupported)\n{\n     // Yay, the device is no older than 2012!\n}```", "Update 2023: New intrinsics and cross-vendor APIs\nThe intrinsics supported by NVIDIA GPUs are not limited to warp shuffle. In fact, warp shuffle and related functions are now available through cross-vendor intrinsics in both DirectX 12 and Vulkan, and there is no need to use NVAPI for them. For more information about DirectX 12 wave intrinsics, see Wave Intrinsics. For more information about Vulkan subgroup operations, see the Vulkan subgroup tutorial.\nThe complete list of intrinsics supported by NVIDIA GPUs can be found in the NVAPI header file called nvHLSLExtns.h, which is now available on GitHub. The functions declared in this file can be subdivided into a few general categories:\nOlder warp operations: shuffle, vote, ballot, lane index ( ```NvShfl*```, ```NvAny```, ```NvAll```, ```NvBallot```, ```NvGetLaneId``` )\nNewer warp operations: wave match ( ```NvWaveMatch``` ). ```NvWaveMatch``` returns a mask of active lanes in the warp that passed the same parameter value as the current lane.\nSpecial register access ( ```NvGetSpecial``` )\nExtended atomic operations on FP16, FP32, and Uint64 variables ( ```NvInterlocked*``` )\nVariable rate shading ( ```NvGetShadingRate```, ```NvEvaluateAttribute*``` )\nTexture footprint evaluation ( ```NvFootprint*``` )\nWaveMultiPrefix functions ( ```NvWaveMultiPrefix*``` ). These functions are just algorithms built on top of other intrinsics.\nRay tracing micromap extensions ( ```NvRtMicroTriangle*```, ```NvRtMicroVertex*``` )\nRay tracing shader execution reordering ( ```NvHitObject```, ```NvReorderThread``` )", "Update: Compiling shaders with the correct options\nCurrently, there is a known issue in the NVIDIA GPU drivers that affects HLSL intrinsics. Specifically, the intrinsics do NOT work properly if the shader is compiled with the ```D3DCOMPILE_SKIP_OPTIMIZATION``` flag, or the ```/Od``` command line option passed to FXC. If you see that the intrinsics have no effect, please make sure that this flag is not specified.", "Conclusion\nFor more information about NVAPI functions and structures, see the comments in NVAPI header files. For more use cases and examples of intrinsics, see the following resources:\nReading Between the Threads: Shader Intrinsics\nNVIDIA NVAPI SDK: Now at the /NVIDIA/nvapi GitHub repo!\nFaster Parallel Reductions on Kepler\nCUDA Pro Tip: Do The Kepler Shuffle\nCUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics\nShuffle: Tips and Tricks (GTC session)"], "document_title": "Unlocking GPU Intrinsics in HLSL", "document_url": "https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/", "document_date": "2023-11-21T18:08:42", "document_date_modified": "2023-11-30T19:43:27", "document_full_text": "Unlocking GPU Intrinsics in HLSL\nThere are some useful intrinsic functions in the NVIDIA GPU instruction set that are not included in standard graphics APIs.\nUpdated from the original 2016 post to add information about new intrinsics and cross-vendor APIs in DirectX and Vulkan.\nFor example, a shader can use warp shuffle instructions to exchange data between threads in a warp without going through shared memory, which is especially valuable in pixel shaders where there is no shared memory. Or a shader can perform atomic additions on half-precision floating-point numbers in global memory.\nThe Reading Between The Threads: Shader Intrinsics post showed you how the intrinsic instructions worked. Now, I take you into the machinery to make them work in DirectX.\nNone of the intrinsics are possible in standard DirectX or OpenGL. [ 2023: This is no longer true. More information is shared later in this post. ] But they have been supported and well-documented in CUDA for years. A mechanism to support them in DirectX has been available for a while but not widely documented. I happen to have an old NVAPI version 343 on my system from October 2014 and the intrinsics are supported in DirectX by that version and probably earlier versions. This post explains the mechanism for using them in DirectX.\nUnlike OpenGL or Vulkan, DirectX unfortunately doesn\u2019t have a native mechanism for vendor-specific extensions. However, there is still a way to make all this functionality available in DirectX 11 or 12 through custom intrinsics. That mechanism is implemented in the graphics driver and accessible through the NVAPI library.\nExtending HLSL shaders\nTo use the intrinsics, they have to be encoded as special sequences of regular HLSL instructions that the driver can recognize and turn into the intended operations. These special sequences are provided in one of the header files that comes with the NVAPI SDK: ```nvHLSLExtns.h```.\nOne important thing about these instruction sequences is that they have to pass through the HLSL compiler without optimizations because the compiler does not understand their true meaning and therefore could modify them beyond recognition, change their order, or even completely remove them.\nTo prevent the compiler from doing that, the sequences use atomic operations on a UAV buffer. The HLSL compiler cannot optimize away these instructions because it is unaware of possible dependencies, even though there are none. That UAV buffer is basically a fake and it is not used by the actual shader after it\u2019s passed through the NVIDIA GPU driver. But the applications still have to allocate a UAV slot for it and tell the driver which slot that is.\nFor example, the ```NvShfl``` function that implements warp shuffle looks like the following code example, as defined in ```nvHLSLExtns.h```:\n```int NvShfl(int val, uint srcLane, int width = NV_WARP_SIZE)\n{\n     uint index = g_NvidiaExt.IncrementCounter();\n     g_NvidiaExt[index].src0u.x  =  val;          // variable to be shuffled\n     g_NvidiaExt[index].src0u.y  =  srcLane;      // source lane\n     g_NvidiaExt[index].src0u.z  =  __NvGetShflMaskFromWidth(width);\n     g_NvidiaExt[index].opcode   =  NV_EXTN_OP_SHFL;\n\t    \n// result is returned as the return value of IncrementCounter on fake UAV slot\n     return g_NvidiaExt.IncrementCounter();\n}``` A shader that uses this function would look something like the following code example:\n```// Declare that the driver should use UAV 0 to encode the instruction sequences.\n// It's a pixel shader with one output, so u0 is taken by the render target - use u1.\n#define NV_SHADER_EXTN_SLOT u1\n\n// On DirectX12 and Shader Model 5.1, you can also define the register space for that UAV.\n#define NV_SHADER_EXTN_REGISTER_SPACE space0\n\n// Include the header - note that the UAV slot has to be declared before including it.\n#include \"nvHLSLExtns.h\"\n\nTexture2D tex : register(t0);\nSamplerState samp : register(s0);\n\nfloat4 main(in float2 texCoord : UV) : SV_Target\n{\n     float4 color = tex.Sample(samp, texCoord);\n\n     // Use NvShfl to distribute the color from lane 0 to all other lanes in the warp.\n     // The NvShfl function accepts and returns uint data, so use asuint/asfloat to pass float values.\n\tcolor.r = asfloat(NvShfl(asuint(color.r), 0));\n\tcolor.g = asfloat(NvShfl(asuint(color.g), 0));\n\tcolor.b = asfloat(NvShfl(asuint(color.b), 0));\n\tcolor.a = asfloat(NvShfl(asuint(color.a), 0));\n\n\treturn color;\n}``` This example may look like it\u2019s doing something meaningless, and it is. Realistic use cases of the intrinsics in graphics applications are usually complicated. For example, warp shuffle can be used to optimize memory access in algorithms like light culling. Floating-point atomics are used in VXGI to accumulate emittance during voxelization. However, those applications require a significant amount of shader and host code to work. This example, on the other hand, can be plugged into virtually any pixel shader, and the effect is obvious.\nWhen you compile this shader, each call to ```NvShfl``` is expanded into this sequence, give or take the register names:\n```imm_atomic_alloc r1.x, u1\nmov r3.yz, l(0,0,31,0)\nmov r3.x, r2.z\nstore_structured u1.xyz, r1.x, l(76), r3.xyzx\nstore_structured u1.x, r1.x, l(0), l(1)\nimm_atomic_alloc r0.y, u1``` And when this shader passes through the driver\u2019s JIT compiler, each ```NvShfl``` function maps to just one GPU instruction:\n```SHFL.IDX        PT, R3, R3, RZ, 0x1f;```\nCreating extended shaders in DirectX 11\nTo actually use this shader, its runtime object has to be created in a special way. A regular call to ```ID3D11Device::CreatePixelShader``` does not suffice because the driver must know that the shader intends to use intrinsics. It also has to know which UAV slot is used.\nIf you\u2019re working with DirectX 11, use the ```NvAPI_D3D11_SetNvShaderExtnSlot``` function before and after calling ```CreatePixelShader```:\n```// Do this one time during app initialization.\nNvAPI_Initialize();\n\nID3D11PixelShader* pShader = nullptr;\nHRESULT D3DResult = E_FAIL;\n\n// First, enable compilation of intrinsics. \n// The second parameter is the UAV slot index that is used in the shader: u1.\nNvAPI_Status NvapiStatus = NvAPI_D3D11_SetNvShaderExtnSlot(pDevice, 1);\nif(NvapiStatus == NVAPI_OK)\n{\n     // Then create the shader as usual...\n     D3DResult = pDevice->CreatePixelShader(pBytecode, BytecodeLength, nullptr, &pShader);\n\n     // And disable again by telling the driver to use an invalid UAV slot.\n     NvAPI_D3D11_SetNvShaderExtnSlot(pDevice, ~0u);\n}\n\nif(FAILED(D3DResult))\n{\n     // ...Handle the error...\n}``` This method works with any shader that can reference a UAV. So, in DirectX 11.0 it works with pixel and compute shaders. In DirectX 11.1 and later, it should work with all kinds of shaders.\nCreating extended pipeline state objects in DirectX 12\nIf you\u2019re working with DirectX 12, there are no individual shader objects. Instead, complete pipeline states (PSOs) are created.\nThere are various other NVIDIA-specific pipeline state extensions that can be accessed through NVAPI, so to avoid a combinatorial explosion of functions that create PSOs with various sets of extensions, NVIDIA made just two functions, one for graphics and one for compute, that accept a list of extensions to use:\n```NvAPI_D3D12_CreateGraphicsPipelineState```\n```NvAPI_D3D12_CreateComputePipelineState```\nThe HLSL extension is described by the ```NVAPI_D3D12_PSO_SET_SHADER_EXTENSION_SLOT_DESC``` structure. There\u2019s only one for the whole pipeline state though, so if two or more shaders in the pipeline use intrinsics, they must use the same UAV slot for it.\n```// Do this one time during app initialization.\nNvAPI_Initialize();\n\n// Fill the PSO description structure\nD3D12_GRAPHICS_PIPELINE_STATE_DESC PsoDesc;\nPsoDesc.VS = { pVSBytecode, VSBytecodeLength };\n// ...And so on, as usual...\n\n// Also fill the extension structure. \n// Use the same UAV slot index and register space that are declared in the shader.\nNVAPI_D3D12_PSO_SET_SHADER_EXTENSION_SLOT_DESC ExtensionDesc;       \nExtensionDesc.baseVersion = NV_PSO_EXTENSION_DESC_VER;\nExtensionDesc.psoExtension = NV_PSO_SET_SHADER_EXTNENSION_SLOT_AND_SPACE;\nExtensionDesc.version = NV_SET_SHADER_EXTENSION_SLOT_DESC_VER;\nExtensionDesc.uavSlot = 1;\nExtensionDesc.registerSpace = 0;\n\n// Put the pointer to the extension into an array. There can be multiple extensions enabled at one time.\n// Other supported extensions are: \n       //     - Extended rasterizer state\n       //  - Pass-through geometry shader, implicit or explicit\n       //  - Depth bound test\n       const NVAPI_D3D12_PSO_EXTENSION_DESC* pExtensions[] = { &ExtensionDesc };\n\n// Now create the PSO.\nID3D12PipelineState* pPSO = nullptr;\nNvAPI_Status NvapiStatus = NvAPI_D3D12_CreateGraphicsPipelineState(pDevice, &PsoDesc, ARRAYSIZE(pExtensions), pExtensions, &pPSO);\n\nif(NvapiStatus != NVAPI_OK)\n     {\n        // ...Handle the error...\n     }\n}```\nQuerying GPU feature support\nFinally, before trying to use the intrinsics, you\u2019ll probably want to know whether the device that the app\u2019s working with actually supports those intrinsics. There are two NVAPI functions that can tell you just that:\n```NvAPI_D3D11_IsNvShaderExtnOpCodeSupported```\n```NvAPI_D3D12_IsNvShaderExtnOpCodeSupported```\nThe ```opCode``` parameter identifies the specific operation that you\u2019re interested in. Operation codes are defined in the ```nvShaderExtnEnums.h``` file supplied with NVAPI SDK. For example, to test whether a DirectX 11 device supports warp shuffle, use the following code example:\n```#include \"nvShaderExtnEnums.h\"\n\nbool bSupported = false;\nNvAPI_Status NvapiStatus = NvAPI_D3D11_IsNvShaderExtnOpCodeSupported(pDevice, NV_EXTN_OP_SHFL, &bSupported);\n\nif(NvapiStatus == NVAPI_OK && bSupported)\n{\n     // Yay, the device is no older than 2012!\n}```\nUpdate 2023: New intrinsics and cross-vendor APIs\nThe intrinsics supported by NVIDIA GPUs are not limited to warp shuffle. In fact, warp shuffle and related functions are now available through cross-vendor intrinsics in both DirectX 12 and Vulkan, and there is no need to use NVAPI for them. For more information about DirectX 12 wave intrinsics, see Wave Intrinsics. For more information about Vulkan subgroup operations, see the Vulkan subgroup tutorial.\nThe complete list of intrinsics supported by NVIDIA GPUs can be found in the NVAPI header file called nvHLSLExtns.h, which is now available on GitHub. The functions declared in this file can be subdivided into a few general categories:\nOlder warp operations: shuffle, vote, ballot, lane index ( ```NvShfl*```, ```NvAny```, ```NvAll```, ```NvBallot```, ```NvGetLaneId``` )\nNewer warp operations: wave match ( ```NvWaveMatch``` ). ```NvWaveMatch``` returns a mask of active lanes in the warp that passed the same parameter value as the current lane.\nSpecial register access ( ```NvGetSpecial``` )\nExtended atomic operations on FP16, FP32, and Uint64 variables ( ```NvInterlocked*``` )\nVariable rate shading ( ```NvGetShadingRate```, ```NvEvaluateAttribute*``` )\nTexture footprint evaluation ( ```NvFootprint*``` )\nWaveMultiPrefix functions ( ```NvWaveMultiPrefix*``` ). These functions are just algorithms built on top of other intrinsics.\nRay tracing micromap extensions ( ```NvRtMicroTriangle*```, ```NvRtMicroVertex*``` )\nRay tracing shader execution reordering ( ```NvHitObject```, ```NvReorderThread``` )\nUpdate: Compiling shaders with the correct options\nCurrently, there is a known issue in the NVIDIA GPU drivers that affects HLSL intrinsics. Specifically, the intrinsics do NOT work properly if the shader is compiled with the ```D3DCOMPILE_SKIP_OPTIMIZATION``` flag, or the ```/Od``` command line option passed to FXC. If you see that the intrinsics have no effect, please make sure that this flag is not specified.\nConclusion\nFor more information about NVAPI functions and structures, see the comments in NVAPI header files. For more use cases and examples of intrinsics, see the following resources:\nReading Between the Threads: Shader Intrinsics\nNVIDIA NVAPI SDK: Now at the /NVIDIA/nvapi GitHub repo!\nFaster Parallel Reductions on Kepler\nCUDA Pro Tip: Do The Kepler Shuffle\nCUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics\nShuffle: Tips and Tricks (GTC session)"}], "https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/": [{"text": "The article discusses how AI, specifically NVIDIA TAO, can be used to design custom models for detecting defects in industrial applications, improving product quality and revenue. By leveraging pretrained models like VisualChangeNet and fine-tuning them with the TAO Toolkit, companies can achieve high accuracy in defect detection. The process involves setting up prerequisites, downloading the VisualChangeNet model, training the model using TAO, evaluating its performance, and deploying it for real-time inference. The VisualChangeNet model utilizes a Siamese Network architecture to compare current and reference images for detecting changes and anomalies. The article provides detailed steps and code snippets for training and evaluating the model, achieving an overall accuracy of 99.67% on the MVTech dataset. The fine-tuned model can be exported to .onnx format for deployment using NVIDIA DeepStream or Triton. Real-time inference performance metrics on various platforms are also provided. Overall, the article demonstrates how NVIDIA TAO can transform industrial defect detection, allowing companies to reclaim lost profits and enhance quality control.", "text_components": ["Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models\nEfficiency is paramount in industrial manufacturing, where even minor gains can have significant financial implications. According to the American Society of Quality, \u201cMany organizations will have true quality-related costs as high as 15-20% of sales revenue, some going as high as 40% of total operations.\u201d These staggering statistics reveal a stark reality: defects in industrial applications not only jeopardize product quality but also drain a significant portion of a company\u2019s revenue.\nBut what if companies could reclaim these lost profits and channel them back into innovation and expansion? This is where the potential of AI shines.\nThis post explores how NVIDIA TAO can be employed to design custom AI models that pinpoint defects in industrial applications, enhancing overall quality.\nNVIDIA TAO Toolkit is a low-code AI toolkit built on TensorFlow and PyTorch. It simplifies and accelerates the model training process by abstracting away the complexity of AI models and deep learning frameworks. With the TAO Toolkit, developers can use pretrained models and fine-tune them for specific use cases.\nIn this post, we leverage an advanced pretrained model for change detection called VisualChangeNet and fine-tune it with the TAO Toolkit to detect defects in the MV Tech Anomaly detection dataset. This comprehensive benchmarking dataset is designed for anomaly detection in machine vision, consisting of various industrial products with both normal and defective samples.\nUsing the TAO Toolkit, we use transfer learning to train a model that achieves an overall accuracy of 99.67%, 92.3% mIoU, 95.8% mF1, 97.5 mPrecision, and 94.3% mRecall on the bottle class of the MVTec Anomaly dataset. Figure 1 shows the defect mask prediction using the trained model.\nAn image showing a sample of a defective bottle, a reference golden sample of the bottle and the predicted and true defect masks.\nFigure 1. Segmentation predicts a defect mask of a defective object by comparing it with a golden image", "Step 1: Setup prerequisites\nTo follow along with the post and recreate these steps, take the following actions.\nRegister for an account on the NGC Catalog and generate your API key by following the steps provided in the NGC User\u2019s Guide.\nSet up the TAO Launcher by following the TAO Quickstart Guide. Download the VisualChangeNet Segmentation Jupyter Notebook for the MVTec dataset. Launch the Jupyter Notebook and run the cells to follow along with this post.\n*Note that the VisualChangeNet model works only from the 5.1 version.\nDownload and prepare the MVTec anomaly detection dataset by following the prompts to the download page and copying the download link for any of the 15 object classes.\nPaste the download link into the \u201cFIXME\u201d location in section 2.1 of the Jupyter Notebook and run the notebook cell. This post focuses on the bottle object however, all 15 objects work in the notebook. Figure 2 shows the sample defect images in the dataset.\n```\n#Download the data\nimport os\nMVTEC_AD_OBJECT_DOWNLOAD_URL = \"FIXME\"\nmvtec_object = MVTEC_AD_OBJECT_DOWNLOAD_URL.split(\"/\")[-1].split(\".\")[0]\nos.environ[\"URL_DATASET\"]=MVTEC_AD_OBJECT_DOWNLOAD_URL\nos.environ[\"MVTEC_OBJECT\"]=mvtec_object\n!if [ ! -f $HOST_DATA_DIR/$MVTEC_OBJECT.tar.xz ]; then wget $URL_DATASET -O $HOST_DATA_DIR/$MVTEC_OBJECT.tar.xz; else echo \"image archive already downloaded\"; fi\n```\nAn image showing three sample defective objects: a cable, bottle and transistor from the MVTech dataset\nFigure 2. Sample defect images from the MVTech dataset of a cable, bottle, and transistor (left to right)\nFrom MVTec-AD, we leverage the bottle class to showcase automated optical inspection for industrial inspection use cases with VisualChangeNet using the TAO Toolkit.\nAfter the Jupyter Notebook downloads the dataset, run section 2.3 of the notebook to process the dataset into the correct format for VisualChangeNet segmentation.\n```\nimport random \nimport shutil \nfrom PIL import Image\nos.environ[\"HOST_DATA_DIR\"] = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], \"data\", \"changenet\")\nformatted_dir = f\"formatted_{mvtec_object}_dataset\"\n\nDATA_DIR = os.environ[\"HOST_DATA_DIR\"]\nos.environ[\"FORMATTED_DATA_DIR\"] = formatted_dir\n\n#setup dataset folders in expected format \nformatted_path = os.path.join(DATA_DIR, formatted_dir)\na_dir = os.path.join(formatted_path, \"A\")\nb_dir = os.path.join(formatted_path, \"B\")\nlabel_dir = os.path.join(formatted_path, \"label\")\nlist_dir = os.path.join(formatted_path, \"list\")\n\n#Create the expected folders\nos.makedirs(formatted_path, exist_ok=True)\nos.makedirs(a_dir, exist_ok=True)\nos.makedirs(b_dir, exist_ok=True)\nos.makedirs(label_dir, exist_ok=True)\nos.makedirs(list_dir, exist_ok=True)\n```\nThe original dataset was designed for anomaly detection. We merge the two to create a combined dataset of 283 images and then divide them into 253 training set images and 30 testing set images. Both sets include defective samples.\nWe ensured that the test set included 30% of the defective samples from each defect class, as the \u2018bottle\u2019 class predominantly contained \u2018no-defect\u2019 images, with around 20 images for each of the three defect classes.\nAn image showing sample input from the dataset consisting of a test image, a golden image, and a segmentation mask for the defect.\nFigure 3. A sample input from the dataset with a test image, golden image, and segmentation mask showing the defect. The view is of a bottle from the top and the camera is mounted to look straight down", "Step 2: Download the VisualChangeNet model\nVisualChangeNet model is a state-of-the-art transformer-based change detection model. Central to its design is the Siamese Network. A Siamese Network is a unique neural network architecture composed of two or more identical subnetworks. These \u201ctwin\u201d subnetworks accept different inputs but share the same parameters and weights. In the context of VisualChangeNet, this architecture enables the model to compare features between a current image and a reference \u201cgolden\u201d image, pinpointing variations and changes. This capability makes Siamese Networks especially adept at tasks like image comparison and anomaly detection.\nThe model documentation provides more details like architecture and training data. Instead of training a model from scratch, we leverage the pretrained FAN backbone, which was trained on the NV-ImageNet dataset as a starting point. We fine-tune it with the TAO Toolkit on the MVTec-AD dataset for the bottle class.\nRun Section 3 of the notebook to install the NGC command-line tool and download the pretrained backbone from NGC.\n```\n# Installing NGC CLI on the local machine.\n## Download and install\nimport os\n%env CLI=ngccli_cat_linux.zip\n!mkdir -p $HOST_RESULTS_DIR/ngccli\n\n# # Remove any previously existing CLI installations\n!rm -rf $HOST_RESULTS_DIR/ngccli/*\n!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $HOST_RESULTS_DIR/ngccli\n!unzip -u \"$HOST_RESULTS_DIR/ngccli/$CLI\" -d $HOST_RESULTS_DIR/ngccli/\n!rm $HOST_RESULTS_DIR/ngccli/*.zip\nos.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"HOST_RESULTS_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n!mkdir -p $HOST_RESULTS_DIR/pretrained\n!ngc registry model list nvidia/tao/pretrained_fan_classification_nvimagenet*\n!ngc registry model download-version \"nvidia/tao/pretrained_fan_classification_nvimagenet:fan_base_hybrid_nvimagenet\" --dest $HOST_RESULTS_DIR/pretrained\n```", "Step 3: Train the model using the TAO Toolkit\nIn this section, we go into the details of training the VisualChangeNet model using the TAO Toolkit. You can find the details of the Visual ChangeNet models along with the supported pretrained weights in the model card. You can also use the pretrained FAN backbone weights as the starting point for fine-tuning VisualChangeNet, which is what we use to fine-tune on the MVTec-AD dataset.\nAs shown in Figure 4, the training algorithm updates the parameters across all the subnetworks in tandem. In TAO, Visual ChangeNet supports two images as input\u2014a golden sample and a test \u200csample. The goal is to detect a change between the \u201cgolden or reference\u201d image and the \u201ctest\u201d image. TAO supports the FAN backbone network for Visual ChangeNet architectures.\nTAO supports two types of Change Detection networks: Visual ChangeNet-Segmentation and Visual ChangeNet-Classification. In this post, we leverage the Visual ChangeNet-Segmentation model to demonstrate change detection by segmenting the changed pixels between the two input images from the MVTec-AD dataset.\nAn image showing the architecture of the segmentation algorithm that detects changes between a golden image and a test image of the bottle class.\nFigure 4. An Architecture diagram of the Visual ChangeNet-Segmentation algorithm that detects changes between a golden image and a test image of the bottle class\nFine-tuning the VisualChangeNet model is easy with the TAO Toolkit and requires zero coding experience. Simply load the data in the TAO Toolkit, set up the experiment configuration, and run the train command.\nThe experiment config file defines the hyperparameters for the VisualChangeNet model\u2019s architecture, training, and evaluation. In the Jupyter Notebook, you can view and edit the config file before training the model.\nWe use this config for fine-tuning the Visual ChangeNet model. In the config, let\u2019s define a Visual ChangeNet model with a pretrained FAN-Hybrid-Base backbone, which is the baseline model. Let\u2019s train the model for 30 epochs with batch size 8. The following section demonstrates a partial experiment config, showing some key parameters. The full experiment config is viewable in the Jupyter Notebook.\n```\nencryption_key: tlt_encode\ntask: segment\ntrain:\n  resume_training_checkpoint_path: null\n  pretrained_model_path: null\n  segment:\n    loss: \"ce\"\n    weights: [0.5, 0.5, 0.5, 0.8, 1.0]\n  num_epochs: 30\n  num_nodes: 1\n  val_interval: 1\n  checkpoint_interval: 1\n  optim:\n    lr: 0.0002\n    optim: \"adamw\"\n    policy: \"linear\" \n    momentum: 0.9\n    weight_decay: 0.01\nresults_dir: \"/results\"\nmodel:\n  backbone:\n    type: \"fan_base_16_p4_hybrid\"\n    pretrained_backbone_path: /results/pretrained/pretrained_fan_classification_nvimagenet_vfan_base_hybrid_nvimagenet/fan_base_hybrid_nvimagenet.pth\n```\nSome common values that can be modified to tune the performance of the model are the number of training epochs, the learning rate (lr), the optimizer, and the pretrained backbone. To train from scratch, the pretrained_backbone_path can be set to null, however, this will likely increase the number of epochs and amount of data needed to achieve high accuracy. For more information about the parameters in the experiment config file, see the VisualChangeNet User\u2019s Guide.\nNow that the dataset and experiment config is ready, let\u2019s start the training in the TAO Toolkit. Run the code block in section 5.1 to launch a Visual ChangeNet training with a single GPU.\n```\nprint(\"Train model\")\n!tao model visual_changenet train \\\n                  -e $SPECS_DIR/experiment.yaml \\\n                    train.num_epochs=$NUM_EPOCHS \\\n                    dataset.segment.root_dir=$DATA_DIR \\\n                    model.backbone.pretrained_backbone_path=$BACKBONE_PATH\n```\nThis cell will begin training the Visual ChangeNet Segmentation model on the MVTec dataset. During training, the model will learn how to identify defective objects and output a segmentation mask showing the defective region. The training log, which includes accuracy on the validation dataset, training loss, learning rate, and trained model, is saved in the results directory set in the experiment config.", "Step 4: Evaluate the model\nAfter training is complete, we can use TAO to evaluate the model on a validation dataset. For Visual ChangeNet Segmentation, the output is a segmentation change map for the 2 given input images denoting the pixel-level defects. Section 6 of the notebook will run the command to evaluate the model\u2019s performance.\n```\n!tao model visual_changenet evaluate \\\n                   -e $SPECS_DIR/experiment.yaml \\\n                    evaluate.checkpoint=$RESULTS_DIR/train/changenet.pth \\\n                    dataset.segment.root_dir=$DATA_DIR\n```\nThe evaluate command in TAO will return several KPIs on the validation set such as accuracy, precision, recall, F1 score, and IoU for the defect class (defect pixels).\nOA = overall accuracy of change/no change pixels (input dimension \u2013 256\u00d7256)\nMVTec-AD Binary CD (Bottle Class)\nModel\nBackbone\nmPrecision\nmRecall\nmF1\nmIOU\nOA\nVisualChangeNet\nFAN-Hybrid-B (pretrained)\n97.5\n94.3\n95.8\n92.3\n99.67\nTable 1. Evaluation metrics of the VisualChangeNet model for MVTec-AD Binary CD (Bottle Class)", "Step 5: Deploy the model\nYou can use this fine-tuned model and deploy it using NVIDIA DeepStream or NVIDIA Triton. Let\u2019s export it to the .onnx format. Section 8 of the notebook will run the TAO export command.\n```\n!tao model visual_changenet export \\\n                    -e $SPECS_DIR/experiment.yaml \\\n                        export.checkpoint=$RESULTS_DIR/train/changenet.pth \\\n                        export.onnx_file=$RESULTS_DIR/export/changenet.onnx\n```\nThe output .onnx model is saved in the same directory as the trained .pth model. To deploy to Triton, check out the tao-toolkit-triton repository on GitHub. This project provides reference implementations to deploy many TAO models, including Visual ChangeNet Segmentation, to a Triton inference server.", "Real-time inference performance\nThe inference is run on the provided unpruned model at FP16 precision. The inference performance is run using trtexec on embedded Jetson Orin GPUs and data center GPUs. The Jetson devices are running at Max-N configuration for maximum GPU frequency.\nRun the following command to run trtexec:\n/usr/src/tensorrt/bin/trtexec --onnx=<ONNX path> --minShapes=input0:1x3x512x512,input1:1x3x512x512 --maxShapes=input0:8x3x512x512,input1:8x3x512x512 --optShapes=input0:4x3x512x512,input1:4x3x512x512\n--saveEngine=<engine path>\nThe performance shown here is the inference-only performance. The end-to-end performance with streaming video data might vary depending on other bottlenecks in the hardware and software.\nPlatform\nBatch Size\nFPS\nNVIDIA Jetson Orin Nano 8 GB\n16\n15.19\nNVIDIA Jetson Orin NX 16 GB\n16\n21.92\nNVIDIA Jetson AGX Orin 64 GB\n16\n55.07\nNVIDIA A2 Tensor Core GPU\n16\n36.02\nNVIDIA T4 Tensor Core GPU\n16\n59.7\nNVIDIA L4 Tensor Core GPU\n8\n131.48\nNVIDIA A30 Tensor Core GPU\n16\n204.12\nNVIDIA L40 GPU\n8\n364\nNVIDIA A100 Tensor Core GPU\n32\n435.18\nNVIDIA H100 Tensor Core GPU\n32\n841.68\nTable 2. Performance metrics for inference of unpruned model at FP16 precision on different platforms", "Summary\nIn this post, we learned how to use the TAO Toolkit to fine-tune the VisualChangeNet model and use it for segmenting defects in the MVTech dataset, achieving an overall accuracy of 99.67%.\nYou can also now leverage NVIDIA TAO to detect defects in your manufacturing workflows.\nTo get started:\nDownload the VisualChangeNet model from the NVIDIA NGC catalog.\nFollow the TAO Quickstart Guide to set up the TAO launcher.\nDownload the Visual ChangeNet Segmentation Notebook from GitHub\nLearn more about the NVIDIA TAO Toolkit from NVIDIA Docs."], "document_title": "Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models", "document_url": "https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/", "document_date": "2023-11-20T17:00:00", "document_date_modified": "2023-12-07T16:59:55", "document_full_text": "Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models\nEfficiency is paramount in industrial manufacturing, where even minor gains can have significant financial implications. According to the American Society of Quality, \u201cMany organizations will have true quality-related costs as high as 15-20% of sales revenue, some going as high as 40% of total operations.\u201d These staggering statistics reveal a stark reality: defects in industrial applications not only jeopardize product quality but also drain a significant portion of a company\u2019s revenue.\nBut what if companies could reclaim these lost profits and channel them back into innovation and expansion? This is where the potential of AI shines.\nThis post explores how NVIDIA TAO can be employed to design custom AI models that pinpoint defects in industrial applications, enhancing overall quality.\nNVIDIA TAO Toolkit is a low-code AI toolkit built on TensorFlow and PyTorch. It simplifies and accelerates the model training process by abstracting away the complexity of AI models and deep learning frameworks. With the TAO Toolkit, developers can use pretrained models and fine-tune them for specific use cases.\nIn this post, we leverage an advanced pretrained model for change detection called VisualChangeNet and fine-tune it with the TAO Toolkit to detect defects in the MV Tech Anomaly detection dataset. This comprehensive benchmarking dataset is designed for anomaly detection in machine vision, consisting of various industrial products with both normal and defective samples.\nUsing the TAO Toolkit, we use transfer learning to train a model that achieves an overall accuracy of 99.67%, 92.3% mIoU, 95.8% mF1, 97.5 mPrecision, and 94.3% mRecall on the bottle class of the MVTec Anomaly dataset. Figure 1 shows the defect mask prediction using the trained model.\nAn image showing a sample of a defective bottle, a reference golden sample of the bottle and the predicted and true defect masks.\nFigure 1. Segmentation predicts a defect mask of a defective object by comparing it with a golden image\nStep 1: Setup prerequisites\nTo follow along with the post and recreate these steps, take the following actions.\nRegister for an account on the NGC Catalog and generate your API key by following the steps provided in the NGC User\u2019s Guide.\nSet up the TAO Launcher by following the TAO Quickstart Guide. Download the VisualChangeNet Segmentation Jupyter Notebook for the MVTec dataset. Launch the Jupyter Notebook and run the cells to follow along with this post.\n*Note that the VisualChangeNet model works only from the 5.1 version.\nDownload and prepare the MVTec anomaly detection dataset by following the prompts to the download page and copying the download link for any of the 15 object classes.\nPaste the download link into the \u201cFIXME\u201d location in section 2.1 of the Jupyter Notebook and run the notebook cell. This post focuses on the bottle object however, all 15 objects work in the notebook. Figure 2 shows the sample defect images in the dataset.\n```\n#Download the data\nimport os\nMVTEC_AD_OBJECT_DOWNLOAD_URL = \"FIXME\"\nmvtec_object = MVTEC_AD_OBJECT_DOWNLOAD_URL.split(\"/\")[-1].split(\".\")[0]\nos.environ[\"URL_DATASET\"]=MVTEC_AD_OBJECT_DOWNLOAD_URL\nos.environ[\"MVTEC_OBJECT\"]=mvtec_object\n!if [ ! -f $HOST_DATA_DIR/$MVTEC_OBJECT.tar.xz ]; then wget $URL_DATASET -O $HOST_DATA_DIR/$MVTEC_OBJECT.tar.xz; else echo \"image archive already downloaded\"; fi\n```\nAn image showing three sample defective objects: a cable, bottle and transistor from the MVTech dataset\nFigure 2. Sample defect images from the MVTech dataset of a cable, bottle, and transistor (left to right)\nFrom MVTec-AD, we leverage the bottle class to showcase automated optical inspection for industrial inspection use cases with VisualChangeNet using the TAO Toolkit.\nAfter the Jupyter Notebook downloads the dataset, run section 2.3 of the notebook to process the dataset into the correct format for VisualChangeNet segmentation.\n```\nimport random \nimport shutil \nfrom PIL import Image\nos.environ[\"HOST_DATA_DIR\"] = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], \"data\", \"changenet\")\nformatted_dir = f\"formatted_{mvtec_object}_dataset\"\n\nDATA_DIR = os.environ[\"HOST_DATA_DIR\"]\nos.environ[\"FORMATTED_DATA_DIR\"] = formatted_dir\n\n#setup dataset folders in expected format \nformatted_path = os.path.join(DATA_DIR, formatted_dir)\na_dir = os.path.join(formatted_path, \"A\")\nb_dir = os.path.join(formatted_path, \"B\")\nlabel_dir = os.path.join(formatted_path, \"label\")\nlist_dir = os.path.join(formatted_path, \"list\")\n\n#Create the expected folders\nos.makedirs(formatted_path, exist_ok=True)\nos.makedirs(a_dir, exist_ok=True)\nos.makedirs(b_dir, exist_ok=True)\nos.makedirs(label_dir, exist_ok=True)\nos.makedirs(list_dir, exist_ok=True)\n```\nThe original dataset was designed for anomaly detection. We merge the two to create a combined dataset of 283 images and then divide them into 253 training set images and 30 testing set images. Both sets include defective samples.\nWe ensured that the test set included 30% of the defective samples from each defect class, as the \u2018bottle\u2019 class predominantly contained \u2018no-defect\u2019 images, with around 20 images for each of the three defect classes.\nAn image showing sample input from the dataset consisting of a test image, a golden image, and a segmentation mask for the defect.\nFigure 3. A sample input from the dataset with a test image, golden image, and segmentation mask showing the defect. The view is of a bottle from the top and the camera is mounted to look straight down\nStep 2: Download the VisualChangeNet model\nVisualChangeNet model is a state-of-the-art transformer-based change detection model. Central to its design is the Siamese Network. A Siamese Network is a unique neural network architecture composed of two or more identical subnetworks. These \u201ctwin\u201d subnetworks accept different inputs but share the same parameters and weights. In the context of VisualChangeNet, this architecture enables the model to compare features between a current image and a reference \u201cgolden\u201d image, pinpointing variations and changes. This capability makes Siamese Networks especially adept at tasks like image comparison and anomaly detection.\nThe model documentation provides more details like architecture and training data. Instead of training a model from scratch, we leverage the pretrained FAN backbone, which was trained on the NV-ImageNet dataset as a starting point. We fine-tune it with the TAO Toolkit on the MVTec-AD dataset for the bottle class.\nRun Section 3 of the notebook to install the NGC command-line tool and download the pretrained backbone from NGC.\n```\n# Installing NGC CLI on the local machine.\n## Download and install\nimport os\n%env CLI=ngccli_cat_linux.zip\n!mkdir -p $HOST_RESULTS_DIR/ngccli\n\n# # Remove any previously existing CLI installations\n!rm -rf $HOST_RESULTS_DIR/ngccli/*\n!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $HOST_RESULTS_DIR/ngccli\n!unzip -u \"$HOST_RESULTS_DIR/ngccli/$CLI\" -d $HOST_RESULTS_DIR/ngccli/\n!rm $HOST_RESULTS_DIR/ngccli/*.zip\nos.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"HOST_RESULTS_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n!mkdir -p $HOST_RESULTS_DIR/pretrained\n!ngc registry model list nvidia/tao/pretrained_fan_classification_nvimagenet*\n!ngc registry model download-version \"nvidia/tao/pretrained_fan_classification_nvimagenet:fan_base_hybrid_nvimagenet\" --dest $HOST_RESULTS_DIR/pretrained\n```\nStep 3: Train the model using the TAO Toolkit\nIn this section, we go into the details of training the VisualChangeNet model using the TAO Toolkit. You can find the details of the Visual ChangeNet models along with the supported pretrained weights in the model card. You can also use the pretrained FAN backbone weights as the starting point for fine-tuning VisualChangeNet, which is what we use to fine-tune on the MVTec-AD dataset.\nAs shown in Figure 4, the training algorithm updates the parameters across all the subnetworks in tandem. In TAO, Visual ChangeNet supports two images as input\u2014a golden sample and a test \u200csample. The goal is to detect a change between the \u201cgolden or reference\u201d image and the \u201ctest\u201d image. TAO supports the FAN backbone network for Visual ChangeNet architectures.\nTAO supports two types of Change Detection networks: Visual ChangeNet-Segmentation and Visual ChangeNet-Classification. In this post, we leverage the Visual ChangeNet-Segmentation model to demonstrate change detection by segmenting the changed pixels between the two input images from the MVTec-AD dataset.\nAn image showing the architecture of the segmentation algorithm that detects changes between a golden image and a test image of the bottle class.\nFigure 4. An Architecture diagram of the Visual ChangeNet-Segmentation algorithm that detects changes between a golden image and a test image of the bottle class\nFine-tuning the VisualChangeNet model is easy with the TAO Toolkit and requires zero coding experience. Simply load the data in the TAO Toolkit, set up the experiment configuration, and run the train command.\nThe experiment config file defines the hyperparameters for the VisualChangeNet model\u2019s architecture, training, and evaluation. In the Jupyter Notebook, you can view and edit the config file before training the model.\nWe use this config for fine-tuning the Visual ChangeNet model. In the config, let\u2019s define a Visual ChangeNet model with a pretrained FAN-Hybrid-Base backbone, which is the baseline model. Let\u2019s train the model for 30 epochs with batch size 8. The following section demonstrates a partial experiment config, showing some key parameters. The full experiment config is viewable in the Jupyter Notebook.\n```\nencryption_key: tlt_encode\ntask: segment\ntrain:\n  resume_training_checkpoint_path: null\n  pretrained_model_path: null\n  segment:\n    loss: \"ce\"\n    weights: [0.5, 0.5, 0.5, 0.8, 1.0]\n  num_epochs: 30\n  num_nodes: 1\n  val_interval: 1\n  checkpoint_interval: 1\n  optim:\n    lr: 0.0002\n    optim: \"adamw\"\n    policy: \"linear\" \n    momentum: 0.9\n    weight_decay: 0.01\nresults_dir: \"/results\"\nmodel:\n  backbone:\n    type: \"fan_base_16_p4_hybrid\"\n    pretrained_backbone_path: /results/pretrained/pretrained_fan_classification_nvimagenet_vfan_base_hybrid_nvimagenet/fan_base_hybrid_nvimagenet.pth\n```\nSome common values that can be modified to tune the performance of the model are the number of training epochs, the learning rate (lr), the optimizer, and the pretrained backbone. To train from scratch, the pretrained_backbone_path can be set to null, however, this will likely increase the number of epochs and amount of data needed to achieve high accuracy. For more information about the parameters in the experiment config file, see the VisualChangeNet User\u2019s Guide.\nNow that the dataset and experiment config is ready, let\u2019s start the training in the TAO Toolkit. Run the code block in section 5.1 to launch a Visual ChangeNet training with a single GPU.\n```\nprint(\"Train model\")\n!tao model visual_changenet train \\\n                  -e $SPECS_DIR/experiment.yaml \\\n                    train.num_epochs=$NUM_EPOCHS \\\n                    dataset.segment.root_dir=$DATA_DIR \\\n                    model.backbone.pretrained_backbone_path=$BACKBONE_PATH\n```\nThis cell will begin training the Visual ChangeNet Segmentation model on the MVTec dataset. During training, the model will learn how to identify defective objects and output a segmentation mask showing the defective region. The training log, which includes accuracy on the validation dataset, training loss, learning rate, and trained model, is saved in the results directory set in the experiment config.\nStep 4: Evaluate the model\nAfter training is complete, we can use TAO to evaluate the model on a validation dataset. For Visual ChangeNet Segmentation, the output is a segmentation change map for the 2 given input images denoting the pixel-level defects. Section 6 of the notebook will run the command to evaluate the model\u2019s performance.\n```\n!tao model visual_changenet evaluate \\\n                   -e $SPECS_DIR/experiment.yaml \\\n                    evaluate.checkpoint=$RESULTS_DIR/train/changenet.pth \\\n                    dataset.segment.root_dir=$DATA_DIR\n```\nThe evaluate command in TAO will return several KPIs on the validation set such as accuracy, precision, recall, F1 score, and IoU for the defect class (defect pixels).\nOA = overall accuracy of change/no change pixels (input dimension \u2013 256\u00d7256)\nMVTec-AD Binary CD (Bottle Class)\nModel\nBackbone\nmPrecision\nmRecall\nmF1\nmIOU\nOA\nVisualChangeNet\nFAN-Hybrid-B (pretrained)\n97.5\n94.3\n95.8\n92.3\n99.67\nTable 1. Evaluation metrics of the VisualChangeNet model for MVTec-AD Binary CD (Bottle Class)\nStep 5: Deploy the model\nYou can use this fine-tuned model and deploy it using NVIDIA DeepStream or NVIDIA Triton. Let\u2019s export it to the .onnx format. Section 8 of the notebook will run the TAO export command.\n```\n!tao model visual_changenet export \\\n                    -e $SPECS_DIR/experiment.yaml \\\n                        export.checkpoint=$RESULTS_DIR/train/changenet.pth \\\n                        export.onnx_file=$RESULTS_DIR/export/changenet.onnx\n```\nThe output .onnx model is saved in the same directory as the trained .pth model. To deploy to Triton, check out the tao-toolkit-triton repository on GitHub. This project provides reference implementations to deploy many TAO models, including Visual ChangeNet Segmentation, to a Triton inference server.\nReal-time inference performance\nThe inference is run on the provided unpruned model at FP16 precision. The inference performance is run using trtexec on embedded Jetson Orin GPUs and data center GPUs. The Jetson devices are running at Max-N configuration for maximum GPU frequency.\nRun the following command to run trtexec:\n/usr/src/tensorrt/bin/trtexec --onnx=<ONNX path> --minShapes=input0:1x3x512x512,input1:1x3x512x512 --maxShapes=input0:8x3x512x512,input1:8x3x512x512 --optShapes=input0:4x3x512x512,input1:4x3x512x512\n--saveEngine=<engine path>\nThe performance shown here is the inference-only performance. The end-to-end performance with streaming video data might vary depending on other bottlenecks in the hardware and software.\nPlatform\nBatch Size\nFPS\nNVIDIA Jetson Orin Nano 8 GB\n16\n15.19\nNVIDIA Jetson Orin NX 16 GB\n16\n21.92\nNVIDIA Jetson AGX Orin 64 GB\n16\n55.07\nNVIDIA A2 Tensor Core GPU\n16\n36.02\nNVIDIA T4 Tensor Core GPU\n16\n59.7\nNVIDIA L4 Tensor Core GPU\n8\n131.48\nNVIDIA A30 Tensor Core GPU\n16\n204.12\nNVIDIA L40 GPU\n8\n364\nNVIDIA A100 Tensor Core GPU\n32\n435.18\nNVIDIA H100 Tensor Core GPU\n32\n841.68\nTable 2. Performance metrics for inference of unpruned model at FP16 precision on different platforms\nSummary\nIn this post, we learned how to use the TAO Toolkit to fine-tune the VisualChangeNet model and use it for segmenting defects in the MVTech dataset, achieving an overall accuracy of 99.67%.\nYou can also now leverage NVIDIA TAO to detect defects in your manufacturing workflows.\nTo get started:\nDownload the VisualChangeNet model from the NVIDIA NGC catalog.\nFollow the TAO Quickstart Guide to set up the TAO launcher.\nDownload the Visual ChangeNet Segmentation Notebook from GitHub\nLearn more about the NVIDIA TAO Toolkit from NVIDIA Docs."}], "https://developer.nvidia.com/blog/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/": [{"text": "NVIDIA Isaac Transport for ROS (NITROS) enhances ROS 2 nodes by enabling hardware-acceleration features like type adaptation and negotiation, reducing memory copies between the CPU and GPU. This efficient acceleration is showcased through a comparison between inefficient and NITROS-enabled hardware acceleration. By using Managed NITROS Publisher and Subscriber, CUDA nodes can communicate with NITROS-accelerated nodes and improve parallel compute performance. The Isaac ROS NITROS bridge facilitates accelerated computing for ROS 1 applications, eliminating CPU memory copies and reducing latency. Integrating ROS 2 nodes with NITROS offers improved performance, compatibility with legacy nodes, and a modular software design. The YOLOv8 object detection sample demonstrates how Managed NITROS utilities can be used with custom ROS decoders for accelerated object detection. Overall, integrating ROS 2 nodes with NITROS offers various benefits such as improved performance, compatibility, modular design, and accelerated computing for both ROS 1 and ROS 2 applications.", "text_components": ["Boosting Custom ROS Graphs Using NVIDIA Isaac Transport for ROS\nNVIDIA Isaac Transport for ROS ( NITROS ) is the implementation of two hardware-acceleration features introduced with ROS 2 Humble-type adaptation and type negotiation.\nType adaptation enables ROS nodes to work in a data format optimized for specific hardware accelerators. The adapted type is used by processing graphs to eliminate memory copies between the CPU and the memory accelerator.\nThrough type negotiation, different ROS nodes in a processing graph can advertise their supported types and the ROS framework can choose data formats, resulting in ideal performance.\nGIF showing a comparison between inefficient hardware acceleration and NITROS-enabled efficient hardware acceleration. In the first case, image data is copied multiple times between CPU and GPU resulting in slow data transfer. In the second case, image data is transferred once from CPU to GPU, and is then accessible directly from GPU memory to NITROS-compatible nodes.\nFigure 1. NITROS enables efficient acceleration by reducing memory copies between CPU and GPU When two NITROS-capable ROS nodes are next to each other in a graph, they can discover each other through type negotiation and then use type adaptation for sharing data. Together, type adaptation and type negotiation significantly improve the performance of AI and computer vision tasks in ROS-based applications, by removing unnecessary memory copies.\nThis reduces CPU overhead and optimizes performance on the underlying hardware. Figure 1 shows efficient hardware acceleration using NITROS. Data is accessible from GPU memory instead of frequent CPU copies.\nYou can use a combination of NITROS-based Isaac ROS nodes and other ROS nodes in your processing graphs, as the ROS framework maintains compatibility with legacy nodes that don\u2019t support negotiation. A NITROS-capable node functions like a typical ROS 2 node while communicating with a non-NITROS node. Most Isaac ROS GEMs are NITROS-accelerated.\nLearn more about NITROS and system assumptions from NVIDIA NITROS docs.", "NVIDIA CUDA with NITROS\nNVIDIA CUDA is a parallel computing programming model that can drastically speed up functions in robotic systems with GPUs. Your custom ROS 2 nodes can use CUDA with NITROS through the Managed NITROS Publisher and Managed NITROS Subscriber.\nBlock diagram showing CUDA with NITROS. Your ROS 2 node can use a Managed NITROS Subscriber \u200cor Publisher to communicate with other NITROS-capable nodes.\nFigure 2. Overview of CUDA with NITROS\nCUDA code in a ROS node can share its output buffers in GPU memory with NITROS-capable Isaac ROS nodes using the Managed NITROS Publisher. This removes expensive CPU memory copies, improving performance as a result. NITROS also maintains compatibility with non-NITROS nodes, by publishing the same data as a normal ROS 2 message.\nBlock diagram showing the use of Managed NITROS Publisher in your ROS 2 node. Your node can then communicate with both NITROS-capable and non-NITROS nodes (like RViz) through type adaptation.\nFigure 3. NITROS Publisher in a ROS 2 node\nOn the Subscriber side, CUDA code in a ROS node can receive input in GPU memory using the Managed NITROS Subscriber. Input can come from either a NITROS-capable Isaac ROS node or another CUDA-enabled ROS node using a NITROS Publisher. Just like the Managed NITROS Publisher, this gives better performance by increasing the parallel compute between the GPU and CPU.\nlock diagram showing the use of Managed NITROS Subscriber in your ROS 2 node. Your node can then subscribe to NITROS-typed messages from a NITROS-capable node through type adaptation.\nFigure 4. NITROS Subscriber in a ROS 2 node\nTo understand this better, let\u2019s consider an example graph performing DNN-based point cloud segmentation. At a high level, these are the three main components using CUDA with NITROS:\nEncoder node with Managed NITROS Publisher to convert a sensor_msgs/PointCloud2 message into a NitrosTensorList\nIsaac ROS TensorRT node to perform DNN inference, taking in an input NitrosTensorList and producing an output NitrosTensorList\nDecoder node with Managed NITROS Subscriber to convert the output NitrosTensorList into a segmented sensor_msgs/PointCloud2 message\nThe Managed NITROS Publisher and Subscriber offer a familiar interface, comparable to the standard rclcpp::Publisher and rclcpp::Subscriber APIs, making integration with existing ROS 2 nodes intuitive. CUDA with NITROS also enables a more modular software design. With Managed NITROS Publishers and Subscribers, CUDA nodes can be used anywhere in a graph with Isaac ROS nodes and other CUDA nodes to get the advantages of accelerated computing in each node.\nDigging just a little deeper, NITROS is based on the NVIDIA Graph Execution Framework (GXF), an extensible framework for building high-performance compute graphs. NITROS leverages GXF to achieve efficient ROS application graphs. CUDA with NITROS removes the need for developers to understand the underlying workings of GXF as a prerequisite to making their nodes NITROS-capable. The GXF layer is abstracted away\u2014making it easy and speedy for users to write ROS 2 nodes like they usually do, with straightforward tweaks to enable NITROS.\nLearn more about the core concepts of CUDA with NITROS.\nCurrently, the Managed NITROS Publisher and Subscriber are only compatible with the Isaac ROS NitrosTensorList message type. Visit isaac_ros_nitros_type for a complete list of NITROS data types.", "Object detection using CUDA with NITROS and YOLOv8\nIsaac ROS provides a YOLOv8 sample showing how to use Managed NITROS utilities with your custom ROS decoders to take advantage of NITROS. This sample uses packages from Isaac ROS DNN Inference to perform TensorRT accelerated object detection using YOLOv8. The Managed NITROS Publisher and Subscriber use NITROS-typed messages and currently are only compatible with the Isaac ROS NitrosTensorList message type. This message type is used to share tensors between your nodes and Isaac ROS DNN Inference nodes.\nAn image of a group of people on bicycles with bounding boxes drawn around detected objects in the image. These are the results of object detection using YOLOv8 and Isaac ROS DNN Inference.\nFigure 5. YOLOv8 object detection using Isaac ROS DNN Inference\nLet\u2019s say you want to use a custom object detection model with Isaac ROS DNN Inference and CUDA NITROS acceleration. There are three main steps involved in the detection pipeline: input image encoding, DNN inference, and output decoding. Isaac ROS DNN Inference has implementations for the first two steps.\nIn the decoding step, relevant information must be extracted from inferred results, which are tensors. For a task like 2D object detection, relevant information includes bounding boxes and class scores for each detected output in the image.\nLet\u2019s look into each step in some more detail.", "Step 1: Encoding\nOn the input side, Isaac ROS provides a NITROS-accelerated DNN image encoder. This preprocesses input images and converts them into tensors, which are communicated through the isaac_ros_tensor_list type to the TensorRT or Triton nodes for inference.\nYou can specify parameters like image size and the input size your network expects, for various preprocessing functions like resizing. Note that you\u2019ll need different encoders based on the task. For instance, you can\u2019t use this image encoder with language models because the networks expect different input encodings.\nDiagram showing an overview of the Isaac ROS DNN Image Encoder node. The node takes in a ROS 2 image message as input, encodes it into a list of tensors and outputs an Isaac ROS TensorList message. This message is passed onto the Isaac ROS inference node.\nFigure 6. Overview of the Isaac ROS DNN image encoder node", "Step 2: Inference\nIsaac ROS provides two ROS nodes for DNN inference\u2014the TensorRT node and Triton node. Of these, the YOLOv8 sample currently uses the TensorRT node. You provide your trained model to the TensorRT node, which performs inference and outputs a tensor containing detection results.\nThis output tensor list is passed onto the decoder node. You can specify parameters like dimensions and tensor names expected by the network\u2014information that can be found easily from the ONNX model using tools like Netron.\nBlock diagram showing an overview of the Isaac ROS TensorRT inference node. It takes a trained model and an input tensor list from the image encoder node as input, performs inference, and outputs a tensor list containing inference results to the decoder node.\nFigure 7. Overview of the Isaac ROS TensorRT inference node", "Step 3: Decoding\nThe inferred output tensor from the TensorRT or Triton node must be parsed into the desired bounding box and class information. Let\u2019s say you\u2019ve written your model\u2019s decoder as a ROS 2 node (not NITROS-capable yet). The decoder node doesn\u2019t support NITROS-typed messages and expects a typical ROS 2 message from the inference node. This still works because NITROS maintains compatibility with non-NITROS nodes.\nHowever, in this case, the output NITROS-typed message from the inference node (in GPU memory) is converted to a ROS 2 message and brought over to the CPU memory for the decoder to consume. This introduces some overhead as the data now lives in CPU memory, resulting in CPU memory copies while working with downstream ROS nodes.\nNow let\u2019s say you want to upgrade your decoder to communicate with the inference node (and other NITROS-accelerated nodes) through NITROS, instead of incurring the CPU memory copying cost. All the data stays in GPU memory in this case.\nThis is made easy by using Managed NITROS Subscriber in your decoder node. It subscribes to the NITROS-typed output message from the inference node and uses NITROS Views to obtain the CUDA buffer containing the detection output. You can then implement your decoding logic to this data and publish the results through an appropriate ROS message type.\nThe YOLOv8 decoder can be configured with parameters such as NMS threshold and confidence threshold to filter candidate detections. A simple visualization node can be used to subscribe to the resultant ROS message and draw bounding boxes on the input image. Note that Managed NITROS can only be integrated with CPP ROS 2 nodes.\nDiagram showing an overview of the YOLOv8 Decoder node. This node takes in an encoded tensor list from the inference node, extracts required information from the detection results and output results as a Detection2DArray ROS 2 message.\nFigure 8. Overview of the YOLOv8 Decoder node", "Isaac ROS NITROS bridge\nIf your robotics applications are currently based on ROS 1, you can still get the benefits of accelerated computing using the newly released Isaac ROS NITROS bridge. This is also helpful for developers using ROS 2 versions where type adaptation and negotiation aren\u2019t available (pre-Humble versions).\nTo highlight the speedups achievable, the NITROS bridge moves 1080p images between ROS 1 Noetic and NITROS packages up to 2.5x faster than the ROS 1 bridge.\nThe ROS bridge includes a CPU-based memory copy cost, which the Isaac ROS NITROS bridge eliminates by moving data from CPU to GPU. This data can be used in place in GPU memory.\nNITROS bridge consists of two converter nodes. One is used on the ROS (for example Noetic) side and the other on the ROS 2 (for example Humble) side. Using the ROS bridge without NITROS converters results in images being sent from Noetic to Humble and back through copies across ROS processes in CPU memory, increasing latency. This problem is especially apparent between nodes sending huge amounts of data like segmented point clouds.\nBlock diagram of ROS bridge without NITROS converters.\nFigure 9. ROS bridge without NITROS converters\nThe NITROS bridge is designed with the goal of reducing end-to-end latency across ROS versions. Consider the same example, this time using NITROS converters. The converter on the Noetic side (Figure 10) moves the image to GPU memory, avoiding CPU memory copies over the bridge. The converter on the Humble side (Figure 10) converts the image in GPU memory to a NITROS image type that is compatible to be used with other NITROS-accelerated nodes.\nThings work similarly in the reverse direction\u2014with the image data being sent as a NITROS image from Humble through the converter on either side to an image in CPU-accessible memory in Noetic.\nFor more information about performance gains, visit Isaac ROS Benchmark for NITROS bridge and ros1_bridge. Note that the Isaac ROS NITROS bridge doesn\u2019t support NVIDIA Jetson platforms yet.\nBlock diagram showing an overview of NITROSb. On the ROS 1 side, an image in CPU memory is copied over to the GPU through the NITROS Converter ROS node. This image can be used on the ROS 2 side through the NITROS Converter ROS 2 node without any CPU copies since it is available in GPU memory. In this way, the image is also accessible to other NITROS nodes.\nFigure 10. Overview of NITROS bridge", "Benefits of integrating ROS 2 nodes with NITROS\nThe following summarizes the many benefits of integrating your ROS 2 nodes with NITROS:\nImproved performance by reducing CPU memory copies.\nCompatibility with other non-NITROS ROS nodes such as RViz.\nEasy integration of custom ROS 2 nodes with hardware-accelerated Isaac ROS nodes through Managed NITROS Publisher and Subscriber.\nModular software design using CUDA with NITROS.\nImproved performance of applications based on earlier ROS versions using NITROS bridge.\nTry accelerating your own ROS nodes using Isaac ROS NITROS and our YOLOv8 object detection sample!\nVisit the NVIDIA Isaac ROS documentation page to learn more about our hardware-accelerated packages. Check out the Developer Forum for the latest information on Isaac ROS."], "document_title": "Boosting Custom ROS Graphs Using NVIDIA Isaac Transport for ROS", "document_url": "https://developer.nvidia.com/blog/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/", "document_date": "2023-11-17T21:11:29", "document_date_modified": "2023-11-30T19:43:29", "document_full_text": "Boosting Custom ROS Graphs Using NVIDIA Isaac Transport for ROS\nNVIDIA Isaac Transport for ROS ( NITROS ) is the implementation of two hardware-acceleration features introduced with ROS 2 Humble-type adaptation and type negotiation.\nType adaptation enables ROS nodes to work in a data format optimized for specific hardware accelerators. The adapted type is used by processing graphs to eliminate memory copies between the CPU and the memory accelerator.\nThrough type negotiation, different ROS nodes in a processing graph can advertise their supported types and the ROS framework can choose data formats, resulting in ideal performance.\nGIF showing a comparison between inefficient hardware acceleration and NITROS-enabled efficient hardware acceleration. In the first case, image data is copied multiple times between CPU and GPU resulting in slow data transfer. In the second case, image data is transferred once from CPU to GPU, and is then accessible directly from GPU memory to NITROS-compatible nodes.\nFigure 1. NITROS enables efficient acceleration by reducing memory copies between CPU and GPU When two NITROS-capable ROS nodes are next to each other in a graph, they can discover each other through type negotiation and then use type adaptation for sharing data. Together, type adaptation and type negotiation significantly improve the performance of AI and computer vision tasks in ROS-based applications, by removing unnecessary memory copies.\nThis reduces CPU overhead and optimizes performance on the underlying hardware. Figure 1 shows efficient hardware acceleration using NITROS. Data is accessible from GPU memory instead of frequent CPU copies.\nYou can use a combination of NITROS-based Isaac ROS nodes and other ROS nodes in your processing graphs, as the ROS framework maintains compatibility with legacy nodes that don\u2019t support negotiation. A NITROS-capable node functions like a typical ROS 2 node while communicating with a non-NITROS node. Most Isaac ROS GEMs are NITROS-accelerated.\nLearn more about NITROS and system assumptions from NVIDIA NITROS docs.\nNVIDIA CUDA with NITROS\nNVIDIA CUDA is a parallel computing programming model that can drastically speed up functions in robotic systems with GPUs. Your custom ROS 2 nodes can use CUDA with NITROS through the Managed NITROS Publisher and Managed NITROS Subscriber.\nBlock diagram showing CUDA with NITROS. Your ROS 2 node can use a Managed NITROS Subscriber \u200cor Publisher to communicate with other NITROS-capable nodes.\nFigure 2. Overview of CUDA with NITROS\nCUDA code in a ROS node can share its output buffers in GPU memory with NITROS-capable Isaac ROS nodes using the Managed NITROS Publisher. This removes expensive CPU memory copies, improving performance as a result. NITROS also maintains compatibility with non-NITROS nodes, by publishing the same data as a normal ROS 2 message.\nBlock diagram showing the use of Managed NITROS Publisher in your ROS 2 node. Your node can then communicate with both NITROS-capable and non-NITROS nodes (like RViz) through type adaptation.\nFigure 3. NITROS Publisher in a ROS 2 node\nOn the Subscriber side, CUDA code in a ROS node can receive input in GPU memory using the Managed NITROS Subscriber. Input can come from either a NITROS-capable Isaac ROS node or another CUDA-enabled ROS node using a NITROS Publisher. Just like the Managed NITROS Publisher, this gives better performance by increasing the parallel compute between the GPU and CPU.\nlock diagram showing the use of Managed NITROS Subscriber in your ROS 2 node. Your node can then subscribe to NITROS-typed messages from a NITROS-capable node through type adaptation.\nFigure 4. NITROS Subscriber in a ROS 2 node\nTo understand this better, let\u2019s consider an example graph performing DNN-based point cloud segmentation. At a high level, these are the three main components using CUDA with NITROS:\nEncoder node with Managed NITROS Publisher to convert a sensor_msgs/PointCloud2 message into a NitrosTensorList\nIsaac ROS TensorRT node to perform DNN inference, taking in an input NitrosTensorList and producing an output NitrosTensorList\nDecoder node with Managed NITROS Subscriber to convert the output NitrosTensorList into a segmented sensor_msgs/PointCloud2 message\nThe Managed NITROS Publisher and Subscriber offer a familiar interface, comparable to the standard rclcpp::Publisher and rclcpp::Subscriber APIs, making integration with existing ROS 2 nodes intuitive. CUDA with NITROS also enables a more modular software design. With Managed NITROS Publishers and Subscribers, CUDA nodes can be used anywhere in a graph with Isaac ROS nodes and other CUDA nodes to get the advantages of accelerated computing in each node.\nDigging just a little deeper, NITROS is based on the NVIDIA Graph Execution Framework (GXF), an extensible framework for building high-performance compute graphs. NITROS leverages GXF to achieve efficient ROS application graphs. CUDA with NITROS removes the need for developers to understand the underlying workings of GXF as a prerequisite to making their nodes NITROS-capable. The GXF layer is abstracted away\u2014making it easy and speedy for users to write ROS 2 nodes like they usually do, with straightforward tweaks to enable NITROS.\nLearn more about the core concepts of CUDA with NITROS.\nCurrently, the Managed NITROS Publisher and Subscriber are only compatible with the Isaac ROS NitrosTensorList message type. Visit isaac_ros_nitros_type for a complete list of NITROS data types.\nObject detection using CUDA with NITROS and YOLOv8\nIsaac ROS provides a YOLOv8 sample showing how to use Managed NITROS utilities with your custom ROS decoders to take advantage of NITROS. This sample uses packages from Isaac ROS DNN Inference to perform TensorRT accelerated object detection using YOLOv8. The Managed NITROS Publisher and Subscriber use NITROS-typed messages and currently are only compatible with the Isaac ROS NitrosTensorList message type. This message type is used to share tensors between your nodes and Isaac ROS DNN Inference nodes.\nAn image of a group of people on bicycles with bounding boxes drawn around detected objects in the image. These are the results of object detection using YOLOv8 and Isaac ROS DNN Inference.\nFigure 5. YOLOv8 object detection using Isaac ROS DNN Inference\nLet\u2019s say you want to use a custom object detection model with Isaac ROS DNN Inference and CUDA NITROS acceleration. There are three main steps involved in the detection pipeline: input image encoding, DNN inference, and output decoding. Isaac ROS DNN Inference has implementations for the first two steps.\nIn the decoding step, relevant information must be extracted from inferred results, which are tensors. For a task like 2D object detection, relevant information includes bounding boxes and class scores for each detected output in the image.\nLet\u2019s look into each step in some more detail.\nStep 1: Encoding\nOn the input side, Isaac ROS provides a NITROS-accelerated DNN image encoder. This preprocesses input images and converts them into tensors, which are communicated through the isaac_ros_tensor_list type to the TensorRT or Triton nodes for inference.\nYou can specify parameters like image size and the input size your network expects, for various preprocessing functions like resizing. Note that you\u2019ll need different encoders based on the task. For instance, you can\u2019t use this image encoder with language models because the networks expect different input encodings.\nDiagram showing an overview of the Isaac ROS DNN Image Encoder node. The node takes in a ROS 2 image message as input, encodes it into a list of tensors and outputs an Isaac ROS TensorList message. This message is passed onto the Isaac ROS inference node.\nFigure 6. Overview of the Isaac ROS DNN image encoder node\nStep 2: Inference\nIsaac ROS provides two ROS nodes for DNN inference\u2014the TensorRT node and Triton node. Of these, the YOLOv8 sample currently uses the TensorRT node. You provide your trained model to the TensorRT node, which performs inference and outputs a tensor containing detection results.\nThis output tensor list is passed onto the decoder node. You can specify parameters like dimensions and tensor names expected by the network\u2014information that can be found easily from the ONNX model using tools like Netron.\nBlock diagram showing an overview of the Isaac ROS TensorRT inference node. It takes a trained model and an input tensor list from the image encoder node as input, performs inference, and outputs a tensor list containing inference results to the decoder node.\nFigure 7. Overview of the Isaac ROS TensorRT inference node\nStep 3: Decoding\nThe inferred output tensor from the TensorRT or Triton node must be parsed into the desired bounding box and class information. Let\u2019s say you\u2019ve written your model\u2019s decoder as a ROS 2 node (not NITROS-capable yet). The decoder node doesn\u2019t support NITROS-typed messages and expects a typical ROS 2 message from the inference node. This still works because NITROS maintains compatibility with non-NITROS nodes.\nHowever, in this case, the output NITROS-typed message from the inference node (in GPU memory) is converted to a ROS 2 message and brought over to the CPU memory for the decoder to consume. This introduces some overhead as the data now lives in CPU memory, resulting in CPU memory copies while working with downstream ROS nodes.\nNow let\u2019s say you want to upgrade your decoder to communicate with the inference node (and other NITROS-accelerated nodes) through NITROS, instead of incurring the CPU memory copying cost. All the data stays in GPU memory in this case.\nThis is made easy by using Managed NITROS Subscriber in your decoder node. It subscribes to the NITROS-typed output message from the inference node and uses NITROS Views to obtain the CUDA buffer containing the detection output. You can then implement your decoding logic to this data and publish the results through an appropriate ROS message type.\nThe YOLOv8 decoder can be configured with parameters such as NMS threshold and confidence threshold to filter candidate detections. A simple visualization node can be used to subscribe to the resultant ROS message and draw bounding boxes on the input image. Note that Managed NITROS can only be integrated with CPP ROS 2 nodes.\nDiagram showing an overview of the YOLOv8 Decoder node. This node takes in an encoded tensor list from the inference node, extracts required information from the detection results and output results as a Detection2DArray ROS 2 message.\nFigure 8. Overview of the YOLOv8 Decoder node\nIsaac ROS NITROS bridge\nIf your robotics applications are currently based on ROS 1, you can still get the benefits of accelerated computing using the newly released Isaac ROS NITROS bridge. This is also helpful for developers using ROS 2 versions where type adaptation and negotiation aren\u2019t available (pre-Humble versions).\nTo highlight the speedups achievable, the NITROS bridge moves 1080p images between ROS 1 Noetic and NITROS packages up to 2.5x faster than the ROS 1 bridge.\nThe ROS bridge includes a CPU-based memory copy cost, which the Isaac ROS NITROS bridge eliminates by moving data from CPU to GPU. This data can be used in place in GPU memory.\nNITROS bridge consists of two converter nodes. One is used on the ROS (for example Noetic) side and the other on the ROS 2 (for example Humble) side. Using the ROS bridge without NITROS converters results in images being sent from Noetic to Humble and back through copies across ROS processes in CPU memory, increasing latency. This problem is especially apparent between nodes sending huge amounts of data like segmented point clouds.\nBlock diagram of ROS bridge without NITROS converters.\nFigure 9. ROS bridge without NITROS converters\nThe NITROS bridge is designed with the goal of reducing end-to-end latency across ROS versions. Consider the same example, this time using NITROS converters. The converter on the Noetic side (Figure 10) moves the image to GPU memory, avoiding CPU memory copies over the bridge. The converter on the Humble side (Figure 10) converts the image in GPU memory to a NITROS image type that is compatible to be used with other NITROS-accelerated nodes.\nThings work similarly in the reverse direction\u2014with the image data being sent as a NITROS image from Humble through the converter on either side to an image in CPU-accessible memory in Noetic.\nFor more information about performance gains, visit Isaac ROS Benchmark for NITROS bridge and ros1_bridge. Note that the Isaac ROS NITROS bridge doesn\u2019t support NVIDIA Jetson platforms yet.\nBlock diagram showing an overview of NITROSb. On the ROS 1 side, an image in CPU memory is copied over to the GPU through the NITROS Converter ROS node. This image can be used on the ROS 2 side through the NITROS Converter ROS 2 node without any CPU copies since it is available in GPU memory. In this way, the image is also accessible to other NITROS nodes.\nFigure 10. Overview of NITROS bridge\nBenefits of integrating ROS 2 nodes with NITROS\nThe following summarizes the many benefits of integrating your ROS 2 nodes with NITROS:\nImproved performance by reducing CPU memory copies.\nCompatibility with other non-NITROS ROS nodes such as RViz.\nEasy integration of custom ROS 2 nodes with hardware-accelerated Isaac ROS nodes through Managed NITROS Publisher and Subscriber.\nModular software design using CUDA with NITROS.\nImproved performance of applications based on earlier ROS versions using NITROS bridge.\nTry accelerating your own ROS nodes using Isaac ROS NITROS and our YOLOv8 object detection sample!\nVisit the NVIDIA Isaac ROS documentation page to learn more about our hardware-accelerated packages. Check out the Developer Forum for the latest information on Isaac ROS."}], "https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/": [{"text": "Large language models (LLMs) are powerful but costly to train and compute-intensive during inference. Stacking transformer layers in LLMs can lead to better accuracies and few-shot learning capabilities. Challenges in LLM inference include managing long inputs and optimizing the decode phase. Batching can improve GPU utilization, but traditional methods may be suboptimal. Model parallelization, such as pipeline and tensor parallelism, can distribute memory and compute across multiple GPUs. Optimizing the attention mechanism, such as multi-head and multi-query attention, can improve efficiency. Flash attention and efficient KV cache management with paging can further optimize memory usage. Model optimizations like quantization, sparsity, and distillation can reduce memory consumption. Model serving techniques like in-flight batching and speculative inference can enhance GPU utilization. NVIDIA TensorRT-LLM offers optimized solutions for LLM inference, allowing for high performance and efficiency across various deployment scenarios.", "text_components": ["Mastering LLM Techniques: Inference Optimization\nStacking transformer layers to create large models results in better accuracies, few-shot learning capabilities, and even near-human emergent abilities on a wide range of language tasks. These foundation models are expensive to train, and they can be memory- and compute-intensive during inference (a recurring cost). The most popular large language models (LLMs) today can reach tens to hundreds of billions of parameters in size and, depending on the use case, may require ingesting long inputs (or contexts), which can also add expense. For example, retrieval-augmented generation (RAG) pipelines require putting large amounts of information into the input of the model, greatly increasing the amount of processing work the LLM has to do.\nThis post discusses the most pressing challenges in LLM inference, along with some practical solutions. Readers should have a basic understanding of transformer architecture and the attention mechanism in general. It is essential to have a grasp of the intricacies of LLM inference, which we will address in the next section.", "Understanding LLM inference\nMost of the popular decoder-only LLMs (GPT-3, for example) are pretrained on the causal modeling objective, essentially as next-word predictors. These LLMs take a series of tokens as inputs, and generate subsequent tokens autoregressively until they meet a stopping criteria (a limit on the number of tokens to generate or a list of stop words, for example) or until it generates a special ```<end>``` token marking the end of generation. This process involves two phases: the prefill phase and the decode phase.\nNote that tokens are the atomic parts of language that a model processes. One token is approximately four English characters. All inputs in natural language are converted to tokens before inputting into the model.", "Prefill phase or processing the input\nIn the prefill phase, the LLM processes the input tokens to compute the intermediate states (keys and values), which are used to generate the \u201cfirst\u201d new token. Each new token depends on all the previous tokens, but because the full extent of the input is known, at a high level this is a matrix-matrix operation that\u2019s highly parallelized. It effectively saturates GPU utilization.", "Decode phase or generating the output\nIn the decode phase, the LLM generates output tokens autoregressively one at a time, until a stopping criteria is met. Each sequential output token needs to know all the previous iterations\u2019 output states (keys and values). This is like a matrix-vector operation that underutilizes the GPU compute ability compared to the prefill phase. The speed at which the data (weights, keys, values, activations) is transferred to the GPU from memory dominates the latency, not how fast the computation actually happens. In other words, this is a memory-bound operation.\nMany of the inference challenges and corresponding solutions featured in this post concern the optimization of this decode phase: efficient attention modules, managing the keys and values effectively, and others.\nDifferent LLMs may use different tokenizers, and thus, comparing output tokens between them may not be straightforward. When comparing inference throughput, even if two LLMs have similar tokens per second output, they may not be equivalent if they use different tokenizers. This is because corresponding tokens may represent a different number of characters.", "Batching\nThe simplest way to improve GPU utilization, and effectively throughput, is through batching. Since multiple requests use the same model, the memory cost of the weights is spread out. Larger batches getting transferred to the GPU to be processed all at once will leverage more of the compute available.\nBatch sizes, however, can only be increased up to a certain limit, at which point they may lead to a memory overflow. To better understand why this happens requires looking at key-value (KV) caching and LLM memory requirements.\nTraditional batching (also called static batching) is suboptimal. This is because for each request in a batch, the LLM may generate a different number of completion tokens, and subsequently they have different execution times. As a result, all requests in the batch must wait until the longest request is finished, which can be exacerbated by a large variance in the generation lengths. There are methods to mitigate this, such as in-flight batching, which will be discussed later.", "Key-value caching\nOne common optimization for the decode phase is KV caching. The decode phase generates a single token at each time step, but each token depends on the key and value tensors of all previous tokens (including the input tokens\u2019 KV tensors computed at prefill, and any new KV tensors computed until the current time step).\nTo avoid recomputing all these tensors for all tokens at each time step, it\u2019s possible to cache them in GPU memory. Every iteration, when new elements are computed, they are simply added to the running cache to be used in the next iteration. In some implementations, there is one KV cache for each layer of the model.\nAn illustration of KV caching depicted in Prefill and Decode phases. Prefill is a highly parallelized operation where the KV tensors of all input tokens can be computed simultaneously. During decode, new KV tensors and subsequently the output token at each step is computed autoregressively.\nFigure 1. An illustration of the key-value caching mechanism", "LLM memory requirement\nIn effect, the two main contributors to the GPU LLM memory requirement are model weights and the KV cache.\nModel weights: Memory is occupied by the model parameters. As an example, a model with 7 billion parameters (such as Llama 2 7B ), loaded in 16-bit precision (FP16 or BF16) would take roughly 7B * sizeof(FP16) ~= 14 GB in memory.\nKV caching: Memory is occupied by the caching of self-attention tensors to avoid redundant computation.\nWith batching, the KV cache of each of the requests in the batch must still be allocated separately, and can have a large memory footprint. The formula below delineates the size of the KV cache, applicable to most common LLM architectures today.\nSize of KV cache per token in bytes = 2 * (num_layers) * (num_heads * dim_head) * precision_in_bytes\nThe first factor of 2 accounts for the K and V matrices. Commonly, the value of (num_heads * dim_head) is the same as the hidden_size (or dimension of the model, d_model) of the transformer. These model attributes are commonly found in model cards or associated config files.\nThis memory size is required for each token in the input sequence, across the batch of inputs. Assuming half-precision, the total size of KV cache is given by the formula below.\nTotal size of KV cache in bytes = (batch_size) * (sequence_length) * 2 * (num_layers) * (hidden_size) * sizeof(FP16)\nFor example, with a Llama 2 7B model in 16-bit precision and a batch size of 1, the size of the KV cache will be 1 * 4096 * 2 * 32 * 4096 * 2 bytes, which is ~2 GB.\nManaging this KV cache efficiently is a challenging endeavor. Growing linearly with batch size and sequence length, the memory requirement can quickly scale. Consequently, it limits the throughput that can be served, and poses challenges for long-context inputs. This is the motivation behind several optimizations featured in this post.", "Scaling up LLMs with model parallelization\nOne way to reduce the per-device memory footprint of the model weights is to distribute the model over several GPUs. Spreading the memory and compute footprint enables running larger models, or larger batches of inputs. Model parallelization is a necessity to train or infer on a model requiring more memory than available on a single device, and to make training times and inference measures (latency or throughput) suitable for certain use cases. There are several ways of parallelizing the model based on how the model weights are split.\nNote that data parallelism is also a technique often mentioned in the same context as the others listed below. In this, weights of the model are copied over multiple devices, and the (global) batch size of inputs is sharded across each of the devices into microbatches. It reduces the overall execution time by processing larger batches. However, it is a training time optimization that is less relevant during inference.", "Pipeline parallelism\nPipeline parallelism involves sharding the model (vertically) into chunks, where each chunk comprises a subset of layers that is executed on a separate device. Figure 2a is an illustration of four-way pipeline parallelism, where the model is sequentially partitioned and a quarter subset of all layers are executed on each device. The outputs of a group of operations on one device are passed to the next, which continues executing the subsequent chunk. F_n\nand B_n\nindicate forward and backward passes respectively on device n. The memory requirement for storing model weights on each device is effectively quartered.\nThe main limitation of this method is that, due to the sequential nature of the processing, some devices or layers may remain idle while waiting for the output (activations, gradients) of previous layers. This results in inefficiencies or \u201cpipeline bubbles\u201d in both the forward and backward passes. In Figure 2b, the white empty areas are the large pipeline bubbles with naive pipeline parallelism where devices are idle and underutilized.\nMicrobatching can mitigate this to some extent, as shown in Figure 2c. The global batch size of inputs is split into sub-batches, which are processed one by one, with gradients being accumulated at the end. Note that F_{n,m}\nand B_{n,m}\nindicate forward and backward passes respectively on device n\nwith microbatch m\n. This approach shrinks the size of pipeline bubbles, but it does not completely eliminate them.\nDepiction of four-way pipeline parallelism. (a) Model is partitioned across layers in 4 parts, each subset executed on a separate device. (b) Naive pipeline parallelism results in large pipeline bubbles and GPU under-utilization. (c) Micro-batching reduces the size of pipeline bubbles, and improves GPU utilization.\nFigure 2. An illustration of four-way pipeline parallelism. Credit: GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism", "Tensor parallelism\nTensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of tensor parallelism. In multi-head attention blocks, each head or group of heads can be assigned to a different device so they can be computed independently and in parallel.\nIllustration of Tensor Parallelism in MLPs and Self-Attention Layers. In MLPs, the weight matrix is partitioned across multiple devices, enabling simultaneous computation on a batch of inputs using the split weights. In self-attention layers, the multiple attention heads are naturally parallel and can be distributed across devices.\nFigure 3. Illustration of tensor parallelism in multi-layer perceptron (MLP) and self-attention layers. Credit: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism Figure 3a shows an example of two-way tensor parallelism on a two-layer MLP, with each layer represented by a rounded box. Within the first layer, the weight matrix A\nis split into A_1\nand A_2\n. The computations XA_1\nand XA_2\ncan be independently executed on the same batch ( f\nis an identity operation) of inputs X\non two different devices. This effectively halves the memory requirement of storing weights on each device. A reduction operation g\ncombines the outputs in the second layer.\nFigure 3b is an example of two-way tensor parallelism in the self-attention layer. The multiple attention heads are parallel by nature and can be split across devices.", "Sequence parallelism\nTensor parallelism has limitations, as it requires layers to be divided into independent, manageable blocks. It\u2019s not applicable to operations like LayerNorm and Dropout, which are instead replicated across the tensor-parallel group. While LayerNorm and Dropout are computationally inexpensive, they do require a considerable amount of memory to store (redundant) activations.\nAs shown in Reducing Activation Recomputation in Large Transformer Models, these operations are independent across the input sequence, and these ops can be partitioned along that \u201csequence-dimension,\u201d making them more memory efficient. This is called sequence parallelism.\nIllustration of a transformer layer with both Tensor parallelism and Sequence parallelism. Sequence parallelism is applicable for operations like LayerNorm and Dropout, which are not well-suited for tensor parallelism.\nFigure 4. An illustration of a transformer layer with both tensor and sequence parallelism. Credit: Reducing Activation Recomputation in Large Transformer Models Techniques for model parallelism are not exclusive and can be used in conjunction. They can help scale and reduce the per-GPU memory footprint of LLMs, but there are also optimization techniques specifically for the attention module.", "Optimizing the attention mechanism\nThe scaled dot-product attention (SDPA) operation maps query and key-value pairs to an output, as described in Attention Is All You Need.", "Multi-head attention\nAs an enhancement to the SDPA, executing the attention layer multiple times in parallel with different, learned projections of the Q, K, and V matrices, enables the model to jointly attend to information from different representational subspaces at different positions. These subspaces are learned independently, providing the model with a richer understanding of different positions in the input.\nAs depicted in Figure 5, the outputs from the multiple parallel attention operations are concatenated and linearly projected to combine them. Each parallel attention layer is called a \u2018head,\u2019 and this approach is called multi-head attention (MHA).\nIn the original work, each attention head operates on a reduced dimension of the model (such as d_{model}/8\n) when using eight parallel attention heads. This keeps the computational cost similar to single-head attention.\nAn illustration of the scaled dot-product attention and multi-head attention.\nFigure 5. An illustration of the scaled dot-product attention (left) and multi-head attention (right), which is simply multiple SDPA heads in parallel. Credit: Attention Is All You Need", "Multi-query attention\nOne of the inference optimizations to MHA, called multi-query attention (MQA), as proposed in Fast Transformer Decoding, shares the keys and values among the multiple attention heads. The query vector is still projected multiple times, as before.\nWhile the amount of computation done in MQA is identical to MHA, the amount of data (keys, values) read from memory is a fraction of before. When bound by memory-bandwidth, this enables better compute utilization. It also reduces the size of the KV-cache in memory, allowing space for larger batch sizes.\nThe reduction in key-value heads comes with a potential accuracy drop. Additionally, models that need to leverage this optimization at inference need to train (or at least fine-tuned with ~5% of training volume) with MQA enabled.", "Grouped-query attention\nGrouped-query attention (GQA) strikes a balance between MHA and MQA by projecting key and values to a few groups of query heads (Figure 6). Within each of the groups, it behaves like multi-query attention.\nFigure 6 shows that multi-head attention has multiple key-value heads (left). Grouped-query attention (center) has more key-value heads than one, but fewer than the number of query heads, which is a balance between memory requirement and model quality. Multi-query attention (right) has a single key-value head to help save memory.\nDifferent attention mechanisms compared. Left: Multi-head attention has multiple key-value heads. Right: Multi-query attention has a single key-value head, which reduces memory requirements. Center: Grouped-query attention has a few key-value heads, balancing memory and model quality.\nFigure 6. A comparison of different attention mechanisms. Credit: GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints Models originally trained with MHA, can be \u201cuptrained\u201d with GQA using a fraction of the original training compute. They attain quality close to MHA while maintaining a computational efficiency closer to MQA. Llama 2 70B is an example of a model that leverages GQA.\nOptimizations like MQA and GQA help reduce the memory required by KV caches by reducing the number of key and value heads that are stored. There may still be inefficiencies in how this KV cache is managed. Of a different flavor than optimizing the attention module itself, the next section presents a technique for more efficient KV cache management.", "Flash attention\nAnother way of optimizing the attention mechanism is to modify the ordering of certain computations to take better advantage of the memory hierarchy of GPUs. Neural networks are generally described in terms of layers, and most implementations are laid out that way as well, with one kind of computation done on the input data at a time in sequence. This doesn\u2019t always lead to optimal performance, since it can be beneficial to do more calculations on values that have already been brought into the higher, more performant levels of the memory hierarchy.\nFusing multiple layers together during the actual computation can enable minimizing the number of times the GPU needs to read from and write to its memory and to group together calculations that require the same data, even if they are parts of different layers in the neural network.\nOne very popular fusion is FlashAttention, an I/O aware exact attention algorithm, as detailed in FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Exact attention means that it is mathematically identical to the standard multi-head attention (with variants available for multi-query and grouped-query attention), and so can be swapped into an existing model architecture or even an already-trained model with no modifications.\nI/O aware means it takes into account some of the memory movement costs previously discussed when fusing operations together. In particular, FlashAttention uses \u201ctiling\u201d to fully compute and write out a small part of the final matrix at once, rather than doing part of the computation on the whole matrix in steps, writing out the intermediate values in between.\nFigure 7 shows the tiled FlashAttention computation pattern and the memory hierarchy on a 40 GB GPU. The chart on the right shows the relative speedup that comes from fusing and reordering the different components of the Attention mechanism.\nDiagram depicting the memory hierarchy and the FlashAttention computation.\nFigure 7. The tiled FlashAttention computation pattern and the memory hierarchy on a 40 GB GPU. Credit: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "Efficient management of KV cache with paging\nAt times, KV caches are statically \u201cover-provisioned\u201d to account for the largest possible input (the supported sequence length) because the size of inputs is unpredictable. For example, if the supported maximum sequence length of a model is 2,048, then regardless of the size of input and the generated output in a request, a reservation of size 2,048 would be made in memory. This space may be contiguously allocated, and often, much of it remains unused, leading to memory waste or fragmentation. This reserved space is tied up for the lifetime of the request.\nAn illustration of memory wastage and fragmentation due to over-provisioning and inefficient management of KV cache. 1) \u201creserved\u201d indicates memory set aside for future use that is reserved for the entirety of the request duration. 2) \u201cinternal fragmentation\u201d happens because it\u2019s hard to predict how long the generation will be and thus memory is overprovisioned to account for the maximum sequence length. 3) \u201cexternal fragmentation\u201d indicates inefficiencies due to requests in a batch requiring different pre-allocated sizes.\nFigure 8. An illustration of memory wastage and fragmentation due to over-provisioning and inefficient KV cache management. Credit: Efficient Memory Management for Large Language Model Serving with PagedAttention Inspired by paging in operating systems, the PagedAttention algorithm enables storing continuous keys and values in noncontiguous space in memory. It partitions the KV cache of each request into blocks representing a fixed number of tokens, which can be stored non-contiguously.\nThese blocks are fetched as required during attention computation using a block table that keeps account. As new tokens are generated, new block allocations are made. The size of these blocks is fixed, eliminating inefficiencies arising from challenges like different requests requiring different allocations. This significantly limits memory wastage, enabling larger batch sizes (and, consequently, throughput).", "Model optimization techniques\nSo far, we\u2019ve discussed the different ways LLMs consume memory, some of the ways memory can be distributed across several different GPUs, and optimizing the attention mechanism and KV cache. There are also several model optimization techniques to reduce the memory use on each GPU by making modifications to the model weights themselves. GPUs also have dedicated hardware for accelerating operations on these modified values, providing even more speedups for models.", "Quantization\nQuantization is the process of reducing the precision of a model\u2019s weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory\u2014a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.\nFigure 9 shows the distribution of values before and after one possible method of quantization. In this case, some precision is lost to rounding, and some dynamic range is lost to clipping, allowing the values to be represented in a much smaller format.\nTwo distribution plots, one showing the full range of values at high precision and another showing the compressed and rounded range at low precision.\nFigure 9. The distribution of values before and after one possible method of quantization\nReducing the precision of a model can yield several benefits. If the model takes up less space in memory, you can fit larger models on the same amount of hardware. Quantization also means you can transfer more parameters over the same amount of bandwidth, which can help to accelerate models that are bandwidth-limited.\nThere are many different quantization techniques for LLMs involving reduced precision on either the activations, the weights, or both. It\u2019s much more straightforward to quantize the weights because they are fixed after training. However, this can leave some performance on the table because the activations remain at higher precisions. GPUs don\u2019t have dedicated hardware for multiplying INT8 and FP16 numbers, so the weights must be converted back into a higher precision for the actual operations.\nIt\u2019s also possible to quantize the activations, the inputs of transformer blocks and network layers, but this comes with its own challenges. Activation vectors often contain outliers, effectively increasing their dynamic range and making it more challenging to represent these values at a lower precision than with the weights.\nOne option is to find out where those outliers are likely to show up by passing a representative dataset through the model, and choosing to represent certain activations at a higher precision than others (LLM.int8()). Another option is to borrow the dynamic range of the weights, which are easy to quantize, and reuse that range in the activations.", "Sparsity\nSimilar to quantization, it\u2019s been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. Sparse matrices are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.\nA sparse matrix represented in a compressed format.\nFigure 10. A sparse matrix represented in a compressed format consisting of non-zero data values and their corresponding two-bit indices\nGPUs in particular have hardware acceleration for a certain kind of structured sparsity, where two out of every four values are represented by zeros. Sparse representations can also be combined with quantization to achieve even greater speedups in execution. Finding the best way to represent large language models in a sparse format is still an active area of research, and offers a promising direction for future improvements to inference speeds.", "Distillation\nAnother approach to shrinking the size of a model is to transfer its knowledge to a smaller model through a process called distillation. This process involves training a smaller model (called a student) to mimic the behavior of a larger model (a teacher).\nSuccessful examples of distilled models include DistilBERT, which compresses a BERT model by 40% while retaining 97% of its language understanding capabilities at a speed 60% faster.\nWhile distillation in LLMs is an active field of research, the general approach was first described for neural networks in Distilling the Knowledge in a Neural Network:\nThe student network is trained to mirror the performance of a larger teacher network, using a loss function that measures the discrepancy between their outputs. This objective is in addition to potentially including the original loss function of matching the student\u2019s outputs with the ground-truth labels.\nThe teacher\u2019s outputs that are matched can be the very last layer (called logits ) or intermediate layer activations.\nFigure 11 shows a general framework for knowledge distillation. The logits of the teacher are soft targets that the student optimizes for using a distillation loss. Other distillation methods may use other measures of loss to \u201cdistill\u201d knowledge from the teacher.\nFigure depicting a general framework for knowledge distillation using a distillation loss between the logits of the teacher and student.\nFigure 11. A general framework for knowledge distillation. Credit: Knowledge Distillation: A Survey An alternative approach to distillation is to use data synthesized by the teacher for supervised training of a student LLM, which is especially useful when human annotations are scarce or not available. Distilling Step by Step! goes one step further by extracting rationales from a teacher LLM in addition to the labels that serve as ground truth. These rationales serve as intermediate reasoning steps to train smaller student LLMs in a data-efficient way.\nIt\u2019s important to note that many state-of-the-art LLMs today have restrictive licenses that prohibit using their outputs to train other LLMs, making it challenging to find a suitable teacher model.", "Model serving techniques\nModel execution is frequently memory-bandwidth bound\u2014in particular, bandwidth-bound in the weights. Even after applying all the model optimizations previously described, it\u2019s still very likely to be memory bound. So you want to do as much as possible with your model weights when they are loaded. In other words, try doing things in parallel. Two approaches can be taken:\nIn-flight batching involves executing multiple different requests at the same time.\nSpeculative inference involves executing multiple different steps of the sequence in parallel to try to save time.", "In-flight batching\nLLMs have some unique execution characteristics that can make it difficult to effectively batch requests in practice. A single model can be used simultaneously for a variety of tasks that look very different from one another. From a simple question-and-answer response in a chatbot to the summarization of a document or the generation of a long chunk of code, workloads are highly dynamic, with outputs varying in size by several orders of magnitude.\nThis versatility can make it challenging to batch requests and execute them in parallel effectively\u2014a common optimization for serving neural networks. This could result in some requests finishing much earlier than others.\nTo manage these dynamic loads, many LLM serving solutions include an optimized scheduling technique called continuous or in-flight batching. This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model.\nWith in-flight batching, rather than waiting for the whole batch to finish before moving on to the next set of requests, the server runtime immediately evicts finished sequences from the batch. It then begins executing new requests while other requests are still in flight. In-flight batching can therefore greatly increase the overall GPU utilization in real-world use cases.", "Speculative inference\nAlso known as speculative sampling, assisted generation, or blockwise parallel decoding, speculative inference is a different way of parallelizing the execution of LLMs. Normally, GPT-style large language models are autoregressive models that generate text token by token.\nEvery token that is generated relies on all of the tokens that come before it to provide context. This means that in regular execution, it\u2019s impossible to generate multiple tokens from the same sequence in parallel\u2014you have to wait for the nth token to be generated before you can generate n+1.\nFigure 12 shows an example of speculative inference in which a draft model temporarily predicts multiple future steps that are verified or rejected in parallel. In this case, the first two predicted tokens in the draft are accepted, while the last is rejected and removed before continuing with the generation.\nFrom the prompt \u201cI saw a dog ride\u201d, the draft model predicts \u201cin the bus\u201d. The verification model predicts \u201cin the car\u201d in parallel, so we reject the \u201ccar\u201d token.\nFigure 12. An example of speculative inference. Credit: Blockwise Parallel Decoding for Deep Autoregressive Models Speculative sampling offers a workaround. The basic idea of this approach is to use some \u201ccheaper\u201d process to generate a draft continuation that is several tokens long. Then, execute the main \u201cverification\u201d model at multiple steps in parallel, using the cheap draft as \u201cspeculative\u201d context for the execution steps where it is needed.\nIf the verification model generates the same tokens as the draft, then you know to accept those tokens for the output. Otherwise, you can throw out everything after the first non-matching token, and repeat the process with a new draft.\nThere are many different options for how to generate draft tokens, and each comes with different tradeoffs. You can train multiple models, or fine-tune multiple heads on a single pretrained model, that predict tokens that are multiple steps in the future. Or, you can use a small model as the draft model, and a larger, more capable model as the verifier.", "Conclusion\nThis post outlines many of the most popular solutions to help optimize and serve LLMs efficiently, be it in the data center or at the edge on a PC. Many of these techniques are optimized and available through NVIDIA TensorRT-LLM, an open-source library consisting of the TensorRT deep learning compiler alongside optimized kernels, preprocessing and postprocessing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs. To learn more, see Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available.\nNVIDIA TensorRT-LLM is now supported by NVIDIA Triton Inference Server, enabling enterprises to serve multiple AI models concurrently across different AI frameworks, hardware accelerators, and deployment models with peak throughput and minimum latency.\nTensorRT-LLM also powers NVIDIA NeMo, which provides an end-to-end cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. Get started with NeMo."], "document_title": "Mastering LLM Techniques: Inference Optimization", "document_url": "https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/", "document_date": "2023-11-17T15:00:00", "document_date_modified": "2024-01-25T18:57:32", "document_full_text": "Mastering LLM Techniques: Inference Optimization\nStacking transformer layers to create large models results in better accuracies, few-shot learning capabilities, and even near-human emergent abilities on a wide range of language tasks. These foundation models are expensive to train, and they can be memory- and compute-intensive during inference (a recurring cost). The most popular large language models (LLMs) today can reach tens to hundreds of billions of parameters in size and, depending on the use case, may require ingesting long inputs (or contexts), which can also add expense. For example, retrieval-augmented generation (RAG) pipelines require putting large amounts of information into the input of the model, greatly increasing the amount of processing work the LLM has to do.\nThis post discusses the most pressing challenges in LLM inference, along with some practical solutions. Readers should have a basic understanding of transformer architecture and the attention mechanism in general. It is essential to have a grasp of the intricacies of LLM inference, which we will address in the next section.\nUnderstanding LLM inference\nMost of the popular decoder-only LLMs (GPT-3, for example) are pretrained on the causal modeling objective, essentially as next-word predictors. These LLMs take a series of tokens as inputs, and generate subsequent tokens autoregressively until they meet a stopping criteria (a limit on the number of tokens to generate or a list of stop words, for example) or until it generates a special ```<end>``` token marking the end of generation. This process involves two phases: the prefill phase and the decode phase.\nNote that tokens are the atomic parts of language that a model processes. One token is approximately four English characters. All inputs in natural language are converted to tokens before inputting into the model.\nPrefill phase or processing the input\nIn the prefill phase, the LLM processes the input tokens to compute the intermediate states (keys and values), which are used to generate the \u201cfirst\u201d new token. Each new token depends on all the previous tokens, but because the full extent of the input is known, at a high level this is a matrix-matrix operation that\u2019s highly parallelized. It effectively saturates GPU utilization.\nDecode phase or generating the output\nIn the decode phase, the LLM generates output tokens autoregressively one at a time, until a stopping criteria is met. Each sequential output token needs to know all the previous iterations\u2019 output states (keys and values). This is like a matrix-vector operation that underutilizes the GPU compute ability compared to the prefill phase. The speed at which the data (weights, keys, values, activations) is transferred to the GPU from memory dominates the latency, not how fast the computation actually happens. In other words, this is a memory-bound operation.\nMany of the inference challenges and corresponding solutions featured in this post concern the optimization of this decode phase: efficient attention modules, managing the keys and values effectively, and others.\nDifferent LLMs may use different tokenizers, and thus, comparing output tokens between them may not be straightforward. When comparing inference throughput, even if two LLMs have similar tokens per second output, they may not be equivalent if they use different tokenizers. This is because corresponding tokens may represent a different number of characters.\nBatching\nThe simplest way to improve GPU utilization, and effectively throughput, is through batching. Since multiple requests use the same model, the memory cost of the weights is spread out. Larger batches getting transferred to the GPU to be processed all at once will leverage more of the compute available.\nBatch sizes, however, can only be increased up to a certain limit, at which point they may lead to a memory overflow. To better understand why this happens requires looking at key-value (KV) caching and LLM memory requirements.\nTraditional batching (also called static batching) is suboptimal. This is because for each request in a batch, the LLM may generate a different number of completion tokens, and subsequently they have different execution times. As a result, all requests in the batch must wait until the longest request is finished, which can be exacerbated by a large variance in the generation lengths. There are methods to mitigate this, such as in-flight batching, which will be discussed later.\nKey-value caching\nOne common optimization for the decode phase is KV caching. The decode phase generates a single token at each time step, but each token depends on the key and value tensors of all previous tokens (including the input tokens\u2019 KV tensors computed at prefill, and any new KV tensors computed until the current time step).\nTo avoid recomputing all these tensors for all tokens at each time step, it\u2019s possible to cache them in GPU memory. Every iteration, when new elements are computed, they are simply added to the running cache to be used in the next iteration. In some implementations, there is one KV cache for each layer of the model.\nAn illustration of KV caching depicted in Prefill and Decode phases. Prefill is a highly parallelized operation where the KV tensors of all input tokens can be computed simultaneously. During decode, new KV tensors and subsequently the output token at each step is computed autoregressively.\nFigure 1. An illustration of the key-value caching mechanism\nLLM memory requirement\nIn effect, the two main contributors to the GPU LLM memory requirement are model weights and the KV cache.\nModel weights: Memory is occupied by the model parameters. As an example, a model with 7 billion parameters (such as Llama 2 7B ), loaded in 16-bit precision (FP16 or BF16) would take roughly 7B * sizeof(FP16) ~= 14 GB in memory.\nKV caching: Memory is occupied by the caching of self-attention tensors to avoid redundant computation.\nWith batching, the KV cache of each of the requests in the batch must still be allocated separately, and can have a large memory footprint. The formula below delineates the size of the KV cache, applicable to most common LLM architectures today.\nSize of KV cache per token in bytes = 2 * (num_layers) * (num_heads * dim_head) * precision_in_bytes\nThe first factor of 2 accounts for the K and V matrices. Commonly, the value of (num_heads * dim_head) is the same as the hidden_size (or dimension of the model, d_model) of the transformer. These model attributes are commonly found in model cards or associated config files.\nThis memory size is required for each token in the input sequence, across the batch of inputs. Assuming half-precision, the total size of KV cache is given by the formula below.\nTotal size of KV cache in bytes = (batch_size) * (sequence_length) * 2 * (num_layers) * (hidden_size) * sizeof(FP16)\nFor example, with a Llama 2 7B model in 16-bit precision and a batch size of 1, the size of the KV cache will be 1 * 4096 * 2 * 32 * 4096 * 2 bytes, which is ~2 GB.\nManaging this KV cache efficiently is a challenging endeavor. Growing linearly with batch size and sequence length, the memory requirement can quickly scale. Consequently, it limits the throughput that can be served, and poses challenges for long-context inputs. This is the motivation behind several optimizations featured in this post.\nScaling up LLMs with model parallelization\nOne way to reduce the per-device memory footprint of the model weights is to distribute the model over several GPUs. Spreading the memory and compute footprint enables running larger models, or larger batches of inputs. Model parallelization is a necessity to train or infer on a model requiring more memory than available on a single device, and to make training times and inference measures (latency or throughput) suitable for certain use cases. There are several ways of parallelizing the model based on how the model weights are split.\nNote that data parallelism is also a technique often mentioned in the same context as the others listed below. In this, weights of the model are copied over multiple devices, and the (global) batch size of inputs is sharded across each of the devices into microbatches. It reduces the overall execution time by processing larger batches. However, it is a training time optimization that is less relevant during inference.\nPipeline parallelism\nPipeline parallelism involves sharding the model (vertically) into chunks, where each chunk comprises a subset of layers that is executed on a separate device. Figure 2a is an illustration of four-way pipeline parallelism, where the model is sequentially partitioned and a quarter subset of all layers are executed on each device. The outputs of a group of operations on one device are passed to the next, which continues executing the subsequent chunk. F_n\nand B_n\nindicate forward and backward passes respectively on device n. The memory requirement for storing model weights on each device is effectively quartered.\nThe main limitation of this method is that, due to the sequential nature of the processing, some devices or layers may remain idle while waiting for the output (activations, gradients) of previous layers. This results in inefficiencies or \u201cpipeline bubbles\u201d in both the forward and backward passes. In Figure 2b, the white empty areas are the large pipeline bubbles with naive pipeline parallelism where devices are idle and underutilized.\nMicrobatching can mitigate this to some extent, as shown in Figure 2c. The global batch size of inputs is split into sub-batches, which are processed one by one, with gradients being accumulated at the end. Note that F_{n,m}\nand B_{n,m}\nindicate forward and backward passes respectively on device n\nwith microbatch m\n. This approach shrinks the size of pipeline bubbles, but it does not completely eliminate them.\nDepiction of four-way pipeline parallelism. (a) Model is partitioned across layers in 4 parts, each subset executed on a separate device. (b) Naive pipeline parallelism results in large pipeline bubbles and GPU under-utilization. (c) Micro-batching reduces the size of pipeline bubbles, and improves GPU utilization.\nFigure 2. An illustration of four-way pipeline parallelism. Credit: GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism\nTensor parallelism\nTensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of tensor parallelism. In multi-head attention blocks, each head or group of heads can be assigned to a different device so they can be computed independently and in parallel.\nIllustration of Tensor Parallelism in MLPs and Self-Attention Layers. In MLPs, the weight matrix is partitioned across multiple devices, enabling simultaneous computation on a batch of inputs using the split weights. In self-attention layers, the multiple attention heads are naturally parallel and can be distributed across devices.\nFigure 3. Illustration of tensor parallelism in multi-layer perceptron (MLP) and self-attention layers. Credit: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism Figure 3a shows an example of two-way tensor parallelism on a two-layer MLP, with each layer represented by a rounded box. Within the first layer, the weight matrix A\nis split into A_1\nand A_2\n. The computations XA_1\nand XA_2\ncan be independently executed on the same batch ( f\nis an identity operation) of inputs X\non two different devices. This effectively halves the memory requirement of storing weights on each device. A reduction operation g\ncombines the outputs in the second layer.\nFigure 3b is an example of two-way tensor parallelism in the self-attention layer. The multiple attention heads are parallel by nature and can be split across devices.\nSequence parallelism\nTensor parallelism has limitations, as it requires layers to be divided into independent, manageable blocks. It\u2019s not applicable to operations like LayerNorm and Dropout, which are instead replicated across the tensor-parallel group. While LayerNorm and Dropout are computationally inexpensive, they do require a considerable amount of memory to store (redundant) activations.\nAs shown in Reducing Activation Recomputation in Large Transformer Models, these operations are independent across the input sequence, and these ops can be partitioned along that \u201csequence-dimension,\u201d making them more memory efficient. This is called sequence parallelism.\nIllustration of a transformer layer with both Tensor parallelism and Sequence parallelism. Sequence parallelism is applicable for operations like LayerNorm and Dropout, which are not well-suited for tensor parallelism.\nFigure 4. An illustration of a transformer layer with both tensor and sequence parallelism. Credit: Reducing Activation Recomputation in Large Transformer Models Techniques for model parallelism are not exclusive and can be used in conjunction. They can help scale and reduce the per-GPU memory footprint of LLMs, but there are also optimization techniques specifically for the attention module.\nOptimizing the attention mechanism\nThe scaled dot-product attention (SDPA) operation maps query and key-value pairs to an output, as described in Attention Is All You Need.\nMulti-head attention\nAs an enhancement to the SDPA, executing the attention layer multiple times in parallel with different, learned projections of the Q, K, and V matrices, enables the model to jointly attend to information from different representational subspaces at different positions. These subspaces are learned independently, providing the model with a richer understanding of different positions in the input.\nAs depicted in Figure 5, the outputs from the multiple parallel attention operations are concatenated and linearly projected to combine them. Each parallel attention layer is called a \u2018head,\u2019 and this approach is called multi-head attention (MHA).\nIn the original work, each attention head operates on a reduced dimension of the model (such as d_{model}/8\n) when using eight parallel attention heads. This keeps the computational cost similar to single-head attention.\nAn illustration of the scaled dot-product attention and multi-head attention.\nFigure 5. An illustration of the scaled dot-product attention (left) and multi-head attention (right), which is simply multiple SDPA heads in parallel. Credit: Attention Is All You Need\nMulti-query attention\nOne of the inference optimizations to MHA, called multi-query attention (MQA), as proposed in Fast Transformer Decoding, shares the keys and values among the multiple attention heads. The query vector is still projected multiple times, as before.\nWhile the amount of computation done in MQA is identical to MHA, the amount of data (keys, values) read from memory is a fraction of before. When bound by memory-bandwidth, this enables better compute utilization. It also reduces the size of the KV-cache in memory, allowing space for larger batch sizes.\nThe reduction in key-value heads comes with a potential accuracy drop. Additionally, models that need to leverage this optimization at inference need to train (or at least fine-tuned with ~5% of training volume) with MQA enabled.\nGrouped-query attention\nGrouped-query attention (GQA) strikes a balance between MHA and MQA by projecting key and values to a few groups of query heads (Figure 6). Within each of the groups, it behaves like multi-query attention.\nFigure 6 shows that multi-head attention has multiple key-value heads (left). Grouped-query attention (center) has more key-value heads than one, but fewer than the number of query heads, which is a balance between memory requirement and model quality. Multi-query attention (right) has a single key-value head to help save memory.\nDifferent attention mechanisms compared. Left: Multi-head attention has multiple key-value heads. Right: Multi-query attention has a single key-value head, which reduces memory requirements. Center: Grouped-query attention has a few key-value heads, balancing memory and model quality.\nFigure 6. A comparison of different attention mechanisms. Credit: GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints Models originally trained with MHA, can be \u201cuptrained\u201d with GQA using a fraction of the original training compute. They attain quality close to MHA while maintaining a computational efficiency closer to MQA. Llama 2 70B is an example of a model that leverages GQA.\nOptimizations like MQA and GQA help reduce the memory required by KV caches by reducing the number of key and value heads that are stored. There may still be inefficiencies in how this KV cache is managed. Of a different flavor than optimizing the attention module itself, the next section presents a technique for more efficient KV cache management.\nFlash attention\nAnother way of optimizing the attention mechanism is to modify the ordering of certain computations to take better advantage of the memory hierarchy of GPUs. Neural networks are generally described in terms of layers, and most implementations are laid out that way as well, with one kind of computation done on the input data at a time in sequence. This doesn\u2019t always lead to optimal performance, since it can be beneficial to do more calculations on values that have already been brought into the higher, more performant levels of the memory hierarchy.\nFusing multiple layers together during the actual computation can enable minimizing the number of times the GPU needs to read from and write to its memory and to group together calculations that require the same data, even if they are parts of different layers in the neural network.\nOne very popular fusion is FlashAttention, an I/O aware exact attention algorithm, as detailed in FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Exact attention means that it is mathematically identical to the standard multi-head attention (with variants available for multi-query and grouped-query attention), and so can be swapped into an existing model architecture or even an already-trained model with no modifications.\nI/O aware means it takes into account some of the memory movement costs previously discussed when fusing operations together. In particular, FlashAttention uses \u201ctiling\u201d to fully compute and write out a small part of the final matrix at once, rather than doing part of the computation on the whole matrix in steps, writing out the intermediate values in between.\nFigure 7 shows the tiled FlashAttention computation pattern and the memory hierarchy on a 40 GB GPU. The chart on the right shows the relative speedup that comes from fusing and reordering the different components of the Attention mechanism.\nDiagram depicting the memory hierarchy and the FlashAttention computation.\nFigure 7. The tiled FlashAttention computation pattern and the memory hierarchy on a 40 GB GPU. Credit: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nEfficient management of KV cache with paging\nAt times, KV caches are statically \u201cover-provisioned\u201d to account for the largest possible input (the supported sequence length) because the size of inputs is unpredictable. For example, if the supported maximum sequence length of a model is 2,048, then regardless of the size of input and the generated output in a request, a reservation of size 2,048 would be made in memory. This space may be contiguously allocated, and often, much of it remains unused, leading to memory waste or fragmentation. This reserved space is tied up for the lifetime of the request.\nAn illustration of memory wastage and fragmentation due to over-provisioning and inefficient management of KV cache. 1) \u201creserved\u201d indicates memory set aside for future use that is reserved for the entirety of the request duration. 2) \u201cinternal fragmentation\u201d happens because it\u2019s hard to predict how long the generation will be and thus memory is overprovisioned to account for the maximum sequence length. 3) \u201cexternal fragmentation\u201d indicates inefficiencies due to requests in a batch requiring different pre-allocated sizes.\nFigure 8. An illustration of memory wastage and fragmentation due to over-provisioning and inefficient KV cache management. Credit: Efficient Memory Management for Large Language Model Serving with PagedAttention Inspired by paging in operating systems, the PagedAttention algorithm enables storing continuous keys and values in noncontiguous space in memory. It partitions the KV cache of each request into blocks representing a fixed number of tokens, which can be stored non-contiguously.\nThese blocks are fetched as required during attention computation using a block table that keeps account. As new tokens are generated, new block allocations are made. The size of these blocks is fixed, eliminating inefficiencies arising from challenges like different requests requiring different allocations. This significantly limits memory wastage, enabling larger batch sizes (and, consequently, throughput).\nModel optimization techniques\nSo far, we\u2019ve discussed the different ways LLMs consume memory, some of the ways memory can be distributed across several different GPUs, and optimizing the attention mechanism and KV cache. There are also several model optimization techniques to reduce the memory use on each GPU by making modifications to the model weights themselves. GPUs also have dedicated hardware for accelerating operations on these modified values, providing even more speedups for models.\nQuantization\nQuantization is the process of reducing the precision of a model\u2019s weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory\u2014a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.\nFigure 9 shows the distribution of values before and after one possible method of quantization. In this case, some precision is lost to rounding, and some dynamic range is lost to clipping, allowing the values to be represented in a much smaller format.\nTwo distribution plots, one showing the full range of values at high precision and another showing the compressed and rounded range at low precision.\nFigure 9. The distribution of values before and after one possible method of quantization\nReducing the precision of a model can yield several benefits. If the model takes up less space in memory, you can fit larger models on the same amount of hardware. Quantization also means you can transfer more parameters over the same amount of bandwidth, which can help to accelerate models that are bandwidth-limited.\nThere are many different quantization techniques for LLMs involving reduced precision on either the activations, the weights, or both. It\u2019s much more straightforward to quantize the weights because they are fixed after training. However, this can leave some performance on the table because the activations remain at higher precisions. GPUs don\u2019t have dedicated hardware for multiplying INT8 and FP16 numbers, so the weights must be converted back into a higher precision for the actual operations.\nIt\u2019s also possible to quantize the activations, the inputs of transformer blocks and network layers, but this comes with its own challenges. Activation vectors often contain outliers, effectively increasing their dynamic range and making it more challenging to represent these values at a lower precision than with the weights.\nOne option is to find out where those outliers are likely to show up by passing a representative dataset through the model, and choosing to represent certain activations at a higher precision than others (LLM.int8()). Another option is to borrow the dynamic range of the weights, which are easy to quantize, and reuse that range in the activations.\nSparsity\nSimilar to quantization, it\u2019s been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. Sparse matrices are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.\nA sparse matrix represented in a compressed format.\nFigure 10. A sparse matrix represented in a compressed format consisting of non-zero data values and their corresponding two-bit indices\nGPUs in particular have hardware acceleration for a certain kind of structured sparsity, where two out of every four values are represented by zeros. Sparse representations can also be combined with quantization to achieve even greater speedups in execution. Finding the best way to represent large language models in a sparse format is still an active area of research, and offers a promising direction for future improvements to inference speeds.\nDistillation\nAnother approach to shrinking the size of a model is to transfer its knowledge to a smaller model through a process called distillation. This process involves training a smaller model (called a student) to mimic the behavior of a larger model (a teacher).\nSuccessful examples of distilled models include DistilBERT, which compresses a BERT model by 40% while retaining 97% of its language understanding capabilities at a speed 60% faster.\nWhile distillation in LLMs is an active field of research, the general approach was first described for neural networks in Distilling the Knowledge in a Neural Network:\nThe student network is trained to mirror the performance of a larger teacher network, using a loss function that measures the discrepancy between their outputs. This objective is in addition to potentially including the original loss function of matching the student\u2019s outputs with the ground-truth labels.\nThe teacher\u2019s outputs that are matched can be the very last layer (called logits ) or intermediate layer activations.\nFigure 11 shows a general framework for knowledge distillation. The logits of the teacher are soft targets that the student optimizes for using a distillation loss. Other distillation methods may use other measures of loss to \u201cdistill\u201d knowledge from the teacher.\nFigure depicting a general framework for knowledge distillation using a distillation loss between the logits of the teacher and student.\nFigure 11. A general framework for knowledge distillation. Credit: Knowledge Distillation: A Survey An alternative approach to distillation is to use data synthesized by the teacher for supervised training of a student LLM, which is especially useful when human annotations are scarce or not available. Distilling Step by Step! goes one step further by extracting rationales from a teacher LLM in addition to the labels that serve as ground truth. These rationales serve as intermediate reasoning steps to train smaller student LLMs in a data-efficient way.\nIt\u2019s important to note that many state-of-the-art LLMs today have restrictive licenses that prohibit using their outputs to train other LLMs, making it challenging to find a suitable teacher model.\nModel serving techniques\nModel execution is frequently memory-bandwidth bound\u2014in particular, bandwidth-bound in the weights. Even after applying all the model optimizations previously described, it\u2019s still very likely to be memory bound. So you want to do as much as possible with your model weights when they are loaded. In other words, try doing things in parallel. Two approaches can be taken:\nIn-flight batching involves executing multiple different requests at the same time.\nSpeculative inference involves executing multiple different steps of the sequence in parallel to try to save time.\nIn-flight batching\nLLMs have some unique execution characteristics that can make it difficult to effectively batch requests in practice. A single model can be used simultaneously for a variety of tasks that look very different from one another. From a simple question-and-answer response in a chatbot to the summarization of a document or the generation of a long chunk of code, workloads are highly dynamic, with outputs varying in size by several orders of magnitude.\nThis versatility can make it challenging to batch requests and execute them in parallel effectively\u2014a common optimization for serving neural networks. This could result in some requests finishing much earlier than others.\nTo manage these dynamic loads, many LLM serving solutions include an optimized scheduling technique called continuous or in-flight batching. This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model.\nWith in-flight batching, rather than waiting for the whole batch to finish before moving on to the next set of requests, the server runtime immediately evicts finished sequences from the batch. It then begins executing new requests while other requests are still in flight. In-flight batching can therefore greatly increase the overall GPU utilization in real-world use cases.\nSpeculative inference\nAlso known as speculative sampling, assisted generation, or blockwise parallel decoding, speculative inference is a different way of parallelizing the execution of LLMs. Normally, GPT-style large language models are autoregressive models that generate text token by token.\nEvery token that is generated relies on all of the tokens that come before it to provide context. This means that in regular execution, it\u2019s impossible to generate multiple tokens from the same sequence in parallel\u2014you have to wait for the nth token to be generated before you can generate n+1.\nFigure 12 shows an example of speculative inference in which a draft model temporarily predicts multiple future steps that are verified or rejected in parallel. In this case, the first two predicted tokens in the draft are accepted, while the last is rejected and removed before continuing with the generation.\nFrom the prompt \u201cI saw a dog ride\u201d, the draft model predicts \u201cin the bus\u201d. The verification model predicts \u201cin the car\u201d in parallel, so we reject the \u201ccar\u201d token.\nFigure 12. An example of speculative inference. Credit: Blockwise Parallel Decoding for Deep Autoregressive Models Speculative sampling offers a workaround. The basic idea of this approach is to use some \u201ccheaper\u201d process to generate a draft continuation that is several tokens long. Then, execute the main \u201cverification\u201d model at multiple steps in parallel, using the cheap draft as \u201cspeculative\u201d context for the execution steps where it is needed.\nIf the verification model generates the same tokens as the draft, then you know to accept those tokens for the output. Otherwise, you can throw out everything after the first non-matching token, and repeat the process with a new draft.\nThere are many different options for how to generate draft tokens, and each comes with different tradeoffs. You can train multiple models, or fine-tune multiple heads on a single pretrained model, that predict tokens that are multiple steps in the future. Or, you can use a small model as the draft model, and a larger, more capable model as the verifier.\nConclusion\nThis post outlines many of the most popular solutions to help optimize and serve LLMs efficiently, be it in the data center or at the edge on a PC. Many of these techniques are optimized and available through NVIDIA TensorRT-LLM, an open-source library consisting of the TensorRT deep learning compiler alongside optimized kernels, preprocessing and postprocessing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs. To learn more, see Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available.\nNVIDIA TensorRT-LLM is now supported by NVIDIA Triton Inference Server, enabling enterprises to serve multiple AI models concurrently across different AI frameworks, hardware accelerators, and deployment models with peak throughput and minimum latency.\nTensorRT-LLM also powers NVIDIA NeMo, which provides an end-to-end cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. Get started with NeMo."}], "https://developer.nvidia.com/blog/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/": [{"text": "The article discusses the power of NVIDIA Grace and NVIDIA Hopper architectures in high-performance computing (HPC) applications, especially in conjunction with AI. NVIDIA offers a software stack that includes tools, libraries, and compilers to help developers optimize their applications for these architectures. The NVIDIA HPC SDK 23.11 introduces new capabilities for GPU programming, enabling bidirectional communication between CPU and GPU memory for improved performance. Additionally, NVIDIA Performance Libraries (NVPL) provide optimized math libraries for Arm 64-bit architectures, enhancing the performance of HPC applications on NVIDIA Grace CPUs. The article also introduces new libraries like NVIDIA cuDSS for solving linear systems and cuTENSOR 2.0 for accelerating applications in HPC and AI. Finally, NVIDIA Nsight Systems 2023.4 is highlighted as a performance analysis tool for tuning applications on NVIDIA Grace CPUs. Overall, NVIDIA offers a comprehensive ecosystem of tools and libraries to unlock the full potential of accelerated computing for HPC applications.", "text_components": ["Unlock the Power of NVIDIA Grace and NVIDIA Hopper Architectures with Foundational HPC Software\nHigh-performance computing (HPC) powers applications in simulation and modeling, healthcare and life sciences, industry and engineering, and more. In the modern data center, HPC synergizes with AI, harnessing data in transformative new ways.\nThe performance and throughput demands of next-generation HPC applications call for an accelerated computing platform that can handle diverse workloads and has a tight coupling between the CPU and GPU. The NVIDIA Grace CPU and NVIDIA Hopper GPU are industry-leading hardware ecosystems for HPC development.\nNVIDIA provides tools, libraries, and compilers to help developers take advantage of the NVIDIA Grace and NVIDIA Grace Hopper architectures. These tools support innovation and help applications make full use of accelerated computing. This foundational software stack provides the means for GPU acceleration, and porting and optimizing your applications on NVIDIA Grace-based systems. For more information about NVIDIA Grace compilers, tools, and libraries, see the NVIDIA Grace product page.", "NVIDIA HPC SDK 23.11\nThe new hardware developments in the NVIDIA Grace Hopper systems enable dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that you can develop your application for both processors while using a single, unified address space.\nEach processor retains its own physical memory that is designed with the bandwidth, latency, and capacity characteristics matched to the workloads most suited for each processor. Code written for existing discrete-memory GPU systems continues to run performantly without modification for the new NVIDIA Grace Hopper architecture.\nAll application threads (GPU or CPU) can directly access the application\u2019s system-allocated memory, removing the need to copy data between processors. This new ability to read or write directly to the full application memory address space significantly improves programmer productivity for all programming models built on top of NVIDIA CUDA:\nCUDA C++\nCUDA Fortran\nStandard parallelism in ISO C++, ISO Fortran, OpenACC, OpenMP\n\u2026and many others\nNVIDIA HPC SDK 23.11 introduces new unified memory programming support, enabling workloads bottlenecked by host-to-device or device-to-host transfers to achieve up to a 7x speedup due to the chip-to-chip (C2C) interconnect in NVIDIA Grace Hopper systems. Application development can also be dramatically simplified because considerations for data location and movement are handled automatically by the system.\nFor more information about how HPC compilers use these new hardware capabilities to simplify GPU programming with ISO C++, ISO Fortran, OpenACC, and CUDA Fortran, see Simplifying GPU Programming for HPC with the NVIDIA Grace Hopper Superchip.\nGet started with the NVIDIA HPC SDK for free and download version 23.11 now.", "NVIDIA Performance Libraries\nNVIDIA has grown to become a full-stack, enterprise platform provider, now offering CPUs as well as GPUs and DPUs. NVIDIA math software offerings now support CPU-only workloads in addition to existing GPU-centric solutions.\nNVIDIA Performance Libraries (NVPL) are a collection of essential math libraries optimized for Arm 64-bit architectures. Many HPC applications rely on mathematical APIs like BLAS and LAPACK, which are crucial to their performance. NVPL math libraries are drop-in replacements for these standardized math APIs.\nThey are optimized for the NVIDIA Grace CPU. Applications being ported to or built on NVIDIA Grace-based platforms can fully use the high-performance and high-efficiency architecture. A primary goal of NVPL is to provide developers and system administrators with the smoothest experience porting and deploying existing HPC applications to the NVIDIA Grace platform with no source code changes required to achieve maximal performance when using CPU-based, standardized math libraries.\nThe beta release of NVPL, available now, includes BLAS, LAPACK, FFT, RAND, and SPARSE to accelerate your applications on the NVIDIA Grace CPU.\nLearn more and download the NVPL beta.", "NVIDIA CUDA Direct Sparse Solvers\nA new standard math library is being introduced to the suite of NVIDIA GPU-accelerated libraries. The NVIDIA CUDA Direct Sparse Solvers library, NVIDIA cuDSS, is optimized for solving linear systems with very sparse matrices. While the first version of cuDSS supports execution on a single-GPU, multi-GPU, and multi-node support will be added in an upcoming release.\nHoneywell is one of the early adopters of cuDSS and is in the final phase of performance benchmarking in its UniSim Design process simulation product.\nThe cuDSS preview is available to download now. For more information about supported features, see the NVIDIA cuDSS documentation.", "NVIDIA cuTENSOR 2.0\nNVIDIA cuTENSOR 2.0 is a performant and flexible library for accelerating your applications at the intersection of HPC and AI.\nIn this major release, cuTENSOR 2.0 adds new features and performance improvements, including for arbitrarily high dimensional tensors. To make the new optimizations easily extensible across all tensor operations uniformly, while delivering high performance, the cuTENSOR 2.0 APIs have been completely revised with a focus on flexibility and extensibility.\ncuTENSOR 2.0 API flowchart.\nFigure 1. cuTENSOR APIs are now shared across different tensor operations\nThe plan-based multi-stage API is extended to all operations through a set of shared APIs. The new APIs can take opaque heap-allocated data structures as input for passing any operation-specific problem descriptors defined for that execution.\ncuTENSOR 2.0 also adds support for just-in-time (JIT) kernels.\ncuTENSOR 2.0 benchmark performance.\nFigure 2. Average incremental performance improvements from using JIT for various input tensor types compared for two benchmarks: QC-like and Rand1000. Performance improvements from JIT are significant for QC-like test cases with high dimensional tensors\nUsing JIT kernels helps realize unparalleled performance by tuning the right configuration and optimization knobs for the target configuration at runtime, supporting a myriad of high-dimensional tensors not achievable through generic pre-compiled kernels that the library can ship.\ncuTENSOR 2.0 speedup graph.\nFigure 3. cuTENSOR 2.0.0 performance gains over the previous 1.7.0 version when tuned with JIT and other capabilities\nLearn about migration and download cuTENSOR 2.0 now.", "NVIDIA Grace CPU performance tuning with NVIDIA Nsight Systems 2023.4\nApplications on NVIDIA Grace-based platforms benefit from tuning instruction execution on the CPU cores, and from optimizing the CPU\u2019s interaction with other hardware units in the system. When porting applications to NVIDIA Grace CPUs, insight into functions at the hardware level helps you configure your software for the new platform.\nNVIDIA Nsight Systems is a system-wide performance analysis tool that collects hardware and API metrics and correlates them on a unified timeline. For NVIDIA Grace CPU performance tuning, Nsight Systems samples instruction pointers and backtraces to visualize where CPU code is busiest, and how the CPU is using resources across the system. Nsight Systems also captures context switching to build a utilization graph for all the NVIDIA Grace CPU cores.\nNVIDIA Grace CPU core event rates, like CPU cycles and instructions retired, show how the NVIDIA Grace cores are handling work. The summary view for backtrace samples also helps you quickly identify which instruction pointers are causing hotspots.\nNow available in Nsight Systems 2023.4, NVIDIA Grace CPU uncore event rates monitor activity outside of the cores\u2014like NVLink-C2C and PCIe activity. Uncore metrics show how activity between sockets supports the work of the cores, helping you find ways to improve the NVIDIA Grace CPU\u2019s integration with the rest of the system.\nNVIDIA Grace CPU uncore and core event sampling in Nsight Systems 2023.4 help you find the best optimizations for code running on NVIDIA Grace. For more information about performance tuning, as well as tips on optimizing your CUDA code in conjunction, see the following video.\nVideo 1. NVIDIA Grace CPU Performance Tuning with NVIDIA Nsight Tools Learn more and get started with Nsight Systems 2023.4. Nsight Systems is also available in the HPC SDK and CUDA Toolkit.", "Accelerated computing for HPC\nNVIDIA provides an ecosystem of tools, libraries, and compilers for accelerated computing on the NVIDIA Grace and Hopper architectures. The HPC software stack is foundational for research and science on NVIDIA data center silicon.\nDive deeper into accelerated computing topics in the Developer Forums."], "document_title": "Unlock the Power of NVIDIA Grace and NVIDIA Hopper Architectures with Foundational HPC Software", "document_url": "https://developer.nvidia.com/blog/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/", "document_date": "2023-11-16T19:07:51", "document_date_modified": "2023-11-30T19:43:30", "document_full_text": "Unlock the Power of NVIDIA Grace and NVIDIA Hopper Architectures with Foundational HPC Software\nHigh-performance computing (HPC) powers applications in simulation and modeling, healthcare and life sciences, industry and engineering, and more. In the modern data center, HPC synergizes with AI, harnessing data in transformative new ways.\nThe performance and throughput demands of next-generation HPC applications call for an accelerated computing platform that can handle diverse workloads and has a tight coupling between the CPU and GPU. The NVIDIA Grace CPU and NVIDIA Hopper GPU are industry-leading hardware ecosystems for HPC development.\nNVIDIA provides tools, libraries, and compilers to help developers take advantage of the NVIDIA Grace and NVIDIA Grace Hopper architectures. These tools support innovation and help applications make full use of accelerated computing. This foundational software stack provides the means for GPU acceleration, and porting and optimizing your applications on NVIDIA Grace-based systems. For more information about NVIDIA Grace compilers, tools, and libraries, see the NVIDIA Grace product page.\nNVIDIA HPC SDK 23.11\nThe new hardware developments in the NVIDIA Grace Hopper systems enable dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that you can develop your application for both processors while using a single, unified address space.\nEach processor retains its own physical memory that is designed with the bandwidth, latency, and capacity characteristics matched to the workloads most suited for each processor. Code written for existing discrete-memory GPU systems continues to run performantly without modification for the new NVIDIA Grace Hopper architecture.\nAll application threads (GPU or CPU) can directly access the application\u2019s system-allocated memory, removing the need to copy data between processors. This new ability to read or write directly to the full application memory address space significantly improves programmer productivity for all programming models built on top of NVIDIA CUDA:\nCUDA C++\nCUDA Fortran\nStandard parallelism in ISO C++, ISO Fortran, OpenACC, OpenMP\n\u2026and many others\nNVIDIA HPC SDK 23.11 introduces new unified memory programming support, enabling workloads bottlenecked by host-to-device or device-to-host transfers to achieve up to a 7x speedup due to the chip-to-chip (C2C) interconnect in NVIDIA Grace Hopper systems. Application development can also be dramatically simplified because considerations for data location and movement are handled automatically by the system.\nFor more information about how HPC compilers use these new hardware capabilities to simplify GPU programming with ISO C++, ISO Fortran, OpenACC, and CUDA Fortran, see Simplifying GPU Programming for HPC with the NVIDIA Grace Hopper Superchip.\nGet started with the NVIDIA HPC SDK for free and download version 23.11 now.\nNVIDIA Performance Libraries\nNVIDIA has grown to become a full-stack, enterprise platform provider, now offering CPUs as well as GPUs and DPUs. NVIDIA math software offerings now support CPU-only workloads in addition to existing GPU-centric solutions.\nNVIDIA Performance Libraries (NVPL) are a collection of essential math libraries optimized for Arm 64-bit architectures. Many HPC applications rely on mathematical APIs like BLAS and LAPACK, which are crucial to their performance. NVPL math libraries are drop-in replacements for these standardized math APIs.\nThey are optimized for the NVIDIA Grace CPU. Applications being ported to or built on NVIDIA Grace-based platforms can fully use the high-performance and high-efficiency architecture. A primary goal of NVPL is to provide developers and system administrators with the smoothest experience porting and deploying existing HPC applications to the NVIDIA Grace platform with no source code changes required to achieve maximal performance when using CPU-based, standardized math libraries.\nThe beta release of NVPL, available now, includes BLAS, LAPACK, FFT, RAND, and SPARSE to accelerate your applications on the NVIDIA Grace CPU.\nLearn more and download the NVPL beta.\nNVIDIA CUDA Direct Sparse Solvers\nA new standard math library is being introduced to the suite of NVIDIA GPU-accelerated libraries. The NVIDIA CUDA Direct Sparse Solvers library, NVIDIA cuDSS, is optimized for solving linear systems with very sparse matrices. While the first version of cuDSS supports execution on a single-GPU, multi-GPU, and multi-node support will be added in an upcoming release.\nHoneywell is one of the early adopters of cuDSS and is in the final phase of performance benchmarking in its UniSim Design process simulation product.\nThe cuDSS preview is available to download now. For more information about supported features, see the NVIDIA cuDSS documentation.\nNVIDIA cuTENSOR 2.0\nNVIDIA cuTENSOR 2.0 is a performant and flexible library for accelerating your applications at the intersection of HPC and AI.\nIn this major release, cuTENSOR 2.0 adds new features and performance improvements, including for arbitrarily high dimensional tensors. To make the new optimizations easily extensible across all tensor operations uniformly, while delivering high performance, the cuTENSOR 2.0 APIs have been completely revised with a focus on flexibility and extensibility.\ncuTENSOR 2.0 API flowchart.\nFigure 1. cuTENSOR APIs are now shared across different tensor operations\nThe plan-based multi-stage API is extended to all operations through a set of shared APIs. The new APIs can take opaque heap-allocated data structures as input for passing any operation-specific problem descriptors defined for that execution.\ncuTENSOR 2.0 also adds support for just-in-time (JIT) kernels.\ncuTENSOR 2.0 benchmark performance.\nFigure 2. Average incremental performance improvements from using JIT for various input tensor types compared for two benchmarks: QC-like and Rand1000. Performance improvements from JIT are significant for QC-like test cases with high dimensional tensors\nUsing JIT kernels helps realize unparalleled performance by tuning the right configuration and optimization knobs for the target configuration at runtime, supporting a myriad of high-dimensional tensors not achievable through generic pre-compiled kernels that the library can ship.\ncuTENSOR 2.0 speedup graph.\nFigure 3. cuTENSOR 2.0.0 performance gains over the previous 1.7.0 version when tuned with JIT and other capabilities\nLearn about migration and download cuTENSOR 2.0 now.\nNVIDIA Grace CPU performance tuning with NVIDIA Nsight Systems 2023.4\nApplications on NVIDIA Grace-based platforms benefit from tuning instruction execution on the CPU cores, and from optimizing the CPU\u2019s interaction with other hardware units in the system. When porting applications to NVIDIA Grace CPUs, insight into functions at the hardware level helps you configure your software for the new platform.\nNVIDIA Nsight Systems is a system-wide performance analysis tool that collects hardware and API metrics and correlates them on a unified timeline. For NVIDIA Grace CPU performance tuning, Nsight Systems samples instruction pointers and backtraces to visualize where CPU code is busiest, and how the CPU is using resources across the system. Nsight Systems also captures context switching to build a utilization graph for all the NVIDIA Grace CPU cores.\nNVIDIA Grace CPU core event rates, like CPU cycles and instructions retired, show how the NVIDIA Grace cores are handling work. The summary view for backtrace samples also helps you quickly identify which instruction pointers are causing hotspots.\nNow available in Nsight Systems 2023.4, NVIDIA Grace CPU uncore event rates monitor activity outside of the cores\u2014like NVLink-C2C and PCIe activity. Uncore metrics show how activity between sockets supports the work of the cores, helping you find ways to improve the NVIDIA Grace CPU\u2019s integration with the rest of the system.\nNVIDIA Grace CPU uncore and core event sampling in Nsight Systems 2023.4 help you find the best optimizations for code running on NVIDIA Grace. For more information about performance tuning, as well as tips on optimizing your CUDA code in conjunction, see the following video.\nVideo 1. NVIDIA Grace CPU Performance Tuning with NVIDIA Nsight Tools Learn more and get started with Nsight Systems 2023.4. Nsight Systems is also available in the HPC SDK and CUDA Toolkit.\nAccelerated computing for HPC\nNVIDIA provides an ecosystem of tools, libraries, and compilers for accelerated computing on the NVIDIA Grace and Hopper architectures. The HPC software stack is foundational for research and science on NVIDIA data center silicon.\nDive deeper into accelerated computing topics in the Developer Forums."}], "https://developer.nvidia.com/blog/mastering-llm-techniques-training/": [{"text": "The article delves into the training of large language models (LLMs) using transformer networks, discussing model architectures, tokenization, attention mechanisms, embedding techniques, and training strategies. LLMs, built using transformer networks, have the potential to revolutionize society but are challenging to train. Various model architectures like BERT, GPT, and Text-To-Text Transformers are explored, along with attention mechanisms like self-attention and multi-head attention. The article also discusses tokenization methods such as subword tokenization and different embedding techniques like positional encoding and Rotary Position Embeddings (RoPE). Training strategies like model parallelism, sequence parallelism, and quantization-aware training are also covered. The article highlights the importance of efficient training methods for LLMs to handle the massive amount of data and parameters involved. It also mentions the availability of tools like NVIDIA NeMo for accelerated training workflows and customization techniques. Overall, the article provides insights into the complex process of training LLMs using transformer networks.", "text_components": ["Mastering LLM Techniques: Training\u00a0\nLarge language models (LLMs) are a class of generative AI models built using transformer networks that can recognize, summarize, translate, predict, and generate language using very large datasets. LLMs have the promise of transforming society as we know it, yet training these foundation models is incredibly challenging.\nThis blog articulates the basic principles behind LLMs, built using transformer networks, spanning model architectures, \u200cattention mechanisms, \u200cembedding techniques, and foundation model training strategies.", "Model architectures\nModel architectures define the backbone of transformer networks, broadly dictating the capabilities and limitations of the model. The architecture of an LLM is often called an encoder, decoder, or encoder-decoder model.\nImage depicts the canonical architecture of transformer networks, including encoder-decoder, encoder-only, and decoder-only architectures.\nFigure 1. The canonical structure of large language models\nSome popular architectures include:\nArchitecture\nDescription\nSuitable for\nBi-directional Encoder Representation from Transformers (BERT)\nEncoder-only architecture, best suited for tasks that can understand language.\nClassification and sentiment analysis\nGenerative Pre-trained Transformer (GPT)\nDecoder-only architecture suited for generative tasks and fine-tuned with labeled data on discriminative tasks.\nGiven the unidirectional architecture, context only flows forward. The GPT framework helps achieve strong natural language understanding using a single-task-agnostic model through generative pre-training and discriminative fine-tuning.\nTextual entailment, sentence similarity, question answering.\nText-To-Text Transformer (Sequence-to-Sequence models)\nEncoder-decoder architecture. It leverages the transfer learning approach to convert every text-based language problem into a text-to-text format, that is taking text as input and producing the next text as output. With a bidirectional architecture, context flows in both directions.\nTranslation, Question & Answering, Summarization.\nMixture of Experts (MoE)\nModel architecture decisions that can be applied to any of the\u200c architectures. Designed to scale up model capacity substantially while adding minimal computation overhead, converting dense models into sparse models. The MoE layer consists of many expert models and a sparse gating function. The gates route each input to the top-K (K>=2 or K=1) best experts during inference.\nGeneralize well across tasks for computational efficiency during inference, with low latency\nAnother popular architecture decision is to expand to multimodal models that combine information from multiple modalities or forms of data such as text, images, audio, and video. Although challenging to train, multimodal models offer key benefits of complementary information from different modalities, much as humans understand by analyzing data from multiple senses.\nThese models contain separate encoders for each modality, like a CNN for images, and transformers for text to extract high-level feature representations from the respective input data. The combination of features extracted from multiple modalities can be a challenge. It can be addressed by fusing features extracted from each modality, or by using attention mechanisms to weigh the contribution of each modality relative to the task.\nThe joint representation captures interactions between modalities. The model architecture may contain additional decoders for generating task-specific outputs like classifications, caption generation, translation, image generation given prompt text, image editing given prompt text, and the like.", "Delving into transformer networks\nWithin the realm of transformer networks, the process of tokenization assumes a pivotal role in fragmenting text into smaller units known as tokens.", "Tokenizers\nTokenization is the first step to building a model, which involves splitting text into smaller units called tokens that become the basic building blocks for LLMs. These extracted tokens are used to build a vocabulary index mapping tokens to numeric IDs, to numerically represent text suitable for deep learning computations. During the encoding process, these numeric tokens are encoded into vectors representing each token\u2019s meaning. During the decoding process, when LLMs perform generation, tokenizers decode the numeric vectors back into readable text sequences.\nThe process begins with normalization to process lowercase, pruning punctuation and whitespaces, stemming, lemmatization, handling contractions, and \u200cremoving accents. Once the text is cleaned up, the next step is to segment the text by recognizing word and sentence boundaries. Depending on the boundary, tokenizers can be at word, sub-word, or character-level granularity.\nAlthough word and character-based tokenizers are prevalent, there are challenges with these. Word-based tokenizers lead to a large vocabulary size and words not seen during the tokenizer training process cause many out-of-vocabulary tokens. Character-based tokenizers lead to long sequences and less meaningful individual tokens.\nDue to these shortcomings, subword-based tokenizers have gained popularity. The focus of subword tokenization algorithms is to split rare words into smaller, meaningful subwords, based on common character n-grams and patterns. For This technique enables the representation of rare and unseen words via known subwords, resulting in a reduced vocabulary size. During inference, it also handles out-of-vocabulary words effectively reducing vocabulary size, while handling out-of-vocabulary words gracefully during inference.\nPopular subword tokenization algorithms include Byte Pair Encoding (BPE), WordPiece, Unigram, and SentencePiece.\nBPE starts with character vocabulary and iteratively merges frequent adjacent character pairs into new vocabulary terms, achieving text compression with faster inference at decoding time by replacing most common words with single tokens.\nWordPiece is similar to BPE in doing merge operations, however, this leverages the probabilistic nature of the language to merge characters to maximize training data likelihood.\nUnigram starts with a large vocabulary, calculates the probability of tokens, and removes tokens based on a loss function until it reaches the desired vocabulary size.\nSentencePiece learns subword units from raw text based on language modeling objectives and uses Unigram or BPE tokenization algorithms to construct the vocabulary.", "Attention Mechanisms\nAs \u200ctraditional seq-2-seq encoder-decoder language models like Recurrent Neural Networks (RNNs) don\u2019t scale well with the length of the input sequence, the concept of attention was introduced and has proved to be seminal. The attention mechanism enables the decoder to use the most relevant parts of the input sequence weighted by the encoded input sequence, with the most relevant tokens being assigned the highest weight. This concept improves the scaling of input sequence lengths by carefully selecting \u200ctokens by importance.\nThis idea was furthered with self-attention and introduced in 2017 with the transformer model architecture, removing the need for RNNs. Self-attention mechanisms create representations of the input sequence relying on the relationship between different words in the same sequence. By enhancing the information content of an input embedding through the inclusion of input context, self-attention mechanisms play a crucial role in \u200ctransformer architectures.\nComputational steps in self-attention.\nFigure 2. Self-attention architecture ( source: Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch )\nSelf-attention is called scaled-dot product attention because of how it achieves context-aware input representation. Each token in the input sequence is used to project itself into Query (Q), Key (K), and Value (V) sequences using their respective weight matrices. The goal is to compute an attention-weighted version of each input token given all the other input tokens as its context. By computing a scaled dotduct of Q and K matrices with relevant pairs determined by the V matrix getting higher weights, the self-attention mechanism finds a suitable vector for each input token (Q) given all key-value pairs that are other tokens in the sequence.\nSelf-attention further evolved into multi-head attention. The three matrices (Q,K,V) described preceding can be considered as single-head. Multi-head self-attention is when multiple such heads are used. These heads function like multiple kernels in CNNs, attending to different parts of the sequence, focusing on longer-term compared to shorter-term dependencies.\nMultiple heads focusing on different parts of the sequence.\nFigure 3. Multi-head self-attention ( source: Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch )\nAnd finally, the concept of cross-attention came about, where instead of a single input sequence as in the case of self-attention, this involves two different input sequences. In the transformer model architecture, that\u2019s one input sequence from the encoder and another processed by the decoder.", "FlashAttention\nTransformers of a larger size are limited by the memory requirements of the attention layer, which increases in proportion to the length of the sequence. This growth is quadratic. To speed up \u200cattention layer computations and reduce its memory footprint, FlashAttention optimizes the naive implementation bottlenecked by repeated reads and writes from slower GPU high bandwidth memory (HBM).\nFlashAttention uses classical tiling to load blocks of query, key, and value from GPU HBM (its main memory) to SRAM (its fast cache) for attention computation, and then writes back the output to HBM. It also improves upon memory usage, by not storing large attention matrices from the forward pass; instead relies on recomputing the attention matrix during backprop in SRAM. With these optimizations, FlashAttention brings significant speedup (2-4x) for longer sequences.\nSpeedup and memory savings from using FlashAttention.\nFigure 4. FlashAttention fast and memory-efficient exact attention with IO-awareness (s ource: https://github.com/Dao-AILab/flash-attention )\nFurther improved FlashAttention-2 is 2x faster than FlashAttention by adding further optimizations with sequence parallelism, better work partitioning, and reducing non-matmul FLOPs. This newer version also supports multi-query attention as well as grouped-query attention that we describe next.", "Multi-Query Attention (MQA)\nA variant of attention where multiple heads of query attend to the same head of key and value projections. This reduces the KV cache size and hence the memory bandwidth requirements of incremental decoding. The resulting models support faster autoregressive decoding during inference with minor quality degradation than the baseline multi-head attention architecture.", "Group Query Attention (GQA)\nGrouped-query attention shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.Group-query attention (GQA) is an improvement over MQA to overcome quality degradation issues while retaining the speed-up at inference time. Moreover, models trained using multi-head attention don't have to be retrained from scratch and can employ GQA during inference by up-training existing model checkpoints using only 5% of original training compute. Also, this is a generalization of MQA using an intermediate (more than one, less than number of query heads) number of key-value heads. GQA achieves quality close to baseline multi-head attention with comparable speed to MQA.\nEmbedding techniques\nThe order in which words appear in a sentence is important. This Information is encoded in LLMs using positional encoding by assigning the order of occurrence of each input token to a 2D positional encoding matrix. Each row of the matrix represents an encoded token of the sequence summed with its positional information. This allows the model to differentiate between words with similar meanings but different positions in the sentence and enables encoding of the relative position of words.\nThe original transformer architecture combines absolute positional encoding with word embeddings using sinusoidal functions. However, this approach doesn\u2019t allow extrapolation to longer sequences at inference time than those seen during training. Relative position encoding solved this challenge. In this, the content representations for query and key vectors are combined with positional representations that are trainable, relative to the distance between a query and a key that is clipped beyond a certain distance.\nRoPE\nFigure 5. Grouped-query attention architecture ( source: https://github.com/fkodom/grouped-query-attention-pytorch ) Group-query attention (GQA) is an improvement over MQA to overcome quality degradation issues while retaining the speed-up at inference time. Moreover, models trained using multi-head attention don\u2019t have to be retrained from scratch. They can employ GQA during inference by up-training existing model checkpoints using only 5% of the original training compute. Also, this is a generalization of MQA using an intermediate (more than one, less than number of query heads) number of key-value heads. GQA achieves quality close to baseline multi-head attention with comparable speed to MQA.", "Embedding techniques\nThe order in which words appear in a sentence is important. This information is encoded in LLMs using positional encoding by assigning the order of occurrence of each input token to a 2D positional encoding matrix. Each row of the matrix represents an encoded token of the sequence summed with its positional information. This allows the model to differentiate between words with similar meanings but different positions in the sentence and enables encoding of the relative position of words.\nThe original transformer architecture combines absolute positional encoding with word embeddings using sinusoidal functions. However, this approach doesn\u2019t allow extrapolation to longer sequences at inference time than those seen during training. Relative position encoding solved this challenge. In this, the content representations for query and key vectors are combined with positional representations that are trainable, relative to the distance between a query and a key that is clipped beyond a certain distance.", "RoPE\nEnhancing linear self-attention with relative position encoding. Absolute position is encoded using a rotation matrix.\nFigure 6. Implementation of Rotary Position Embedding (source: RoFormer: Enhanced Transformer with Rotary Position Embedding )\nSource:\nRotary Position Embeddings (RoPE) combines the concepts of absolute and relative position embeddings. The absolute position is encoded using a rotation matrix. The relative position dependency is incorporated in self-attention formulation and added to the contextual representation in a multiplicative manner. This technique retains the benefit of sequence length flexibility introduced in the transformer\u2019s sinusoidal position embedding while equipping linear self-attention with relative position encoding. It also introduces decaying inter-token dependency with increasing relative distances, enabling extrapolation to longer sequences at inference time.", "AliBi\nTransformer-based LLMs don\u2019t scale well to longer sequences due to the quadratic cost of self-attention, which limits the number of tokens of context. Additionally, the sinusoidal position method introduced in the original transformer architecture doesn\u2019t extrapolate to sequences that are longer than it saw during training. This limits the set of real-world use cases where LLMs can be applied. To overcome this, Attention with Linear Biases (ALiBi) was introduced. This technique does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance.\nTo facilitate efficient extrapolation for much longer sequences than seen at training time, ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Compared to sinusoidal models, this method requires no additional runtime or parameters and incurs a negligible (0\u20130.7%) memory increase. ALiBi\u2019s edge over sinusoidal embeddings is largely explained by its improved avoidance of the early token curse. This method can also achieve further gains by more efficiently exploiting longer context histories.", "Training transformer networks\nWhile training LLMs, there are several techniques to improve efficiency and optimize resource usage of underlying hardware configurations. Scaling these massively large AI models with billions of parameters and trillions of tokens comes with huge memory capacity requirements.\nTo alleviate this requirement, a few methods such as model parallelism and activation recomputation are popular. Model parallelism partitions the model parameters and optimizer states across multiple GPUs so that each GPU stores a subset of the model parameters. It is further classified into tensor and pipeline parallelism.\nTensor parallelism splits operations across GPUs, often known as intra-layer parallelism focused on parallelizing computation within an operation such as matrix-matrix multiplication. This technique requires additional communication to make sure that the result is correct.\nPipeline parallelism splits model layers across GPUs, also known as inter-layer parallelization, focused on splitting the model by layers into chunks. Each device computes for its chunk and passes intermediate activations to the next stage. This could lead to bubble time where some devices are engaged in computation and others waiting, leading to a waste of computational resources.\nSequence parallelism expands upon tensor-level model parallelism by noticing that the regions of a transformer layer that haven\u2019t previously been parallelized and are independent along the sequence dimension. Splitting these layers along the sequence dimension enables distribution of the compute as well as the activation memory for these regions across the tensor parallel devices. Since activations are distributed and have a smaller memory footprint, more activations can be saved for the backward pass.\nSelective activation recomputation goes hand-in-hand with sequence parallelism. It improves cases where memory constraints force the recomputation of some, but not all, of the activations, by noticing that different activations require different numbers of operations to recompute. Instead of checkpointing and recomputing full transformer layers, it\u2019s possible to checkpoint and recompute only parts of each transformer layer that take up a lot of memory but aren\u2019t computationally expensive to recompute.\nAll techniques add communication or computation overhead. Therefore, finding the configuration that achieves maximum performance and then scaling training with data parallelism is essential for efficient LLM training.\nIn data parallel training, the dataset is split into several shards, where each shard is allocated to a device. This is equivalent to parallelizing the training process along the batch dimension. Each device will hold a full copy of the model replica and train on the dataset shard allocated. After back-propagation, the gradients of the model will be all-reduced so that the model parameters on different devices can stay synchronized.\nA variant of this is called the fully sharded data parallelism (FSDP) technique. It shards model parameters and training data uniformly across data parallel workers, where the computation for each micro-batch of data is local to each GPU worker.\nFSDP offers configurable sharding strategies that can be customized to match the physical interconnect topology of the cluster to handle hardware heterogeneity. It can minimize bubbles to overlap communication with computation aggressively through operation reordering and parameter prefetching. And lastly, FSDP optimizes memory usage by restricting the number of blocks allocated for inflight unsharded parameters. Due to these optimizations, FSDP provides support for significantly larger models with near-linear scalability in terms of TFLOPS.", "Quantization Aware Training\nQuantization is the process in which deep learning models perform all or part of the computation in reduced precision as compared to full precision (floating point) values. This technique enables inference speedups, memory savings, and cost reduction of using deep learning models with minimal accuracy loss.\nQuantization Aware Training (QAT) is a method that takes into account the impact of quantization during the training process. The model is trained with quantization-aware operations that mimic the quantization process during training. Models learn how to perform well in \u200cquantized representations, leading to improved accuracy compared to post-training quantization. The forward pass quantizes weights and activations to low-precision representations. The backward pass computes gradients using full-precision weights and activations. This enables the model to learn parameters that are robust to quantization errors introduced in the forward pass. The result is a trained model that can be quantized post-training with minimal impact on accuracy.", "Train LLMs today\nThis post covered various model training techniques and when to use them. Check out the post on Mastering LLM Techniques: Customization, to continue your learning journey on the LLM workflow.\nMany of the training methods are supported on NVIDIA NeMo, which provides an accelerated workflow for training with 3D parallelism techniques. It also offers a choice of several customization techniques. It is optimized for at-scale inference of large-scale models for language and image workloads, with multi-GPU and multi-node configurations. Download the NeMo framework today and train LLMs on your preferred on-premises and cloud platforms."], "document_title": "Mastering LLM Techniques: Training\u00a0", "document_url": "https://developer.nvidia.com/blog/mastering-llm-techniques-training/", "document_date": "2023-11-16T14:00:00", "document_date_modified": "2024-01-22T22:05:25", "document_full_text": "Mastering LLM Techniques: Training\u00a0\nLarge language models (LLMs) are a class of generative AI models built using transformer networks that can recognize, summarize, translate, predict, and generate language using very large datasets. LLMs have the promise of transforming society as we know it, yet training these foundation models is incredibly challenging.\nThis blog articulates the basic principles behind LLMs, built using transformer networks, spanning model architectures, \u200cattention mechanisms, \u200cembedding techniques, and foundation model training strategies.\nModel architectures\nModel architectures define the backbone of transformer networks, broadly dictating the capabilities and limitations of the model. The architecture of an LLM is often called an encoder, decoder, or encoder-decoder model.\nImage depicts the canonical architecture of transformer networks, including encoder-decoder, encoder-only, and decoder-only architectures.\nFigure 1. The canonical structure of large language models\nSome popular architectures include:\nArchitecture\nDescription\nSuitable for\nBi-directional Encoder Representation from Transformers (BERT)\nEncoder-only architecture, best suited for tasks that can understand language.\nClassification and sentiment analysis\nGenerative Pre-trained Transformer (GPT)\nDecoder-only architecture suited for generative tasks and fine-tuned with labeled data on discriminative tasks.\nGiven the unidirectional architecture, context only flows forward. The GPT framework helps achieve strong natural language understanding using a single-task-agnostic model through generative pre-training and discriminative fine-tuning.\nTextual entailment, sentence similarity, question answering.\nText-To-Text Transformer (Sequence-to-Sequence models)\nEncoder-decoder architecture. It leverages the transfer learning approach to convert every text-based language problem into a text-to-text format, that is taking text as input and producing the next text as output. With a bidirectional architecture, context flows in both directions.\nTranslation, Question & Answering, Summarization.\nMixture of Experts (MoE)\nModel architecture decisions that can be applied to any of the\u200c architectures. Designed to scale up model capacity substantially while adding minimal computation overhead, converting dense models into sparse models. The MoE layer consists of many expert models and a sparse gating function. The gates route each input to the top-K (K>=2 or K=1) best experts during inference.\nGeneralize well across tasks for computational efficiency during inference, with low latency\nAnother popular architecture decision is to expand to multimodal models that combine information from multiple modalities or forms of data such as text, images, audio, and video. Although challenging to train, multimodal models offer key benefits of complementary information from different modalities, much as humans understand by analyzing data from multiple senses.\nThese models contain separate encoders for each modality, like a CNN for images, and transformers for text to extract high-level feature representations from the respective input data. The combination of features extracted from multiple modalities can be a challenge. It can be addressed by fusing features extracted from each modality, or by using attention mechanisms to weigh the contribution of each modality relative to the task.\nThe joint representation captures interactions between modalities. The model architecture may contain additional decoders for generating task-specific outputs like classifications, caption generation, translation, image generation given prompt text, image editing given prompt text, and the like.\nDelving into transformer networks\nWithin the realm of transformer networks, the process of tokenization assumes a pivotal role in fragmenting text into smaller units known as tokens.\nTokenizers\nTokenization is the first step to building a model, which involves splitting text into smaller units called tokens that become the basic building blocks for LLMs. These extracted tokens are used to build a vocabulary index mapping tokens to numeric IDs, to numerically represent text suitable for deep learning computations. During the encoding process, these numeric tokens are encoded into vectors representing each token\u2019s meaning. During the decoding process, when LLMs perform generation, tokenizers decode the numeric vectors back into readable text sequences.\nThe process begins with normalization to process lowercase, pruning punctuation and whitespaces, stemming, lemmatization, handling contractions, and \u200cremoving accents. Once the text is cleaned up, the next step is to segment the text by recognizing word and sentence boundaries. Depending on the boundary, tokenizers can be at word, sub-word, or character-level granularity.\nAlthough word and character-based tokenizers are prevalent, there are challenges with these. Word-based tokenizers lead to a large vocabulary size and words not seen during the tokenizer training process cause many out-of-vocabulary tokens. Character-based tokenizers lead to long sequences and less meaningful individual tokens.\nDue to these shortcomings, subword-based tokenizers have gained popularity. The focus of subword tokenization algorithms is to split rare words into smaller, meaningful subwords, based on common character n-grams and patterns. For This technique enables the representation of rare and unseen words via known subwords, resulting in a reduced vocabulary size. During inference, it also handles out-of-vocabulary words effectively reducing vocabulary size, while handling out-of-vocabulary words gracefully during inference.\nPopular subword tokenization algorithms include Byte Pair Encoding (BPE), WordPiece, Unigram, and SentencePiece.\nBPE starts with character vocabulary and iteratively merges frequent adjacent character pairs into new vocabulary terms, achieving text compression with faster inference at decoding time by replacing most common words with single tokens.\nWordPiece is similar to BPE in doing merge operations, however, this leverages the probabilistic nature of the language to merge characters to maximize training data likelihood.\nUnigram starts with a large vocabulary, calculates the probability of tokens, and removes tokens based on a loss function until it reaches the desired vocabulary size.\nSentencePiece learns subword units from raw text based on language modeling objectives and uses Unigram or BPE tokenization algorithms to construct the vocabulary.\nAttention Mechanisms\nAs \u200ctraditional seq-2-seq encoder-decoder language models like Recurrent Neural Networks (RNNs) don\u2019t scale well with the length of the input sequence, the concept of attention was introduced and has proved to be seminal. The attention mechanism enables the decoder to use the most relevant parts of the input sequence weighted by the encoded input sequence, with the most relevant tokens being assigned the highest weight. This concept improves the scaling of input sequence lengths by carefully selecting \u200ctokens by importance.\nThis idea was furthered with self-attention and introduced in 2017 with the transformer model architecture, removing the need for RNNs. Self-attention mechanisms create representations of the input sequence relying on the relationship between different words in the same sequence. By enhancing the information content of an input embedding through the inclusion of input context, self-attention mechanisms play a crucial role in \u200ctransformer architectures.\nComputational steps in self-attention.\nFigure 2. Self-attention architecture ( source: Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch )\nSelf-attention is called scaled-dot product attention because of how it achieves context-aware input representation. Each token in the input sequence is used to project itself into Query (Q), Key (K), and Value (V) sequences using their respective weight matrices. The goal is to compute an attention-weighted version of each input token given all the other input tokens as its context. By computing a scaled dotduct of Q and K matrices with relevant pairs determined by the V matrix getting higher weights, the self-attention mechanism finds a suitable vector for each input token (Q) given all key-value pairs that are other tokens in the sequence.\nSelf-attention further evolved into multi-head attention. The three matrices (Q,K,V) described preceding can be considered as single-head. Multi-head self-attention is when multiple such heads are used. These heads function like multiple kernels in CNNs, attending to different parts of the sequence, focusing on longer-term compared to shorter-term dependencies.\nMultiple heads focusing on different parts of the sequence.\nFigure 3. Multi-head self-attention ( source: Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch )\nAnd finally, the concept of cross-attention came about, where instead of a single input sequence as in the case of self-attention, this involves two different input sequences. In the transformer model architecture, that\u2019s one input sequence from the encoder and another processed by the decoder.\nFlashAttention\nTransformers of a larger size are limited by the memory requirements of the attention layer, which increases in proportion to the length of the sequence. This growth is quadratic. To speed up \u200cattention layer computations and reduce its memory footprint, FlashAttention optimizes the naive implementation bottlenecked by repeated reads and writes from slower GPU high bandwidth memory (HBM).\nFlashAttention uses classical tiling to load blocks of query, key, and value from GPU HBM (its main memory) to SRAM (its fast cache) for attention computation, and then writes back the output to HBM. It also improves upon memory usage, by not storing large attention matrices from the forward pass; instead relies on recomputing the attention matrix during backprop in SRAM. With these optimizations, FlashAttention brings significant speedup (2-4x) for longer sequences.\nSpeedup and memory savings from using FlashAttention.\nFigure 4. FlashAttention fast and memory-efficient exact attention with IO-awareness (s ource: https://github.com/Dao-AILab/flash-attention )\nFurther improved FlashAttention-2 is 2x faster than FlashAttention by adding further optimizations with sequence parallelism, better work partitioning, and reducing non-matmul FLOPs. This newer version also supports multi-query attention as well as grouped-query attention that we describe next.\nMulti-Query Attention (MQA)\nA variant of attention where multiple heads of query attend to the same head of key and value projections. This reduces the KV cache size and hence the memory bandwidth requirements of incremental decoding. The resulting models support faster autoregressive decoding during inference with minor quality degradation than the baseline multi-head attention architecture.\nGroup Query Attention (GQA)\nGrouped-query attention shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.Group-query attention (GQA) is an improvement over MQA to overcome quality degradation issues while retaining the speed-up at inference time. Moreover, models trained using multi-head attention don't have to be retrained from scratch and can employ GQA during inference by up-training existing model checkpoints using only 5% of original training compute. Also, this is a generalization of MQA using an intermediate (more than one, less than number of query heads) number of key-value heads. GQA achieves quality close to baseline multi-head attention with comparable speed to MQA.\nEmbedding techniques\nThe order in which words appear in a sentence is important. This Information is encoded in LLMs using positional encoding by assigning the order of occurrence of each input token to a 2D positional encoding matrix. Each row of the matrix represents an encoded token of the sequence summed with its positional information. This allows the model to differentiate between words with similar meanings but different positions in the sentence and enables encoding of the relative position of words.\nThe original transformer architecture combines absolute positional encoding with word embeddings using sinusoidal functions. However, this approach doesn\u2019t allow extrapolation to longer sequences at inference time than those seen during training. Relative position encoding solved this challenge. In this, the content representations for query and key vectors are combined with positional representations that are trainable, relative to the distance between a query and a key that is clipped beyond a certain distance.\nRoPE\nFigure 5. Grouped-query attention architecture ( source: https://github.com/fkodom/grouped-query-attention-pytorch ) Group-query attention (GQA) is an improvement over MQA to overcome quality degradation issues while retaining the speed-up at inference time. Moreover, models trained using multi-head attention don\u2019t have to be retrained from scratch. They can employ GQA during inference by up-training existing model checkpoints using only 5% of the original training compute. Also, this is a generalization of MQA using an intermediate (more than one, less than number of query heads) number of key-value heads. GQA achieves quality close to baseline multi-head attention with comparable speed to MQA.\nEmbedding techniques\nThe order in which words appear in a sentence is important. This information is encoded in LLMs using positional encoding by assigning the order of occurrence of each input token to a 2D positional encoding matrix. Each row of the matrix represents an encoded token of the sequence summed with its positional information. This allows the model to differentiate between words with similar meanings but different positions in the sentence and enables encoding of the relative position of words.\nThe original transformer architecture combines absolute positional encoding with word embeddings using sinusoidal functions. However, this approach doesn\u2019t allow extrapolation to longer sequences at inference time than those seen during training. Relative position encoding solved this challenge. In this, the content representations for query and key vectors are combined with positional representations that are trainable, relative to the distance between a query and a key that is clipped beyond a certain distance.\nRoPE\nEnhancing linear self-attention with relative position encoding. Absolute position is encoded using a rotation matrix.\nFigure 6. Implementation of Rotary Position Embedding (source: RoFormer: Enhanced Transformer with Rotary Position Embedding )\nSource:\nRotary Position Embeddings (RoPE) combines the concepts of absolute and relative position embeddings. The absolute position is encoded using a rotation matrix. The relative position dependency is incorporated in self-attention formulation and added to the contextual representation in a multiplicative manner. This technique retains the benefit of sequence length flexibility introduced in the transformer\u2019s sinusoidal position embedding while equipping linear self-attention with relative position encoding. It also introduces decaying inter-token dependency with increasing relative distances, enabling extrapolation to longer sequences at inference time.\nAliBi\nTransformer-based LLMs don\u2019t scale well to longer sequences due to the quadratic cost of self-attention, which limits the number of tokens of context. Additionally, the sinusoidal position method introduced in the original transformer architecture doesn\u2019t extrapolate to sequences that are longer than it saw during training. This limits the set of real-world use cases where LLMs can be applied. To overcome this, Attention with Linear Biases (ALiBi) was introduced. This technique does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance.\nTo facilitate efficient extrapolation for much longer sequences than seen at training time, ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Compared to sinusoidal models, this method requires no additional runtime or parameters and incurs a negligible (0\u20130.7%) memory increase. ALiBi\u2019s edge over sinusoidal embeddings is largely explained by its improved avoidance of the early token curse. This method can also achieve further gains by more efficiently exploiting longer context histories.\nTraining transformer networks\nWhile training LLMs, there are several techniques to improve efficiency and optimize resource usage of underlying hardware configurations. Scaling these massively large AI models with billions of parameters and trillions of tokens comes with huge memory capacity requirements.\nTo alleviate this requirement, a few methods such as model parallelism and activation recomputation are popular. Model parallelism partitions the model parameters and optimizer states across multiple GPUs so that each GPU stores a subset of the model parameters. It is further classified into tensor and pipeline parallelism.\nTensor parallelism splits operations across GPUs, often known as intra-layer parallelism focused on parallelizing computation within an operation such as matrix-matrix multiplication. This technique requires additional communication to make sure that the result is correct.\nPipeline parallelism splits model layers across GPUs, also known as inter-layer parallelization, focused on splitting the model by layers into chunks. Each device computes for its chunk and passes intermediate activations to the next stage. This could lead to bubble time where some devices are engaged in computation and others waiting, leading to a waste of computational resources.\nSequence parallelism expands upon tensor-level model parallelism by noticing that the regions of a transformer layer that haven\u2019t previously been parallelized and are independent along the sequence dimension. Splitting these layers along the sequence dimension enables distribution of the compute as well as the activation memory for these regions across the tensor parallel devices. Since activations are distributed and have a smaller memory footprint, more activations can be saved for the backward pass.\nSelective activation recomputation goes hand-in-hand with sequence parallelism. It improves cases where memory constraints force the recomputation of some, but not all, of the activations, by noticing that different activations require different numbers of operations to recompute. Instead of checkpointing and recomputing full transformer layers, it\u2019s possible to checkpoint and recompute only parts of each transformer layer that take up a lot of memory but aren\u2019t computationally expensive to recompute.\nAll techniques add communication or computation overhead. Therefore, finding the configuration that achieves maximum performance and then scaling training with data parallelism is essential for efficient LLM training.\nIn data parallel training, the dataset is split into several shards, where each shard is allocated to a device. This is equivalent to parallelizing the training process along the batch dimension. Each device will hold a full copy of the model replica and train on the dataset shard allocated. After back-propagation, the gradients of the model will be all-reduced so that the model parameters on different devices can stay synchronized.\nA variant of this is called the fully sharded data parallelism (FSDP) technique. It shards model parameters and training data uniformly across data parallel workers, where the computation for each micro-batch of data is local to each GPU worker.\nFSDP offers configurable sharding strategies that can be customized to match the physical interconnect topology of the cluster to handle hardware heterogeneity. It can minimize bubbles to overlap communication with computation aggressively through operation reordering and parameter prefetching. And lastly, FSDP optimizes memory usage by restricting the number of blocks allocated for inflight unsharded parameters. Due to these optimizations, FSDP provides support for significantly larger models with near-linear scalability in terms of TFLOPS.\nQuantization Aware Training\nQuantization is the process in which deep learning models perform all or part of the computation in reduced precision as compared to full precision (floating point) values. This technique enables inference speedups, memory savings, and cost reduction of using deep learning models with minimal accuracy loss.\nQuantization Aware Training (QAT) is a method that takes into account the impact of quantization during the training process. The model is trained with quantization-aware operations that mimic the quantization process during training. Models learn how to perform well in \u200cquantized representations, leading to improved accuracy compared to post-training quantization. The forward pass quantizes weights and activations to low-precision representations. The backward pass computes gradients using full-precision weights and activations. This enables the model to learn parameters that are robust to quantization errors introduced in the forward pass. The result is a trained model that can be quantized post-training with minimal impact on accuracy.\nTrain LLMs today\nThis post covered various model training techniques and when to use them. Check out the post on Mastering LLM Techniques: Customization, to continue your learning journey on the LLM workflow.\nMany of the training methods are supported on NVIDIA NeMo, which provides an accelerated workflow for training with 3D parallelism techniques. It also offers a choice of several customization techniques. It is optimized for at-scale inference of large-scale models for language and image workloads, with multi-GPU and multi-node configurations. Download the NeMo framework today and train LLMs on your preferred on-premises and cloud platforms."}], "https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/": [{"text": "Large language models (LLMs) bring powerful enhancements to text-processing applications but also introduce security risks like prompt injection, information leaks, and LLM reliability issues. Prompt injection allows attackers to control LLM output, while information leaks can expose private data used in training or runtime. LLM reliability can lead to incorrect information output. Best practices for securing LLM-enabled applications include parameterizing plug-ins, sanitizing inputs, and strict user authorization. To prevent information leaks, avoid training LLMs on sensitive data and limit queries. Application-related leaks can occur through improper logging, so track user authorization and ensure completions are logged securely. LLM reliability issues require robust error handling and user education. By considering these risks and implementing mitigation strategies, the security of LLM-enabled applications can be improved. Join events like LLM Developer Day for more insights on defending machine learning models.", "text_components": ["Best Practices for Securing LLM-Enabled Applications\nLarge language models (LLMs) provide a wide range of powerful enhancements to nearly any application that processes text. And yet they also introduce new risks, including:\nPrompt injection, which may enable attackers to control the output of the LLM or LLM-enabled application.\nInformation leaks, which occur when private data used to train the LLM or used at runtime can be inferred or extracted by an attacker.\nLLM reliability, which is a threat when LLMs occasionally produce incorrect information simply by chance.\nThis post walks through these security vulnerabilities in detail and outlines best practices for designing or evaluating a secure LLM-enabled application.", "Prompt injection\nPrompt injection is the most common and well-known LLM attack. It enables attackers to control the output of the LLM, potentially affecting the behavior of downstream queries and plugins connected to the LLM. This can have additional downstream consequences or responses for future users. Prompt injection attacks can be either direct or indirect.", "Direct prompt injection\nIn the case of direct prompt injection attacks, the attacker interacts with the LLM directly, attempting to make the LLM produce a specific response. An example of a direct prompt injection leading to remote code execution is shown in Figure 1. For more details about direct prompt injection, see Securing LLM Systems Against Prompt Injection.\nA text listing, showing a request to an LLM that says \u201cplease solve the following problem\u201d and then lists some python code invoking an os.system call; the result shows a listing of the /etc/password file on the system hosting the LLM, indicating a successful code execution.\nFigure 1. An example of a direct prompt injection attack in which an LLM-powered application is made to execute attacker code", "Indirect prompt injection\nIndirect prompt injection relies on the LLM having access to an external data source that it used when constructing queries to the system. An attacker can insert malicious content into these external data sources, which is ingested by the LLM and inserted into the prompt to produce the response desired by the attacker. For more information about indirect prompt injection, see Mitigating Stored Prompt Injection Attacks Against LLM Applications.", "Trust boundaries\nWith both direct and indirect prompt injection, once the attacker is able to successfully introduce their input into the LLM context, they have significant influence (if not outright control) over the output of the LLM. Because the external sources that LLMs may use can be so difficult to control, and LLM users themselves may be malicious, it\u2019s important to treat any LLM responses as potentially untrustworthy.\nA trust boundary must be established between those responses and any responses that process them. Some practical steps to enforce this separation are listed below.\nParameterize plug-ins. Strictly limit the number of actions that a given plug-in can perform. For example, a plug-in to operate on a user\u2019s email might require a message ID, a specific operation such as \u2018reply\u2019 or \u2018forward\u2019, or only accept free-form text to be inserted into the body of an email.\nSanitize inputs to the plug-in before use. Email body text might have any HTML elements forcibly removed before inserting, for example. Or a forward email operation might require that the recipient be present in the user\u2019s address book.\nRequest explicit user authorization from the user when a plug-in operates on a sensitive system. Any such operation should result in an immediate re-request for explicit authorization from the user to perform the action, as well as provide a summary of the action that is about to be performed.\nRequire specific authorization from the user when multiple plug-ins are called in sequence. This pattern\u2014allowing the output of one plug-in to be fed to another plug-in\u2014can quite rapidly lead to unexpected and even dangerous behavior. Allowing the user to check and verify which plug-ins are being called and what action they will take can help mitigate the issue.\nManage plug-in authorization carefully. Separate any service account from the LLM service account. If user authorization is required for a plug-in\u2019s action, then that authorization should be delegated to the plug-in using a secure method such as OAuth2.", "Information leaks\nInformation leaks from the LLM and LLM-enabled applications create confidentiality risk. If an LLM is either trained or customized on private data, a skilled attacker can perform model inversion or training data extraction attacks to access data that application developers considered private.\nLogging of both prompts and completions can accidentally leak data across permission boundaries by violating service-side role-based access controls for data at rest. If the LLM itself is provided access rights to information, or stores logs, it can often be induced into revealing this data.", "Leaks from the LLM itself\nThe LLM itself can leak information to an attacker in several ways. With prompt extraction attacks, an attacker can use prompt injection techniques to induce the LLM to reveal information contained in its prompt template, such as model instructions, model persona information, or even secrets such as passwords.\nWith model inversion attacks, an attacker can recover some of the data used to train the model. Depending on the details of the attack, these records might be recovered at random, or the attacker may be able to bias the search to particular records they suspect might be present. For instance, they might be able to extract examples of Personal Identifiable Information (PII) used in training the LLM. To learn more, see Algorithms that Remember: Model Inversion Attacks and Data Protection Law.\nFinally, training data membership inference attacks enable an attacker to determine whether a particular bit of information already known to them was likely contained within the training data of the model. For instance, they might be able to determine whether their PII in particular was used to train the LLM.\nFortunately, mitigation for these attacks is relatively straightforward.\nTo avoid the risk of prompt extraction attacks, do not share any information that the current LLM user is not authorized to see in the system prompt template. This may include information retrieved from a retrieval augmented generation (RAG) architecture. Assume that anything included in a prompt template is visible to a sufficiently motivated attacker. In particular, passwords, access tokens, or API keys should never be placed in the prompt, or anywhere else directly accessible to the LLM. Strict isolation of information is the best defense.\nTo reduce the risk of sensitive training data being extracted from the model, the best approach is simply to not train on it. Given enough queries, it is inevitable that the LLM will eventually incorporate some element of that sensitive data into its response. If the model must be able to use or answer questions about sensitive information, a RAG architecture may be a more secure approach.\nIn such an architecture, the LLM is not trained on sensitive documents, but is given access to a document store that is capable of 1) identifying and returning relevant sensitive documents to the LLM to assist in generation, and 2) verifying the authorization of the current user to access those documents.\nWhile this avoids the need to train the LLM on sensitive data to produce acceptable results, it does introduce additional complexity to the application with respect to conveying authorization and tracking document permissions. This must be carefully handled to prevent other confidentiality violations.\nIf the sensitive data has already been trained into the model, then the risk can still be somewhat mitigated by rate-limiting queries, not providing detailed information about probabilities of the LLM completions back to the user, and adding logging and alerting to the application.\nRestricting the query budget to the minimum that is consistent with the functionality of the LLM-enabled application, and not providing any detailed probability information to the final user, make both the inversion and inference attacks extremely difficult and time-consuming to execute.\nWorking with an AI Red Team to evaluate data leakage may be helpful in quantifying risk, setting appropriate rate limits for a particular application, and identifying queries or patterns of queries within user sessions that might indicate an attempt to extract training data that should be alerted on.", "Application-related leaks\nIn addition to LLM-specific attacks, the novelty of LLMs can lead to more basic errors in constructing an LLM-enabled application. Logging of prompts and responses can often lead to service-side information leaks. Either a user who has not been properly educated introduces proprietary or sensitive information into the application, or the LLM provides a response based on sensitive information which is logged without the appropriate access controls.\nIn Figure 2, a user makes a request to a RAG system, which requests documents the user alone is authorized to see on the user\u2019s behalf in order to fulfill the request. Unfortunately, the request and response\u2014which contain information related to the privileged document\u2014are logged in a system with a different access level, thus leaking information.\nA system diagram showing a \u2018user device\u2019 and \u2018document store\u2019 within a green confidentiality boundary; they both have a bidirectional connection to an \u2018inference service\u2019 that is not within a confidentiality boundary (as it is stateless). There is an arrow from the inference service to a \u2018logging and monitoring\u2019 service that is within a red confidentiality boundary.\nFigure 2. An example of data leakage through logging If RAG is being used to improve LLM responses, it\u2019s important to track user authorization with respect to the documents being retrieved, and where the responses are being logged. The LLM should only be able to access documents that the current user is authorized to access. The completions (which by design incorporate some of the information contained in those access-controlled documents) should be logged in a manner such that unauthorized users cannot see the summaries of the sensitive documents.\nIt is therefore extremely important that authentication and authorization mechanisms are executed outside the context of the LLM. If relying on transmitting user context as part of the prompt, a sufficiently skilled attacker can use prompt injection to impersonate other users.\nFinally, the behavior of any plug-ins should be scrutinized to ensure that they do not maintain any state that could lead to cross-user information leakage. For instance, if a search plug-in happens to cache queries, then the speed with which it returns information might allow an attacker to infer what topics other users of the application query most often.", "LLM reliability\nDespite significant improvements in the reliability and accuracy of LLM generations, they are still subject to some amount of random error. How words are sampled randomly from the set of possible next words increases the \u201ccreativity\u201d of LLMs, but also increases the chance that they will produce incorrect results.\nThis has the potential to impact both users, who may act on inaccurate information, and downstream processes, plug-ins, or other computations that may fail or produce additional inaccurate results based on the inaccurate input (Figure 3).\nAn image displaying two turns of interaction with an LLM. The LLM user requested that the LLM tell a joke without the letter \u2018e\u2019. The LLM provides a joke that contains two instances of the letter \u2018e\u2019 and then asserts that the letter \u2018e\u2019 appears zero times in the joke. The user then asks the LLM to count again, and the LLM correctly identifies that the letter \u2018e\u2019 appears twice, but incorrectly states that one of those instances is in the word \u2018salad\u2019.\nFigure 3. An example of an LLM failing to complete a task and correctly answer a related question Downstream processes and plug-ins must be designed with the potential for LLM errors in mind. As with prompt injection, good security design up front, including parameterization of plug-ins, sanitization of inputs, robust error handling, and ensuring that the user authorization is explicitly requested when performing a sensitive operation. All of these approaches help mitigate risks associated with LLMs.\nIn addition, ensure that any LLM orchestration layer can terminate early and inform the user in the event of an invalid request or LLM generation. This helps avoid compounding errors if a sequence of plugins is called. Compounding errors across LLM and plug-in calls is the most common way exploitation vectors are built for these systems. The standard practice of failing closed when bad data is identified should be used here.\nUser education around the scope, reliability, and applicability of the LLM powering the application is important. Users should be reminded that the LLM-enabled application is intended to supplement\u2014not replace\u2014their skills, knowledge, and creativity. The final responsibility for the use of any result, LLM-derived or not, rests with the user.", "Conclusion\nLLMs can provide significant value to both users and the organizations that deploy them. However, as with any new technology, new security risks come along with them. Prompt injection techniques are \u200cthe best known, and any application, including an LLM, should be designed with that risk in mind.\nLess familiar security risks include the various forms of information leaks that LLMs can create, which require careful tracing of data flows and management of authorization. The occasionally unreliable nature of LLMs must also be considered, both from a user reliability standpoint and from an application standpoint.\nMaking your application robust to both natural and malicious errors can increase its security. By considering the risks outlined in this post, and applying the mitigation strategies and best practices described, you can reduce your exposure to these risks and help ensure a successful deployment.\nTo learn more about attacking and defending machine learning models, check out the NVIDIA training at Black Hat Europe 2023.\nRegister for LLM Developer Day, a free virtual event on November 17, and join us for the session, Reinventing the Complete Cybersecurity Stack with AI Language Models."], "document_title": "Best Practices for Securing LLM-Enabled Applications", "document_url": "https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/", "document_date": "2023-11-15T18:00:00", "document_date_modified": "2023-11-30T19:43:31", "document_full_text": "Best Practices for Securing LLM-Enabled Applications\nLarge language models (LLMs) provide a wide range of powerful enhancements to nearly any application that processes text. And yet they also introduce new risks, including:\nPrompt injection, which may enable attackers to control the output of the LLM or LLM-enabled application.\nInformation leaks, which occur when private data used to train the LLM or used at runtime can be inferred or extracted by an attacker.\nLLM reliability, which is a threat when LLMs occasionally produce incorrect information simply by chance.\nThis post walks through these security vulnerabilities in detail and outlines best practices for designing or evaluating a secure LLM-enabled application.\nPrompt injection\nPrompt injection is the most common and well-known LLM attack. It enables attackers to control the output of the LLM, potentially affecting the behavior of downstream queries and plugins connected to the LLM. This can have additional downstream consequences or responses for future users. Prompt injection attacks can be either direct or indirect.\nDirect prompt injection\nIn the case of direct prompt injection attacks, the attacker interacts with the LLM directly, attempting to make the LLM produce a specific response. An example of a direct prompt injection leading to remote code execution is shown in Figure 1. For more details about direct prompt injection, see Securing LLM Systems Against Prompt Injection.\nA text listing, showing a request to an LLM that says \u201cplease solve the following problem\u201d and then lists some python code invoking an os.system call; the result shows a listing of the /etc/password file on the system hosting the LLM, indicating a successful code execution.\nFigure 1. An example of a direct prompt injection attack in which an LLM-powered application is made to execute attacker code\nIndirect prompt injection\nIndirect prompt injection relies on the LLM having access to an external data source that it used when constructing queries to the system. An attacker can insert malicious content into these external data sources, which is ingested by the LLM and inserted into the prompt to produce the response desired by the attacker. For more information about indirect prompt injection, see Mitigating Stored Prompt Injection Attacks Against LLM Applications.\nTrust boundaries\nWith both direct and indirect prompt injection, once the attacker is able to successfully introduce their input into the LLM context, they have significant influence (if not outright control) over the output of the LLM. Because the external sources that LLMs may use can be so difficult to control, and LLM users themselves may be malicious, it\u2019s important to treat any LLM responses as potentially untrustworthy.\nA trust boundary must be established between those responses and any responses that process them. Some practical steps to enforce this separation are listed below.\nParameterize plug-ins. Strictly limit the number of actions that a given plug-in can perform. For example, a plug-in to operate on a user\u2019s email might require a message ID, a specific operation such as \u2018reply\u2019 or \u2018forward\u2019, or only accept free-form text to be inserted into the body of an email.\nSanitize inputs to the plug-in before use. Email body text might have any HTML elements forcibly removed before inserting, for example. Or a forward email operation might require that the recipient be present in the user\u2019s address book.\nRequest explicit user authorization from the user when a plug-in operates on a sensitive system. Any such operation should result in an immediate re-request for explicit authorization from the user to perform the action, as well as provide a summary of the action that is about to be performed.\nRequire specific authorization from the user when multiple plug-ins are called in sequence. This pattern\u2014allowing the output of one plug-in to be fed to another plug-in\u2014can quite rapidly lead to unexpected and even dangerous behavior. Allowing the user to check and verify which plug-ins are being called and what action they will take can help mitigate the issue.\nManage plug-in authorization carefully. Separate any service account from the LLM service account. If user authorization is required for a plug-in\u2019s action, then that authorization should be delegated to the plug-in using a secure method such as OAuth2.\nInformation leaks\nInformation leaks from the LLM and LLM-enabled applications create confidentiality risk. If an LLM is either trained or customized on private data, a skilled attacker can perform model inversion or training data extraction attacks to access data that application developers considered private.\nLogging of both prompts and completions can accidentally leak data across permission boundaries by violating service-side role-based access controls for data at rest. If the LLM itself is provided access rights to information, or stores logs, it can often be induced into revealing this data.\nLeaks from the LLM itself\nThe LLM itself can leak information to an attacker in several ways. With prompt extraction attacks, an attacker can use prompt injection techniques to induce the LLM to reveal information contained in its prompt template, such as model instructions, model persona information, or even secrets such as passwords.\nWith model inversion attacks, an attacker can recover some of the data used to train the model. Depending on the details of the attack, these records might be recovered at random, or the attacker may be able to bias the search to particular records they suspect might be present. For instance, they might be able to extract examples of Personal Identifiable Information (PII) used in training the LLM. To learn more, see Algorithms that Remember: Model Inversion Attacks and Data Protection Law.\nFinally, training data membership inference attacks enable an attacker to determine whether a particular bit of information already known to them was likely contained within the training data of the model. For instance, they might be able to determine whether their PII in particular was used to train the LLM.\nFortunately, mitigation for these attacks is relatively straightforward.\nTo avoid the risk of prompt extraction attacks, do not share any information that the current LLM user is not authorized to see in the system prompt template. This may include information retrieved from a retrieval augmented generation (RAG) architecture. Assume that anything included in a prompt template is visible to a sufficiently motivated attacker. In particular, passwords, access tokens, or API keys should never be placed in the prompt, or anywhere else directly accessible to the LLM. Strict isolation of information is the best defense.\nTo reduce the risk of sensitive training data being extracted from the model, the best approach is simply to not train on it. Given enough queries, it is inevitable that the LLM will eventually incorporate some element of that sensitive data into its response. If the model must be able to use or answer questions about sensitive information, a RAG architecture may be a more secure approach.\nIn such an architecture, the LLM is not trained on sensitive documents, but is given access to a document store that is capable of 1) identifying and returning relevant sensitive documents to the LLM to assist in generation, and 2) verifying the authorization of the current user to access those documents.\nWhile this avoids the need to train the LLM on sensitive data to produce acceptable results, it does introduce additional complexity to the application with respect to conveying authorization and tracking document permissions. This must be carefully handled to prevent other confidentiality violations.\nIf the sensitive data has already been trained into the model, then the risk can still be somewhat mitigated by rate-limiting queries, not providing detailed information about probabilities of the LLM completions back to the user, and adding logging and alerting to the application.\nRestricting the query budget to the minimum that is consistent with the functionality of the LLM-enabled application, and not providing any detailed probability information to the final user, make both the inversion and inference attacks extremely difficult and time-consuming to execute.\nWorking with an AI Red Team to evaluate data leakage may be helpful in quantifying risk, setting appropriate rate limits for a particular application, and identifying queries or patterns of queries within user sessions that might indicate an attempt to extract training data that should be alerted on.\nApplication-related leaks\nIn addition to LLM-specific attacks, the novelty of LLMs can lead to more basic errors in constructing an LLM-enabled application. Logging of prompts and responses can often lead to service-side information leaks. Either a user who has not been properly educated introduces proprietary or sensitive information into the application, or the LLM provides a response based on sensitive information which is logged without the appropriate access controls.\nIn Figure 2, a user makes a request to a RAG system, which requests documents the user alone is authorized to see on the user\u2019s behalf in order to fulfill the request. Unfortunately, the request and response\u2014which contain information related to the privileged document\u2014are logged in a system with a different access level, thus leaking information.\nA system diagram showing a \u2018user device\u2019 and \u2018document store\u2019 within a green confidentiality boundary; they both have a bidirectional connection to an \u2018inference service\u2019 that is not within a confidentiality boundary (as it is stateless). There is an arrow from the inference service to a \u2018logging and monitoring\u2019 service that is within a red confidentiality boundary.\nFigure 2. An example of data leakage through logging If RAG is being used to improve LLM responses, it\u2019s important to track user authorization with respect to the documents being retrieved, and where the responses are being logged. The LLM should only be able to access documents that the current user is authorized to access. The completions (which by design incorporate some of the information contained in those access-controlled documents) should be logged in a manner such that unauthorized users cannot see the summaries of the sensitive documents.\nIt is therefore extremely important that authentication and authorization mechanisms are executed outside the context of the LLM. If relying on transmitting user context as part of the prompt, a sufficiently skilled attacker can use prompt injection to impersonate other users.\nFinally, the behavior of any plug-ins should be scrutinized to ensure that they do not maintain any state that could lead to cross-user information leakage. For instance, if a search plug-in happens to cache queries, then the speed with which it returns information might allow an attacker to infer what topics other users of the application query most often.\nLLM reliability\nDespite significant improvements in the reliability and accuracy of LLM generations, they are still subject to some amount of random error. How words are sampled randomly from the set of possible next words increases the \u201ccreativity\u201d of LLMs, but also increases the chance that they will produce incorrect results.\nThis has the potential to impact both users, who may act on inaccurate information, and downstream processes, plug-ins, or other computations that may fail or produce additional inaccurate results based on the inaccurate input (Figure 3).\nAn image displaying two turns of interaction with an LLM. The LLM user requested that the LLM tell a joke without the letter \u2018e\u2019. The LLM provides a joke that contains two instances of the letter \u2018e\u2019 and then asserts that the letter \u2018e\u2019 appears zero times in the joke. The user then asks the LLM to count again, and the LLM correctly identifies that the letter \u2018e\u2019 appears twice, but incorrectly states that one of those instances is in the word \u2018salad\u2019.\nFigure 3. An example of an LLM failing to complete a task and correctly answer a related question Downstream processes and plug-ins must be designed with the potential for LLM errors in mind. As with prompt injection, good security design up front, including parameterization of plug-ins, sanitization of inputs, robust error handling, and ensuring that the user authorization is explicitly requested when performing a sensitive operation. All of these approaches help mitigate risks associated with LLMs.\nIn addition, ensure that any LLM orchestration layer can terminate early and inform the user in the event of an invalid request or LLM generation. This helps avoid compounding errors if a sequence of plugins is called. Compounding errors across LLM and plug-in calls is the most common way exploitation vectors are built for these systems. The standard practice of failing closed when bad data is identified should be used here.\nUser education around the scope, reliability, and applicability of the LLM powering the application is important. Users should be reminded that the LLM-enabled application is intended to supplement\u2014not replace\u2014their skills, knowledge, and creativity. The final responsibility for the use of any result, LLM-derived or not, rests with the user.\nConclusion\nLLMs can provide significant value to both users and the organizations that deploy them. However, as with any new technology, new security risks come along with them. Prompt injection techniques are \u200cthe best known, and any application, including an LLM, should be designed with that risk in mind.\nLess familiar security risks include the various forms of information leaks that LLMs can create, which require careful tracing of data flows and management of authorization. The occasionally unreliable nature of LLMs must also be considered, both from a user reliability standpoint and from an application standpoint.\nMaking your application robust to both natural and malicious errors can increase its security. By considering the risks outlined in this post, and applying the mitigation strategies and best practices described, you can reduce your exposure to these risks and help ensure a successful deployment.\nTo learn more about attacking and defending machine learning models, check out the NVIDIA training at Black Hat Europe 2023.\nRegister for LLM Developer Day, a free virtual event on November 17, and join us for the session, Reinventing the Complete Cybersecurity Stack with AI Language Models."}], "https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/": [{"text": "Businesses are increasingly relying on data and AI to innovate and stay competitive, leading to the emergence of machine learning operations (MLOps) to manage code, data, and models effectively. With the rise of large language models (LLMs) and generative AI, new areas like GenAIOps and LLMOps have been developed to address the challenges of developing and managing these technologies in production. GenAIOps extends MLOps to develop generative AI solutions, while LLMOps focuses specifically on LLM-based solutions. RAGOps, a subclass of LLMOps, enhances the capabilities of general-purpose LLMs through retrieval augmented generation. These specialized operations offer benefits such as faster time-to-market, increased innovation, risk mitigation, streamlined collaboration, lean operations, and reproducibility. Incorporating GenAIOps into business strategies can enhance user experiences, unlock new revenue streams, and set ethical standards in the rapidly evolving world of AI. Organizations can get started with state-of-the-art generative AI models and LLM development on NVIDIA platforms or through expert-led training courses. By mastering GenAIOps, businesses can shape the trajectory of the generative AI revolution.", "text_components": ["Mastering LLM Techniques: LLMOps\nBusinesses rely more than ever on data and AI to innovate, offer value to customers, and stay competitive. The adoption of machine learning (ML), created a need for tools, processes, and organizational principles to manage code, data, and models that work reliably, cost-effectively, and at scale. This is broadly known as machine learning operations ( MLOps ).\nThe world is venturing rapidly into a new generative AI era powered by foundation models and large language models (LLMs) in particular. The release of ChatGPT further accelerated this transition.\nNew and specialized areas of generative AI operations (GenAIOps) and large language model operations (LLMOps) emerged as an evolution of MLOps for addressing the challenges of developing and managing generative AI and LLM-powered apps in production.\nIn this post, we outline the generative AI app development journey, define the concepts of GenAIOps and LLMOps, and compare them with MLOps. We also explain why mastering operations becomes paramount for business leaders executing an enterprise-wide AI transformation.", "Building modern generative AI apps for enterprises\nThe journey towards a modern generative AI app starts from a foundation model, which goes through a pretraining stage to learn the foundational knowledge about the world and gain emergent capabilities. The next step is aligning the model with human preferences, behavior, and values using a curated dataset of human-generated prompts and responses. This gives the model precise instruction-following capabilities. Users can choose to train their own foundation model or use a pretrained model.\nFor example, various foundation models such as NVIDIA Nemotron-3 and community models like Llama are available through NVIDIA AI Foundations. These are all enhanced with NVIDIA proprietary algorithmic and system optimizations, security, and enterprise-grade support covered by NVIDIA AI Enterprise.\nDiagram of lifecycle of model customization techniques and retrieval augmented generation.\nFigure 1. A lifecycle of a generative AI application powered by a customized foundation model and retrieval augmented generation\nNext, comes the customization stage. A foundation model is combined with a task-specific prompt or fine-tuned on a curated enterprise dataset. The knowledge of a foundation model is limited to the pretraining and fine-tuning data, becoming outdated over time unless the model is continuously retrained, which can be costly.\nA retrieval augmented generation ( RAG ) workflow is used to maintain freshness and keep the model grounded with external knowledge during query time. This is one of the most critical steps in the generative AI app development lifecycle and when a model learns unique relationships hidden in enterprise data.\nAfter customization, the model is ready for real-world use either independently or as a part of a chain, which combines multiple foundation models and APIs to deliver the end-to-end application logic. At this point, it is crucial to test the complete AI system for accuracy, speed, and vulnerabilities, and add guardrails to ensure the model outputs are accurate, safe, and secure.\nFinally, the feedback loop is closed. Users interact with an app through the user interface or collect data automatically using system instrumentation. This information can be used to update the model and the A/B test continuously, increasing its value to the customers.\nAn enterprise typically has many generative AI apps tailored to different use cases, business functions, and workflows. This AI portfolio requires continuous oversight and risk management to ensure smooth operation, ethical use, and prompt alerts for addressing incidents, biases, or regressions.\nGenAIOps accelerates this journey from research to production through automation. It optimizes development and operational costs, improves the quality of models, adds robustness to the model evaluation process, and guarantees sustained operations at scale.", "Understanding GenAIOps, LLMOps, and RAGOps\nThere are several terms associated with generative AI. We outline the definitions in the following section.\nAn illustration showing the nested relationship from MLOps, GenAIOps, LLMOps, and RAGOps.\nFigure 2. A hierarchy of AI types and associated Ops organized by the level of specialization\nThink of AI as a series of nested layers. At the outermost layer, ML covers intelligent automation, where the logic of the program is not explicitly defined but learned from data. As we dive deeper, we encounter specialized AI types, like those built on LLMs or RAGs. Similarly, there are nested concepts enabling reproducibility, reuse, scalability, reliability, and efficiency.\nEach one builds on the previous, adding, or refining capabilities\u2013from foundational MLOps to the newly developed RAGOps lifecycle:\nMLOps is the overarching concept covering the core tools, processes, and best practices for end-to-end machine learning system development and operations in production.\nGenAIOps extends MLOps to develop and operationalize generative AI solutions. The distinct characteristic of GenAIOps is the management of and interaction with a foundation model.\nLLMOps is a distinct type of GenAIOps focused specifically on developing and productionizing LLM-based solutions.\nRAGOps is a subclass of LLMOps focusing on the delivery and operation of RAGs, which can also be considered the ultimate reference architecture for generative AI and LLMs, driving massive adoption.\nGenAIOps and LLMOps span the entire AI lifecycle. This includes foundation model pretraining, model alignment through supervised fine-tuning, and reinforcement learning from human feedback (RLHF), customization to a specific use case coupled with pre/post-processing logic, chaining with other foundation models, APIs, and guardrails. RAGOps scope doesn\u2019t include pretraining and assumes that a foundation model is provided as an input into the RAG lifecycle.\nGenAIOps, LLMOps, and RAGOps are not only about tools or platform capabilities to enable AI development. They also cover methodologies for setting goals and KPIs, organizing teams, measuring progress, and continuously improving operational processes.", "Extending MLOps for generative AI and LLMs\nWith the key concepts defined, we can focus on the nuances differentiating one from the other.\nNew GenAIOps-specific capabilities, including Synthetic Data Management, Embedding Management, Agent / Chain Management, Guardrails, and Prompt Management.\nFigure 3. An end-2-end machine learning lifecycle showcasing core MLOps (gray) and GenAIOps capabilities (green)", "MLOps\nMLOps lays the foundation for a structured approach to the development, training, evaluation, optimization, deployment, inference, and monitoring of machine learning models in production.\nThe key MLOps ideas and capabilities are relevant for generative AI, including the following.\nInfrastructure management: request, provision, and configure compute, storage, and networking resources to access the underlying hardware programmatically.\nData management: collect, ingest, store, process, and label data for training and evaluation. Configure role-based access control; dataset search, browsing, and exploration; data provenance tracking, data logging, dataset versioning, metadata indexing, data quality validation, dataset cards, and dashboards for data visualization.\nWorkflow and pipeline management: work with cloud resources or a local workstation; connect data preparation, model training, model evaluation, model optimization, and model deployment steps into an end-to-end automated and scalable workflow combining data and compute.\nModel management: train, evaluate, and optimize models for production; store and version models along with their model cards in a centralized model registry; assess model risks, and ensure compliance with standards.\nExperiment management and observability: track and compare different machine learning model experiments, including changes in training data, models, and hyperparameters. Automatically search the space of possible model architectures and hyperparameters for a given model architecture; analyze model performance during inference, monitor model inputs and outputs for concept drift.\nInteractive development: manage development environments and integrate with external version control systems, desktop IDEs, and other standalone developer tools, making it easier for teams to prototype, launch jobs, and collaborate on projects.", "GenAIOps\nGenAIOps encompasses MLOps, code development operations (DevOps), data operations (DataOps), and model operations (ModelOps), for all generative AI workloads from language, to image, to multimodal. Data curation and model training, customization, evaluation, optimization, deployment, and risk management must be rethought for generative AI.\nNew emerging GenAIOps capabilities include:\nSynthetic data management: extend data management with a new native generative AI capability. Generate synthetic training data through domain randomization to increase transfer learning capabilities. Declaratively define and generate edge cases to evaluate, validate, and certify model accuracy and robustness.\nEmbedding management: represent data samples of any modality as dense multi-dimensional embedding vectors; generate, store, and version embeddings in a vector database. Visualize embeddings for improvised exploration. Find relevant contextual information through vector similarity search for RAGs, data labeling, or data curation as a part of the active learning loop. For GenAIOps, using embeddings and vector databases replaces feature management and feature stores relevant to MLOps.\nAgent/chain management: define complex multi-step application logic. Combine multiple foundation models and APIs together, and augment a foundation model with external memory and knowledge, following the RAG pattern. Debug, test, and trace chains with non-deterministic outputs or complex planning strategies, visualize, and inspect the execution flow of a multi-step chain in real-time and offline. Agent/chain management is valuable throughout the entire generative AI lifecycle as a key part of the inference pipeline. It serves as an extension of the workflow/pipeline management for MLOps.\nGuardrails: intercept adversarial or unsupported inputs before sending them to a foundation model. Make sure that model outputs are accurate, relevant, safe, and secure. Maintain and check the state of the conversation and active context, detect intents, and decide actions while enforcing content policies. Guardrails build upon rule-based pre/post-processing of AI inputs/outputs covered under model management.\nPrompt management: create, store, compare, optimize, and version prompts. Analyze inputs and outputs and manage test cases during prompt engineering. Create parameterized prompt templates, select optimal inference-time hyperparameters and system prompts serving as the starting point during the interaction of a user with an app; and adjust prompts for each foundation model. Prompt management, with its distinct capabilities, is a logical extension of experiment management for generative AI.", "LLMOps\nLLMOps is a subset of the broader GenAIOps paradigm, focused on operationalizing transformer-based networks for language use cases in production applications. Language is a foundational modality that can be combined with other modalities to guide AI system behavior, for example, NVIDIA Picasso is a multimodal system combining text and image modalities for visual content production.\nIn this case, text drives the control loop of an AI system with other data modalities and foundation models being used as plug-ins for specific tasks. The natural language interface expands the user and developer bases and decreases the AI adoption barrier. The set of operations encompassed under LLMOps includes prompt management, agent management, and RAGOps.", "Driving generative AI adoption with RAGOps\nRAG is a workflow designed to enhance the capabilities of general-purpose LLMs. Incorporating information from proprietary datasets during query time and grounding generated answers on facts guarantees factual correctness. While traditional models can be fine-tuned for tasks like sentiment analysis without needing external knowledge, RAG is tailored for tasks that benefit from accessing external knowledge sources, like question answering.\nRAG integrates an information retrieval component with a text generator. This process consists of two steps:\nDocument retrieval and ingestion \u2014the process of ingesting documents and chunking the text with an embedding model to convert them into vectors and store them in a vector database.\nUser query and response generation \u2014a user query is converted to the embedding space at a query time along with the embedding model, which in turn is used to search against the vector database for the closest matching chunks and documents. The original user query and the top documents are fed into a customized generator LLM, which generates a final response and renders it back to the user.\nIt also offers the advantage of updating its knowledge without the need for comprehensive retraining. This approach ensures reliability in generated responses and addresses the issue of \u201challucination\u201d in outputs.\nDiagram: User query gets transformed into an embedding vector and then it is matched to document chunks, also represented as embeddings, with the help of the vector database.\nFigure 4. Retrieval Augmented Generation (RAG) sequence diagram\nRAGOps is an extension of LLMOps. This involves managing documents and databases, both in the traditional sense, as well as in the vectorized formats, alongside embedding and retrieval models. RAGOps distills the complexities of generative AI app development into one pattern. Thus, it enables more developers to build new powerful applications and decreases the AI adoption barrier.", "GenAIOps offers many business benefits\nAs researchers and developers master GenAIOps to expand beyond DevOps, DataOps, and ModelOps, there are many business benefits. These include the following.\nFaster time-to-market: Automation and acceleration of end-to-end generative AI workflows lead to shorter AI product iteration cycles, making the organization more dynamic and adaptable to new challenges.\nHigher yield and innovation: Simplifying the AI system development process and increasing the level of abstraction, enables GenAIOps more experiments, and higher enterprise application developer engagement, optimizing AI product releases.\nRisk mitigation: Foundation models hold the potential to revolutionize industries but also run the risk of amplifying inherent biases or inaccuracies from their training data. The defects of one foundation model propagate to all downstream models and chains. GenAIOps ensures that there is a proactive stance on minimizing these defects and addressing ethical challenges head-on.\nStreamlined collaboration: GenAIOps enables smooth handoffs across teams, from data engineering to research to product engineering inside one project, and facilitates artifacts and knowledge sharing across projects. It requires stringent operational rigor, standardization, and collaborative tooling to keep multiple teams in sync.\nLean operations: GenAIOps helps reduce waste through workload optimization, automation of routine tasks, and availability of specialized tools for every stage in the AI lifecycle. This leads to higher productivity and lower TCO (TCO).\nReproducibility: GenAIOps helps maintain a record of code, data, models, and configurations ensuring that a successful experiment run can be reproduced on-demand. This becomes especially critical for regulated industries, where reproducibility is no longer a feature but a hard requirement to be in business.", "The transformational potential of generative AI\nIncorporating GenAIOps into the organizational fabric is not just a technical upgrade. It is a strategic move with long-term positive effects for both customers and end users across the enterprise.\nEnhancing user experiences: GenAIOps provides optimal performance of AI apps in production. Businesses can offer enhanced user experiences. Be it through chatbots, autonomous agents, content generators, or data analysis tools.\nUnlocking new revenue streams: With tailored applications of generative AI, facilitated by GenAIOps, businesses can venture into previously uncharted territories, unlocking new revenue streams and diversifying their offerings.\nLeading ethical standards: In a world where brand image is closely tied to ethical considerations, businesses that proactively address AI\u2019s potential pitfalls, guided by GenAIOps, can emerge as industry leaders, setting benchmarks for others to follow.\nThe world of AI is dynamic, rapidly evolving, and brimming with potential. Foundation models, with their unparalleled capabilities in understanding and generating text, images, molecules, and music, are at the forefront of this revolution.\nWhen examining the evolution of AI operations, from MLOps to GenAIOps, LLMOps, and RAGOps, businesses must be flexible, advance, and prioritize precision in operations. With a comprehensive understanding and strategic application of GenAIOps, organizations stand poised to shape the trajectory of the generative AI revolution.", "How to get started\nTry state-of-the-art generative AI models running on an optimized NVIDIA accelerated hardware/software stack from your browser using NVIDIA AI Foundation Models.\nGet started with LLM development on NVIDIA NeMo, an end-to-end, cloud-native framework for building, customizing, and deploying generative AI models anywhere.\nOr, begin your learning journey with NVIDIA training. Our expert-led courses and workshops provide learners with the knowledge and hands-on experience necessary to unlock the full potential of NVIDIA solutions. For generative AI and LLMs, check out our focused Gen AI/LLM learning path."], "document_title": "Mastering LLM Techniques: LLMOps", "document_url": "https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/", "document_date": "2023-11-15T18:00:00", "document_date_modified": "2023-12-08T18:53:36", "document_full_text": "Mastering LLM Techniques: LLMOps\nBusinesses rely more than ever on data and AI to innovate, offer value to customers, and stay competitive. The adoption of machine learning (ML), created a need for tools, processes, and organizational principles to manage code, data, and models that work reliably, cost-effectively, and at scale. This is broadly known as machine learning operations ( MLOps ).\nThe world is venturing rapidly into a new generative AI era powered by foundation models and large language models (LLMs) in particular. The release of ChatGPT further accelerated this transition.\nNew and specialized areas of generative AI operations (GenAIOps) and large language model operations (LLMOps) emerged as an evolution of MLOps for addressing the challenges of developing and managing generative AI and LLM-powered apps in production.\nIn this post, we outline the generative AI app development journey, define the concepts of GenAIOps and LLMOps, and compare them with MLOps. We also explain why mastering operations becomes paramount for business leaders executing an enterprise-wide AI transformation.\nBuilding modern generative AI apps for enterprises\nThe journey towards a modern generative AI app starts from a foundation model, which goes through a pretraining stage to learn the foundational knowledge about the world and gain emergent capabilities. The next step is aligning the model with human preferences, behavior, and values using a curated dataset of human-generated prompts and responses. This gives the model precise instruction-following capabilities. Users can choose to train their own foundation model or use a pretrained model.\nFor example, various foundation models such as NVIDIA Nemotron-3 and community models like Llama are available through NVIDIA AI Foundations. These are all enhanced with NVIDIA proprietary algorithmic and system optimizations, security, and enterprise-grade support covered by NVIDIA AI Enterprise.\nDiagram of lifecycle of model customization techniques and retrieval augmented generation.\nFigure 1. A lifecycle of a generative AI application powered by a customized foundation model and retrieval augmented generation\nNext, comes the customization stage. A foundation model is combined with a task-specific prompt or fine-tuned on a curated enterprise dataset. The knowledge of a foundation model is limited to the pretraining and fine-tuning data, becoming outdated over time unless the model is continuously retrained, which can be costly.\nA retrieval augmented generation ( RAG ) workflow is used to maintain freshness and keep the model grounded with external knowledge during query time. This is one of the most critical steps in the generative AI app development lifecycle and when a model learns unique relationships hidden in enterprise data.\nAfter customization, the model is ready for real-world use either independently or as a part of a chain, which combines multiple foundation models and APIs to deliver the end-to-end application logic. At this point, it is crucial to test the complete AI system for accuracy, speed, and vulnerabilities, and add guardrails to ensure the model outputs are accurate, safe, and secure.\nFinally, the feedback loop is closed. Users interact with an app through the user interface or collect data automatically using system instrumentation. This information can be used to update the model and the A/B test continuously, increasing its value to the customers.\nAn enterprise typically has many generative AI apps tailored to different use cases, business functions, and workflows. This AI portfolio requires continuous oversight and risk management to ensure smooth operation, ethical use, and prompt alerts for addressing incidents, biases, or regressions.\nGenAIOps accelerates this journey from research to production through automation. It optimizes development and operational costs, improves the quality of models, adds robustness to the model evaluation process, and guarantees sustained operations at scale.\nUnderstanding GenAIOps, LLMOps, and RAGOps\nThere are several terms associated with generative AI. We outline the definitions in the following section.\nAn illustration showing the nested relationship from MLOps, GenAIOps, LLMOps, and RAGOps.\nFigure 2. A hierarchy of AI types and associated Ops organized by the level of specialization\nThink of AI as a series of nested layers. At the outermost layer, ML covers intelligent automation, where the logic of the program is not explicitly defined but learned from data. As we dive deeper, we encounter specialized AI types, like those built on LLMs or RAGs. Similarly, there are nested concepts enabling reproducibility, reuse, scalability, reliability, and efficiency.\nEach one builds on the previous, adding, or refining capabilities\u2013from foundational MLOps to the newly developed RAGOps lifecycle:\nMLOps is the overarching concept covering the core tools, processes, and best practices for end-to-end machine learning system development and operations in production.\nGenAIOps extends MLOps to develop and operationalize generative AI solutions. The distinct characteristic of GenAIOps is the management of and interaction with a foundation model.\nLLMOps is a distinct type of GenAIOps focused specifically on developing and productionizing LLM-based solutions.\nRAGOps is a subclass of LLMOps focusing on the delivery and operation of RAGs, which can also be considered the ultimate reference architecture for generative AI and LLMs, driving massive adoption.\nGenAIOps and LLMOps span the entire AI lifecycle. This includes foundation model pretraining, model alignment through supervised fine-tuning, and reinforcement learning from human feedback (RLHF), customization to a specific use case coupled with pre/post-processing logic, chaining with other foundation models, APIs, and guardrails. RAGOps scope doesn\u2019t include pretraining and assumes that a foundation model is provided as an input into the RAG lifecycle.\nGenAIOps, LLMOps, and RAGOps are not only about tools or platform capabilities to enable AI development. They also cover methodologies for setting goals and KPIs, organizing teams, measuring progress, and continuously improving operational processes.\nExtending MLOps for generative AI and LLMs\nWith the key concepts defined, we can focus on the nuances differentiating one from the other.\nNew GenAIOps-specific capabilities, including Synthetic Data Management, Embedding Management, Agent / Chain Management, Guardrails, and Prompt Management.\nFigure 3. An end-2-end machine learning lifecycle showcasing core MLOps (gray) and GenAIOps capabilities (green)\nMLOps\nMLOps lays the foundation for a structured approach to the development, training, evaluation, optimization, deployment, inference, and monitoring of machine learning models in production.\nThe key MLOps ideas and capabilities are relevant for generative AI, including the following.\nInfrastructure management: request, provision, and configure compute, storage, and networking resources to access the underlying hardware programmatically.\nData management: collect, ingest, store, process, and label data for training and evaluation. Configure role-based access control; dataset search, browsing, and exploration; data provenance tracking, data logging, dataset versioning, metadata indexing, data quality validation, dataset cards, and dashboards for data visualization.\nWorkflow and pipeline management: work with cloud resources or a local workstation; connect data preparation, model training, model evaluation, model optimization, and model deployment steps into an end-to-end automated and scalable workflow combining data and compute.\nModel management: train, evaluate, and optimize models for production; store and version models along with their model cards in a centralized model registry; assess model risks, and ensure compliance with standards.\nExperiment management and observability: track and compare different machine learning model experiments, including changes in training data, models, and hyperparameters. Automatically search the space of possible model architectures and hyperparameters for a given model architecture; analyze model performance during inference, monitor model inputs and outputs for concept drift.\nInteractive development: manage development environments and integrate with external version control systems, desktop IDEs, and other standalone developer tools, making it easier for teams to prototype, launch jobs, and collaborate on projects.\nGenAIOps\nGenAIOps encompasses MLOps, code development operations (DevOps), data operations (DataOps), and model operations (ModelOps), for all generative AI workloads from language, to image, to multimodal. Data curation and model training, customization, evaluation, optimization, deployment, and risk management must be rethought for generative AI.\nNew emerging GenAIOps capabilities include:\nSynthetic data management: extend data management with a new native generative AI capability. Generate synthetic training data through domain randomization to increase transfer learning capabilities. Declaratively define and generate edge cases to evaluate, validate, and certify model accuracy and robustness.\nEmbedding management: represent data samples of any modality as dense multi-dimensional embedding vectors; generate, store, and version embeddings in a vector database. Visualize embeddings for improvised exploration. Find relevant contextual information through vector similarity search for RAGs, data labeling, or data curation as a part of the active learning loop. For GenAIOps, using embeddings and vector databases replaces feature management and feature stores relevant to MLOps.\nAgent/chain management: define complex multi-step application logic. Combine multiple foundation models and APIs together, and augment a foundation model with external memory and knowledge, following the RAG pattern. Debug, test, and trace chains with non-deterministic outputs or complex planning strategies, visualize, and inspect the execution flow of a multi-step chain in real-time and offline. Agent/chain management is valuable throughout the entire generative AI lifecycle as a key part of the inference pipeline. It serves as an extension of the workflow/pipeline management for MLOps.\nGuardrails: intercept adversarial or unsupported inputs before sending them to a foundation model. Make sure that model outputs are accurate, relevant, safe, and secure. Maintain and check the state of the conversation and active context, detect intents, and decide actions while enforcing content policies. Guardrails build upon rule-based pre/post-processing of AI inputs/outputs covered under model management.\nPrompt management: create, store, compare, optimize, and version prompts. Analyze inputs and outputs and manage test cases during prompt engineering. Create parameterized prompt templates, select optimal inference-time hyperparameters and system prompts serving as the starting point during the interaction of a user with an app; and adjust prompts for each foundation model. Prompt management, with its distinct capabilities, is a logical extension of experiment management for generative AI.\nLLMOps\nLLMOps is a subset of the broader GenAIOps paradigm, focused on operationalizing transformer-based networks for language use cases in production applications. Language is a foundational modality that can be combined with other modalities to guide AI system behavior, for example, NVIDIA Picasso is a multimodal system combining text and image modalities for visual content production.\nIn this case, text drives the control loop of an AI system with other data modalities and foundation models being used as plug-ins for specific tasks. The natural language interface expands the user and developer bases and decreases the AI adoption barrier. The set of operations encompassed under LLMOps includes prompt management, agent management, and RAGOps.\nDriving generative AI adoption with RAGOps\nRAG is a workflow designed to enhance the capabilities of general-purpose LLMs. Incorporating information from proprietary datasets during query time and grounding generated answers on facts guarantees factual correctness. While traditional models can be fine-tuned for tasks like sentiment analysis without needing external knowledge, RAG is tailored for tasks that benefit from accessing external knowledge sources, like question answering.\nRAG integrates an information retrieval component with a text generator. This process consists of two steps:\nDocument retrieval and ingestion \u2014the process of ingesting documents and chunking the text with an embedding model to convert them into vectors and store them in a vector database.\nUser query and response generation \u2014a user query is converted to the embedding space at a query time along with the embedding model, which in turn is used to search against the vector database for the closest matching chunks and documents. The original user query and the top documents are fed into a customized generator LLM, which generates a final response and renders it back to the user.\nIt also offers the advantage of updating its knowledge without the need for comprehensive retraining. This approach ensures reliability in generated responses and addresses the issue of \u201challucination\u201d in outputs.\nDiagram: User query gets transformed into an embedding vector and then it is matched to document chunks, also represented as embeddings, with the help of the vector database.\nFigure 4. Retrieval Augmented Generation (RAG) sequence diagram\nRAGOps is an extension of LLMOps. This involves managing documents and databases, both in the traditional sense, as well as in the vectorized formats, alongside embedding and retrieval models. RAGOps distills the complexities of generative AI app development into one pattern. Thus, it enables more developers to build new powerful applications and decreases the AI adoption barrier.\nGenAIOps offers many business benefits\nAs researchers and developers master GenAIOps to expand beyond DevOps, DataOps, and ModelOps, there are many business benefits. These include the following.\nFaster time-to-market: Automation and acceleration of end-to-end generative AI workflows lead to shorter AI product iteration cycles, making the organization more dynamic and adaptable to new challenges.\nHigher yield and innovation: Simplifying the AI system development process and increasing the level of abstraction, enables GenAIOps more experiments, and higher enterprise application developer engagement, optimizing AI product releases.\nRisk mitigation: Foundation models hold the potential to revolutionize industries but also run the risk of amplifying inherent biases or inaccuracies from their training data. The defects of one foundation model propagate to all downstream models and chains. GenAIOps ensures that there is a proactive stance on minimizing these defects and addressing ethical challenges head-on.\nStreamlined collaboration: GenAIOps enables smooth handoffs across teams, from data engineering to research to product engineering inside one project, and facilitates artifacts and knowledge sharing across projects. It requires stringent operational rigor, standardization, and collaborative tooling to keep multiple teams in sync.\nLean operations: GenAIOps helps reduce waste through workload optimization, automation of routine tasks, and availability of specialized tools for every stage in the AI lifecycle. This leads to higher productivity and lower TCO (TCO).\nReproducibility: GenAIOps helps maintain a record of code, data, models, and configurations ensuring that a successful experiment run can be reproduced on-demand. This becomes especially critical for regulated industries, where reproducibility is no longer a feature but a hard requirement to be in business.\nThe transformational potential of generative AI\nIncorporating GenAIOps into the organizational fabric is not just a technical upgrade. It is a strategic move with long-term positive effects for both customers and end users across the enterprise.\nEnhancing user experiences: GenAIOps provides optimal performance of AI apps in production. Businesses can offer enhanced user experiences. Be it through chatbots, autonomous agents, content generators, or data analysis tools.\nUnlocking new revenue streams: With tailored applications of generative AI, facilitated by GenAIOps, businesses can venture into previously uncharted territories, unlocking new revenue streams and diversifying their offerings.\nLeading ethical standards: In a world where brand image is closely tied to ethical considerations, businesses that proactively address AI\u2019s potential pitfalls, guided by GenAIOps, can emerge as industry leaders, setting benchmarks for others to follow.\nThe world of AI is dynamic, rapidly evolving, and brimming with potential. Foundation models, with their unparalleled capabilities in understanding and generating text, images, molecules, and music, are at the forefront of this revolution.\nWhen examining the evolution of AI operations, from MLOps to GenAIOps, LLMOps, and RAGOps, businesses must be flexible, advance, and prioritize precision in operations. With a comprehensive understanding and strategic application of GenAIOps, organizations stand poised to shape the trajectory of the generative AI revolution.\nHow to get started\nTry state-of-the-art generative AI models running on an optimized NVIDIA accelerated hardware/software stack from your browser using NVIDIA AI Foundation Models.\nGet started with LLM development on NVIDIA NeMo, an end-to-end, cloud-native framework for building, customizing, and deploying generative AI models anywhere.\nOr, begin your learning journey with NVIDIA training. Our expert-led courses and workshops provide learners with the knowledge and hands-on experience necessary to unlock the full potential of NVIDIA solutions. For generative AI and LLMs, check out our focused Gen AI/LLM learning path."}], "https://developer.nvidia.com/blog/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/": [{"text": "The article discusses the deployment of large language models (LLMs) at the edge using the NVIDIA IGX Orin Developer Kit. It addresses the challenges of running advanced LLMs at the edge due to computational requirements and accessibility issues. The NVIDIA IGX Orin platform, coupled with the RTX A6000 GPU, provides an industrial-grade edge AI platform for industrial and medical environments. The article highlights the potential applications of open-source LLMs for real-time streaming, such as medical chatbots and surgical case note generation. It also discusses model quantization for optimal accuracy and memory usage, as well as the opportunities presented by running state-of-the-art LLMs on commodity hardware. The article emphasizes the importance of leveraging open-source LLMs and provides resources for developers to get started deploying LLMs at the edge with the IGX Orin Developer Kit. Overall, the article showcases the potential of integrating LLMs into edge AI applications using the NVIDIA IGX Orin platform.", "text_components": ["Deploy Large Language Models at the Edge with NVIDIA IGX Orin Developer Kit\nAs large language models (LLMs) become more powerful and techniques for reducing their computational requirements mature, two compelling questions emerge. First, what is the most advanced LLM that can be run and deployed at the edge? And second, how can real-world applications leverage these advancements?\nRunning a state-of-the-art open-source LLM like Llama 2 70B, even at reduced FP16 precision, requires more than 140 GB of GPU VRAM (70 billion parameters x 2 bytes = 140 GB in FP16, plus more for KV Cache). For most developers and smaller companies, this amount of VRAM is not easily accessible. Additionally, whether due to cost, bandwidth, latency, or data privacy issues, application-specific requirements may preclude the option of hosting an LLM using cloud computing resources.\nNVIDIA IGX Orin Developer Kit and NVIDIA Holoscan SDK address these challenges, bringing the power of LLMs to the edge. The NVIDIA IGX Orin Developer Kit coupled with a discrete NVIDIA RTX A6000 GPU delivers an industrial-grade edge AI platform tailored to the demands of industrial and medical environments. This includes NVIDIA Holoscan, an SDK that harmonizes data movement, accelerated computing, real-time visualization, and AI inferencing.\nThis platform enables developers to add open-source LLMs into edge AI streaming workflows and products, offering new possibilities in real-time AI-enabled sensor processing while ensuring that sensitive data remains within the secure boundaries of the IGX hardware.", "Open-source LLMs for real-time streaming\nThe recent rapid development of open-source LLMs has transformed the perception of what is possible in a real-time streaming application. The prevailing perception was that any application requiring human-like abilities was exclusive to closed-source LLMs powered by enterprise-grade GPUs at data center scale. However, thanks to the recent surge in the performance of new open-source LLMs, models such as Falcon, MPT, and Llama 2 can now provide a viable alternative to closed-source black-box LLMs.\nThere are many possible applications that can make use of these open-source models on the edge, and many of them involve the distillation of streaming sensor data into natural language summaries. Such possibilities include real-time monitoring of surgical videos to keep families informed of progress, summarizing recent radar contacts for air traffic controllers, and even converting the play-by-play commentary of a soccer match into another language.\nAccess to powerful, open-source LLMs has also inspired a community devoted to refining the accuracy of these models, as well as reducing the computation required to run them. This vibrant community is active on the Hugging Face Open LLM Leaderboard, which is updated often with the latest top-performing models.", "AI capabilities at the edge\nThe NVIDIA IGX Orin platform is uniquely positioned to leverage the surge in available open-source LLMs and supporting software. The NVIDIA RTX A6000 GPU provides an ample 48 GB of VRAM, enabling it to run some of the largest open-source models. For example, a version of Llama 2 70B whose model weights have been quantized to 4 bits of precision, rather than the standard 32 bits, can run entirely on the GPU at 14 tokens per second.\nDiverse problems and use cases can be addressed by the robust Llama 2 model, bolstered by the security measures of the NVIDIA IGX Orin platform, and seamlessly incorporated into low-latency Holoscan SDK pipelines. This convergence not only marks a significant advancement in AI capabilities at the edge, but also unlocks the potential for transformative solutions across various domains.\nOne notable application could capitalize on the newly introduced Clinical Camel, a fine-tuned Llama 2 70B variant specialized in medical knowledge. By creating a localized medical chatbot based on this model, it ensures that sensitive patient data remains confined within the secure boundaries of the IGX hardware. Applications where privacy, bandwidth concerns, or real-time feedback are critical are where the IGX platform really shines.\nImagine inputting a patient\u2019s medical records and querying the bot to discover similar cases, gaining fresh insights into hard-to-diagnose patients, or even equipping medical professionals with a short list of potential medications that circumvents interactions with existing prescriptions. All of this could\u200c be automated through a Holoscan application that converts real-time audio from physician-patient interactions into text and seamlessly feeds it into the Clinical Camel model.\nClinical notes between a doctor and patient, generated by the Clinical Camel model.\nFigure 1. Clinical note generated by the Clinical Camel model from the example dialogue\nExpanding on its potential, the NVIDIA IGX platform extends the capabilities of LLMs beyond text-only applications, thanks to its exceptional optimization for low-latency sensor input. While the medical chatbot presents a compelling use case, the power of the IGX Orin Developer Kit is its capacity to seamlessly integrate real-time data from various sensors.\nDesigned for edge environments, the IGX Orin can process streaming information from cameras, lidar sensors, radio antennas, accelerometers, ultrasound probes, and more. This versatility empowers cutting-edge applications that seamlessly merge LLM prowess with real-time data streams.\nIntegrated into Holoscan operators, these LLMs have the potential to significantly enhance the capabilities and functionalities of AI-enabled sensor processing pipelines. Specific examples are detailed below.\nMultimodal medical assistants: Enhance LLMs with the capability to interpret not only text but also medical imaging, as demonstrated by projects like Med-Flamingo, which interprets MRIs, x-rays, and histology images.\nRationale and answer generated by Med-Flamingo in response to image and question about lesions in aorta.\nFigure 2. LLMs can interpret text and extract relevant insights from medical images Signals Intelligence (SIGINT): Derive natural language summaries from real-time electronic signals captured by communication systems and radars, providing insights that bridge technical data and human understanding.\nSurgical case note generation: Channel endoscopy video, audio, system data, and patient records to multimodal LLMs to generate comprehensive surgical case notes that are automatically uploaded to a patient\u2019s electronic medical records.\nSmart agriculture: Tap into soil sensors monitoring pH, moisture, and nutrient levels, enabling LLMs to offer actionable insights for optimized planting, irrigation, and pest control strategies.\nSoftware development copilots for education, troubleshooting, or productivity enhancement agents are another novel use case of LLMs. These models help developers to develop more efficient code and thorough documentation.\nThe Holoscan team recently released HoloChat, an AI-driven chatbot serving as a developer\u2019s copilot in Holoscan development. It generates human-like responses to questions about Holoscan and writing code. For details, visit nvidia-holoscan/holohub on GitHub.\nThe HoloChat local hosting approach is designed to provide developers with the same benefits as popular closed-source chatbots while eliminating the privacy and security concerns associated with sending data to third-party remote servers for processing.", "Model quantization for optimal accuracy and memory usage\nWith the influx of open-source models being released under Apache 2, MIT, and commercially viable licenses, anyone can download and use these model weights. However, just because this is possible, does not mean that it is feasible for the vast majority of developers.\nModel quantization provides one solution. Model quantization reduces the computational and memory costs of running inference by representing the weights and activations as low-precision data types (int8 and int4) instead of higher-precision data types (FP16 and FP32).\nHowever, removing this precision from the model does result in some degradation in the accuracy of the model. Yet research indicates that, given a memory budget, the best LLM performance is achieved by using the largest possible model that will fit into memory when the parameters are stored in 4-bit precision. For more details, see The Case for 4-bit Precision: k-Bit Inference Scaling Laws.\nAs such, the Llama 2 70B model achieves its optimal balance of accuracy and memory usage when it is implemented in a 4-bit quantization, which reduces the RAM required to about 35 GB only. This memory requirement is within reach for smaller development teams or even individuals. It is easily met with the single NVIDIA RTX A6000 48 GB GPU that is optionally included with the IGX Orin.", "Open-source LLMs open new development opportunities\nWith the ability to run state-of-the-art LLMs on commodity hardware, the open-source community has exploded with new libraries that support running locally and offer tools that extend the capabilities of these models beyond just predicting a sentence\u2019s next word.\nLibraries such as Llama.cpp, ExLlama, and AutoGPTQ enable you to quantize your own models and run blazing-fast inference on your local GPUs. Quantizing your own models is an entirely optional step, however, as the HuggingFace.co model repository is full of quantized models ready for use. This is thanks in large part to power users like /TheBloke, who upload newly quantized models daily.\nWhile these models on their own offer exciting development opportunities, augmenting them with additional tools from a host of newly created libraries makes them all the more powerful. Examples include:\nLangChain, a library with 58,000 GitHub stars that provides everything from vector database integration to enable document Q&A, to multi-step agent frameworks that enable LLMs to browse the web.\nHaystack, which enables scalable semantic search.\nMagentic, offering easy integration of LLMs into your Python code.\nOobabooga, a web UI for running quantized LLMs locally.\nIf you have an LLM use case, an open-source library is likely available that will provide most of what you need.", "Get started deploying LLMs at the edge\nDeploying cutting-edge LLMs at the edge with NVIDIA IGX Orin Developer Kit opens untapped development opportunities. To get started, check out the comprehensive tutorial detailing the creation of a simple chatbot application on IGX Orin, Deploying Llama 2 70B Model on the Edge with IGX Orin.\nThis tutorial illustrates the seamless integration of Llama 2 on IGX Orin, and guides you through developing a Python application using Gradio. This is the initial step toward harnessing any of the exceptional LLM libraries mentioned in this post. IGX Orin delivers resilience, unparalleled performance, and end-to-end security, empowering developers to forge innovative Holoscan-optimized applications around state-of-the-art LLMs operating at the edge.\nOn Friday, November 17, join the NVIDIA Deep Learning Institute for LLM Developer Day, a free virtual event delving into cutting-edge techniques in LLM application development. Register to access the event live or on demand."], "document_title": "Deploy Large Language Models at the Edge with NVIDIA IGX Orin Developer Kit", "document_url": "https://developer.nvidia.com/blog/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/", "document_date": "2023-11-15T17:30:00", "document_date_modified": "2023-11-30T19:43:32", "document_full_text": "Deploy Large Language Models at the Edge with NVIDIA IGX Orin Developer Kit\nAs large language models (LLMs) become more powerful and techniques for reducing their computational requirements mature, two compelling questions emerge. First, what is the most advanced LLM that can be run and deployed at the edge? And second, how can real-world applications leverage these advancements?\nRunning a state-of-the-art open-source LLM like Llama 2 70B, even at reduced FP16 precision, requires more than 140 GB of GPU VRAM (70 billion parameters x 2 bytes = 140 GB in FP16, plus more for KV Cache). For most developers and smaller companies, this amount of VRAM is not easily accessible. Additionally, whether due to cost, bandwidth, latency, or data privacy issues, application-specific requirements may preclude the option of hosting an LLM using cloud computing resources.\nNVIDIA IGX Orin Developer Kit and NVIDIA Holoscan SDK address these challenges, bringing the power of LLMs to the edge. The NVIDIA IGX Orin Developer Kit coupled with a discrete NVIDIA RTX A6000 GPU delivers an industrial-grade edge AI platform tailored to the demands of industrial and medical environments. This includes NVIDIA Holoscan, an SDK that harmonizes data movement, accelerated computing, real-time visualization, and AI inferencing.\nThis platform enables developers to add open-source LLMs into edge AI streaming workflows and products, offering new possibilities in real-time AI-enabled sensor processing while ensuring that sensitive data remains within the secure boundaries of the IGX hardware.\nOpen-source LLMs for real-time streaming\nThe recent rapid development of open-source LLMs has transformed the perception of what is possible in a real-time streaming application. The prevailing perception was that any application requiring human-like abilities was exclusive to closed-source LLMs powered by enterprise-grade GPUs at data center scale. However, thanks to the recent surge in the performance of new open-source LLMs, models such as Falcon, MPT, and Llama 2 can now provide a viable alternative to closed-source black-box LLMs.\nThere are many possible applications that can make use of these open-source models on the edge, and many of them involve the distillation of streaming sensor data into natural language summaries. Such possibilities include real-time monitoring of surgical videos to keep families informed of progress, summarizing recent radar contacts for air traffic controllers, and even converting the play-by-play commentary of a soccer match into another language.\nAccess to powerful, open-source LLMs has also inspired a community devoted to refining the accuracy of these models, as well as reducing the computation required to run them. This vibrant community is active on the Hugging Face Open LLM Leaderboard, which is updated often with the latest top-performing models.\nAI capabilities at the edge\nThe NVIDIA IGX Orin platform is uniquely positioned to leverage the surge in available open-source LLMs and supporting software. The NVIDIA RTX A6000 GPU provides an ample 48 GB of VRAM, enabling it to run some of the largest open-source models. For example, a version of Llama 2 70B whose model weights have been quantized to 4 bits of precision, rather than the standard 32 bits, can run entirely on the GPU at 14 tokens per second.\nDiverse problems and use cases can be addressed by the robust Llama 2 model, bolstered by the security measures of the NVIDIA IGX Orin platform, and seamlessly incorporated into low-latency Holoscan SDK pipelines. This convergence not only marks a significant advancement in AI capabilities at the edge, but also unlocks the potential for transformative solutions across various domains.\nOne notable application could capitalize on the newly introduced Clinical Camel, a fine-tuned Llama 2 70B variant specialized in medical knowledge. By creating a localized medical chatbot based on this model, it ensures that sensitive patient data remains confined within the secure boundaries of the IGX hardware. Applications where privacy, bandwidth concerns, or real-time feedback are critical are where the IGX platform really shines.\nImagine inputting a patient\u2019s medical records and querying the bot to discover similar cases, gaining fresh insights into hard-to-diagnose patients, or even equipping medical professionals with a short list of potential medications that circumvents interactions with existing prescriptions. All of this could\u200c be automated through a Holoscan application that converts real-time audio from physician-patient interactions into text and seamlessly feeds it into the Clinical Camel model.\nClinical notes between a doctor and patient, generated by the Clinical Camel model.\nFigure 1. Clinical note generated by the Clinical Camel model from the example dialogue\nExpanding on its potential, the NVIDIA IGX platform extends the capabilities of LLMs beyond text-only applications, thanks to its exceptional optimization for low-latency sensor input. While the medical chatbot presents a compelling use case, the power of the IGX Orin Developer Kit is its capacity to seamlessly integrate real-time data from various sensors.\nDesigned for edge environments, the IGX Orin can process streaming information from cameras, lidar sensors, radio antennas, accelerometers, ultrasound probes, and more. This versatility empowers cutting-edge applications that seamlessly merge LLM prowess with real-time data streams.\nIntegrated into Holoscan operators, these LLMs have the potential to significantly enhance the capabilities and functionalities of AI-enabled sensor processing pipelines. Specific examples are detailed below.\nMultimodal medical assistants: Enhance LLMs with the capability to interpret not only text but also medical imaging, as demonstrated by projects like Med-Flamingo, which interprets MRIs, x-rays, and histology images.\nRationale and answer generated by Med-Flamingo in response to image and question about lesions in aorta.\nFigure 2. LLMs can interpret text and extract relevant insights from medical images Signals Intelligence (SIGINT): Derive natural language summaries from real-time electronic signals captured by communication systems and radars, providing insights that bridge technical data and human understanding.\nSurgical case note generation: Channel endoscopy video, audio, system data, and patient records to multimodal LLMs to generate comprehensive surgical case notes that are automatically uploaded to a patient\u2019s electronic medical records.\nSmart agriculture: Tap into soil sensors monitoring pH, moisture, and nutrient levels, enabling LLMs to offer actionable insights for optimized planting, irrigation, and pest control strategies.\nSoftware development copilots for education, troubleshooting, or productivity enhancement agents are another novel use case of LLMs. These models help developers to develop more efficient code and thorough documentation.\nThe Holoscan team recently released HoloChat, an AI-driven chatbot serving as a developer\u2019s copilot in Holoscan development. It generates human-like responses to questions about Holoscan and writing code. For details, visit nvidia-holoscan/holohub on GitHub.\nThe HoloChat local hosting approach is designed to provide developers with the same benefits as popular closed-source chatbots while eliminating the privacy and security concerns associated with sending data to third-party remote servers for processing.\nModel quantization for optimal accuracy and memory usage\nWith the influx of open-source models being released under Apache 2, MIT, and commercially viable licenses, anyone can download and use these model weights. However, just because this is possible, does not mean that it is feasible for the vast majority of developers.\nModel quantization provides one solution. Model quantization reduces the computational and memory costs of running inference by representing the weights and activations as low-precision data types (int8 and int4) instead of higher-precision data types (FP16 and FP32).\nHowever, removing this precision from the model does result in some degradation in the accuracy of the model. Yet research indicates that, given a memory budget, the best LLM performance is achieved by using the largest possible model that will fit into memory when the parameters are stored in 4-bit precision. For more details, see The Case for 4-bit Precision: k-Bit Inference Scaling Laws.\nAs such, the Llama 2 70B model achieves its optimal balance of accuracy and memory usage when it is implemented in a 4-bit quantization, which reduces the RAM required to about 35 GB only. This memory requirement is within reach for smaller development teams or even individuals. It is easily met with the single NVIDIA RTX A6000 48 GB GPU that is optionally included with the IGX Orin.\nOpen-source LLMs open new development opportunities\nWith the ability to run state-of-the-art LLMs on commodity hardware, the open-source community has exploded with new libraries that support running locally and offer tools that extend the capabilities of these models beyond just predicting a sentence\u2019s next word.\nLibraries such as Llama.cpp, ExLlama, and AutoGPTQ enable you to quantize your own models and run blazing-fast inference on your local GPUs. Quantizing your own models is an entirely optional step, however, as the HuggingFace.co model repository is full of quantized models ready for use. This is thanks in large part to power users like /TheBloke, who upload newly quantized models daily.\nWhile these models on their own offer exciting development opportunities, augmenting them with additional tools from a host of newly created libraries makes them all the more powerful. Examples include:\nLangChain, a library with 58,000 GitHub stars that provides everything from vector database integration to enable document Q&A, to multi-step agent frameworks that enable LLMs to browse the web.\nHaystack, which enables scalable semantic search.\nMagentic, offering easy integration of LLMs into your Python code.\nOobabooga, a web UI for running quantized LLMs locally.\nIf you have an LLM use case, an open-source library is likely available that will provide most of what you need.\nGet started deploying LLMs at the edge\nDeploying cutting-edge LLMs at the edge with NVIDIA IGX Orin Developer Kit opens untapped development opportunities. To get started, check out the comprehensive tutorial detailing the creation of a simple chatbot application on IGX Orin, Deploying Llama 2 70B Model on the Edge with IGX Orin.\nThis tutorial illustrates the seamless integration of Llama 2 on IGX Orin, and guides you through developing a Python application using Gradio. This is the initial step toward harnessing any of the exceptional LLM libraries mentioned in this post. IGX Orin delivers resilience, unparalleled performance, and end-to-end security, empowering developers to forge innovative Holoscan-optimized applications around state-of-the-art LLMs operating at the edge.\nOn Friday, November 17, join the NVIDIA Deep Learning Institute for LLM Developer Day, a free virtual event delving into cutting-edge techniques in LLM application development. Register to access the event live or on demand."}], "https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/": [{"text": "The article discusses how developers can build custom enterprise-grade generative AI using NVIDIA AI Foundation Models. These models are curated and optimized for peak performance, offering a range of capabilities for different use cases. The NVIDIA Nemotron-3 8B family of models, in particular, provides a foundation for production-ready generative AI applications with multilingual capabilities. Additionally, NVIDIA offers optimized community models that can be customized for enterprise applications. Developers can easily interact with these models through APIs or graphical user interfaces, fine-tune them with proprietary data using tools like NVIDIA NeMo, and deploy them on their own infrastructure or through NVIDIA AI Foundation Endpoints or AI Enterprise. The article provides a step-by-step guide on how to experience, customize, and deploy these models, showcasing the flexibility and scalability of NVIDIA's generative AI solutions for enterprise developers.", "text_components": ["Build Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\u00a0\nIn the realm of generative AI, building enterprise-grade large language models (LLMs) requires expertise collecting high-quality data, setting up the accelerated infrastructure, and optimizing the models.\nDevelopers can begin with pretrained models and fine-tune them for their use case, saving time and getting their solutions faster to market. Developers need an easy way to try out models and evaluate their capabilities by integrating them through APIs. This helps them decide which model is best for their application.", "NVIDIA AI Foundation Models\nNVIDIA AI Foundation Models are a curated set of community and NVIDIA-built models, optimized for peak performance. Developers can quickly use them directly from their browser through APIs or \u200cgraphical user interface, without any setup. The models \u200care optimized with NVIDIA TensorRT-LLM and Activation-aware Weight Quantization (AWQ) to identify the configuration for the highest throughput and lowest latency and run at scale on the NVIDIA accelerated computing stack.", "Introducing the NVIDIA Nemotron-3 8B family of LLMs\nThe NVIDIA Nemotron-3 8B family of models offers a foundation for customers looking to build production-ready generative AI applications. These models are constructed on responsibly sourced datasets and operate at comparable performance to much larger models, making them ideal for enterprise deployments.\nA key differentiator of the NVIDIA Nemotron-3 8B family of models is its multilingual capabilities, which make it ideal for global enterprise. Out of the box, these models are proficient in 53 languages, including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch.\nThe family of models also features a range of alignment techniques, including supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), as well as the new NVIDIA SteerLM customization technique, where customers can tune models at inference. These variants provide a variety of starting points for supporting different use cases, whether customizing or running the models from scratch.\nThe Nemotron-3 8B family of models includes:\nNemotron-3-8B-Chat-SteerLM, a generative language model based on the NV-Nemotron-3-8B base model that is customized for user control of model outputs during inference using the SteerLM technique\nNemotron-3-8B-QA, a generative language model based on the NV-Nemotron-3-8B base model that further fine-tunes for instructions for Question Answering.", "NVIDIA-optimized community models\nAdditionally, NVIDIA offers leading community models that \u200care optimized using NVIDIA TensorRT-LLM to deliver the highest performance per dollar that organizations can customize for their enterprise applications. These include:\nLlama 2, one of the most popular LLMs capable of generating text from prompts.\nStable Diffusion XL, a popular Generative AI model that can create expressive images with text.\nCode Llama, a fine-tuned version of Llama 2 model that can generate code in several popular languages such as Java, C++, Python, and more.\nMistral 7B, an LLM that can follow instructions, complete requests, and generate creative text formats.\nContrastive Language-Image Pre-Training (CLIP), a popular open-source model that understands images and text together, enabling tasks like image classification and object detection.\nAs developers identify the right foundation models, there\u2019s an easy path for them to fine-tune and deploy these models either on their own or on NVIDIA-maintained infrastructure through NVIDIA DGX Cloud.\nLet\u2019s walk through the steps to experience, customize, and deploy the fine-tuned Llama 2 model.", "Experience Llama 2\nNVIDIA offers an easy-to-use interface to interact with the Llama 2 model directly from your browser. Simply enter the text in the prompt field, click generate, and the model will begin generating informative responses instantly.\nIn Figure 1, a user asked the model for an SQL query to retrieve the list of customers who spent at least $50,000 in the first quarter of 2021. The model interpreted the user\u2019s query correctly and provided the answer with a detailed explanation.\nAn image showing the output of the Llama 2 model for an SQL query to get the list of customers who spent at least $50,000 in the first quarter of 2021.\nFigure 1. Image of the Llama 2 model response for an SQL query to get the list of customers who spent at least $50,000 in the first quarter of 2021\nDevelopers are often more interested in working with code. NVIDIA also offers an API widget directly in the browser, making it seamless to experience the models through an API.\nTo try the in-browser API, click on API mode, and select the language you prefer from the dropdown. Figure 2 shows the API instructions for calling the API through cURL.\nImage showing the in-browser API widget for Llama 2 model for cURL.\nFigure 2. Picture of the in-browser API Widget for the Llama 2 model for cURL", "Customize the model\nIt\u2019s often the case that the models don\u2019t meet the developer\u2019s \u200cneeds as-is and must be fine-tuned with proprietary data. NVIDIA offers various paths for customizing the available models.\nNVIDIA NeMo is an end-to-end, enterprise-grade cloud-native framework for developers to build, customize, and deploy generative AI models with billions of parameters. NeMo also provides APIs to fine-tune LLMs like Llama 2.\nTo get started quickly, we are offering an NVIDIA LaunchPad lab\u2014a universal proving ground, offering comprehensive testing of the latest NVIDIA enterprise hardware and software.\nThe following example in the LaunchPad lab experience, fine-tunes a Llama 2 7B text-to-text model using a custom dataset to better perform a Question-Answering task.\nTo get started, click on the Llama 2 fine-tuning lab and request access. When working on Launchpad, the Llama 2 model files are pre-downloaded as a .nemo checkpoints, which enable fine-tuning compatibility with NVIDIA NeMo Framework.\nAfter the model is prepared, we load the Dolly dataset from Hugging Face and preprocess it by getting rid of unnecessary fields, renaming certain fields to better fit the p-tuning task, and splitting the dataset into train and test files.\ndataset = load_dataset(\"aisquared/databricks-dolly-15k\") A sample of the data is shown. The datasets can be swapped to fit specific use cases.\n```\n{\n\"question\": \"When did Virgin Australia start operating?\", \n\"context\": \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]\", \n\"answer\": \"Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\", \n\"taskname\": \"genqa\"\n}\n```\nThe fine-tuning job is then configured by updating certain fields of a default NeMo config file to fit the training task at hand. The job is then launched using a NeMo training script, which runs the fine-tuning and generates model checkpoints along the way.\nOnce the fine-tuning task is completed, we\u2019ll be able to run in-notebook inference to generate a few example outputs and evaluate the performance of the fine-tuned model.\nLet\u2019s look at one such example output. We provided the context with descriptions of the two products: a lawn mower and a kitchen robot, and asked the model whether the lawn mower was solar-powered.\nThe model accurately grasped the context and responded with a \u201cYes.\u201d Fine-tuning the model enabled it to respond accurately to our questions from the provided context.\n{\n\"input\": \"Context: The Auto Chef Master is a personal kitchen robot that effortlessly turns raw ingredients into gourmet meals with the precision of a Michelin-star chef. The Eco Lawn Mower is a solar powered high-tech lawn mower that provides an eco-friendly and efficient way to maintain your lawn. Question: Is the lawn mower product solar powered?\nAnswer:\",\n\"pred\": \"Yes\",\n\"label\": \"Yes, the Eco Lawn Mower is solar powered.\",\n\"taskname\": \"genqa\"\n}", "Deploy the model\nNVIDIA AI Foundation Endpoints provide fully serverless and scalable APIs that can be deployed on either your own or NVIDIA DGX Cloud. Complete this form to get started with AI Foundation Endpoints.\nYou can also deploy on your own cloud or data center infrastructure with NVIDIA AI Enterprise. This end-to-end, cloud-native software platform accelerates the development and deployment of production-grade generative AI with enterprise-grade security, stability, manageability, and support. When you are ready to move from experimentation to production, you can leverage NVIDIA AI Enterprise for enterprise-grade runtimes to fine-tune and deploy these models.", "Learn more\nIn this post, we learned how NVIDIA AI Foundation models enable enterprise developers to find the right model for various use cases by providing an easy-to-use interface to experience the models and simplified paths to fine-tune and deploy them.\nExplore the different AI Foundation models available on the NVIDIA NGC Catalog and find the right model for you."], "document_title": "Build Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\u00a0", "document_url": "https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/", "document_date": "2023-11-15T16:00:00", "document_date_modified": "2024-01-02T18:37:01", "document_full_text": "Build Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\u00a0\nIn the realm of generative AI, building enterprise-grade large language models (LLMs) requires expertise collecting high-quality data, setting up the accelerated infrastructure, and optimizing the models.\nDevelopers can begin with pretrained models and fine-tune them for their use case, saving time and getting their solutions faster to market. Developers need an easy way to try out models and evaluate their capabilities by integrating them through APIs. This helps them decide which model is best for their application.\nNVIDIA AI Foundation Models\nNVIDIA AI Foundation Models are a curated set of community and NVIDIA-built models, optimized for peak performance. Developers can quickly use them directly from their browser through APIs or \u200cgraphical user interface, without any setup. The models \u200care optimized with NVIDIA TensorRT-LLM and Activation-aware Weight Quantization (AWQ) to identify the configuration for the highest throughput and lowest latency and run at scale on the NVIDIA accelerated computing stack.\nIntroducing the NVIDIA Nemotron-3 8B family of LLMs\nThe NVIDIA Nemotron-3 8B family of models offers a foundation for customers looking to build production-ready generative AI applications. These models are constructed on responsibly sourced datasets and operate at comparable performance to much larger models, making them ideal for enterprise deployments.\nA key differentiator of the NVIDIA Nemotron-3 8B family of models is its multilingual capabilities, which make it ideal for global enterprise. Out of the box, these models are proficient in 53 languages, including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch.\nThe family of models also features a range of alignment techniques, including supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), as well as the new NVIDIA SteerLM customization technique, where customers can tune models at inference. These variants provide a variety of starting points for supporting different use cases, whether customizing or running the models from scratch.\nThe Nemotron-3 8B family of models includes:\nNemotron-3-8B-Chat-SteerLM, a generative language model based on the NV-Nemotron-3-8B base model that is customized for user control of model outputs during inference using the SteerLM technique\nNemotron-3-8B-QA, a generative language model based on the NV-Nemotron-3-8B base model that further fine-tunes for instructions for Question Answering.\nNVIDIA-optimized community models\nAdditionally, NVIDIA offers leading community models that \u200care optimized using NVIDIA TensorRT-LLM to deliver the highest performance per dollar that organizations can customize for their enterprise applications. These include:\nLlama 2, one of the most popular LLMs capable of generating text from prompts.\nStable Diffusion XL, a popular Generative AI model that can create expressive images with text.\nCode Llama, a fine-tuned version of Llama 2 model that can generate code in several popular languages such as Java, C++, Python, and more.\nMistral 7B, an LLM that can follow instructions, complete requests, and generate creative text formats.\nContrastive Language-Image Pre-Training (CLIP), a popular open-source model that understands images and text together, enabling tasks like image classification and object detection.\nAs developers identify the right foundation models, there\u2019s an easy path for them to fine-tune and deploy these models either on their own or on NVIDIA-maintained infrastructure through NVIDIA DGX Cloud.\nLet\u2019s walk through the steps to experience, customize, and deploy the fine-tuned Llama 2 model.\nExperience Llama 2\nNVIDIA offers an easy-to-use interface to interact with the Llama 2 model directly from your browser. Simply enter the text in the prompt field, click generate, and the model will begin generating informative responses instantly.\nIn Figure 1, a user asked the model for an SQL query to retrieve the list of customers who spent at least $50,000 in the first quarter of 2021. The model interpreted the user\u2019s query correctly and provided the answer with a detailed explanation.\nAn image showing the output of the Llama 2 model for an SQL query to get the list of customers who spent at least $50,000 in the first quarter of 2021.\nFigure 1. Image of the Llama 2 model response for an SQL query to get the list of customers who spent at least $50,000 in the first quarter of 2021\nDevelopers are often more interested in working with code. NVIDIA also offers an API widget directly in the browser, making it seamless to experience the models through an API.\nTo try the in-browser API, click on API mode, and select the language you prefer from the dropdown. Figure 2 shows the API instructions for calling the API through cURL.\nImage showing the in-browser API widget for Llama 2 model for cURL.\nFigure 2. Picture of the in-browser API Widget for the Llama 2 model for cURL\nCustomize the model\nIt\u2019s often the case that the models don\u2019t meet the developer\u2019s \u200cneeds as-is and must be fine-tuned with proprietary data. NVIDIA offers various paths for customizing the available models.\nNVIDIA NeMo is an end-to-end, enterprise-grade cloud-native framework for developers to build, customize, and deploy generative AI models with billions of parameters. NeMo also provides APIs to fine-tune LLMs like Llama 2.\nTo get started quickly, we are offering an NVIDIA LaunchPad lab\u2014a universal proving ground, offering comprehensive testing of the latest NVIDIA enterprise hardware and software.\nThe following example in the LaunchPad lab experience, fine-tunes a Llama 2 7B text-to-text model using a custom dataset to better perform a Question-Answering task.\nTo get started, click on the Llama 2 fine-tuning lab and request access. When working on Launchpad, the Llama 2 model files are pre-downloaded as a .nemo checkpoints, which enable fine-tuning compatibility with NVIDIA NeMo Framework.\nAfter the model is prepared, we load the Dolly dataset from Hugging Face and preprocess it by getting rid of unnecessary fields, renaming certain fields to better fit the p-tuning task, and splitting the dataset into train and test files.\ndataset = load_dataset(\"aisquared/databricks-dolly-15k\") A sample of the data is shown. The datasets can be swapped to fit specific use cases.\n```\n{\n\"question\": \"When did Virgin Australia start operating?\", \n\"context\": \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]\", \n\"answer\": \"Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\", \n\"taskname\": \"genqa\"\n}\n```\nThe fine-tuning job is then configured by updating certain fields of a default NeMo config file to fit the training task at hand. The job is then launched using a NeMo training script, which runs the fine-tuning and generates model checkpoints along the way.\nOnce the fine-tuning task is completed, we\u2019ll be able to run in-notebook inference to generate a few example outputs and evaluate the performance of the fine-tuned model.\nLet\u2019s look at one such example output. We provided the context with descriptions of the two products: a lawn mower and a kitchen robot, and asked the model whether the lawn mower was solar-powered.\nThe model accurately grasped the context and responded with a \u201cYes.\u201d Fine-tuning the model enabled it to respond accurately to our questions from the provided context.\n{\n\"input\": \"Context: The Auto Chef Master is a personal kitchen robot that effortlessly turns raw ingredients into gourmet meals with the precision of a Michelin-star chef. The Eco Lawn Mower is a solar powered high-tech lawn mower that provides an eco-friendly and efficient way to maintain your lawn. Question: Is the lawn mower product solar powered?\nAnswer:\",\n\"pred\": \"Yes\",\n\"label\": \"Yes, the Eco Lawn Mower is solar powered.\",\n\"taskname\": \"genqa\"\n}\nDeploy the model\nNVIDIA AI Foundation Endpoints provide fully serverless and scalable APIs that can be deployed on either your own or NVIDIA DGX Cloud. Complete this form to get started with AI Foundation Endpoints.\nYou can also deploy on your own cloud or data center infrastructure with NVIDIA AI Enterprise. This end-to-end, cloud-native software platform accelerates the development and deployment of production-grade generative AI with enterprise-grade security, stability, manageability, and support. When you are ready to move from experimentation to production, you can leverage NVIDIA AI Enterprise for enterprise-grade runtimes to fine-tune and deploy these models.\nLearn more\nIn this post, we learned how NVIDIA AI Foundation models enable enterprise developers to find the right model for various use cases by providing an easy-to-use interface to experience the models and simplified paths to fine-tune and deploy them.\nExplore the different AI Foundation models available on the NVIDIA NGC Catalog and find the right model for you."}], "https://developer.nvidia.com/blog/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/": [{"text": "NVIDIA and Microsoft are collaborating to provide enterprises with tools for building, optimizing, and deploying AI applications, including generative AI, using NVIDIA AI on Azure Machine Learning. The NeMo Framework and NVIDIA AI Foundation Models are now available in the Azure Machine Learning Model Catalog, allowing for customization and deployment of generative AI models with billions of parameters. The Triton Inference Server, which optimizes inference for various query types, is also integrated with Azure ML-managed endpoints, offering stability, security, and API support through NVIDIA AI Enterprise. Enterprises can easily deploy AI models using Triton Inference Server on Azure ML-managed endpoints, benefiting from GPU acceleration and cloud resources. The collaboration between NVIDIA and Microsoft aims to revolutionize AI and cloud computing, with businesses able to efficiently develop and deploy AI models with the combined power of NVIDIA GPUs and Azure Machine Learning. Visit NVIDIA at Microsoft Ignite for more information on their innovative solutions and collaborations.", "text_components": ["Elevate Enterprise Generative AI App Development with NVIDIA AI on Azure Machine Learning\nGenerative AI is revolutionizing how organizations across all industries are leveraging data to increase productivity, advance personalized customer engagement, and foster innovation. Given its tremendous value, enterprises are looking for tools and expertise that help them integrate this new technology into their business operations and strategies effectively and reliably.\nNVIDIA and Microsoft are working together to provide enterprises with a comprehensive solution for building, optimizing, and deploying AI applications, including generative AI, using NVIDIA AI on Azure Machine Learning (Azure ML).\nThis week at Microsoft Ignite, NVIDIA and Microsoft announced two additional milestones, bringing new capabilities to Azure ML for managing production AI and developing generative AI applications.\nNVIDIA NeMo, a framework for building and customizing generative AI models, and NVIDIA AI Foundation Models, including the new NVIDIA Nemotron-3 8B family of models, are available in the Azure Machine Learning Model Catalog.\nNVIDIA Triton Inference Server, which scales AI in production, is generally available with Azure ML-managed endpoints.\nIn June, we published a post explaining the NVIDIA AI Enterprise software integration with Azure Machine Learning, and how to get started. This post provides updates to the progress made by the NVIDIA and Azure teams, explains the benefits of the two new integrations, and steps for accessing them.", "NeMo Framework integration in Azure Machine Learning Model Catalog\nLLMs are gaining significant attention due to their ability to perform a variety of tasks, such as text summarization, language translation, and text generation. Open-source and proprietary LLMs available as model weights or APIs are pretrained on a large corpus of data using different generative AI frameworks.\nCustom LLMs, tailored for domain-specific insights using generative AI frameworks, are also finding increased traction in the enterprise domain.\nNeMo is an end-to-end, cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. It includes training and inferencing frameworks, guard-railing toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI.\nFor a secure, optimized full-stack solution designed to accelerate enterprises with support, security, and API stability, NeMo is available as part of NVIDIA AI Enterprise. Azure ML customers can now customize, optimize, and deploy through the user interface in a no-code flow. Customers can also get support directly from NVIDIA on their generative AI projects, including best practices for performance and accuracy.\nWith the availability of the NVIDIA Nemotron-3 8B family of foundational models and NeMo Framework within the Azure Machine Learning Model Catalog, users can now access, customize, and deploy these models out of the box. The framework offers a choice of several customization techniques and is optimized for at-scale inference of models for language and image applications.", "Triton Inference Server Integration in Azure ML-managed endpoint\nTriton Inference Server is a multi-framework, open-source software that optimizes inference for multiple query types, such as real-time, batch, and streaming. It also supports model ensembles and is included with NVIDIA AI Enterprise. Triton is compatible with various machine learning frameworks, such as TensorFlow, ONNX Runtime, PyTorch, and NVIDIA TensorRT. It can be used for CPU and GPU workloads.\nTriton is available on Azure ML, which provides dynamic batching, concurrent execution, and optimal model configuration. It also offers enterprise-grade security, manageability, and API stability through NVIDIA AI Enterprise within Azure ML.\nAzure ML-managed endpoints make it easy for enterprises to monitor, deploy, and scale AI models, reducing the complexity of setting up and managing your own AI infrastructure.\nThe GA release is based on production branches, exclusively available with NVIDIA AI Enterprise. Production branches provide stability and security for applications built on NVIDIA AI, offering nine months of support, API stability, and monthly fixes for software vulnerabilities. Learn more about production branches.", "Get started with Triton Inference Server\nDeploying your model in Triton Inference Server on an Azure ML-managed endpoint is simple. Watch the following video and follow the teps listed for guidance.\nIn Azure Machine Learning, go to Models and register your model in Triton format. Confirm that the type is Triton.\nUnder Endpoints, choose Create to create a real-time online endpoint. Select the Triton server for deployment.\nConfigure your deployment parameters and choose Next. In the Environment section, the environment and scoring scripts are preselected. Choose Next.\nConfirm the model and environment and choose Create to deploy for model inference.\nReview the test page.\nScreenshot of a test endpoint page on Azure Machine Learning endpoints.\nFigure 1. Test endpoint on Azure Machine Learning", "Get started with NVIDIA on Azure Machine Learning\nCombining NVIDIA AI Enterprise and Azure Machine Learning creates powerful GPU-accelerated computing and a comprehensive cloud-based machine learning platform, enabling businesses to develop and deploy AI models more efficiently. Enterprises can take advantage of cloud resources and the performance benefits of NVIDIA GPUs and software with this synergy.\nVisit NVIDIA at Microsoft Ignite to learn more about the latest innovations and collaborations propelling AI and cloud computing into the future.\nGain insights into groundbreaking NVIDIA solutions by participating in our sponsor sessions or stop by our EMU demo space #311. Check out the NVIDIA showcase page for more information.\nReady to get started now? Check out NVIDIA AI Foundation Models and NeMo Framework in the Azure Machine Learning Model Catalog and NVIDIA Triton Inference Server on Azure ML endpoints."], "document_title": "Elevate Enterprise Generative AI App Development with NVIDIA AI on Azure Machine Learning", "document_url": "https://developer.nvidia.com/blog/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/", "document_date": "2023-11-15T16:00:00", "document_date_modified": "2023-12-30T00:41:50", "document_full_text": "Elevate Enterprise Generative AI App Development with NVIDIA AI on Azure Machine Learning\nGenerative AI is revolutionizing how organizations across all industries are leveraging data to increase productivity, advance personalized customer engagement, and foster innovation. Given its tremendous value, enterprises are looking for tools and expertise that help them integrate this new technology into their business operations and strategies effectively and reliably.\nNVIDIA and Microsoft are working together to provide enterprises with a comprehensive solution for building, optimizing, and deploying AI applications, including generative AI, using NVIDIA AI on Azure Machine Learning (Azure ML).\nThis week at Microsoft Ignite, NVIDIA and Microsoft announced two additional milestones, bringing new capabilities to Azure ML for managing production AI and developing generative AI applications.\nNVIDIA NeMo, a framework for building and customizing generative AI models, and NVIDIA AI Foundation Models, including the new NVIDIA Nemotron-3 8B family of models, are available in the Azure Machine Learning Model Catalog.\nNVIDIA Triton Inference Server, which scales AI in production, is generally available with Azure ML-managed endpoints.\nIn June, we published a post explaining the NVIDIA AI Enterprise software integration with Azure Machine Learning, and how to get started. This post provides updates to the progress made by the NVIDIA and Azure teams, explains the benefits of the two new integrations, and steps for accessing them.\nNeMo Framework integration in Azure Machine Learning Model Catalog\nLLMs are gaining significant attention due to their ability to perform a variety of tasks, such as text summarization, language translation, and text generation. Open-source and proprietary LLMs available as model weights or APIs are pretrained on a large corpus of data using different generative AI frameworks.\nCustom LLMs, tailored for domain-specific insights using generative AI frameworks, are also finding increased traction in the enterprise domain.\nNeMo is an end-to-end, cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. It includes training and inferencing frameworks, guard-railing toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI.\nFor a secure, optimized full-stack solution designed to accelerate enterprises with support, security, and API stability, NeMo is available as part of NVIDIA AI Enterprise. Azure ML customers can now customize, optimize, and deploy through the user interface in a no-code flow. Customers can also get support directly from NVIDIA on their generative AI projects, including best practices for performance and accuracy.\nWith the availability of the NVIDIA Nemotron-3 8B family of foundational models and NeMo Framework within the Azure Machine Learning Model Catalog, users can now access, customize, and deploy these models out of the box. The framework offers a choice of several customization techniques and is optimized for at-scale inference of models for language and image applications.\nTriton Inference Server Integration in Azure ML-managed endpoint\nTriton Inference Server is a multi-framework, open-source software that optimizes inference for multiple query types, such as real-time, batch, and streaming. It also supports model ensembles and is included with NVIDIA AI Enterprise. Triton is compatible with various machine learning frameworks, such as TensorFlow, ONNX Runtime, PyTorch, and NVIDIA TensorRT. It can be used for CPU and GPU workloads.\nTriton is available on Azure ML, which provides dynamic batching, concurrent execution, and optimal model configuration. It also offers enterprise-grade security, manageability, and API stability through NVIDIA AI Enterprise within Azure ML.\nAzure ML-managed endpoints make it easy for enterprises to monitor, deploy, and scale AI models, reducing the complexity of setting up and managing your own AI infrastructure.\nThe GA release is based on production branches, exclusively available with NVIDIA AI Enterprise. Production branches provide stability and security for applications built on NVIDIA AI, offering nine months of support, API stability, and monthly fixes for software vulnerabilities. Learn more about production branches.\nGet started with Triton Inference Server\nDeploying your model in Triton Inference Server on an Azure ML-managed endpoint is simple. Watch the following video and follow the teps listed for guidance.\nIn Azure Machine Learning, go to Models and register your model in Triton format. Confirm that the type is Triton.\nUnder Endpoints, choose Create to create a real-time online endpoint. Select the Triton server for deployment.\nConfigure your deployment parameters and choose Next. In the Environment section, the environment and scoring scripts are preselected. Choose Next.\nConfirm the model and environment and choose Create to deploy for model inference.\nReview the test page.\nScreenshot of a test endpoint page on Azure Machine Learning endpoints.\nFigure 1. Test endpoint on Azure Machine Learning\nGet started with NVIDIA on Azure Machine Learning\nCombining NVIDIA AI Enterprise and Azure Machine Learning creates powerful GPU-accelerated computing and a comprehensive cloud-based machine learning platform, enabling businesses to develop and deploy AI models more efficiently. Enterprises can take advantage of cloud resources and the performance benefits of NVIDIA GPUs and software with this synergy.\nVisit NVIDIA at Microsoft Ignite to learn more about the latest innovations and collaborations propelling AI and cloud computing into the future.\nGain insights into groundbreaking NVIDIA solutions by participating in our sponsor sessions or stop by our EMU demo space #311. Check out the NVIDIA showcase page for more information.\nReady to get started now? Check out NVIDIA AI Foundation Models and NeMo Framework in the Azure Machine Learning Model Catalog and NVIDIA Triton Inference Server on Azure ML endpoints."}], "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/": [{"text": "NVIDIA AI Foundation Models, specifically the Nemotron-3 8B family, offer powerful tools for building custom enterprise chatbots and co-pilots using Large Language Models (LLMs). These models, available in Azure AI Model Catalog, HuggingFace, and NVIDIA AI Foundation Model hub, support a wide range of use cases, including base, chat, and question-and-answer models. The Nemotron-3-8B base model is compact and multilingual, while the chat models offer different options for customization and alignment. The question-and-answer model achieves state-of-the-art performance. The NeMo framework simplifies the customization and deployment of these models for specific enterprise needs, ensuring optimal performance and data privacy compliance. Deployment on Azure ML or other clouds is straightforward, and running inference can be done using NeMo client APIs or PyTriton. Further training and customization options are available for domain-specific datasets, making the Nemotron-3-8B family versatile and competitive across various benchmarks and languages. The models are designed for rapid deployment at scale without compromising quality or security standards.", "text_components": ["NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs\nLarge language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.\nThe NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise\u2013fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.\nThese new foundation models join NVIDIA NeMo, an end-to-end framework for building, customizing, and deploying LLMs tailored for enterprise use. Businesses can now use these tools for developing AI applications quickly, cost-effectively, and on a large scale. These applications can run in the cloud, data center, and on Windows desktop and laptop machines.\nThe Nemotron-3 8B family is available in the Azure AI Model Catalog, HuggingFace, and the NVIDIA AI Foundation Model hub on the NVIDIA NGC Catalog. It includes base, chat, and question-and-answer (Q&A) models that are designed to solve a variety of downstream tasks. Table 1 shows the full family of foundation models.\nModel\nVariant\nKey Benefit\nBase\nNemotron-3-8B-Base\nEnables customization, including parameter-efficient fine-tuning and continuous pretraining for domain-adapted LLMs\nChat\nNemotron-3-8B-Chat-SFT\nA building block for instruction tuning custom models or user-defined alignment, such as RLHF or SteerLM models\nNemotron-3-8B-Chat-RLHF\nBest out-of-the-box chat model performance\nNemotron-3-8B-Chat-SteerLM\nBest out-of-the-box chat model with flexible alignment at inference time\nQuestion-and- Answer\nNemotron-3-8B-QA\nQ&A LLMs customized on knowledge bases\nTable 1. The Nemotron-3 8B family of foundation models supports a wide range of LLM use cases", "Designing foundation models for production use\nFoundation models are a powerful building block that reduces the time and resources required to build useful, custom applications. However, organizations must make sure that these models meet their enterprise requirements.\nNVIDIA AI Foundation Models are trained on responsibly sourced datasets, capturing myriad voices and experiences. Rigorous monitoring provides data fidelity and compliance with evolving legal stipulations. Any arising data issues are swiftly addressed, making sure that businesses are armed with AI applications that comply with both legal norms and user privacy. These models can assimilate both publicly available and proprietary datasets.", "Nemotron-3-8B base\nThe Nemotron-3-8B base model is a compact, high-performance model for generating human-like text or code. The model has an MMLU 5-shot average of 54.4. The base model also caters to the needs of global enterprises with multilingual capabilities, as it is proficient in 53 languages including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch. This base model is also trained on 37 different coding languages.", "Nemotron-3-8B chat\nAdding to this suite are the Nemotron-3-8B chat models, which target LLM-powered chatbot interactions. There are three chat model versions, each designed for unique user-specific adjustments:\nSupervised fine-tuning (SFT)\nReinforcement learning from human feedback (RLHF)\nNVIDIA SteerLM\nThe Nemotron-3-8B-SFT model is the first step in instruct-tuning, from which we build our RLHF model that has the highest MT-Bench score within the 8B category, the most cited metric for chat quality. We suggest the user begin with 8B-chat-RLHF for the best immediate chat interaction, but for enterprises interested in uniquely aligning with their end users\u2019 preferences, we recommend the SFT model while applying their own RLHF.\nFinally, the latest alignment method, SteerLM, offers a new level of flexibility for training and customizing LLMs at inference. With SteerLM, users define all the attributes they want and embed them in a single model. Then, they can choose the combination they need for a given use case while the model is running.\nThis method enables a continuous improvement cycle. Responses from a custom model can serve as data for a future training run that dials the model into new levels of usefulness.", "Nemotron-3-8B question-and-answer\nThe Nemotron-3-8B-QA model is a question-and-answer (QA) model fine-tuned on a large amount of data focused on the target use case.\nThe Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in \u200cQA.\nThe Nemotron-3-8B-QA model has been tested against other state-of-the-art language models that have a larger parameter size. This testing was conducted on datasets created by NVIDIA, as well as on Natural Questions and Doc2Dial datasets. Results show that this model performs well.", "Building custom LLMs with NVIDIA NeMo framework\nNVIDIA NeMo simplifies the path to building customized, enterprise generative AI models by providing end-to-end capabilities and containerized recipes for several model architectures. With the Nemotron-3-8B family of models, developers have access to pretrained models from NVIDIA that can be easily adapted for their specific use cases.", "Fast model deployment\nWhen using the NeMo framework, there isn\u2019t a need for collecting data or setting up infrastructure. NeMo streamlines the process. Developers can customize existing models and deploy them in production quickly.", "Optimal model performance\nFurthermore, it integrates seamlessly with the NVIDIA TensorRT-LLM open-source library, which optimizes model performance, along with NVIDIA Triton Inference Server, which accelerates the inference serving process. This combination of tools enables cutting-edge accuracy, low latency, and high throughput.", "Data privacy and security\nNeMo enables secure, efficient large-scale deployments that comply with safety and security regulations. For example, if data privacy is a key concern for your business, you can use NeMo Guardrails to store customer data securely without compromising on performance or reliability.\nOverall, building custom LLMs with the NeMo framework is an effective way to create enterprise AI applications quickly without sacrificing quality or security standards. It offers developers flexibility in terms of customization while providing the robust tools needed for rapid deployment at scale.", "Getting started with Nemotron-3-8B\nYou can easily run inference on the Nemotron-3-8B model with the NeMo framework, which leverages TensorRT-LLM, an open source library that provides advanced optimizations for efficient and easy LLM inference on NVIDIA GPUs. It has built-in support for various optimization techniques including:\nKV caching\nEfficient Attention modules (including MQA, GQA, and Paged Attention)\nIn-flight (or continuous) batching\nSupport for low-precision (INT8/FP8) quantization, among other optimizations.\nThe NeMo framework inference container includes all the necessary scripts and dependencies to apply TensorRT-LLM optimizations on NeMo models such as the Nemotron-3-8B family, and host them with Triton Inference Server. With deployment, it exposes an endpoint you can send your inference queries to.", "Prerequisites\nTo follow the instructions to deploy and infer, you\u2019ll need access to:\nNVIDIA Data Center GPUs: at least (1) A100 \u2013 40 GB / 80 GB, (2) H100 \u2013 80 GB, or (3) L40S.\nNVIDIA NeMo framework: provides you with both the training and inference containers to customize \u200cor deploy the Nemotron-3-8B family of models.", "Steps to deploy on Azure ML\nThe models in the Nemotron-3-8B family are available in the Azure ML Model Catalog for deploying in Azure ML-managed endpoints. AzureML provides an easy to use \u2018no-code deployment\u2019 flow hat makes deploying Nemotron-3-8B family models very easy. The underlying plumbing that is the NeMo framework inference container is integrated within the platform.\nScreenshot showing real-time endpoint selection for model deployment within Azure ML.\nFigure 1. Select real-time endpoint in Azure ML\nTo deploy the NVIDIA foundation model on Azure ML for inference, follow the following steps:\nLog in to your Azure account: https://portal.azure.com/#home\nNavigate to Azure ML Machine Learning Studio\nSelect your workspace & navigate to the model catalog\nNVIDIA AI Foundation models are available for fine-tuning, evaluation, and deployment on Azure. Customization of the models can be done within Azure ML using the NeMo training framework. The NeMo framework, consisting of training and inference containers, is already integrated within AzureML.\nTo fine-tune the base model, select your favorite model variant, click \u2018fine-tune\u2019, fill in parameters like task type, custom training data, train and validation split, and the compute cluster.\nTo deploy the model, select your favorite model variant, click \u2018Real-time endpoint\u2019, select instance, endpoint, and other parameters for customizing deployment. Click deploy to deploy the model for inference to an endpoint.\nAzure CLI and SDK support are also available for running fine-tuning jobs and deployments on Azure ML. For more information, please refer to Foundation Models in Azure ML documentation.", "Steps for deploying on-prem or on other clouds\nThe models in the Nemotron-3-8B family have distinct prompt templates for inference requests that are recommended as best practice, however, the instructions to deploy them are similar, as they share the same base architecture.\nFor the latest deployment instructions using the NeMo framework inference container, see https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference.\nTo demonstrate, let\u2019s deploy Nemotron-3-8B-Base-4k.\nLog in to the NGC Catalog, and fetch the Inference container.\n# log in to your NGC organization\ndocker login nvcr.io\n# Fetch the NeMo framework inference container\ndocker pull nvcr.io/ea-bignlp/ga-participants/nemofw-inference:23.10\nDownload the Nemotron-3-8B-Base-4k model. The 8B family of models is available on NGC Catalog, as well as on Hugging Face. You may choose to download the model from either one.\nNVIDIA NGC\nThe easiest way to download the model from NGC is to use the CLI. If you don\u2019t have the NGC CLI installed, follow the Getting Started instructions to install and configure it.\n# Downloading using CLI. The model path can be obtained from it\u2019s page on NGC\nngc registry model download-version \"dztrnjtldi02/nemotron-3-8b-base-4k:1.0\" Hugging Face Hub\nThe following command uses git-lfs, but you may use any of the methods supported by Hugging Face to download models.\ngit lfs install\ngit clone https://huggingface.co/nvidia/nemotron-3-8b-base-4k nemotron-3-8b-base-4k_v1.0\nRun the NeMo inference container in interactive mode, mounting the relevant paths\n# Create a folder to cache the built TRT engines. This is recommended so they don\u2019t have to be built on every deployment call.\nmkdir -p trt-cache\n# Run the container, mounting the checkpoint and the cache directory\ndocker run --rm --net=host \\\n--gpus=all \\\n-v $(pwd)/nemotron-3-8b-base-4k_v1.0:/opt/checkpoints/ \\\n-v $(pwd)/trt-cache:/trt-cache \\\n-w /opt/NeMo \\\n-it nvcr.io/ea-bignlp/ga-participants/nemofw-inference:23.10 bash 4. Convert and Deploy the model on Triton Inference Server with TensorRT-LLM backend.\npython scripts/deploy/deploy_triton.py \\\n--nemo_checkpoint /opt/checkpoints/Nemotron-3-8B-Base-4k.nemo \\\n--model_type=\"gptnext\" \\\n--triton_model_name Nemotron-3-8B-4K \\\n--triton_model_repository /trt-cache/8b-base-4k \\\n--max_input_len 3000 \\\n--max_output_len 1000 \\\n--max_batch_size 2 Note that this script will export the built TensorRT-LLM engine files at the path specified in \u201c\u2013triton_model_repository\u201d. To load the exported model without rebuilding engines in subsequent deployments, you can skip the \u201c\u2013nemo_checkpoint\u201d, \u201c\u2013max_input_len\u201d, \u201c\u2013max_output_len\u201d, and \u201cmax_batch_size\u201d arguments. More information about using this script can be found in the documentation.\nWhen this command is successfully completed, it exposes an endpoint you can query. Let\u2019s look at how you can do that.", "Steps to run inference\nThere are several options available to run inference depending on how you want to integrate the service:\nUsing NeMo client APIs available in the NeMo framework inference container\nUsing PyTriton to create a client app in your environment\nUsing any library/tool that can send an HTTP request, as the deployed service exposes an HTTP endpoint.\nAn example of option 1, using NeMo client APIs is as follows. You can use this from the NeMo framework inference container on the same machine or a different machine with access to the service IP and ports.\nfrom nemo.deploy import NemoQuery\n# In this case, we run inference on the same machine\nnq = NemoQuery(url=\"localhost:8000\", model_name=\"Nemotron-3-8B-4K\")\noutput = nq.query_llm(prompts=[\"The meaning of life is\"], max_output_token=200, top_k=1, top_p=0.0, temperature=0.1)\nprint(output) Examples of the other options are available in the README of the inference container.\nNOTE: The chat models (SFT, RLHF and SteerLM) require post-processing the output as these three models have been trained to end their response with \u201c<extra_id_1>\u201d, but the `NemoQuery` API does not yet support stopping the generation automatically when this special token is generated. This can be achieved by modifying `output` as follows:\noutput = nq.query_llm(...)\noutput = [[s.split(\"<extra_id_1>\", 1)[0].strip() for s in out] for out in output]", "Prompting the 8B family of models\nThe models in the NVIDIA Nemotron-3-8B family share a common pretrained foundation. However, the datasets used to tune the chat (SFT, RLHF, SteerLM), and QA models are customized for their specific purposes. Also, building these models employs different training techniques. Consequently, these models are most effective with tailored prompts that follow a template similar to how they were trained.\nThe recommended prompt templates for these models can be found on their respective model cards.\nAs an example, here are the single-turn and multi-turn formats applicable to Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF models:\nNemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF\nSingle-turn prompting\nMulti-turn or Few-shot\n```<extra_id_0>System \n  \n<extra_id_1>User \n{prompt} \n<extra_id_1>Assistant\n<extra_id_0>System \n  \n<extra_id_1>User \n{ prompt 1 } \n<extra_id_1>Assistant \n{ response 1 } \n<extra_id_1>User \n{ prompt 2 } \n<extra_id_1>Assistant \n{ response 2 } \n... \n<extra_id_1>User \n{ prompt N } \n<extra_id_1>Assistant```\nThe prompt and response fields correspond to where your input will go. Here\u2019s an example of formatting your input using the single-turn template.\nPROMPT_TEMPLATE = \"\"\"<extra_id_0>System\n{system}\n<extra_id_1>User\n{prompt}\n<extra_id_1>Assistant\n\"\"\"\nsystem = \"\"\nprompt = \"Write a poem on NVIDIA in the style of Shakespeare\"\nprompt = PROMPT_TEMPLATE.format(prompt=prompt, system=system)\nprint(prompt) NOTE: For the Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF models, we recommend keeping the system prompt empty.", "Further training and customization\nThe NVIDIA Nemotron-3-8B family of models is suitable for further customization for domain-specific datasets. There are several options for this, such as continuing pretraining from checkpoints, SFT or parameter-efficient fine-tuning, alignment on human demonstrations with RLHF, or using the new SteerLM technique from NVIDIA.\nEasy-to-use scripts for the mentioned techniques exist in the NeMo framework training container. We also provide tools for data curation, identifying optimal hyperparameters for training and inference, and running the NeMo framework on your choice of hardware that is on-prem DGX cloud, Kubernetes-enabled platform, or a Cloud Service Provider.\nFor more information, check out the NeMo Framework User Guide or container README.\nThe Nemotron-3-8B family of models is designed for diverse use cases, that not only perform competitively on various benchmarks but are also capable of multiple languages.\nTake them for a spin, and let us know what you think in the comments."], "document_title": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs", "document_url": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/", "document_date": "2023-11-15T16:00:00", "document_date_modified": "2023-11-28T23:57:22", "document_full_text": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs\nLarge language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.\nThe NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise\u2013fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.\nThese new foundation models join NVIDIA NeMo, an end-to-end framework for building, customizing, and deploying LLMs tailored for enterprise use. Businesses can now use these tools for developing AI applications quickly, cost-effectively, and on a large scale. These applications can run in the cloud, data center, and on Windows desktop and laptop machines.\nThe Nemotron-3 8B family is available in the Azure AI Model Catalog, HuggingFace, and the NVIDIA AI Foundation Model hub on the NVIDIA NGC Catalog. It includes base, chat, and question-and-answer (Q&A) models that are designed to solve a variety of downstream tasks. Table 1 shows the full family of foundation models.\nModel\nVariant\nKey Benefit\nBase\nNemotron-3-8B-Base\nEnables customization, including parameter-efficient fine-tuning and continuous pretraining for domain-adapted LLMs\nChat\nNemotron-3-8B-Chat-SFT\nA building block for instruction tuning custom models or user-defined alignment, such as RLHF or SteerLM models\nNemotron-3-8B-Chat-RLHF\nBest out-of-the-box chat model performance\nNemotron-3-8B-Chat-SteerLM\nBest out-of-the-box chat model with flexible alignment at inference time\nQuestion-and- Answer\nNemotron-3-8B-QA\nQ&A LLMs customized on knowledge bases\nTable 1. The Nemotron-3 8B family of foundation models supports a wide range of LLM use cases\nDesigning foundation models for production use\nFoundation models are a powerful building block that reduces the time and resources required to build useful, custom applications. However, organizations must make sure that these models meet their enterprise requirements.\nNVIDIA AI Foundation Models are trained on responsibly sourced datasets, capturing myriad voices and experiences. Rigorous monitoring provides data fidelity and compliance with evolving legal stipulations. Any arising data issues are swiftly addressed, making sure that businesses are armed with AI applications that comply with both legal norms and user privacy. These models can assimilate both publicly available and proprietary datasets.\nNemotron-3-8B base\nThe Nemotron-3-8B base model is a compact, high-performance model for generating human-like text or code. The model has an MMLU 5-shot average of 54.4. The base model also caters to the needs of global enterprises with multilingual capabilities, as it is proficient in 53 languages including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch. This base model is also trained on 37 different coding languages.\nNemotron-3-8B chat\nAdding to this suite are the Nemotron-3-8B chat models, which target LLM-powered chatbot interactions. There are three chat model versions, each designed for unique user-specific adjustments:\nSupervised fine-tuning (SFT)\nReinforcement learning from human feedback (RLHF)\nNVIDIA SteerLM\nThe Nemotron-3-8B-SFT model is the first step in instruct-tuning, from which we build our RLHF model that has the highest MT-Bench score within the 8B category, the most cited metric for chat quality. We suggest the user begin with 8B-chat-RLHF for the best immediate chat interaction, but for enterprises interested in uniquely aligning with their end users\u2019 preferences, we recommend the SFT model while applying their own RLHF.\nFinally, the latest alignment method, SteerLM, offers a new level of flexibility for training and customizing LLMs at inference. With SteerLM, users define all the attributes they want and embed them in a single model. Then, they can choose the combination they need for a given use case while the model is running.\nThis method enables a continuous improvement cycle. Responses from a custom model can serve as data for a future training run that dials the model into new levels of usefulness.\nNemotron-3-8B question-and-answer\nThe Nemotron-3-8B-QA model is a question-and-answer (QA) model fine-tuned on a large amount of data focused on the target use case.\nThe Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in \u200cQA.\nThe Nemotron-3-8B-QA model has been tested against other state-of-the-art language models that have a larger parameter size. This testing was conducted on datasets created by NVIDIA, as well as on Natural Questions and Doc2Dial datasets. Results show that this model performs well.\nBuilding custom LLMs with NVIDIA NeMo framework\nNVIDIA NeMo simplifies the path to building customized, enterprise generative AI models by providing end-to-end capabilities and containerized recipes for several model architectures. With the Nemotron-3-8B family of models, developers have access to pretrained models from NVIDIA that can be easily adapted for their specific use cases.\nFast model deployment\nWhen using the NeMo framework, there isn\u2019t a need for collecting data or setting up infrastructure. NeMo streamlines the process. Developers can customize existing models and deploy them in production quickly.\nOptimal model performance\nFurthermore, it integrates seamlessly with the NVIDIA TensorRT-LLM open-source library, which optimizes model performance, along with NVIDIA Triton Inference Server, which accelerates the inference serving process. This combination of tools enables cutting-edge accuracy, low latency, and high throughput.\nData privacy and security\nNeMo enables secure, efficient large-scale deployments that comply with safety and security regulations. For example, if data privacy is a key concern for your business, you can use NeMo Guardrails to store customer data securely without compromising on performance or reliability.\nOverall, building custom LLMs with the NeMo framework is an effective way to create enterprise AI applications quickly without sacrificing quality or security standards. It offers developers flexibility in terms of customization while providing the robust tools needed for rapid deployment at scale.\nGetting started with Nemotron-3-8B\nYou can easily run inference on the Nemotron-3-8B model with the NeMo framework, which leverages TensorRT-LLM, an open source library that provides advanced optimizations for efficient and easy LLM inference on NVIDIA GPUs. It has built-in support for various optimization techniques including:\nKV caching\nEfficient Attention modules (including MQA, GQA, and Paged Attention)\nIn-flight (or continuous) batching\nSupport for low-precision (INT8/FP8) quantization, among other optimizations.\nThe NeMo framework inference container includes all the necessary scripts and dependencies to apply TensorRT-LLM optimizations on NeMo models such as the Nemotron-3-8B family, and host them with Triton Inference Server. With deployment, it exposes an endpoint you can send your inference queries to.\nPrerequisites\nTo follow the instructions to deploy and infer, you\u2019ll need access to:\nNVIDIA Data Center GPUs: at least (1) A100 \u2013 40 GB / 80 GB, (2) H100 \u2013 80 GB, or (3) L40S.\nNVIDIA NeMo framework: provides you with both the training and inference containers to customize \u200cor deploy the Nemotron-3-8B family of models.\nSteps to deploy on Azure ML\nThe models in the Nemotron-3-8B family are available in the Azure ML Model Catalog for deploying in Azure ML-managed endpoints. AzureML provides an easy to use \u2018no-code deployment\u2019 flow hat makes deploying Nemotron-3-8B family models very easy. The underlying plumbing that is the NeMo framework inference container is integrated within the platform.\nScreenshot showing real-time endpoint selection for model deployment within Azure ML.\nFigure 1. Select real-time endpoint in Azure ML\nTo deploy the NVIDIA foundation model on Azure ML for inference, follow the following steps:\nLog in to your Azure account: https://portal.azure.com/#home\nNavigate to Azure ML Machine Learning Studio\nSelect your workspace & navigate to the model catalog\nNVIDIA AI Foundation models are available for fine-tuning, evaluation, and deployment on Azure. Customization of the models can be done within Azure ML using the NeMo training framework. The NeMo framework, consisting of training and inference containers, is already integrated within AzureML.\nTo fine-tune the base model, select your favorite model variant, click \u2018fine-tune\u2019, fill in parameters like task type, custom training data, train and validation split, and the compute cluster.\nTo deploy the model, select your favorite model variant, click \u2018Real-time endpoint\u2019, select instance, endpoint, and other parameters for customizing deployment. Click deploy to deploy the model for inference to an endpoint.\nAzure CLI and SDK support are also available for running fine-tuning jobs and deployments on Azure ML. For more information, please refer to Foundation Models in Azure ML documentation.\nSteps for deploying on-prem or on other clouds\nThe models in the Nemotron-3-8B family have distinct prompt templates for inference requests that are recommended as best practice, however, the instructions to deploy them are similar, as they share the same base architecture.\nFor the latest deployment instructions using the NeMo framework inference container, see https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference.\nTo demonstrate, let\u2019s deploy Nemotron-3-8B-Base-4k.\nLog in to the NGC Catalog, and fetch the Inference container.\n# log in to your NGC organization\ndocker login nvcr.io\n# Fetch the NeMo framework inference container\ndocker pull nvcr.io/ea-bignlp/ga-participants/nemofw-inference:23.10\nDownload the Nemotron-3-8B-Base-4k model. The 8B family of models is available on NGC Catalog, as well as on Hugging Face. You may choose to download the model from either one.\nNVIDIA NGC\nThe easiest way to download the model from NGC is to use the CLI. If you don\u2019t have the NGC CLI installed, follow the Getting Started instructions to install and configure it.\n# Downloading using CLI. The model path can be obtained from it\u2019s page on NGC\nngc registry model download-version \"dztrnjtldi02/nemotron-3-8b-base-4k:1.0\" Hugging Face Hub\nThe following command uses git-lfs, but you may use any of the methods supported by Hugging Face to download models.\ngit lfs install\ngit clone https://huggingface.co/nvidia/nemotron-3-8b-base-4k nemotron-3-8b-base-4k_v1.0\nRun the NeMo inference container in interactive mode, mounting the relevant paths\n# Create a folder to cache the built TRT engines. This is recommended so they don\u2019t have to be built on every deployment call.\nmkdir -p trt-cache\n# Run the container, mounting the checkpoint and the cache directory\ndocker run --rm --net=host \\\n--gpus=all \\\n-v $(pwd)/nemotron-3-8b-base-4k_v1.0:/opt/checkpoints/ \\\n-v $(pwd)/trt-cache:/trt-cache \\\n-w /opt/NeMo \\\n-it nvcr.io/ea-bignlp/ga-participants/nemofw-inference:23.10 bash 4. Convert and Deploy the model on Triton Inference Server with TensorRT-LLM backend.\npython scripts/deploy/deploy_triton.py \\\n--nemo_checkpoint /opt/checkpoints/Nemotron-3-8B-Base-4k.nemo \\\n--model_type=\"gptnext\" \\\n--triton_model_name Nemotron-3-8B-4K \\\n--triton_model_repository /trt-cache/8b-base-4k \\\n--max_input_len 3000 \\\n--max_output_len 1000 \\\n--max_batch_size 2 Note that this script will export the built TensorRT-LLM engine files at the path specified in \u201c\u2013triton_model_repository\u201d. To load the exported model without rebuilding engines in subsequent deployments, you can skip the \u201c\u2013nemo_checkpoint\u201d, \u201c\u2013max_input_len\u201d, \u201c\u2013max_output_len\u201d, and \u201cmax_batch_size\u201d arguments. More information about using this script can be found in the documentation.\nWhen this command is successfully completed, it exposes an endpoint you can query. Let\u2019s look at how you can do that.\nSteps to run inference\nThere are several options available to run inference depending on how you want to integrate the service:\nUsing NeMo client APIs available in the NeMo framework inference container\nUsing PyTriton to create a client app in your environment\nUsing any library/tool that can send an HTTP request, as the deployed service exposes an HTTP endpoint.\nAn example of option 1, using NeMo client APIs is as follows. You can use this from the NeMo framework inference container on the same machine or a different machine with access to the service IP and ports.\nfrom nemo.deploy import NemoQuery\n# In this case, we run inference on the same machine\nnq = NemoQuery(url=\"localhost:8000\", model_name=\"Nemotron-3-8B-4K\")\noutput = nq.query_llm(prompts=[\"The meaning of life is\"], max_output_token=200, top_k=1, top_p=0.0, temperature=0.1)\nprint(output) Examples of the other options are available in the README of the inference container.\nNOTE: The chat models (SFT, RLHF and SteerLM) require post-processing the output as these three models have been trained to end their response with \u201c<extra_id_1>\u201d, but the `NemoQuery` API does not yet support stopping the generation automatically when this special token is generated. This can be achieved by modifying `output` as follows:\noutput = nq.query_llm(...)\noutput = [[s.split(\"<extra_id_1>\", 1)[0].strip() for s in out] for out in output]\nPrompting the 8B family of models\nThe models in the NVIDIA Nemotron-3-8B family share a common pretrained foundation. However, the datasets used to tune the chat (SFT, RLHF, SteerLM), and QA models are customized for their specific purposes. Also, building these models employs different training techniques. Consequently, these models are most effective with tailored prompts that follow a template similar to how they were trained.\nThe recommended prompt templates for these models can be found on their respective model cards.\nAs an example, here are the single-turn and multi-turn formats applicable to Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF models:\nNemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF\nSingle-turn prompting\nMulti-turn or Few-shot\n```<extra_id_0>System \n  \n<extra_id_1>User \n{prompt} \n<extra_id_1>Assistant\n<extra_id_0>System \n  \n<extra_id_1>User \n{ prompt 1 } \n<extra_id_1>Assistant \n{ response 1 } \n<extra_id_1>User \n{ prompt 2 } \n<extra_id_1>Assistant \n{ response 2 } \n... \n<extra_id_1>User \n{ prompt N } \n<extra_id_1>Assistant```\nThe prompt and response fields correspond to where your input will go. Here\u2019s an example of formatting your input using the single-turn template.\nPROMPT_TEMPLATE = \"\"\"<extra_id_0>System\n{system}\n<extra_id_1>User\n{prompt}\n<extra_id_1>Assistant\n\"\"\"\nsystem = \"\"\nprompt = \"Write a poem on NVIDIA in the style of Shakespeare\"\nprompt = PROMPT_TEMPLATE.format(prompt=prompt, system=system)\nprint(prompt) NOTE: For the Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF models, we recommend keeping the system prompt empty.\nFurther training and customization\nThe NVIDIA Nemotron-3-8B family of models is suitable for further customization for domain-specific datasets. There are several options for this, such as continuing pretraining from checkpoints, SFT or parameter-efficient fine-tuning, alignment on human demonstrations with RLHF, or using the new SteerLM technique from NVIDIA.\nEasy-to-use scripts for the mentioned techniques exist in the NeMo framework training container. We also provide tools for data curation, identifying optimal hyperparameters for training and inference, and running the NeMo framework on your choice of hardware that is on-prem DGX cloud, Kubernetes-enabled platform, or a Cloud Service Provider.\nFor more information, check out the NeMo Framework User Guide or container README.\nThe Nemotron-3-8B family of models is designed for diverse use cases, that not only perform competitively on various benchmarks but are also capable of multiple languages.\nTake them for a spin, and let us know what you think in the comments."}], "https://developer.nvidia.com/blog/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/": [{"text": "Diamond Light Source is a leading synchrotron facility in the UK that offers advanced imaging techniques like ptychography for nanoscale-resolution images. To speed up live processing of data, they collaborated with NVIDIA Holoscan, an SDK for sensor processing. By optimizing the preprocessing tasks using JAX and CuPy on NVIDIA GPUs, they achieved significant speedups. Holoscan allowed for the development of high-performance, low-latency sensor processing applications that can scale easily. By incorporating streaming IO and AI inference operators, they were able to reduce the time it takes to reconstruct images and provide real-time feedback to users. This approach also enables the processing of multi-dimensional scans at kilohertz speeds and leverages edge computing with AI processing on NVIDIA GPUs to accelerate x-ray microscopy data processing. Overall, this collaboration with NVIDIA Holoscan has improved the efficiency of the ptychography pipeline at Diamond Light Source, leading to faster scientific breakthroughs and enhanced user interaction.", "text_components": ["Accelerating Ptychography Workflows with NVIDIA Holoscan at Diamond Light Source\nDiamond Light Source is a world-renowned synchrotron facility in the UK that provides scientists with access to intense beams of x-rays, infrared, and other forms of light to study materials and biological structures. The facility boasts over 30 experimental stations or beamlines, and is home to some of the most advanced and complex scientific research projects in the world.\nI08-1, a soft x-ray beamline at Diamond Light Source, offers an advanced high-resolution imaging technique called ptychography to provide nanoscale-resolution images. Ptychography uses a computational imaging approach which, from measurements or diffraction patterns resulting from the interaction of an x-ray beam with a sample, can reconstruct an image of the sample to nanometer-scale resolution.\nThis is critical for imaging nanometer-scale features in many biological structures, such as mitochondria and organelles in cells, and also the interior structure and defects in material science samples. This process of reconstructing an image is powerful but can result in a significant gap between measuring the data and seeing an image.\nThe I08-1 detector operates in the range of 25 image frames per second, but detectors that can operate at the thousands of image frames per second range are on the horizon. Accelerated computing at the edge is needed for these sensor instruments.\nFaster scans enable the study of more dynamic working processes. They increase the throughput of experiments, and live processing offers users real-time feedback to adjust the experiment samples, detector settings, and explore the sample to discover interesting scientific results.\nThis post discusses our work with I08-1 to speed up the live processing of beamline experiment data by refactoring the data analysis workflows. It also addresses key challenges, such as the generic ptychography workflow that currently operates in a serial fashion where images are written to disk at a frame rate frequency of 25 Hz.\nThe live-processing pipeline is launched after the scan is finished, and the processing application ( PtyPy ) can work on the complete dataset. The PtyPy application is optimized using GPU acceleration, but I/O communications are a major bottleneck to achieving higher throughputs.\nPtychographic pipeline showing sensor, preprocessing, data loading and 2D reconstruction, and Image display pipeline steps with each step handing off to the next through a written image file. The delay and idle time grows with each step and is shown on the vertical axis.\nFigure 1. Serial file-based ptychographic pipeline before introduction of NVIDIA Holoscan To accelerate the ptychography workflow, we incorporated NVIDIA Holoscan, an SDK for sensor processing that makes it easier for scientists, researchers, and developers to optimize and scale sensor processing workflows (Figure 2). For example, the JAX library was used within a Holoscan operator to speed up the image preprocessing.\nHoloscan enables researchers and developers to develop high-performance, low-latency sensor processing applications that can scale more easily using reference examples in familiar languages.\nThe Holoscan SDK diagram shows a block at the top for Holohub containing reference applications and a block for Model Zoo containing NGC and Monai. The Holoscan SDK block contains blocks for Python, JAX, CuPy, RAPIDS, C++, or Graph Composer. The Holoscan SDK block also contains operator blocks with re-usable code segments with APIs for IO, AI Inference, Visualization, and Customer functions. The Holoscan software stack sits on top of blocks representing I/O libraries with DPDK and Rivermax, AI libraries such as TensorRT and Triton, and Visualization libraries such as Vulcan. NVIDIA Acceleration Libraries is shown as foundation block underlying everything comprising the Holoscan SDK to deliver accelerated computing. Icons at the bottom of the figure denote it is available to run on appliances, workstations, servers, or the cloud.\nFigure 2. NVIDIA Holoscan SDK is designed for sensor applications, from surgery to satellites The soft x-ray ptychography instrument at I08-1 uses a sCMOS camera for collecting diffraction data for the ptychography experiments. The raw data comes as frames of shape ( ```2048, 2048``` ) and type ```uint16```. Before feeding this data into the iterative ptychography solver application (PtyPy), the following routine preprocessing tasks are performed per raw frame:\nSubtraction of a background dark current image\nCropping around the center and re-binning to reduce reconstruction times\nWhile the first task is necessary to obtain clean diffraction images, the other two provide a level of loss-less compression depending on \u200cexperimental circumstances. Ideally, all of these steps are performed as close as possible to the source (on-chip or using FPGAs, for example). Unfortunately, none of these are available options for the particular camera used in this scenario.\nJAX was used to significantly speed up a single-threaded Python script performing the tasks listed above, and with minimal changes to the code. Because the original frame processing code was written in NumPy, the JAX JIT was able to fuse the processing routine into a single GPU kernel, which yielded a speedup of more than 2,000x for a single image over the original NumPy version (ignoring the required data transfers from host to device). Even with data transfers, the speedup is more than 40x over the original CPU-based NumPy implementation.\nWhile acquiring the data can be relatively quick, it can easily take minutes or tens of minutes to reconstruct the data into an image that the investigator or researcher can interpret. This deadtime between a scan and an image is both inefficient and hampers the ability of the investigator to determine if the instrument has been set up properly or if the sample area being scanned is interesting until a scan result is shown.\nBy architecting the ptychography application as a collection of application and Holoscan operator fragments, developers can leverage the PtyPy ptychography code, Holoscan AI inference, and Holoscan network operators, to relatively quickly prototype and test a new GPU-accelerated version of the live-processing ptychography application.\nThe figure shows an application made of two fragments, each composed of chained operators that are serially connected. One example shows two operators connected in parallel.\nFigure 3. Applications in Holoscan are a directed acyclic graph of operators An operator consists of input ports and output ports and contact re-usable algorithm logic inside.\nFigure 4. Operators ingest input data, then process and publish on the output ports The table of Core Holoscan Operators are listed under the categories of I/O, AI Inference, and Visualization. I/O operators are V4L2Source, AJASource, EmergentSource, BasicNetworkRx/Tx, and VideoStreamReplayer. AI inference operators are TensorRTInference and MultiAIInference. Visualization operators are Holoviz, OpenGLRenderer, and SegmentationVisualizer.\nFigure 5. Holoscan SDK reference applications and core operators facilitate streaming edge HPC development The next part of the challenge was how to speed up the live-processing frame rate for the beamline I-08 ptychography workflow to cope with the current and future sCMOS frames rates. By overlapping the serial workflow steps and using Holoscan, this beamline should be able to provide live processing that matches the frame rate of the sensor. This would enable beamline users to observe ptychographically reconstructed images of a sample in real time.\nThe ptychography workflow has been redone replacing all of the file-based IO stages between steps with streaming IO. The steps are Sensor processing with EPICS, preprocessing which is a Python script, data loading and 2D reconstruction which are embodied in the ptychography software, and Image display which provides the user interface. The total time to run is much lower that the original file-based workflow and is shown on the vertical axis with smaller and fewer idle time periods.\nFigure 6. On-the-fly socket-based ptychographic reconstruction pipeline after using Holoscan On the left are 16 out of 1257 diffraction patterns, which are ptychographically processed to reconstruct a 2D image of a butterfly wing. On the right is an image of a butterfly wing at the one-micrometer level of resolution.\nFigure 7. 2D reconstruction from 1,257 diffraction patterns to create a high-resolution image of a butterfly wing Before\nAfter\nData collection\n57 sec\n57 sec\nPreprocessing\n94 sec\n58 sec\nPtyPy data loaded\n119 sec\n61 sec\nPtyPy data reconstructed\n128 sec\n72 sec\nUser wait time\n~ 71 sec\n15 sec\nTable 1. All times are relative to the start of scan except the user wait time, which is relative to the end of scan\ne figure shows the before and after times for the ptychography reconstruction pipeline. The before pipeline does not use Holoscan and has file I/O stage between steps. The times match the numbers in Table 1 above. The after pipeline uses Holoscan for streaming network IO between steps and reduced user waiting time to only 15 seconds.\nFigure 8. Comparison of file-based and streaming ptychography pipelines using Holoscan\n3D reconstruction creates a larger problem that has the same requirements for live-processing. Scaling up multi-GPU and multi-node processing to provide overlapping, parallel processing of many scans may be a means to keep up with the processing and I/O requirements for live processing.\nOur collaboration aims to test various workflow configurations on a local edge server with two NVIDIA A2 GPUs, where preprocessing runs on one GPU, and image reconstruction runs on the second GPU. This approach enables focusing on bespoke ptychography code while leveraging an edge network I/O operator and AI acceleration libraries that can readily be reused and scaled up to multi-node if necessary for production usage.\nHoloscan enables the creation of an end-to-end data streaming pipeline that enables live ptychographic image processing at the I08-1 beam line, which considerably enriches the overall user interaction. As mentioned, other Diamond Light Source beamlines operate in the kilohertz detection range, but none can perform live processing at that rate.\nOn the left, the x-ray sensor scans all positions and produces diffractions data. Ptychography software runs 1 to N times to process scans to construct the sample image.\nFigure 9. Tomography, spectroscopy, or other multi-dimensional scans consist of 1 to N runs of conventional ptychography software Figure 10 shows the x-ray beam-line instrument on the left. It is shown to generate diffraction patterns and the raw data is shown transmitted to a GPU-accelerated edge inference server running a trained AI surrogate model with many parts of the algorithm running in parallel to compute phase and amplitude information. A line shows training data and image outputs are sent to the local supercomputing facility cluster that trains the ptychography reconstruction model. The Edge inference server fine tunes this AI surrogate model using the image estimates.\nFigure 10. Train AI models for tomography, spectroscopy, or other multi-dimensional scans at the supercomputer facility Training AI models on scan data and then using the model to run inference on GPUs at the beamline is a promising method to achieve live processing at kilohertz speeds.", "Summary\nSensor processing pipelines, such as the ptychography pipeline described in this post, combine significant processing and I/O requirements into a single application. As \u200csensor resolution and refresh rates increase, a file-based approach is no longer tenable, pushing processing redesigns to use a real-time streaming workflow.\nThis requires \u200cproper consideration of end-to-end performance, which immediately highlights I/O bottlenecks across the pipeline. The acceleration of the preprocessing and reconstruction operations by NVIDIA GPU edge systems using a combination of JAX, CuPy, and CUDA yields the necessary performance. However, that only amplifies the effect of the I/O bottlenecks, according to our end-to-end analysis.\nHoloscan provides the tools to build streaming processing software pipelines that also leverage the capabilities of the hardware. This includes operators to ingest data directly into the GPU from the network (and the reverse) to better feed the GPUs and increase their utilization. This utilization can be particularly low during pure streaming processing like the preprocessing steps previously highlighted. Because Holoscan is designed to be easily modular, it also enables additional features like the deep learning or live-visualization features discussed.\nBy using GPU-accelerated computing with Holoscan, I08-1 can significantly reduce the time it takes to process x-ray microscopy data and accelerate the frame rates for image processing. The edge nodes are equipped with high-performance computing (HPC) hardware, including NVIDIA GPUs installed on CPU servers. These edge appliances or servers are designed to accelerate image processing and machine learning.\nTo enable real-time processing, Diamond Light Source has a distributed computing architecture that includes multiple edge nodes and a central data processing facility.\nThe chart depicts a canonical architecture for edge processing. On the left starting with data preparation, the data center supercomputer is employed to train the AI model which can be deployed at scale to edge sites next to sensor instruments such as light sheet microscopes, x-ray beam lines, radio telescopes, etc. The streaming data from the sensor instrument on the right is analyzed in edge servers that run AI at the edge. The edge server sends data back to the data center to assist with retraining the model.\nFigure 11. Data center-to-edge computing workflows run AI at the edge The edge nodes are located near the x-ray beamlines and are responsible for processing the data as it is generated. The processed data is then sent to the central data processing facility, where it\u2019s further analyzed and stored.\nEdge HPC with AI processing on NVIDIA GPUs can accelerate x-ray microscopy data processing in several ways. GPUs are highly efficient at processing large amounts of image data in parallel. In x-ray microscopy, NVIDIA GPUs can be used to accelerate tasks such as noise reduction, image registration, and image segmentation.\nMachine learning algorithms, such as deep learning neural networks, can be used to analyze x-ray microscopy data and extract meaningful information. NVIDIA GPUs are well-suited for accelerating the training and inference stages of these algorithms.\nBy using NVIDIA Holoscan in a streaming AI framework for image processing, machine learning, tomographic reconstruction, and data compression, edge AI processing can significantly reduce the time and resources required to process x-ray microscopy data and enable faster scientific breakthroughs.\nWith HPC edge processing, Diamond Light Source is taking a significant step towards the democratization of science by providing scientists with the tools they need to make real-time decisions and accelerate research."], "document_title": "Accelerating Ptychography Workflows with NVIDIA Holoscan at Diamond Light Source", "document_url": "https://developer.nvidia.com/blog/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/", "document_date": "2023-11-14T17:00:00", "document_date_modified": "2023-11-16T19:16:36", "document_full_text": "Accelerating Ptychography Workflows with NVIDIA Holoscan at Diamond Light Source\nDiamond Light Source is a world-renowned synchrotron facility in the UK that provides scientists with access to intense beams of x-rays, infrared, and other forms of light to study materials and biological structures. The facility boasts over 30 experimental stations or beamlines, and is home to some of the most advanced and complex scientific research projects in the world.\nI08-1, a soft x-ray beamline at Diamond Light Source, offers an advanced high-resolution imaging technique called ptychography to provide nanoscale-resolution images. Ptychography uses a computational imaging approach which, from measurements or diffraction patterns resulting from the interaction of an x-ray beam with a sample, can reconstruct an image of the sample to nanometer-scale resolution.\nThis is critical for imaging nanometer-scale features in many biological structures, such as mitochondria and organelles in cells, and also the interior structure and defects in material science samples. This process of reconstructing an image is powerful but can result in a significant gap between measuring the data and seeing an image.\nThe I08-1 detector operates in the range of 25 image frames per second, but detectors that can operate at the thousands of image frames per second range are on the horizon. Accelerated computing at the edge is needed for these sensor instruments.\nFaster scans enable the study of more dynamic working processes. They increase the throughput of experiments, and live processing offers users real-time feedback to adjust the experiment samples, detector settings, and explore the sample to discover interesting scientific results.\nThis post discusses our work with I08-1 to speed up the live processing of beamline experiment data by refactoring the data analysis workflows. It also addresses key challenges, such as the generic ptychography workflow that currently operates in a serial fashion where images are written to disk at a frame rate frequency of 25 Hz.\nThe live-processing pipeline is launched after the scan is finished, and the processing application ( PtyPy ) can work on the complete dataset. The PtyPy application is optimized using GPU acceleration, but I/O communications are a major bottleneck to achieving higher throughputs.\nPtychographic pipeline showing sensor, preprocessing, data loading and 2D reconstruction, and Image display pipeline steps with each step handing off to the next through a written image file. The delay and idle time grows with each step and is shown on the vertical axis.\nFigure 1. Serial file-based ptychographic pipeline before introduction of NVIDIA Holoscan To accelerate the ptychography workflow, we incorporated NVIDIA Holoscan, an SDK for sensor processing that makes it easier for scientists, researchers, and developers to optimize and scale sensor processing workflows (Figure 2). For example, the JAX library was used within a Holoscan operator to speed up the image preprocessing.\nHoloscan enables researchers and developers to develop high-performance, low-latency sensor processing applications that can scale more easily using reference examples in familiar languages.\nThe Holoscan SDK diagram shows a block at the top for Holohub containing reference applications and a block for Model Zoo containing NGC and Monai. The Holoscan SDK block contains blocks for Python, JAX, CuPy, RAPIDS, C++, or Graph Composer. The Holoscan SDK block also contains operator blocks with re-usable code segments with APIs for IO, AI Inference, Visualization, and Customer functions. The Holoscan software stack sits on top of blocks representing I/O libraries with DPDK and Rivermax, AI libraries such as TensorRT and Triton, and Visualization libraries such as Vulcan. NVIDIA Acceleration Libraries is shown as foundation block underlying everything comprising the Holoscan SDK to deliver accelerated computing. Icons at the bottom of the figure denote it is available to run on appliances, workstations, servers, or the cloud.\nFigure 2. NVIDIA Holoscan SDK is designed for sensor applications, from surgery to satellites The soft x-ray ptychography instrument at I08-1 uses a sCMOS camera for collecting diffraction data for the ptychography experiments. The raw data comes as frames of shape ( ```2048, 2048``` ) and type ```uint16```. Before feeding this data into the iterative ptychography solver application (PtyPy), the following routine preprocessing tasks are performed per raw frame:\nSubtraction of a background dark current image\nCropping around the center and re-binning to reduce reconstruction times\nWhile the first task is necessary to obtain clean diffraction images, the other two provide a level of loss-less compression depending on \u200cexperimental circumstances. Ideally, all of these steps are performed as close as possible to the source (on-chip or using FPGAs, for example). Unfortunately, none of these are available options for the particular camera used in this scenario.\nJAX was used to significantly speed up a single-threaded Python script performing the tasks listed above, and with minimal changes to the code. Because the original frame processing code was written in NumPy, the JAX JIT was able to fuse the processing routine into a single GPU kernel, which yielded a speedup of more than 2,000x for a single image over the original NumPy version (ignoring the required data transfers from host to device). Even with data transfers, the speedup is more than 40x over the original CPU-based NumPy implementation.\nWhile acquiring the data can be relatively quick, it can easily take minutes or tens of minutes to reconstruct the data into an image that the investigator or researcher can interpret. This deadtime between a scan and an image is both inefficient and hampers the ability of the investigator to determine if the instrument has been set up properly or if the sample area being scanned is interesting until a scan result is shown.\nBy architecting the ptychography application as a collection of application and Holoscan operator fragments, developers can leverage the PtyPy ptychography code, Holoscan AI inference, and Holoscan network operators, to relatively quickly prototype and test a new GPU-accelerated version of the live-processing ptychography application.\nThe figure shows an application made of two fragments, each composed of chained operators that are serially connected. One example shows two operators connected in parallel.\nFigure 3. Applications in Holoscan are a directed acyclic graph of operators An operator consists of input ports and output ports and contact re-usable algorithm logic inside.\nFigure 4. Operators ingest input data, then process and publish on the output ports The table of Core Holoscan Operators are listed under the categories of I/O, AI Inference, and Visualization. I/O operators are V4L2Source, AJASource, EmergentSource, BasicNetworkRx/Tx, and VideoStreamReplayer. AI inference operators are TensorRTInference and MultiAIInference. Visualization operators are Holoviz, OpenGLRenderer, and SegmentationVisualizer.\nFigure 5. Holoscan SDK reference applications and core operators facilitate streaming edge HPC development The next part of the challenge was how to speed up the live-processing frame rate for the beamline I-08 ptychography workflow to cope with the current and future sCMOS frames rates. By overlapping the serial workflow steps and using Holoscan, this beamline should be able to provide live processing that matches the frame rate of the sensor. This would enable beamline users to observe ptychographically reconstructed images of a sample in real time.\nThe ptychography workflow has been redone replacing all of the file-based IO stages between steps with streaming IO. The steps are Sensor processing with EPICS, preprocessing which is a Python script, data loading and 2D reconstruction which are embodied in the ptychography software, and Image display which provides the user interface. The total time to run is much lower that the original file-based workflow and is shown on the vertical axis with smaller and fewer idle time periods.\nFigure 6. On-the-fly socket-based ptychographic reconstruction pipeline after using Holoscan On the left are 16 out of 1257 diffraction patterns, which are ptychographically processed to reconstruct a 2D image of a butterfly wing. On the right is an image of a butterfly wing at the one-micrometer level of resolution.\nFigure 7. 2D reconstruction from 1,257 diffraction patterns to create a high-resolution image of a butterfly wing Before\nAfter\nData collection\n57 sec\n57 sec\nPreprocessing\n94 sec\n58 sec\nPtyPy data loaded\n119 sec\n61 sec\nPtyPy data reconstructed\n128 sec\n72 sec\nUser wait time\n~ 71 sec\n15 sec\nTable 1. All times are relative to the start of scan except the user wait time, which is relative to the end of scan\ne figure shows the before and after times for the ptychography reconstruction pipeline. The before pipeline does not use Holoscan and has file I/O stage between steps. The times match the numbers in Table 1 above. The after pipeline uses Holoscan for streaming network IO between steps and reduced user waiting time to only 15 seconds.\nFigure 8. Comparison of file-based and streaming ptychography pipelines using Holoscan\n3D reconstruction creates a larger problem that has the same requirements for live-processing. Scaling up multi-GPU and multi-node processing to provide overlapping, parallel processing of many scans may be a means to keep up with the processing and I/O requirements for live processing.\nOur collaboration aims to test various workflow configurations on a local edge server with two NVIDIA A2 GPUs, where preprocessing runs on one GPU, and image reconstruction runs on the second GPU. This approach enables focusing on bespoke ptychography code while leveraging an edge network I/O operator and AI acceleration libraries that can readily be reused and scaled up to multi-node if necessary for production usage.\nHoloscan enables the creation of an end-to-end data streaming pipeline that enables live ptychographic image processing at the I08-1 beam line, which considerably enriches the overall user interaction. As mentioned, other Diamond Light Source beamlines operate in the kilohertz detection range, but none can perform live processing at that rate.\nOn the left, the x-ray sensor scans all positions and produces diffractions data. Ptychography software runs 1 to N times to process scans to construct the sample image.\nFigure 9. Tomography, spectroscopy, or other multi-dimensional scans consist of 1 to N runs of conventional ptychography software Figure 10 shows the x-ray beam-line instrument on the left. It is shown to generate diffraction patterns and the raw data is shown transmitted to a GPU-accelerated edge inference server running a trained AI surrogate model with many parts of the algorithm running in parallel to compute phase and amplitude information. A line shows training data and image outputs are sent to the local supercomputing facility cluster that trains the ptychography reconstruction model. The Edge inference server fine tunes this AI surrogate model using the image estimates.\nFigure 10. Train AI models for tomography, spectroscopy, or other multi-dimensional scans at the supercomputer facility Training AI models on scan data and then using the model to run inference on GPUs at the beamline is a promising method to achieve live processing at kilohertz speeds.\nSummary\nSensor processing pipelines, such as the ptychography pipeline described in this post, combine significant processing and I/O requirements into a single application. As \u200csensor resolution and refresh rates increase, a file-based approach is no longer tenable, pushing processing redesigns to use a real-time streaming workflow.\nThis requires \u200cproper consideration of end-to-end performance, which immediately highlights I/O bottlenecks across the pipeline. The acceleration of the preprocessing and reconstruction operations by NVIDIA GPU edge systems using a combination of JAX, CuPy, and CUDA yields the necessary performance. However, that only amplifies the effect of the I/O bottlenecks, according to our end-to-end analysis.\nHoloscan provides the tools to build streaming processing software pipelines that also leverage the capabilities of the hardware. This includes operators to ingest data directly into the GPU from the network (and the reverse) to better feed the GPUs and increase their utilization. This utilization can be particularly low during pure streaming processing like the preprocessing steps previously highlighted. Because Holoscan is designed to be easily modular, it also enables additional features like the deep learning or live-visualization features discussed.\nBy using GPU-accelerated computing with Holoscan, I08-1 can significantly reduce the time it takes to process x-ray microscopy data and accelerate the frame rates for image processing. The edge nodes are equipped with high-performance computing (HPC) hardware, including NVIDIA GPUs installed on CPU servers. These edge appliances or servers are designed to accelerate image processing and machine learning.\nTo enable real-time processing, Diamond Light Source has a distributed computing architecture that includes multiple edge nodes and a central data processing facility.\nThe chart depicts a canonical architecture for edge processing. On the left starting with data preparation, the data center supercomputer is employed to train the AI model which can be deployed at scale to edge sites next to sensor instruments such as light sheet microscopes, x-ray beam lines, radio telescopes, etc. The streaming data from the sensor instrument on the right is analyzed in edge servers that run AI at the edge. The edge server sends data back to the data center to assist with retraining the model.\nFigure 11. Data center-to-edge computing workflows run AI at the edge The edge nodes are located near the x-ray beamlines and are responsible for processing the data as it is generated. The processed data is then sent to the central data processing facility, where it\u2019s further analyzed and stored.\nEdge HPC with AI processing on NVIDIA GPUs can accelerate x-ray microscopy data processing in several ways. GPUs are highly efficient at processing large amounts of image data in parallel. In x-ray microscopy, NVIDIA GPUs can be used to accelerate tasks such as noise reduction, image registration, and image segmentation.\nMachine learning algorithms, such as deep learning neural networks, can be used to analyze x-ray microscopy data and extract meaningful information. NVIDIA GPUs are well-suited for accelerating the training and inference stages of these algorithms.\nBy using NVIDIA Holoscan in a streaming AI framework for image processing, machine learning, tomographic reconstruction, and data compression, edge AI processing can significantly reduce the time and resources required to process x-ray microscopy data and enable faster scientific breakthroughs.\nWith HPC edge processing, Diamond Light Source is taking a significant step towards the democratization of science by providing scientists with the tools they need to make real-time decisions and accelerate research."}], "https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/": [{"text": "The article discusses the importance of balancing speed and sustainability in high-performance computing (HPC) to address the growing energy consumption in the field. It explores the relationship between task completion rate and energy consumption, highlighting the impact of speed on energy efficiency. The article presents data on various HPC applications, such as FUN3D, GROMACS, ICON, LAMMPS, and MILC, showing their energy usage and performance scalability with different configurations of GPUs and Infiniband connections. The analysis reveals that optimizing the number of resources used per simulation can reduce energy consumption, but other objectives like time to solution must also be considered. The article proposes a multi-objective optimization approach to find a balance between speed and energy efficiency in HPC simulations. By optimizing energy usage without sacrificing performance, researchers can achieve more science per unit of energy, contributing to a more sustainable computing environment. The discussion aims to spark conversations within the HPC community about energy-efficient computing practices and the importance of measuring scientific advancements per unit of energy consumption.", "text_components": ["Energy Efficiency in High-Performance Computing: Balancing Speed and Sustainability\nThe world of computing is on the precipice of a seismic shift.\nThe demand for computing power, particularly in high-performance computing (HPC), is growing year over year, which in turn means so too is energy consumption. However, the underlying issue is, of course, that energy is a resource with limitations. So, the world is faced with the question of how we can best shift our computational focus from performance to energy efficiency.\nWhen considering this question, it is important to take into account the correlation between task completion rate and energy consumption. This relationship is often overlooked, but it can be an essential factor.\nThis post explores the relationship between speed and energy efficiency, and the implications should the shift to completing tasks more quickly \u200coccur.\nTake transportation for example.\nA horizontal bar chart showing modes of transportation from walking (0) to taking a commercial air flight (1277) showing the energy used per meter transported per person.\nFigure 1. Transportation energy-efficiency comparison\nIn the case of object motion, in anything other than a vacuum, drag is proportional to the square of the speed of travel. Meaning it takes four times the force and energy to go twice as fast for a given distance. Movement of people and goods around our planet means traveling through either air or water (both are \u201cfluids\u201d in physics) and this concept helps explain why going faster requires much more energy.\nMost transportation technologies operate on fossil fuels because, even now, the energy density of fossil fuels and the weight of the engines that run on them are difficult to rival. For example, nuclear energy technology presents challenges including waste, and specialized personnel required to operate safely, and weight, meaning nuclear-powered cars, buses, or airplanes are not happening anytime soon.\nA line chart of Power compared to Integer speed showing that as speed increases, the power goes up, linearly at first, then exponentially.\nFigure 2. CPU power compared to a normalized benchmark score\nThe silicon processor has the same kind of relationship with speed. The preceding chart shows that increasing integer processing speed is achieved with an increase in power (energy per unit time). So similar to transportation, the faster the processor goes, the more energy it uses.\nYou might be asking, \u201c Why don\u2019t computers run on tens of kilowatts today? They have been getting faster for 50 years.\u201d\nThe answer is that as processors have gotten faster, they have also gotten smaller. Shrinking silicon features make them more efficient from an energy perspective, and therefore at constant power, can run faster at higher clock speeds. Loosely stated, this is known as Dennard scaling.", "Multi-node parallel computing\nA pivotal concept in HPC is the effect that parallel computing reduces wall-clock runtime at the expense of total aggregate compute time. For HPC applications containing small portions of serial operations (for the uninitiated, almost all do), the runtime asymptotically approaches the runtime of the serial operations as more computational resources are added.\nThis is known as Amdahl\u2019s law.\nThis is important when considering the energy consumed by parallel computing. Assuming a compute unit draws the same amount of power while it performs a computation, means that a computational task will consume more power as it is parallelized among larger and larger sets of compute resources.\nIt means that a computational task will consume more power as it is parallelized among larger and larger sets of compute resources. Simultaneously, the task runs for less time. The hypothesis during this investigation is that the reduction in runtime by adding resources does not keep pace with the additional power required by incremental compute resources, and in the end uses more energy.\nOr, as was discussed in the introduction, the faster you want to go, the more energy you will use.", "Measuring energy usage\nUsing the Selene system, based on the NVIDIA DGX A100, engineers can collect various metrics, which are aggregated using Grafana. With its API, users can query power use over time for individual CPUs, GPUs, and power supply units, either for the whole job or a given time frame.\nUsing this data with timestamps from simulation output quantifies the energy used. The reported numbers for energy consumption do not take into account some factors:\nNetwork switching infrastructure (network cards internal to servers are included)\nShared filesystem usage for data distribution and results collection\nConversion from AC to DC at the power supply\nCooling energy consumption for data center CRAC units\nThe largest of the errors is the item for an air-cooled data center and is closely aligned with the power usage effectiveness (PUE) for the data center (~1.4 for air-cooled and ~1.15 for liquid-cooled). Further, the AC to DC conversion could be approximated by increasing measured energy consumption by 12%. However, this discussion focuses on comparisons of energy, and both of these errors are ignored.\nFor the rest of this post, we use Watts (Joules per second) for power and kilowatt-hours (kWh) for energy.", "Experiment setup\nActive NVIDIA Infiniband connections per node\n1\n2\n4\nGPUs per node\n1\n\u2714\n2\n\u2714\n4\n\u2714\n\u2714\n\u2714\nTable 1. Execution geometry of simulations Due to various configurations of GPU-accelerated HPC systems worldwide, requirements, and stages of adoption for accelerated technologies across HPC simulation applications, it is useful to execute parallel simulations using different configurations of compute nodes. This optimizes performance and efficiency further.\nThe preceding table shows the five different ways that each application was scaled. The following plots use a consistent visual representation for ease of reading. Single NVIDIA InfiniBand connections use a dotted gray or black line. Dual Infiniband connections use a solid green line, and quadruple Infiniband connections use an orange-dashed line.", "HPC application performance and energy\nThis section provides insights into some key HPC applications representing disciplines such as computational fluid dynamics, molecular dynamics, weather simulation, and quantum chromodynamics.", "Applications and datasets\nApplication\nVersion\nDataset\nFUN3D\n14.0-d03712b\nWB.C-30M\nGROMACS\n2023\nSTMV (h-bond)\nICON\n2.6.5\nQUBICC 10 km resolution\nLAMMPS\ndevelop_7ac70ce\nTersoff (85M atoms)\nMILC\ngauge-action-quda_16a2d47119\nNERSC Large\nTable 2. HPC simulation applications used for this study", "FUN3D\nFUN3D was first written in the 1980s to study algorithms and develop new methods for unstructured-grid fluid dynamic simulations for incompressible flow up to transonic flow regimes. Over the last 40 years, the project has blossomed into a suite of tools that cover not only analysis, but adjoint-based error estimation, mesh adaptation, and design optimization. It also handles flow regimes up to the hypersonic.\nCurrently, a US citizen-only code, research, academia, and industry users all take advantage of FUN3D today. For example, Boeing, Lockheed, Cessna, New Piper, and others have used the tools for applications such as high-lift, cruise performance, and studies of revolutionary concepts.", "GROMACS\nGROMACS is a molecular dynamics package that uses Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have many complicated bonded interactions. GROMACS is extremely fast at calculating the non-bonded interactions (that usually dominate simulations) and many groups are also using it for research on non-biological systems (for example, polymers).\nGROMACS supports all the usual algorithms from a modern molecular dynamics implementation.", "ICON modeling framework\nICON modeling framework is a joint project between the Deutscher Wetterdienst (DWD) and the Max-Planck-Institute for Meteorology for developing a unified next-generation global numerical weather prediction (NWP) and climate modeling system. The ICON modeling framework became operational in DWD\u2019s forecast system in January 2015.\nICON sets itself apart from other NWP tools through better conservation properties, with the obligatory requirement of exact local mass conservation and mass-consistent transport.\nIt recognizes that climate is a global issue, and therefore strives for better scalability on future massively parallel HPC architectures.", "LAMMPS\nLAMMPS is a classical molecular dynamics code. As the name implies, it is designed to run well on parallel machines.\nIts focus is on materials modeling. As such, it contains potential models for solid-state materials (metals, semiconductors) soft matter (biomolecules, polymers), and coarse-grained or mesoscopic systems. LAMMPS achieves parallelism using message-passing techniques and a spatial decomposition of the simulation domain. Many of its models have versions that provide accelerated performance on both CPUs and GPUs.", "MILC\nMILC is a collaborative project used and maintained by scientists studying the theory of strong interactions of subatomic physics, otherwise called quantum chromodynamics (QCD). MILC performs simulations of four-dimensional SU(3) lattice gauge theory on MIMD parallel machines. MILC is publicly available for research purposes, and publications of work using MILC or derivatives of this code should acknowledge this use.", "Parallel scalability\nIn the following plots, we show a dimensionless quantity ```SPEEDUP```, or strong scaling, as calculated in the standard fashion:\nSPEEDUP = \\frac{runtime_{1\\ GPU \\ and\\ 1\\ IB}}{runtime_{n\\ GPUs\\ and\\ m\\ IB}}=0\nParallel ```SPEEDUP``` is often plotted against an \u201cideal\u201d ```SPEEDUP```, where employing one computational resource gives a ```SPEEDUP``` value of one, employing two resource units gives a ```SPEEDUP``` value of two, and so on. This is not done here because multiple scaling traces are being shown, and the plots get very crowded with data when each has its own ideal ```SPEEDUP``` reference.\nThe number of GPUs per node and the number of enabled CX6 EDR Infiniband connections for each node sweep are shown in the legend with {# GPUs / node} \u2013 {# IB / node}.\nLeft side: FUN3D Speedup showing GPU per node and IB connections per node perform the same starting with one GPU and diverge slightly when scaled out to 128 GPUs with the 2-2 configuration being the most performant, and the max performance point at 64 GPUs.\nFigure 3. FUN3D Speedup with FUN3D plotted relative to four GPUs each with one active GPU and InfiniBand connection\nGROMACS speedup showing the 4-1 configuration being a poor choice, and both the 2-2 and 4-4 configurations scaling very well.\nFigure 4. GROMACS speedup plotted relative to one GPU with and a single node with one InfiniBand connection All ICON configurations scaling from 64, almost identical except 4-1 is 10% worse.\nFigure 5. ICON Speedup with ICON plotted relative to 64 GPUs with one GPU and one active InfiniBand connection\nAll LAMMPS configurations scaling almost identically except 4-1 is 20% worse.\nFigure 6. LAMMPS Speedup with LAMMPS plotted relative to eight GPUs with one GPU and one active InfiniBand connection\nMILC speedup scaled from 32 to 256 GPUs where 4-2 and 4-4 configurations scale the best and reach the highest performance.\nFigure 7. MILC Speedup plotted relative to 32 nodes with one GPU and one active InfiniBand connection\nFigures 3 through 7 show the scalability of each of the simulations. Generally, most of the data follows the trend expected based on Amdahl\u2019s law. A notable exception is the GROMACS configuration using four GPUs and one InfiniBand connection per node (labeled 4-1). This configuration exhibits negative scaling at first, meaning it slows down as resources are added before it starts to scale.\nGiven the network configuration, it seems that \u200cscaling is bound by the bandwidth of the network and the data each parallel thread is trying to communicate. As the number of resources grows between 16 and 32 GPUs, the task reaches a point where it is no longer bandwidth-bound and scales to a small degree.\nSimilar outlier behavior for the 4-1 configuration is seen for ICON and LAMMPS, though both scale much more readily than GROMACS.\nIt is also interesting to note that the 4-4 configuration is not always the best for performance. For FUN3D (2-2) and a portion of the MILC (4-2), scaling other configurations shows superior performance by small margins. However, in all cases, the 4-4 configuration is among the best configurations.", "Energy Usage\nICON uses the most energy, ranging from 17\u201337 kWh, linearly growing as it scales. MILC's energy consumption has the same shape but ranges between 10 and 30 kWh. FUN3D, LAMMPS, and GROMACS all use less than 5 kWh, and slope up to the right, with LAMMPS being the flattest, ranging between 2.5 and 4 kWh on 16-256 GPUs.\nFigure 8. Energy usage for HPC applications running with four GPUs and four Infiniband connections per node\nFor brevity, Figure 8 shows only the four GPU and four IB connections (4-4) energy consumption in a single plot. The amount of energy used by each simulation is arbitrary and can be adjusted up or down by changing the problem size or analysis type. There are two relevant characteristics to note:\nThe slope for each application is positive, meaning that they use more energy as they scale.\nSome applications like FUN3D increase used energy quickly, while others like LAMMPS increase more gradually.", "Time to solution compared to energy to solution\nSome may conclude from Figure 8 that the best configuration to run these simulations is by minimizing the number of resources used per simulation. While the conclusion from an energy standpoint is correct, there are more objectives to consider beyond energy.\nFor example, in the case of a researcher working against a project deadline, time to solution is crucially important. Or, in the case of a commercial enterprise, final data is often required to start the manufacturing process and get a product ready for release to market. Here, too, the time to solution value may outweigh the extra energy required to produce the simulation output sooner.\nTherefore, this is a multi-objective optimization problem with several solutions depending on the weights of each defined objective.", "The ideal case\nBefore exploring the objectives, consider the following ideal case:\nA perfectly parallel (meaning no serialized operations) accelerated HPC application, which scales linearly as additional processors are added. Graphically, the ```SPEEDUP``` curve is simply a line starting from (1,1) and going up and to the right at a slope of 1.0.\nFor such an application, now consider energy. If every processor added consumes the same amount of power, then power consumption during the run is the following equation:\nPower = n \\times P_1\nIn this case, n\nis the number of processors used and P_1\nis the power used by one GPU.\nEnergy = Power \\times time\nBut time is an inverse function of n\n, so you can rewrite it as follows:\nEnergy = P_1 \\times time_1\nThe n\nvalues cancel, and we see that Energy\n, in the ideal case, is not a function of the number of GPUs and is in fact constant.\nEnergy= n \\times P_1 \\times \\frac{time_1}{n}\nIn several cases, HPC applications are not ideal in either parallel speedup or energy. Because speedup is a case of diminishing returns as resources are added, and energy grows roughly linearly as resources are added, there should be a number of GPUs where the ratio of speedup to energy is a maximum.\nMore formally, assuming equal weights for energy and time:\nSPEEDUP_{Energy} = \\frac{SPEEDUP}{EnergyRatio} = \\frac{\\left [ \\frac{runtime_{1\\ GPU \\ and\\ 1\\ IB}}{runtime_{n\\ GPUs\\ and\\ m\\ IB}} \\right ]}{\\left [ \\frac{energy_{n\\ GPUs\\ and\\ m\\ IB}}{energy_{1\\ GPUs\\ and\\ 1\\ IB}} \\right ]}\\\\ = \\left [ \\frac{runtime_{1\\ GPU \\ and\\ 1\\ IB}}{runtime_{n\\ GPUs\\ and\\ m\\ IB}} \\right ] \\times \\left [ \\frac{energy_{n\\ GPUs\\ and\\ m\\ IB}}{energy_{1\\ GPUs\\ and\\ 1\\ IB}} \\right ]\nIn this formula, the following is constant:\n\\left [ runtime_{1\\ GPU \\ and\\ 1\\ IB} \\times energy_{1\\ GPU \\ and\\ 1\\ IB}\\right ]\nYou should maximize the following:\n\\frac{1}{\\left [ runtime_{n\\ GPU \\ and\\ m\\ IB} \\times energy_{n\\ GPU \\ and\\ m\\ IB}\\right ]}\nOr, you should minimize the following:\nruntime_{n\\ GPU \\ and\\ m\\ IB} \\times energy_{n\\ GPU \\ and\\ m\\ IB}\nIf one plots runtime on one axis and energy on the opposite axis, then the quantity to minimize is simply the area defined by runtime multiplied by energy.\nDefining the problem this way also enables the exploration of multiple independent variables, including but not limited to the number of resources used in a parallel run, GPU clock frequency, network latency, bandwidth, and any other variable that influences runtime and energy usage.", "Balanced speed and energy\nThe following plots are the product of runtime and energy consumption, where the independent variable is the number of GPUs in the parallel run. These plots provide one method to determine one solution among the Pareto front of solutions for optimization of runtime and energy.\nA curved plot showing decreasing then increasing with a minimum at 16 GPUs. The 4-2 and 4-4 configurations are best {right side}.\nFigure 9. Runtime * Energy for FUN3D showing a minimum of 16-32 GPUs\nA less obvious optimum for GROMACS with 2-2 being almost flat between four and 32 GPUs and 4-2 and 4-4 configurations best with the peak at eight GPUs.\nFigure 10. Runtime * Energy for GROMACS\nICON Energy * Runtime decreases with GPUs in all cases, slope reaching very close to zero, so 512 GPUs may be optimal. 4-4 and 4-2 configurations are best.\nFigure 11. Runtime * Energy for ICON\nLAMMPS decreases Runtime * Energy throughout the 8-256 GPU range. 4-4 and 4-2 configurations are best.\nFigure 12. Runtime * Energy for LAMMPS\nMILC shows a clear minimum at 32 GPUs using 4-4 configuration.\nFigure 13. Runtime * Energy for MILC\nThe preceding figures show changes in Runtime * Energy as each of the five applications and five GPU/network configurations are scaled with GPU count. Different from the scalability plots of performance, we see that in all cases the 4-4 configuration (reminder, \u201c4-4\u201d refers to {# GPUs / node} \u2013 {# IB / node}) is the best with the 4-2 configuration occasionally a close second. Aligned with the scalability plots, the 1-1 configuration is consistently the worst configuration, likely because of overhead on each server being mostly idle (that is each node had eight A100 GPUs, and eight CX6 Infiniband adaptors).\nPerhaps more interesting is the fact that both LAMMPS and ICON show they have not reached a minimum in Runtime * Energy for the data we collected. Larger runs should be performed for LAMMPS and ICON to show minima.\nA graph showing energy performance and energy drop.\nFigure 14. Cost and benefits of optimizing time to solution and energy to solution compared to maximum performance The desired outcome of the optimization is to run simulations at increased efficiency. Comparing the performance drop to the energy drop per simulation (normalized by the maximum performance point) is a simple way to check that the optimization output achieved efficiency increase. If the performance drop is less than the energy drop, the efficiency is higher, which is seen for FUN3D, GROMACS, and MILC in Figure 14.\nThere is an opportunity for saving energy and the performance cost compared to the maximum performance point. This is shown in Figure 14. ICON and LAMMPS were not plotted because they did not display a minimum point in the optimization. The desired outcome is that performance drops less than the energy used per simulation, and that is precisely what is seen for FUN3D, GROMACS, and MILC.", "Navigating the intersection of multi-node scalability and energy\nData center growth and the expanding exploration and use of AI will drive increased demand for data centers, data center space, cooling, and electrical energy. Given the fraction of electricity generated from fossil fuels, greenhouse gasses driven by data center energy demands are a problem that must be managed.\nNVIDIA continues to focus on making internal changes and influencing the design of future products, maximizing the positive impact of the AI revolution on society. By using the NVIDIA accelerated computing platform, researchers get more science done per unit of time and use less energy for each scientific result. Such energy savings can be equated to a proportional amount of carbon dioxide emissions avoided, which benefits everyone on the planet.\nRead the recent post, Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO for a deeper dive focused on VASP and energy efficiency.\nThis short discussion and set of results are offered to jump-start a conversation about shifting the way HPC centers allocate and track resources provided to the research community. It may also influence users to consider the impact of their choices when running large parallel simulations, and the downstream effects.\nIt is time to start a conversation regarding energy to solution for HPC and AI simulations and the multi-objective optimization balance with time to solution discussed here. Someday, the HPC community may measure scientific advancements per megawatt-hour, or perhaps even per mTon CO2eq, but that won\u2019t be achieved until conversations begin.\nFor more information, visit NVIDIA sustainable computing. Read about NERSC and their impressions of the accelerated platform. Or, listen to Alan Gray\u2019s NVIDIA 2023 GTC session about tuning the accelerated platform for maximum efficiency."], "document_title": "Energy Efficiency in High-Performance Computing: Balancing Speed and Sustainability", "document_url": "https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/", "document_date": "2023-11-14T16:00:00", "document_date_modified": "2023-12-07T19:19:52", "document_full_text": "Energy Efficiency in High-Performance Computing: Balancing Speed and Sustainability\nThe world of computing is on the precipice of a seismic shift.\nThe demand for computing power, particularly in high-performance computing (HPC), is growing year over year, which in turn means so too is energy consumption. However, the underlying issue is, of course, that energy is a resource with limitations. So, the world is faced with the question of how we can best shift our computational focus from performance to energy efficiency.\nWhen considering this question, it is important to take into account the correlation between task completion rate and energy consumption. This relationship is often overlooked, but it can be an essential factor.\nThis post explores the relationship between speed and energy efficiency, and the implications should the shift to completing tasks more quickly \u200coccur.\nTake transportation for example.\nA horizontal bar chart showing modes of transportation from walking (0) to taking a commercial air flight (1277) showing the energy used per meter transported per person.\nFigure 1. Transportation energy-efficiency comparison\nIn the case of object motion, in anything other than a vacuum, drag is proportional to the square of the speed of travel. Meaning it takes four times the force and energy to go twice as fast for a given distance. Movement of people and goods around our planet means traveling through either air or water (both are \u201cfluids\u201d in physics) and this concept helps explain why going faster requires much more energy.\nMost transportation technologies operate on fossil fuels because, even now, the energy density of fossil fuels and the weight of the engines that run on them are difficult to rival. For example, nuclear energy technology presents challenges including waste, and specialized personnel required to operate safely, and weight, meaning nuclear-powered cars, buses, or airplanes are not happening anytime soon.\nA line chart of Power compared to Integer speed showing that as speed increases, the power goes up, linearly at first, then exponentially.\nFigure 2. CPU power compared to a normalized benchmark score\nThe silicon processor has the same kind of relationship with speed. The preceding chart shows that increasing integer processing speed is achieved with an increase in power (energy per unit time). So similar to transportation, the faster the processor goes, the more energy it uses.\nYou might be asking, \u201c Why don\u2019t computers run on tens of kilowatts today? They have been getting faster for 50 years.\u201d\nThe answer is that as processors have gotten faster, they have also gotten smaller. Shrinking silicon features make them more efficient from an energy perspective, and therefore at constant power, can run faster at higher clock speeds. Loosely stated, this is known as Dennard scaling.\nMulti-node parallel computing\nA pivotal concept in HPC is the effect that parallel computing reduces wall-clock runtime at the expense of total aggregate compute time. For HPC applications containing small portions of serial operations (for the uninitiated, almost all do), the runtime asymptotically approaches the runtime of the serial operations as more computational resources are added.\nThis is known as Amdahl\u2019s law.\nThis is important when considering the energy consumed by parallel computing. Assuming a compute unit draws the same amount of power while it performs a computation, means that a computational task will consume more power as it is parallelized among larger and larger sets of compute resources.\nIt means that a computational task will consume more power as it is parallelized among larger and larger sets of compute resources. Simultaneously, the task runs for less time. The hypothesis during this investigation is that the reduction in runtime by adding resources does not keep pace with the additional power required by incremental compute resources, and in the end uses more energy.\nOr, as was discussed in the introduction, the faster you want to go, the more energy you will use.\nMeasuring energy usage\nUsing the Selene system, based on the NVIDIA DGX A100, engineers can collect various metrics, which are aggregated using Grafana. With its API, users can query power use over time for individual CPUs, GPUs, and power supply units, either for the whole job or a given time frame.\nUsing this data with timestamps from simulation output quantifies the energy used. The reported numbers for energy consumption do not take into account some factors:\nNetwork switching infrastructure (network cards internal to servers are included)\nShared filesystem usage for data distribution and results collection\nConversion from AC to DC at the power supply\nCooling energy consumption for data center CRAC units\nThe largest of the errors is the item for an air-cooled data center and is closely aligned with the power usage effectiveness (PUE) for the data center (~1.4 for air-cooled and ~1.15 for liquid-cooled). Further, the AC to DC conversion could be approximated by increasing measured energy consumption by 12%. However, this discussion focuses on comparisons of energy, and both of these errors are ignored.\nFor the rest of this post, we use Watts (Joules per second) for power and kilowatt-hours (kWh) for energy.\nExperiment setup\nActive NVIDIA Infiniband connections per node\n1\n2\n4\nGPUs per node\n1\n\u2714\n2\n\u2714\n4\n\u2714\n\u2714\n\u2714\nTable 1. Execution geometry of simulations Due to various configurations of GPU-accelerated HPC systems worldwide, requirements, and stages of adoption for accelerated technologies across HPC simulation applications, it is useful to execute parallel simulations using different configurations of compute nodes. This optimizes performance and efficiency further.\nThe preceding table shows the five different ways that each application was scaled. The following plots use a consistent visual representation for ease of reading. Single NVIDIA InfiniBand connections use a dotted gray or black line. Dual Infiniband connections use a solid green line, and quadruple Infiniband connections use an orange-dashed line.\nHPC application performance and energy\nThis section provides insights into some key HPC applications representing disciplines such as computational fluid dynamics, molecular dynamics, weather simulation, and quantum chromodynamics.\nApplications and datasets\nApplication\nVersion\nDataset\nFUN3D\n14.0-d03712b\nWB.C-30M\nGROMACS\n2023\nSTMV (h-bond)\nICON\n2.6.5\nQUBICC 10 km resolution\nLAMMPS\ndevelop_7ac70ce\nTersoff (85M atoms)\nMILC\ngauge-action-quda_16a2d47119\nNERSC Large\nTable 2. HPC simulation applications used for this study\nFUN3D\nFUN3D was first written in the 1980s to study algorithms and develop new methods for unstructured-grid fluid dynamic simulations for incompressible flow up to transonic flow regimes. Over the last 40 years, the project has blossomed into a suite of tools that cover not only analysis, but adjoint-based error estimation, mesh adaptation, and design optimization. It also handles flow regimes up to the hypersonic.\nCurrently, a US citizen-only code, research, academia, and industry users all take advantage of FUN3D today. For example, Boeing, Lockheed, Cessna, New Piper, and others have used the tools for applications such as high-lift, cruise performance, and studies of revolutionary concepts.\nGROMACS\nGROMACS is a molecular dynamics package that uses Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have many complicated bonded interactions. GROMACS is extremely fast at calculating the non-bonded interactions (that usually dominate simulations) and many groups are also using it for research on non-biological systems (for example, polymers).\nGROMACS supports all the usual algorithms from a modern molecular dynamics implementation.\nICON modeling framework\nICON modeling framework is a joint project between the Deutscher Wetterdienst (DWD) and the Max-Planck-Institute for Meteorology for developing a unified next-generation global numerical weather prediction (NWP) and climate modeling system. The ICON modeling framework became operational in DWD\u2019s forecast system in January 2015.\nICON sets itself apart from other NWP tools through better conservation properties, with the obligatory requirement of exact local mass conservation and mass-consistent transport.\nIt recognizes that climate is a global issue, and therefore strives for better scalability on future massively parallel HPC architectures.\nLAMMPS\nLAMMPS is a classical molecular dynamics code. As the name implies, it is designed to run well on parallel machines.\nIts focus is on materials modeling. As such, it contains potential models for solid-state materials (metals, semiconductors) soft matter (biomolecules, polymers), and coarse-grained or mesoscopic systems. LAMMPS achieves parallelism using message-passing techniques and a spatial decomposition of the simulation domain. Many of its models have versions that provide accelerated performance on both CPUs and GPUs.\nMILC\nMILC is a collaborative project used and maintained by scientists studying the theory of strong interactions of subatomic physics, otherwise called quantum chromodynamics (QCD). MILC performs simulations of four-dimensional SU(3) lattice gauge theory on MIMD parallel machines. MILC is publicly available for research purposes, and publications of work using MILC or derivatives of this code should acknowledge this use.\nParallel scalability\nIn the following plots, we show a dimensionless quantity ```SPEEDUP```, or strong scaling, as calculated in the standard fashion:\nSPEEDUP = \\frac{runtime_{1\\ GPU \\ and\\ 1\\ IB}}{runtime_{n\\ GPUs\\ and\\ m\\ IB}}=0\nParallel ```SPEEDUP``` is often plotted against an \u201cideal\u201d ```SPEEDUP```, where employing one computational resource gives a ```SPEEDUP``` value of one, employing two resource units gives a ```SPEEDUP``` value of two, and so on. This is not done here because multiple scaling traces are being shown, and the plots get very crowded with data when each has its own ideal ```SPEEDUP``` reference.\nThe number of GPUs per node and the number of enabled CX6 EDR Infiniband connections for each node sweep are shown in the legend with {# GPUs / node} \u2013 {# IB / node}.\nLeft side: FUN3D Speedup showing GPU per node and IB connections per node perform the same starting with one GPU and diverge slightly when scaled out to 128 GPUs with the 2-2 configuration being the most performant, and the max performance point at 64 GPUs.\nFigure 3. FUN3D Speedup with FUN3D plotted relative to four GPUs each with one active GPU and InfiniBand connection\nGROMACS speedup showing the 4-1 configuration being a poor choice, and both the 2-2 and 4-4 configurations scaling very well.\nFigure 4. GROMACS speedup plotted relative to one GPU with and a single node with one InfiniBand connection All ICON configurations scaling from 64, almost identical except 4-1 is 10% worse.\nFigure 5. ICON Speedup with ICON plotted relative to 64 GPUs with one GPU and one active InfiniBand connection\nAll LAMMPS configurations scaling almost identically except 4-1 is 20% worse.\nFigure 6. LAMMPS Speedup with LAMMPS plotted relative to eight GPUs with one GPU and one active InfiniBand connection\nMILC speedup scaled from 32 to 256 GPUs where 4-2 and 4-4 configurations scale the best and reach the highest performance.\nFigure 7. MILC Speedup plotted relative to 32 nodes with one GPU and one active InfiniBand connection\nFigures 3 through 7 show the scalability of each of the simulations. Generally, most of the data follows the trend expected based on Amdahl\u2019s law. A notable exception is the GROMACS configuration using four GPUs and one InfiniBand connection per node (labeled 4-1). This configuration exhibits negative scaling at first, meaning it slows down as resources are added before it starts to scale.\nGiven the network configuration, it seems that \u200cscaling is bound by the bandwidth of the network and the data each parallel thread is trying to communicate. As the number of resources grows between 16 and 32 GPUs, the task reaches a point where it is no longer bandwidth-bound and scales to a small degree.\nSimilar outlier behavior for the 4-1 configuration is seen for ICON and LAMMPS, though both scale much more readily than GROMACS.\nIt is also interesting to note that the 4-4 configuration is not always the best for performance. For FUN3D (2-2) and a portion of the MILC (4-2), scaling other configurations shows superior performance by small margins. However, in all cases, the 4-4 configuration is among the best configurations.\nEnergy Usage\nICON uses the most energy, ranging from 17\u201337 kWh, linearly growing as it scales. MILC's energy consumption has the same shape but ranges between 10 and 30 kWh. FUN3D, LAMMPS, and GROMACS all use less than 5 kWh, and slope up to the right, with LAMMPS being the flattest, ranging between 2.5 and 4 kWh on 16-256 GPUs.\nFigure 8. Energy usage for HPC applications running with four GPUs and four Infiniband connections per node\nFor brevity, Figure 8 shows only the four GPU and four IB connections (4-4) energy consumption in a single plot. The amount of energy used by each simulation is arbitrary and can be adjusted up or down by changing the problem size or analysis type. There are two relevant characteristics to note:\nThe slope for each application is positive, meaning that they use more energy as they scale.\nSome applications like FUN3D increase used energy quickly, while others like LAMMPS increase more gradually.\nTime to solution compared to energy to solution\nSome may conclude from Figure 8 that the best configuration to run these simulations is by minimizing the number of resources used per simulation. While the conclusion from an energy standpoint is correct, there are more objectives to consider beyond energy.\nFor example, in the case of a researcher working against a project deadline, time to solution is crucially important. Or, in the case of a commercial enterprise, final data is often required to start the manufacturing process and get a product ready for release to market. Here, too, the time to solution value may outweigh the extra energy required to produce the simulation output sooner.\nTherefore, this is a multi-objective optimization problem with several solutions depending on the weights of each defined objective.\nThe ideal case\nBefore exploring the objectives, consider the following ideal case:\nA perfectly parallel (meaning no serialized operations) accelerated HPC application, which scales linearly as additional processors are added. Graphically, the ```SPEEDUP``` curve is simply a line starting from (1,1) and going up and to the right at a slope of 1.0.\nFor such an application, now consider energy. If every processor added consumes the same amount of power, then power consumption during the run is the following equation:\nPower = n \\times P_1\nIn this case, n\nis the number of processors used and P_1\nis the power used by one GPU.\nEnergy = Power \\times time\nBut time is an inverse function of n\n, so you can rewrite it as follows:\nEnergy = P_1 \\times time_1\nThe n\nvalues cancel, and we see that Energy\n, in the ideal case, is not a function of the number of GPUs and is in fact constant.\nEnergy= n \\times P_1 \\times \\frac{time_1}{n}\nIn several cases, HPC applications are not ideal in either parallel speedup or energy. Because speedup is a case of diminishing returns as resources are added, and energy grows roughly linearly as resources are added, there should be a number of GPUs where the ratio of speedup to energy is a maximum.\nMore formally, assuming equal weights for energy and time:\nSPEEDUP_{Energy} = \\frac{SPEEDUP}{EnergyRatio} = \\frac{\\left [ \\frac{runtime_{1\\ GPU \\ and\\ 1\\ IB}}{runtime_{n\\ GPUs\\ and\\ m\\ IB}} \\right ]}{\\left [ \\frac{energy_{n\\ GPUs\\ and\\ m\\ IB}}{energy_{1\\ GPUs\\ and\\ 1\\ IB}} \\right ]}\\\\ = \\left [ \\frac{runtime_{1\\ GPU \\ and\\ 1\\ IB}}{runtime_{n\\ GPUs\\ and\\ m\\ IB}} \\right ] \\times \\left [ \\frac{energy_{n\\ GPUs\\ and\\ m\\ IB}}{energy_{1\\ GPUs\\ and\\ 1\\ IB}} \\right ]\nIn this formula, the following is constant:\n\\left [ runtime_{1\\ GPU \\ and\\ 1\\ IB} \\times energy_{1\\ GPU \\ and\\ 1\\ IB}\\right ]\nYou should maximize the following:\n\\frac{1}{\\left [ runtime_{n\\ GPU \\ and\\ m\\ IB} \\times energy_{n\\ GPU \\ and\\ m\\ IB}\\right ]}\nOr, you should minimize the following:\nruntime_{n\\ GPU \\ and\\ m\\ IB} \\times energy_{n\\ GPU \\ and\\ m\\ IB}\nIf one plots runtime on one axis and energy on the opposite axis, then the quantity to minimize is simply the area defined by runtime multiplied by energy.\nDefining the problem this way also enables the exploration of multiple independent variables, including but not limited to the number of resources used in a parallel run, GPU clock frequency, network latency, bandwidth, and any other variable that influences runtime and energy usage.\nBalanced speed and energy\nThe following plots are the product of runtime and energy consumption, where the independent variable is the number of GPUs in the parallel run. These plots provide one method to determine one solution among the Pareto front of solutions for optimization of runtime and energy.\nA curved plot showing decreasing then increasing with a minimum at 16 GPUs. The 4-2 and 4-4 configurations are best {right side}.\nFigure 9. Runtime * Energy for FUN3D showing a minimum of 16-32 GPUs\nA less obvious optimum for GROMACS with 2-2 being almost flat between four and 32 GPUs and 4-2 and 4-4 configurations best with the peak at eight GPUs.\nFigure 10. Runtime * Energy for GROMACS\nICON Energy * Runtime decreases with GPUs in all cases, slope reaching very close to zero, so 512 GPUs may be optimal. 4-4 and 4-2 configurations are best.\nFigure 11. Runtime * Energy for ICON\nLAMMPS decreases Runtime * Energy throughout the 8-256 GPU range. 4-4 and 4-2 configurations are best.\nFigure 12. Runtime * Energy for LAMMPS\nMILC shows a clear minimum at 32 GPUs using 4-4 configuration.\nFigure 13. Runtime * Energy for MILC\nThe preceding figures show changes in Runtime * Energy as each of the five applications and five GPU/network configurations are scaled with GPU count. Different from the scalability plots of performance, we see that in all cases the 4-4 configuration (reminder, \u201c4-4\u201d refers to {# GPUs / node} \u2013 {# IB / node}) is the best with the 4-2 configuration occasionally a close second. Aligned with the scalability plots, the 1-1 configuration is consistently the worst configuration, likely because of overhead on each server being mostly idle (that is each node had eight A100 GPUs, and eight CX6 Infiniband adaptors).\nPerhaps more interesting is the fact that both LAMMPS and ICON show they have not reached a minimum in Runtime * Energy for the data we collected. Larger runs should be performed for LAMMPS and ICON to show minima.\nA graph showing energy performance and energy drop.\nFigure 14. Cost and benefits of optimizing time to solution and energy to solution compared to maximum performance The desired outcome of the optimization is to run simulations at increased efficiency. Comparing the performance drop to the energy drop per simulation (normalized by the maximum performance point) is a simple way to check that the optimization output achieved efficiency increase. If the performance drop is less than the energy drop, the efficiency is higher, which is seen for FUN3D, GROMACS, and MILC in Figure 14.\nThere is an opportunity for saving energy and the performance cost compared to the maximum performance point. This is shown in Figure 14. ICON and LAMMPS were not plotted because they did not display a minimum point in the optimization. The desired outcome is that performance drops less than the energy used per simulation, and that is precisely what is seen for FUN3D, GROMACS, and MILC.\nNavigating the intersection of multi-node scalability and energy\nData center growth and the expanding exploration and use of AI will drive increased demand for data centers, data center space, cooling, and electrical energy. Given the fraction of electricity generated from fossil fuels, greenhouse gasses driven by data center energy demands are a problem that must be managed.\nNVIDIA continues to focus on making internal changes and influencing the design of future products, maximizing the positive impact of the AI revolution on society. By using the NVIDIA accelerated computing platform, researchers get more science done per unit of time and use less energy for each scientific result. Such energy savings can be equated to a proportional amount of carbon dioxide emissions avoided, which benefits everyone on the planet.\nRead the recent post, Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO for a deeper dive focused on VASP and energy efficiency.\nThis short discussion and set of results are offered to jump-start a conversation about shifting the way HPC centers allocate and track resources provided to the research community. It may also influence users to consider the impact of their choices when running large parallel simulations, and the downstream effects.\nIt is time to start a conversation regarding energy to solution for HPC and AI simulations and the multi-objective optimization balance with time to solution discussed here. Someday, the HPC community may measure scientific advancements per megawatt-hour, or perhaps even per mTon CO2eq, but that won\u2019t be achieved until conversations begin.\nFor more information, visit NVIDIA sustainable computing. Read about NERSC and their impressions of the accelerated platform. Or, listen to Alan Gray\u2019s NVIDIA 2023 GTC session about tuning the accelerated platform for maximum efficiency."}], "https://developer.nvidia.com/blog/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/": [{"text": "NVIDIA has launched a Deep Learning for Science and Engineering Teaching Kit to help educators in various fields incorporate AI into their curriculums. The kit includes lectures, labs, and projects on physics-informed machine learning, neural networks, data quantification, and high-performance computing. It is designed for engineering and science educators to train the next generation of innovators in using AI for problem-solving. The kit leverages the open-source NVIDIA Modulus framework, making it accessible to those without AI or programming expertise. The focus is on solving forward and inverse problems, discovering physical laws, and constructing surrogate models for digital twins. Educators can customize the course to suit their students' needs and anticipate updates as AI technology evolves. Feedback from educators has been positive, with examples and code enabling hands-on learning experiences in applying machine learning to scientific and engineering problems. Educators can access the teaching kit through the NVIDIA DLI Teaching Kit Program and NVIDIA On-Demand.", "text_components": ["NVIDIA Deep Learning Institute Launches Science and Engineering Teaching Kit\nAI is quickly becoming an integral part of diverse industries, from transportation and healthcare to manufacturing and finance. AI powers chatbots, recommender systems, computer vision applications, fraud prevention, and autonomous vehicles. It also has broad applications in engineering and science.\nPhysics-informed machine learning (physics-ML) leverages knowledge of the physical world to train AI models. It is well suited for modeling real-world systems \u2014 some applications include: predicting extreme weather, data center cooling, turbulent flow over a car, and protein modeling.\nA gif showing spinning surrogate model of the Earth with bright colors indicates extreme weather events across the planet\u2019s surface.\nFigure 1. Physics-ML is used to model physical systems such as weather, ocean currents, and tidal activity Academic institutions play a pivotal role in nurturing emerging technologies and driving the innovation needed for their widespread adoption. There is no denying that today\u2019s students who want to succeed in tomorrow\u2019s workplace need to understand how AI can enable solutions.\nTo support this work, NVIDIA is collaborating with pioneers at the intersection of science, engineering, and AI to create the first Deep Learning for Science and Engineering Teaching Kit for educators in academia worldwide.\nThis new teaching kit will enable the next generation of engineers and scientists to leverage AI for innovation in the field of engineering and science. It was created with leading academics including George Karniadakis, professor of Applied Mathematics and Engineering at Brown University, and his team.\n\u201cWe designed this course with my collaborator, Dr. Raj Shukla, to address the urgent need for specific material for scientists and engineers,\u201d said Karniadakis. \u201cWe focused on regression and mimicking the approximation theory and algorithms required in classical numerical analysis courses in the engineering curriculum.\u201d\nEducators can get full, free access to the Deep Learning for Science and Engineering Teaching Kit (and many more) by joining the NVIDIA DLI Teaching Kit Program. Kits include lecture materials, labs, and sample problem sets to facilitate incorporating advanced technology into course curriculums. The entire lecture portion of the course is also available on NVIDIA On-Demand.", "Overview of the Science and Engineering Teaching Kit\nEducators in various fields will find this teaching kit useful, from mechanical, structural, and electrical engineering to atmospheric science, computational science, and more. Materials in the kit include the fundamentals of deep learning as well as advanced topics and hands-on exercises. It contains 15 lectures totaling about 30-35 hours, homework, and 20 projects across different fields.\nThe Deep Learning for Science and Engineering Teaching Kit contains focused modules that combine theory, algorithms, programming, and examples. Highlights include:\nA primer on Python plus scientific and deep learning libraries\nDeep neural network architectures, training, and optimization\nPhysics-informed neural networks\nNeural operators\nData and uncertainty quantification\nHigh-performance computing (HPC) and the NVIDIA Modulus open-source framework\nThis content is ideal for educators in engineering and science. The modular design enables instructors to develop their own custom version of the course to suit the needs of their students.\nThe teaching kit includes dedicated modules for physics-ML, due to its potential to transform HPC simulation workflows across disciplines. These include multi-disciplinary physics including computational fluid dynamics, structural mechanics, and computational chemistry. Because of its broad applicability across science and engineering domains, physics-ML is well suited for modeling real-world multiphysics systems.\nAI surrogate models can help develop a wide range of solutions including weather forecasting, reducing power plant greenhouse gasses, and accelerating clean energy transitions. Such physically consistent surrogate models can underpin the deployment of large-scale digital twins of real-world systems.\n\u201cTo this end,\u201d Karniadakis said, \u201cthe focus of the course is how to solve forward and inverse problems given sparse and noisy data, how to discover governing physical laws, how to construct proper surrogate models for digital twins, and how to quantify uncertainties associated with models and data.\u201d\nThe kit leverages the open-source NVIDIA Modulus framework with hands-on tutorials for project-based learning. Modulus enables engineering and scientific communities that may not have AI or programming expertise. With a Python-based interface, Modulus provides the right tools to combine the governing partial differential equations and other attributes of the problem like the physical geometry and boundary conditions with the training dataset in a simple way.\nIt also provides a variety of reference applications as starting points, across domains (computational fluid dynamics, structures, thermal) applied to problems in different segments from manufacturing to healthcare. To learn more, see Physics-Informed Machine Learning Platform NVIDIA Modulus Is Now Open Source.\nGiven the rapid pace of change in the field of AI, educators can anticipate that the teaching material will be updated, as needed. NVIDIA is committed to providing the best educational materials, and feedback is welcome.\n\u201cThe NVIDIA Teaching Kit on physics-ML has provided me with great resources for use in my machine learning course targeted for our engineering students,\u201d said Hadi Meidani, associate professor of Civil & Environmental Engineering at University of Illinois Urbana-Champaign. \u201cThe examples and code greatly enable hands-on learning experiences on how machine learning is applied to scientific and engineering problems.\u201d", "Get started\nEducators can get full, free access to the Deep Learning for Science and Engineering Teaching Kit (and many more) by joining the NVIDIA DLI Teaching Kit Program. The entire lecture part of the course is also openly available through NVIDIA On-Demand.\nFor access to tutorial content, reach out to the Modulus team. To get started with NVIDIA Modulus hands-on experience, see Introduction to Physics-Informed Machine Learning with Modulus."], "document_title": "NVIDIA Deep Learning Institute Launches Science and Engineering Teaching Kit", "document_url": "https://developer.nvidia.com/blog/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/", "document_date": "2023-11-13T17:30:00", "document_date_modified": "2023-11-16T19:16:38", "document_full_text": "NVIDIA Deep Learning Institute Launches Science and Engineering Teaching Kit\nAI is quickly becoming an integral part of diverse industries, from transportation and healthcare to manufacturing and finance. AI powers chatbots, recommender systems, computer vision applications, fraud prevention, and autonomous vehicles. It also has broad applications in engineering and science.\nPhysics-informed machine learning (physics-ML) leverages knowledge of the physical world to train AI models. It is well suited for modeling real-world systems \u2014 some applications include: predicting extreme weather, data center cooling, turbulent flow over a car, and protein modeling.\nA gif showing spinning surrogate model of the Earth with bright colors indicates extreme weather events across the planet\u2019s surface.\nFigure 1. Physics-ML is used to model physical systems such as weather, ocean currents, and tidal activity Academic institutions play a pivotal role in nurturing emerging technologies and driving the innovation needed for their widespread adoption. There is no denying that today\u2019s students who want to succeed in tomorrow\u2019s workplace need to understand how AI can enable solutions.\nTo support this work, NVIDIA is collaborating with pioneers at the intersection of science, engineering, and AI to create the first Deep Learning for Science and Engineering Teaching Kit for educators in academia worldwide.\nThis new teaching kit will enable the next generation of engineers and scientists to leverage AI for innovation in the field of engineering and science. It was created with leading academics including George Karniadakis, professor of Applied Mathematics and Engineering at Brown University, and his team.\n\u201cWe designed this course with my collaborator, Dr. Raj Shukla, to address the urgent need for specific material for scientists and engineers,\u201d said Karniadakis. \u201cWe focused on regression and mimicking the approximation theory and algorithms required in classical numerical analysis courses in the engineering curriculum.\u201d\nEducators can get full, free access to the Deep Learning for Science and Engineering Teaching Kit (and many more) by joining the NVIDIA DLI Teaching Kit Program. Kits include lecture materials, labs, and sample problem sets to facilitate incorporating advanced technology into course curriculums. The entire lecture portion of the course is also available on NVIDIA On-Demand.\nOverview of the Science and Engineering Teaching Kit\nEducators in various fields will find this teaching kit useful, from mechanical, structural, and electrical engineering to atmospheric science, computational science, and more. Materials in the kit include the fundamentals of deep learning as well as advanced topics and hands-on exercises. It contains 15 lectures totaling about 30-35 hours, homework, and 20 projects across different fields.\nThe Deep Learning for Science and Engineering Teaching Kit contains focused modules that combine theory, algorithms, programming, and examples. Highlights include:\nA primer on Python plus scientific and deep learning libraries\nDeep neural network architectures, training, and optimization\nPhysics-informed neural networks\nNeural operators\nData and uncertainty quantification\nHigh-performance computing (HPC) and the NVIDIA Modulus open-source framework\nThis content is ideal for educators in engineering and science. The modular design enables instructors to develop their own custom version of the course to suit the needs of their students.\nThe teaching kit includes dedicated modules for physics-ML, due to its potential to transform HPC simulation workflows across disciplines. These include multi-disciplinary physics including computational fluid dynamics, structural mechanics, and computational chemistry. Because of its broad applicability across science and engineering domains, physics-ML is well suited for modeling real-world multiphysics systems.\nAI surrogate models can help develop a wide range of solutions including weather forecasting, reducing power plant greenhouse gasses, and accelerating clean energy transitions. Such physically consistent surrogate models can underpin the deployment of large-scale digital twins of real-world systems.\n\u201cTo this end,\u201d Karniadakis said, \u201cthe focus of the course is how to solve forward and inverse problems given sparse and noisy data, how to discover governing physical laws, how to construct proper surrogate models for digital twins, and how to quantify uncertainties associated with models and data.\u201d\nThe kit leverages the open-source NVIDIA Modulus framework with hands-on tutorials for project-based learning. Modulus enables engineering and scientific communities that may not have AI or programming expertise. With a Python-based interface, Modulus provides the right tools to combine the governing partial differential equations and other attributes of the problem like the physical geometry and boundary conditions with the training dataset in a simple way.\nIt also provides a variety of reference applications as starting points, across domains (computational fluid dynamics, structures, thermal) applied to problems in different segments from manufacturing to healthcare. To learn more, see Physics-Informed Machine Learning Platform NVIDIA Modulus Is Now Open Source.\nGiven the rapid pace of change in the field of AI, educators can anticipate that the teaching material will be updated, as needed. NVIDIA is committed to providing the best educational materials, and feedback is welcome.\n\u201cThe NVIDIA Teaching Kit on physics-ML has provided me with great resources for use in my machine learning course targeted for our engineering students,\u201d said Hadi Meidani, associate professor of Civil & Environmental Engineering at University of Illinois Urbana-Champaign. \u201cThe examples and code greatly enable hands-on learning experiences on how machine learning is applied to scientific and engineering problems.\u201d\nGet started\nEducators can get full, free access to the Deep Learning for Science and Engineering Teaching Kit (and many more) by joining the NVIDIA DLI Teaching Kit Program. The entire lecture part of the course is also openly available through NVIDIA On-Demand.\nFor access to tutorial content, reach out to the Modulus team. To get started with NVIDIA Modulus hands-on experience, see Introduction to Physics-Informed Machine Learning with Modulus."}], "https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/": [{"text": "The article discusses the new hardware developments in NVIDIA Grace Hopper Superchip systems, which simplify GPU programming for high-performance computing (HPC). The bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory allows developers to develop applications for both processors using a single, unified address space. The article details the benefits of a single-address space for developers, how it works on systems with NVIDIA GPUs connected to x86_64 CPUs through PCIe, and the improvements in programmer productivity for various programming models. The article also evaluates application performance using unified memory through benchmarks like SPECaccel 2023, LULESH, and POT3D. It explains how to enable and use unified memory in the NVIDIA HPC SDK for programming models like stdpar, OpenACC, and CUDA Fortran. The article concludes by highlighting the simplified GPU programming experience and improved performance available with NVIDIA Grace Hopper systems. Future developments are expected to further enhance code writing and application performance.", "text_components": ["Simplifying GPU Programming for HPC with NVIDIA Grace Hopper Superchip\nThe new hardware developments in NVIDIA Grace Hopper Superchip systems enable some dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that the user can develop their application for both processors while using a single, unified address space.\nEach processor retains its own physical memory that is designed with the bandwidth, latency, and capacity characteristics that are matched to the workloads most suited for each processor. Code written for existing discrete-memory GPU systems will continue to run performantly without modification for the new NVIDIA Grace Hopper architecture.\nOur recent post, Simplifying GPU Application Development with Heterogeneous Memory Management, details some of the benefits that a single-address space brings to developers and how it works on systems with NVIDIA GPUs connected to x86_64 CPUs through PCIe. All application threads (GPU or CPU) can directly access all of the application\u2019s system allocated memory, removing the need to copy data between processors.\nThis new ability to directly read or write to the full application memory address space significantly improves programmer productivity for all programming models built on top of CUDA: CUDA C++, CUDA Fortran, standard parallelism in ISO C++ and ISO Fortran, OpenACC, OpenMP, and many others.\nThis post continues the Heterogeneous Memory Management (HMM) discussion in the context of Grace Hopper hardware, which provides all of the same programming model improvements as HMM-enabled systems, but with added hardware support to make it even better.\nNotably, any workload that is bottlenecked by host-to-device or device-to-host transfers can get up to a 7x speedup due to the chip-to-chip (C2C) interconnect in Grace Hopper systems. Cache coherency enables this performance without having to pin the memory (using ```cudaHostRegister```, for example) if huge pages are used. While HMM and CUDA Managed Memory have historically been limited to migrating whole pages of data reactively on page faults, Grace Hopper is able to make better decisions on where data should reside and when it should migrate.\nWe will detail in this post how the NVIDIA HPC compilers take advantage of these new hardware capabilities to simplify GPU programming with ISO C++, ISO Fortran, OpenACC, and CUDA Fortran.\nNVIDIA Grace Hopper systems provide the best performance available combined with a simplified GPU developer experience. HMM also brings this simplified developer experience to systems that are not Grace Hopper, while providing the optimal performance available when using PCIe. Developers can use these improved and simplified programming models in a portable way to get the best performance available on a wide variety of systems using NVIDIA GPUs.", "Expanding stdpar with Grace Hopper unified memory\nStandard languages such as ISO C++ and ISO Fortran have been gaining features in recent years to enable developers to express the parallelism that is present in their applications directly from the base language itself, without using extensions or compiler directives. The NVIDIA HPC compilers can build these applications to run with high performance on NVIDIA GPUs. These features have been detailed in previous posts.\nMore specifically, we showed how using standard language parallelism, also known as stdpar, can be used to greatly improve developer productivity and simplify GPU application development. However, we also pointed out a few limitations due to the nature of the separate memory spaces of the CPU and GPU. These include the inability to use some types of data in the C++ parallel algorithms, such as data allocated on the stack, global data, or data captured by reference in lambda captures.\nFor Fortran ```do concurrent``` loops, global variables could not be used in routines called from within the ```do concurrent``` loop, and size detection of data by the compiler for assumed-size arrays was limited. Now, Grace Hopper and its unified memory capabilities clear these limitations, and developing applications using stdpar becomes even simpler.", "Simplifying OpenACC and CUDA Fortran\nGPU application developers have long preferred both OpenACC and CUDA Fortran due to their convenience and power. Along with CUDA C++, they have stood the test of time and are used in production by a large number of applications in HPC centers around the world. Both of these models provide robust ways of managing data residency and optimizing data transfer and movement.\nNow with the unified memory capabilities of Grace Hopper, application development can be dramatically simplified because these considerations for data location and movement can be automatically handled by the system. This reduces the effort spent on porting applications to run on GPUs and leaves more time for algorithm development.\nFor fine-tuning performance and optimizations, the developer may choose to selectively add information about data locality using the facilities already available in OpenACC and CUDA Fortran. The data information in existing applications written for discrete memory devices can be used to optimize for Grace Hopper\u2019s unified memory without code changes.", "Evaluating application performance using unified memory\nThe following sections explore several benchmarks and applications to understand how these new features not only simplify code development, but also impact expected runtime performance.", "SPECaccel 2023 benchmark\nThe SPECaccel\u00ae 2023 benchmark suite focuses on single-accelerator performance using directives, OpenACC, and OpenMP. The benchmarks are intended to show general GPU performance and are a good proxy for how many HPC applications can use the new unified memory features of Grace Hopper.\nFigure 1 compares the performance of OpenACC data directives to unified memory enabled through the NVHPC SDK compiler flag ```-gpu=unified```. While the results followed the benchmark\u2019s run rule requirements, they were measured on preproduction hardware, so are considered estimated.\nA bar chart comparing the estimated performance of 13 benchmarks in the SPECaccel 2023 suite when using data directives compared to unified memory. The majority of the bars show very little performance difference between the two versions.\nFigure 1. Estimated performance of multiple SPECaccel 2023 benchmarks using data directives compared to unified memory\nMost of the benchmarks show very little difference between using unified memory and memory managed with the OpenACC data directives with an overall slow-down of only ~1%. 463.swim, which primarily measures the memory performance, gains 28% with unified memory. With the data directives, an entire array is copied back for each time cycle, though only the inner triangular portion of the array is used on the host.\nGiven that the printed data is non-contiguous, with data directives, it is advantageous to copy the entire array as one large block rather than many smaller blocks. With unified memory, far less data is accessed on the host, with only a portion of the array fetched from GPU memory.\nThe only significant slow-down is with the 404.lbm benchmark at 22%. The kernel times for each iteration have a slight overhead of 2 ms when using unified memory. Given that the kernel is executed 2,000 times, the overhead gets accumulated for about 3% of the difference. The larger issue is the entire 5 GB results array is check-pointed every 63 iterations, which needs to be accessed from the host. In this case, the CPU accessing the GPU memory roughly doubles the time, accounting for the remaining 19% of the difference.\nWhile unified memory makes porting code significantly easier, and as in the case of SPECaccel, it generally gives the same performance as using data directives. Programmers still need to be mindful of data placement, as for any other multi-socket system with non-uniform memory access (NUMA) characteristics. However, in most cases, data directives can now be considered a performance tuning option for cases such as 404.lbm where large portions of data are accessed on both the CPU and GPU, rather than a requirement to port code to the GPU.\nSPEC and SPECaccel are registered trademarks of the Standard Performance Evaluation Corporation.", "LULESH\nLULESH is a mini-application designed to simulate a simplified version of shock hydrodynamics representative of LLNL\u2019s ALE3D application. It has been used for over a decade to understand C++ parallel programming models and their interaction with compilers and memory allocators.\nThe stdpar implementation of LULESH uses C++ standard library containers for all the data structures on the GPU, and they depend on the automatic migration of memory between CPU and GPU.\nFigure 2 shows that using unified memory does not affect the performance of LULESH, which makes sense. Both managed and unified memory options lead to a LULESH figure-of-merit (FOM) of 2.09e5 on NVIDIA DGX GH200, which is 40% higher than the FOM than with an NVIDIA H100 PCIe GPU and 6.5x faster than a 56-core Intel Xeon 8480+ CPU system.\nA bar chart comparing the performance of LULESH when run in multiple ways. The performance on an Intel Xeon 8480+ is the baseline. The H100 PCIe bar is 4.61x faster. The performance of the GH200 using managed memory is 6.51x and with the compiler\u2019s unified mode is 6.49x.\nFigure 2. Comparison of LULESH performance using managed and unified memory options on NVIDIA GH200 with NVIDIA H100 PCIe and a modern CPU", "POT3D\nPOT3D approximates solar coronal magnetic fields by computing potential field solutions. It is developed by Predictive Science Inc. using modern Fortran. The application has historically run on GPUs using OpenACC, but the authors have now adopted a mixture of Fortran ```do concurrent``` to express data parallel loops and OpenACC for optimizing data movement with the GPU.\nAs presented in the GTC session, From Directives to DO CONCURRENT: A Case Study in Standard Parallelism, the stdpar version of the code performed roughly 10% slower than the optimized OpenACC code. If OpenACC is used to optimize the data movement of the stdpar version, the performance is nearly identical. This means that the same performance was achieved while maintaining roughly 2,000 fewer lines of code. Does unified memory change this?\nFigure 3 shows the performance of POT3D on Grace Hopper built in two ways. The blue bar is the performance baseline, which is Fortran ```do concurrent``` loops for parallelism and OpenACC data directives to optimize data movement. The green bar is built using the ```-gpu=unified``` option on Grace Hopper and with all OpenACC directives removed.\nThe performance of the code is now the same as the fully optimized code without requiring any OpenACC. With the performance and productivity enhancements unified memory brings, POT3D can now be written in pure Fortran and get the same performance as the previously tuned OpenACC code.\nA bar chart comparing performance of managing memory explicitly in POT3D and using the new unified memory mode. The performance using OpenACC for data management and building without any data directives is equal.\nFigure 3. POT3D performance using OpenACC data directives compared to Grace Hopper unified memory", "How to enable and use unified memory in the NVIDIA HPC SDK\nAs of NVHPC SDK release 23.11, developers aiming to use GPUs with unified memory capability can benefit from simplified programming interfaces. This release introduces a novel compilation mode to the ```nvc++```, ```nvc```, and ```nvfortran``` compilers, which can be enabled by passing the flag ```-gpu=unified```.\nThis section is a deep dive into the specific enhancements for unified memory across various programming models supported by the NVHPC SDK, which leverage the capabilities of the underlying hardware and the CUDA runtime to automatically handle data placement and memory migration between CPU and GPU physical memory.", "stdpar\nFor stdpar, all data access restrictions have been removed. This means global variables can be accessed from CPU or GPU, and unified memory compilation is now the default setting on compatible machines. However, when cross-compiling for different targets, ```-gpu=unified``` flag needs to be passed explicitly to enable the new programming interface.\nIn the original release of stdpar C++ within ```nvc++```, lambda functions in parallel algorithms had several restrictions. These have now been completely lifted. Developers can freely use data across different parallel algorithms and sequential code. This enables capturing variables by reference and accessing global variables within parallel algorithms:\n```\nint init_val = 123;\nvoid foo() {\n  int my_array[ARRAY_SIZE];\n  auto r = std::views::iota(0, ARRAY_SIZE);\n  std::for_each(std::execution::par_unseq, r.begin(), r.end(),\n                [&](auto i) { my_array[i] = init_val; });\n}\n```\nIf this code is compiled as shown below, the array ```my_array``` can be safely initialized on the GPU with each element being set in parallel using the value from the global variable ```init_val```. Previously, accessing both ```my_array``` and ```init_val``` was not supported.\n```\nnvc++ -std=c++20 -stdpar -gpu=unified example.cpp\n```\nIt is now also possible to use ```std::array``` safely with parallel algorithms, as illustrated by the example:\n```\nstd::array<int, 10000> my_array = ...;\nstd::sort(std::execution::par, my_array.begin(), my_array.end());\n```\nThe removal of data access limitations is a notable improvement, but it is important to remember that data races are still possible. For example, accessing global variables within parallel algorithms with simultaneous updates in different lambda instances running on the GPU.\nPorting existing code to stdpar C++ and integrating third-party libraries is also simplified. When pointers to data used in parallel algorithms originate in allocation statements from separate files, those files no longer require compilation with ```nvc++``` or ```-stdpar```.\nFor standard Fortran, some variable uses were previously unsupported. Now, it is possible to access global variables in routines called from ```do concurrent``` loops. Additionally, there were cases where the compiler could not accurately determine variable sizes for implicit data movements between GPU and CPU. These cases can now be handled correctly on targets with unified memory:\n```\nsubroutine r(a, b)\n  integer :: a(*)\n  integer :: b(:)\n  do concurrent (i = 1 : size(b))\n    a(b(i)) = i \n  enddo\nend subroutine\n```\nIn the example above, an access region of an assumed-size array ```a``` inside the ```do concurrent``` construct cannot be determined at compile time because the element index positions are taken from another array ```b``` initialized outside or the routine. This is no longer an issue when such code is compiled as follows:\n```\nnvfortran -stdpar -gpu=unified example.f90\n```\nThe crucial aspect is that the compiler no longer requires precise knowledge of the data segment accessed within the loop. Automatic data transfers between GPU and CPU are now handled seamlessly by the CUDA runtime.\nOpenACC\nNow with unified memory mode, OpenACC programs no longer require explicit data clauses and directives. All variables are now accessible from the OpenACC compute regions. This implementation closely adheres to the shared memory mode detailed in the OpenACC specification.\nThe following C example illustrates an OpenACC parallel loop region that can now be executed correctly on GPUs without requiring any data clauses:\n```\nvoid set(int* ptr, int i, int j, int dim){\n  int idx = i * dim + j;\n  ptr[idx] = someval(i, j);\n}\n\nvoid fill2d(int* ptr, int dim){\n#pragma acc parallel loop\n  for (int i = 0; i < dim; i++)\n    for (int j = 0; j < dim; j++)\n      set(ptr, i, j, dim);\n}\n```\nIn C/C++, native language arrays are implicitly decayed into pointers when passed to functions. Therefore, the original array shape and size information is not preserved across the function invocations. Moreover, arrays with the dynamic size are represented by pointers. The use of pointers poses major challenges for the automatic code optimizations, as compilers lack essential information about the original data.\nWhile the OpenACC compiler has strong support for detecting the data segment accessed in loops to perform the data movements to GPU implicitly, it cannot determine the segment in this case because the array is updated through ```ptr``` in another function ```set``` called inside the loop. Previously, supporting such cases in C was not possible. However, with \u200cunified memory mode enabled as shown below, such examples are now fully supported:\n```\nnvc -acc -gpu=unified example.c\n```\nWithout ```-gpu=unified``` the only way to guarantee correctness for this example is to change the line with the pragma directive:\n```\n#pragma acc parallel loop create(ptr[0:dim*dim]) copyout(ptr[0:dim*dim])\n```\nThis explicitly instructs the OpenACC implementation about the precise data segment used within the parallel loop.\nThe Fortran example below illustrates how a global variable can now be accessed in the OpenACC routine without requiring any explicit annotations.\n```\nmodule m\ninteger :: globmin = 1234\ncontains\nsubroutine findmin(a)\n!$acc routine seq\n  integer, intent(in)  :: a(:)\n  integer :: i\n  do i = 1, size(a)\n    if (a(i) .lt. globmin) then\n      globmin = a(i)\n    endif\n  end do\nend subroutine\nend module m\n```\nWhen this example is compiled as shown below, the source does not need any OpenACC directives in order to access module variable ```globmin``` to read or update its value in the routine invoked from CPU and GPU.\n```\nnvfortran -acc -gpu=unified example.f90\n```\nMoreover, any access to ```globmin``` will be made to the same exact instance of the variable from CPU and GPU keeping its value synchronized automatically. Previously, such behavior could only be achieved by adding a combination of OpenACC ```declare``` and ```update``` directives in the source code.\nIn binaries compiled with ```-gpu=unified```, the OpenACC runtime leverages data action information such as create/delete or copyin/copyout as optimizations to indicate preferable data placement to the CUDA runtime by means of memory hint APIs. For more details, see Simplifying GPU Application Development with Heterogeneous Memory Management.\nSuch actions originate either from the explicit data clauses in the source code or implicitly determined by the compiler. These optimizations can be used to fine-tune application performance by minimizing the amount of \u200cautomatic data migrations.\nFor the C example above, while adding data clauses ```create(ptr[0:dim*dim])``` and ```copyout(ptr[0:dim*dim])``` is optional with ```-gpu=unifie``` d, their use in the OpenACC parallel loop directive may lead to a performance uplift.", "CUDA Fortran\nThe addition of -gpu=unified also simplifies CUDA Fortran programming by removing restrictions on CPU-declared variables passed as arguments to global or device routines executing on the GPU. Moreover, it now permits referencing module or common block variables in such routines without requiring explicit attributes. This change does not affect variables explicitly annotated with existing data attributes: device, managed, constant, shared, or pinned.\n```\nmodule m\ninteger :: globval\ncontains \nattributes(global) subroutine fill(a)\n  integer :: a(*)\n  i = threadIdx%x\n  a(i) = globval\nend subroutine\nend module m\nprogram example\n  use m\n  integer :: a(N)\n  globval = 123\n  call fill<<<1, N>>> (a)\n  e = cudaDeviceSynchronize()\nend program\n```\nIn the example above, the CPU stack-allocated array a is initialized in the kernel fill on the GPU with the value from the global variable globval assigned in the CPU code. As shown, a kernel routine, which is an entry point for execution on GPU, is now enables to directly access variables declared in the regular CPU host.", "Details common across programming models\nBinaries that have not been compiled with the new ```-gpu=unified``` flag will retain their existing performance characteristics on systems with and without unified memory alike. However, binaries compiled with ```-gpu=unified``` can not guarantee correct execution on targets without unified memory capability. When linking the final binary for unified memory targets, passing ```-gpu=unified``` in the linker command line is required for correctness.\nMany applications transitioning to architectures with unified memory can seamlessly recompile with ```-gpu=unified``` without any code modifications. In addition, stdpar C++ and CUDA Fortran object files, whether compiled with or without ```-gpu=unified```, can be linked together. However, linking object files containing OpenACC directives or Fortran DC compiled differently with and without ```-gpu=unified``` is currently not supported.\nManual performance tuning for memory usage is currently achievable through the CUDA memory hints APIs for all the programming models that support unified memory, as well as through data directives for OpenACC programs.\nThe HPC SDK will continue to enhance support for unified memory in upcoming releases. For in-depth information regarding the current status, limitations, and future updates on this new functionality, refer to the NVIDIA HPC SDK documentation.", "Summary\nThe features and performance explained in this post are just the beginning of what NVIDIA Grace Hopper Superchip architecture and the NVIDIA software stack are bringing to developers. Future developments in the driver, CUDA software stack, and the NVIDIA HPC compilers are expected to remove even more restrictions on the way that users write their code, and to improve the performance of the resulting applications.\nLearn more about \u200ccompiler support on the NVIDIA HPC SDK page.\nRead Developing Accelerated Code with Standard Language Parallelism.\nCheck out the blog post series on Fortran Standard Parallelism.\nDownload the NVIDIA HPC SDK for free.\nSPEC and SPECaccel are registered trademarks of the Standard Performance Evaluation Corporation."], "document_title": "Simplifying GPU Programming for HPC with NVIDIA Grace Hopper Superchip", "document_url": "https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/", "document_date": "2023-11-13T17:13:02", "document_date_modified": "2023-11-16T19:16:39", "document_full_text": "Simplifying GPU Programming for HPC with NVIDIA Grace Hopper Superchip\nThe new hardware developments in NVIDIA Grace Hopper Superchip systems enable some dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that the user can develop their application for both processors while using a single, unified address space.\nEach processor retains its own physical memory that is designed with the bandwidth, latency, and capacity characteristics that are matched to the workloads most suited for each processor. Code written for existing discrete-memory GPU systems will continue to run performantly without modification for the new NVIDIA Grace Hopper architecture.\nOur recent post, Simplifying GPU Application Development with Heterogeneous Memory Management, details some of the benefits that a single-address space brings to developers and how it works on systems with NVIDIA GPUs connected to x86_64 CPUs through PCIe. All application threads (GPU or CPU) can directly access all of the application\u2019s system allocated memory, removing the need to copy data between processors.\nThis new ability to directly read or write to the full application memory address space significantly improves programmer productivity for all programming models built on top of CUDA: CUDA C++, CUDA Fortran, standard parallelism in ISO C++ and ISO Fortran, OpenACC, OpenMP, and many others.\nThis post continues the Heterogeneous Memory Management (HMM) discussion in the context of Grace Hopper hardware, which provides all of the same programming model improvements as HMM-enabled systems, but with added hardware support to make it even better.\nNotably, any workload that is bottlenecked by host-to-device or device-to-host transfers can get up to a 7x speedup due to the chip-to-chip (C2C) interconnect in Grace Hopper systems. Cache coherency enables this performance without having to pin the memory (using ```cudaHostRegister```, for example) if huge pages are used. While HMM and CUDA Managed Memory have historically been limited to migrating whole pages of data reactively on page faults, Grace Hopper is able to make better decisions on where data should reside and when it should migrate.\nWe will detail in this post how the NVIDIA HPC compilers take advantage of these new hardware capabilities to simplify GPU programming with ISO C++, ISO Fortran, OpenACC, and CUDA Fortran.\nNVIDIA Grace Hopper systems provide the best performance available combined with a simplified GPU developer experience. HMM also brings this simplified developer experience to systems that are not Grace Hopper, while providing the optimal performance available when using PCIe. Developers can use these improved and simplified programming models in a portable way to get the best performance available on a wide variety of systems using NVIDIA GPUs.\nExpanding stdpar with Grace Hopper unified memory\nStandard languages such as ISO C++ and ISO Fortran have been gaining features in recent years to enable developers to express the parallelism that is present in their applications directly from the base language itself, without using extensions or compiler directives. The NVIDIA HPC compilers can build these applications to run with high performance on NVIDIA GPUs. These features have been detailed in previous posts.\nMore specifically, we showed how using standard language parallelism, also known as stdpar, can be used to greatly improve developer productivity and simplify GPU application development. However, we also pointed out a few limitations due to the nature of the separate memory spaces of the CPU and GPU. These include the inability to use some types of data in the C++ parallel algorithms, such as data allocated on the stack, global data, or data captured by reference in lambda captures.\nFor Fortran ```do concurrent``` loops, global variables could not be used in routines called from within the ```do concurrent``` loop, and size detection of data by the compiler for assumed-size arrays was limited. Now, Grace Hopper and its unified memory capabilities clear these limitations, and developing applications using stdpar becomes even simpler.\nSimplifying OpenACC and CUDA Fortran\nGPU application developers have long preferred both OpenACC and CUDA Fortran due to their convenience and power. Along with CUDA C++, they have stood the test of time and are used in production by a large number of applications in HPC centers around the world. Both of these models provide robust ways of managing data residency and optimizing data transfer and movement.\nNow with the unified memory capabilities of Grace Hopper, application development can be dramatically simplified because these considerations for data location and movement can be automatically handled by the system. This reduces the effort spent on porting applications to run on GPUs and leaves more time for algorithm development.\nFor fine-tuning performance and optimizations, the developer may choose to selectively add information about data locality using the facilities already available in OpenACC and CUDA Fortran. The data information in existing applications written for discrete memory devices can be used to optimize for Grace Hopper\u2019s unified memory without code changes.\nEvaluating application performance using unified memory\nThe following sections explore several benchmarks and applications to understand how these new features not only simplify code development, but also impact expected runtime performance.\nSPECaccel 2023 benchmark\nThe SPECaccel\u00ae 2023 benchmark suite focuses on single-accelerator performance using directives, OpenACC, and OpenMP. The benchmarks are intended to show general GPU performance and are a good proxy for how many HPC applications can use the new unified memory features of Grace Hopper.\nFigure 1 compares the performance of OpenACC data directives to unified memory enabled through the NVHPC SDK compiler flag ```-gpu=unified```. While the results followed the benchmark\u2019s run rule requirements, they were measured on preproduction hardware, so are considered estimated.\nA bar chart comparing the estimated performance of 13 benchmarks in the SPECaccel 2023 suite when using data directives compared to unified memory. The majority of the bars show very little performance difference between the two versions.\nFigure 1. Estimated performance of multiple SPECaccel 2023 benchmarks using data directives compared to unified memory\nMost of the benchmarks show very little difference between using unified memory and memory managed with the OpenACC data directives with an overall slow-down of only ~1%. 463.swim, which primarily measures the memory performance, gains 28% with unified memory. With the data directives, an entire array is copied back for each time cycle, though only the inner triangular portion of the array is used on the host.\nGiven that the printed data is non-contiguous, with data directives, it is advantageous to copy the entire array as one large block rather than many smaller blocks. With unified memory, far less data is accessed on the host, with only a portion of the array fetched from GPU memory.\nThe only significant slow-down is with the 404.lbm benchmark at 22%. The kernel times for each iteration have a slight overhead of 2 ms when using unified memory. Given that the kernel is executed 2,000 times, the overhead gets accumulated for about 3% of the difference. The larger issue is the entire 5 GB results array is check-pointed every 63 iterations, which needs to be accessed from the host. In this case, the CPU accessing the GPU memory roughly doubles the time, accounting for the remaining 19% of the difference.\nWhile unified memory makes porting code significantly easier, and as in the case of SPECaccel, it generally gives the same performance as using data directives. Programmers still need to be mindful of data placement, as for any other multi-socket system with non-uniform memory access (NUMA) characteristics. However, in most cases, data directives can now be considered a performance tuning option for cases such as 404.lbm where large portions of data are accessed on both the CPU and GPU, rather than a requirement to port code to the GPU.\nSPEC and SPECaccel are registered trademarks of the Standard Performance Evaluation Corporation.\nLULESH\nLULESH is a mini-application designed to simulate a simplified version of shock hydrodynamics representative of LLNL\u2019s ALE3D application. It has been used for over a decade to understand C++ parallel programming models and their interaction with compilers and memory allocators.\nThe stdpar implementation of LULESH uses C++ standard library containers for all the data structures on the GPU, and they depend on the automatic migration of memory between CPU and GPU.\nFigure 2 shows that using unified memory does not affect the performance of LULESH, which makes sense. Both managed and unified memory options lead to a LULESH figure-of-merit (FOM) of 2.09e5 on NVIDIA DGX GH200, which is 40% higher than the FOM than with an NVIDIA H100 PCIe GPU and 6.5x faster than a 56-core Intel Xeon 8480+ CPU system.\nA bar chart comparing the performance of LULESH when run in multiple ways. The performance on an Intel Xeon 8480+ is the baseline. The H100 PCIe bar is 4.61x faster. The performance of the GH200 using managed memory is 6.51x and with the compiler\u2019s unified mode is 6.49x.\nFigure 2. Comparison of LULESH performance using managed and unified memory options on NVIDIA GH200 with NVIDIA H100 PCIe and a modern CPU\nPOT3D\nPOT3D approximates solar coronal magnetic fields by computing potential field solutions. It is developed by Predictive Science Inc. using modern Fortran. The application has historically run on GPUs using OpenACC, but the authors have now adopted a mixture of Fortran ```do concurrent``` to express data parallel loops and OpenACC for optimizing data movement with the GPU.\nAs presented in the GTC session, From Directives to DO CONCURRENT: A Case Study in Standard Parallelism, the stdpar version of the code performed roughly 10% slower than the optimized OpenACC code. If OpenACC is used to optimize the data movement of the stdpar version, the performance is nearly identical. This means that the same performance was achieved while maintaining roughly 2,000 fewer lines of code. Does unified memory change this?\nFigure 3 shows the performance of POT3D on Grace Hopper built in two ways. The blue bar is the performance baseline, which is Fortran ```do concurrent``` loops for parallelism and OpenACC data directives to optimize data movement. The green bar is built using the ```-gpu=unified``` option on Grace Hopper and with all OpenACC directives removed.\nThe performance of the code is now the same as the fully optimized code without requiring any OpenACC. With the performance and productivity enhancements unified memory brings, POT3D can now be written in pure Fortran and get the same performance as the previously tuned OpenACC code.\nA bar chart comparing performance of managing memory explicitly in POT3D and using the new unified memory mode. The performance using OpenACC for data management and building without any data directives is equal.\nFigure 3. POT3D performance using OpenACC data directives compared to Grace Hopper unified memory\nHow to enable and use unified memory in the NVIDIA HPC SDK\nAs of NVHPC SDK release 23.11, developers aiming to use GPUs with unified memory capability can benefit from simplified programming interfaces. This release introduces a novel compilation mode to the ```nvc++```, ```nvc```, and ```nvfortran``` compilers, which can be enabled by passing the flag ```-gpu=unified```.\nThis section is a deep dive into the specific enhancements for unified memory across various programming models supported by the NVHPC SDK, which leverage the capabilities of the underlying hardware and the CUDA runtime to automatically handle data placement and memory migration between CPU and GPU physical memory.\nstdpar\nFor stdpar, all data access restrictions have been removed. This means global variables can be accessed from CPU or GPU, and unified memory compilation is now the default setting on compatible machines. However, when cross-compiling for different targets, ```-gpu=unified``` flag needs to be passed explicitly to enable the new programming interface.\nIn the original release of stdpar C++ within ```nvc++```, lambda functions in parallel algorithms had several restrictions. These have now been completely lifted. Developers can freely use data across different parallel algorithms and sequential code. This enables capturing variables by reference and accessing global variables within parallel algorithms:\n```\nint init_val = 123;\nvoid foo() {\n  int my_array[ARRAY_SIZE];\n  auto r = std::views::iota(0, ARRAY_SIZE);\n  std::for_each(std::execution::par_unseq, r.begin(), r.end(),\n                [&](auto i) { my_array[i] = init_val; });\n}\n```\nIf this code is compiled as shown below, the array ```my_array``` can be safely initialized on the GPU with each element being set in parallel using the value from the global variable ```init_val```. Previously, accessing both ```my_array``` and ```init_val``` was not supported.\n```\nnvc++ -std=c++20 -stdpar -gpu=unified example.cpp\n```\nIt is now also possible to use ```std::array``` safely with parallel algorithms, as illustrated by the example:\n```\nstd::array<int, 10000> my_array = ...;\nstd::sort(std::execution::par, my_array.begin(), my_array.end());\n```\nThe removal of data access limitations is a notable improvement, but it is important to remember that data races are still possible. For example, accessing global variables within parallel algorithms with simultaneous updates in different lambda instances running on the GPU.\nPorting existing code to stdpar C++ and integrating third-party libraries is also simplified. When pointers to data used in parallel algorithms originate in allocation statements from separate files, those files no longer require compilation with ```nvc++``` or ```-stdpar```.\nFor standard Fortran, some variable uses were previously unsupported. Now, it is possible to access global variables in routines called from ```do concurrent``` loops. Additionally, there were cases where the compiler could not accurately determine variable sizes for implicit data movements between GPU and CPU. These cases can now be handled correctly on targets with unified memory:\n```\nsubroutine r(a, b)\n  integer :: a(*)\n  integer :: b(:)\n  do concurrent (i = 1 : size(b))\n    a(b(i)) = i \n  enddo\nend subroutine\n```\nIn the example above, an access region of an assumed-size array ```a``` inside the ```do concurrent``` construct cannot be determined at compile time because the element index positions are taken from another array ```b``` initialized outside or the routine. This is no longer an issue when such code is compiled as follows:\n```\nnvfortran -stdpar -gpu=unified example.f90\n```\nThe crucial aspect is that the compiler no longer requires precise knowledge of the data segment accessed within the loop. Automatic data transfers between GPU and CPU are now handled seamlessly by the CUDA runtime.\nOpenACC\nNow with unified memory mode, OpenACC programs no longer require explicit data clauses and directives. All variables are now accessible from the OpenACC compute regions. This implementation closely adheres to the shared memory mode detailed in the OpenACC specification.\nThe following C example illustrates an OpenACC parallel loop region that can now be executed correctly on GPUs without requiring any data clauses:\n```\nvoid set(int* ptr, int i, int j, int dim){\n  int idx = i * dim + j;\n  ptr[idx] = someval(i, j);\n}\n\nvoid fill2d(int* ptr, int dim){\n#pragma acc parallel loop\n  for (int i = 0; i < dim; i++)\n    for (int j = 0; j < dim; j++)\n      set(ptr, i, j, dim);\n}\n```\nIn C/C++, native language arrays are implicitly decayed into pointers when passed to functions. Therefore, the original array shape and size information is not preserved across the function invocations. Moreover, arrays with the dynamic size are represented by pointers. The use of pointers poses major challenges for the automatic code optimizations, as compilers lack essential information about the original data.\nWhile the OpenACC compiler has strong support for detecting the data segment accessed in loops to perform the data movements to GPU implicitly, it cannot determine the segment in this case because the array is updated through ```ptr``` in another function ```set``` called inside the loop. Previously, supporting such cases in C was not possible. However, with \u200cunified memory mode enabled as shown below, such examples are now fully supported:\n```\nnvc -acc -gpu=unified example.c\n```\nWithout ```-gpu=unified``` the only way to guarantee correctness for this example is to change the line with the pragma directive:\n```\n#pragma acc parallel loop create(ptr[0:dim*dim]) copyout(ptr[0:dim*dim])\n```\nThis explicitly instructs the OpenACC implementation about the precise data segment used within the parallel loop.\nThe Fortran example below illustrates how a global variable can now be accessed in the OpenACC routine without requiring any explicit annotations.\n```\nmodule m\ninteger :: globmin = 1234\ncontains\nsubroutine findmin(a)\n!$acc routine seq\n  integer, intent(in)  :: a(:)\n  integer :: i\n  do i = 1, size(a)\n    if (a(i) .lt. globmin) then\n      globmin = a(i)\n    endif\n  end do\nend subroutine\nend module m\n```\nWhen this example is compiled as shown below, the source does not need any OpenACC directives in order to access module variable ```globmin``` to read or update its value in the routine invoked from CPU and GPU.\n```\nnvfortran -acc -gpu=unified example.f90\n```\nMoreover, any access to ```globmin``` will be made to the same exact instance of the variable from CPU and GPU keeping its value synchronized automatically. Previously, such behavior could only be achieved by adding a combination of OpenACC ```declare``` and ```update``` directives in the source code.\nIn binaries compiled with ```-gpu=unified```, the OpenACC runtime leverages data action information such as create/delete or copyin/copyout as optimizations to indicate preferable data placement to the CUDA runtime by means of memory hint APIs. For more details, see Simplifying GPU Application Development with Heterogeneous Memory Management.\nSuch actions originate either from the explicit data clauses in the source code or implicitly determined by the compiler. These optimizations can be used to fine-tune application performance by minimizing the amount of \u200cautomatic data migrations.\nFor the C example above, while adding data clauses ```create(ptr[0:dim*dim])``` and ```copyout(ptr[0:dim*dim])``` is optional with ```-gpu=unifie``` d, their use in the OpenACC parallel loop directive may lead to a performance uplift.\nCUDA Fortran\nThe addition of -gpu=unified also simplifies CUDA Fortran programming by removing restrictions on CPU-declared variables passed as arguments to global or device routines executing on the GPU. Moreover, it now permits referencing module or common block variables in such routines without requiring explicit attributes. This change does not affect variables explicitly annotated with existing data attributes: device, managed, constant, shared, or pinned.\n```\nmodule m\ninteger :: globval\ncontains \nattributes(global) subroutine fill(a)\n  integer :: a(*)\n  i = threadIdx%x\n  a(i) = globval\nend subroutine\nend module m\nprogram example\n  use m\n  integer :: a(N)\n  globval = 123\n  call fill<<<1, N>>> (a)\n  e = cudaDeviceSynchronize()\nend program\n```\nIn the example above, the CPU stack-allocated array a is initialized in the kernel fill on the GPU with the value from the global variable globval assigned in the CPU code. As shown, a kernel routine, which is an entry point for execution on GPU, is now enables to directly access variables declared in the regular CPU host.\nDetails common across programming models\nBinaries that have not been compiled with the new ```-gpu=unified``` flag will retain their existing performance characteristics on systems with and without unified memory alike. However, binaries compiled with ```-gpu=unified``` can not guarantee correct execution on targets without unified memory capability. When linking the final binary for unified memory targets, passing ```-gpu=unified``` in the linker command line is required for correctness.\nMany applications transitioning to architectures with unified memory can seamlessly recompile with ```-gpu=unified``` without any code modifications. In addition, stdpar C++ and CUDA Fortran object files, whether compiled with or without ```-gpu=unified```, can be linked together. However, linking object files containing OpenACC directives or Fortran DC compiled differently with and without ```-gpu=unified``` is currently not supported.\nManual performance tuning for memory usage is currently achievable through the CUDA memory hints APIs for all the programming models that support unified memory, as well as through data directives for OpenACC programs.\nThe HPC SDK will continue to enhance support for unified memory in upcoming releases. For in-depth information regarding the current status, limitations, and future updates on this new functionality, refer to the NVIDIA HPC SDK documentation.\nSummary\nThe features and performance explained in this post are just the beginning of what NVIDIA Grace Hopper Superchip architecture and the NVIDIA software stack are bringing to developers. Future developments in the driver, CUDA software stack, and the NVIDIA HPC compilers are expected to remove even more restrictions on the way that users write their code, and to improve the performance of the resulting applications.\nLearn more about \u200ccompiler support on the NVIDIA HPC SDK page.\nRead Developing Accelerated Code with Standard Language Parallelism.\nCheck out the blog post series on Fortran Standard Parallelism.\nDownload the NVIDIA HPC SDK for free.\nSPEC and SPECaccel are registered trademarks of the Standard Performance Evaluation Corporation."}], "https://developer.nvidia.com/blog/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/": [{"text": "The article discusses the use of synthetic data and novel view synthesis to address challenges in autonomous vehicle perception. Perception algorithms trained on specific sensor configurations may not perform well on vehicles with different sensor setups. Sensitivity analysis and retraining may improve performance but are time-consuming and expensive. Synthetic datasets in NVIDIA DRIVE Sim and novel view synthesis techniques can bridge these gaps by generating data from various viewpoints. The NVS method transforms existing driving data to simulate new sensor positions, improving perception accuracy for different camera heights and angles. Validation of NVS-generated data shows enhanced perception performance, especially for changes in sensor pitch. This approach allows for the reuse of existing datasets across multiple vehicle types, reducing costs and deployment time. Overall, synthetic data and novel view synthesis offer a systematic way to measure perception sensitivity and facilitate the development of robust perception stacks for autonomous vehicles. Researchers are encouraged to explore the provided data and contribute to this field of work.", "text_components": ["Using Synthetic Data to Address Novel Viewpoints for Autonomous Vehicle Perception\nAutonomous vehicles (AV) come in all shapes and sizes, ranging from small passenger cars to multi-axle semi-trucks. However, a perception algorithm deployed on these vehicles must be trained to handle similar situations, like avoiding an obstacle or a pedestrian.\nThe datasets used to develop and validate these algorithms are typically collected by one type of vehicle\u2014 for example sedans outfitted with cameras, radars, lidars, and ultrasonic sensors.\nPerception algorithms trained on fleet sensor data can perform reliably on similar sensor configurations. However, when deploying the same algorithm on a vehicle with a different sensor configuration, perception performance can degrade, as it is seeing the world from a new point of view.\nAddressing any loss in perception accuracy requires measuring the sensitivity of the deep neural network (DNN) to new sensor positions. Using the sensitivity analysis, it is possible to retrain the perception algorithm with data from multiple points of view to improve robustness in a targeted manner.\nHowever, performing a sensitivity analysis and retraining perception both require the collection and annotation of datasets across a variety of sensor configurations. This is a time- and cost-prohibitive process.\nThis post shows how synthetic datasets in NVIDIA DRIVE Sim and the latest NVIDIA research in novel view synthesis (NVS) fill these data gaps and help recover perception accuracy otherwise lost when deploying to new sensor configurations.", "Measuring DNN sensitivity\nBefore creating synthetic datasets for different sensor viewpoints, the first step is to create a digital twin of the test fleet vehicle in NVIDIA DRIVE Sim, along with a sensor rig of simulated cameras that are calibrated to match real-world sensor configurations.\nSynthetic data is generated by driving the ego-vehicle through a predefined scenario where it follows a specific trajectory, and saving the simulated camera data. For each run of the scenario, aspects are varied, such as sensor rig height, pitch, and mount position to emulate other vehicle types.\nUsing the capabilities of NVIDIA Omniverse Replicator, generate the ground truth (GT) labels, such as 3D bounding boxes, and object classes needed to evaluate perception algorithms. This entire workflow is repeatable and enables running well-defined experiments to quickly measure perception sensitivity.\nAfter running DNN inference on the generated datasets, compare the network\u2019s predictions with the GT labels to measure the network\u2019s accuracy for each sensor configuration for different camera heights, as shown in Figures 1 and 2. Each dataset is the same scenario but from different sensor view points. In Figure 1, blue boxes represent GT labels while green boxes show the network\u2019s predictions. In Figure 2, blue boxes represent GT labels while red boxes show the network\u2019s predictions.\nSide-by-side images from four sensor configurations showing a simulated driving scene with bounding boxes around other cars and objects in each frame.\nFigure 1. An example of an object detection DNN running on four different synthetic datasets, focusing on the vehicle object class\nSide-by-side images from four sensor configurations showing a simulated driving scene with bounding boxes around other cars and objects in each frame.\nFigure 2. Example of an object detection DNN running on four different synthetic datasets, focusing on the pedestrian object class\nGiven that the network was trained on data from one vehicle type, detections are more accurate for similar camera positions, and degrade as camera positions change significantly.\nAddressing these gaps in perception and deploying on a new vehicle type requires a targeted dataset for viewpoints that differ from the original data. While existing fleet data can be used with traditional augmentations, this approach does not fully satisfy the need for datasets captured from new points of view.", "Novel view synthesis\nNVS is a computer vision method for generating new, unseen views of a scene from a set of existing images. This capability makes it possible to create images of a scene from different viewpoints or angles not originally captured by the vehicle\u2019s camera.\nFive side-by-side images illustrating the NVS pipeline. It starts with the source image, then depth estimation, followed by mesh creation. Finally, a diagram showing sensors being adjusted to a new viewpoint, resulting in the final novel view image.\nFigure 3. The complete novel view synthesis pipeline\nThe NVIDIA research team recently presented an NVS method that can transform dynamic driving data from one sensor position to new viewpoints emulating other camera heights, pitches, and angles. For details, see Towards Viewpoint Robustness in Bird\u2019s Eye View Segmentation.\nVideo 1. NVIDIA DRIVE Labs video series walks through the latest research on transforming existing driving data to new sensor viewpoints Our approach builds on Worldsheet, a method for using depth estimation and 3D meshing to synthesize new viewpoints of a static scene. A 3D scene mesh is created by warping a lattice grid onto a scene based on predicted depth values. Then, a texture sampler is used to \u201csplat,\u201d or project, the RGB pixel intensities from the original image onto the texture map of the 3D mesh. This approach expands on prior work in this area by using lidar-based depth supervision and automasking to improve the quality of the depth estimation and handle occlusions.\nThe NVS model can now be used to generate data as if it was acquired from different vehicle types, unblocking existing fleet data for use in all future AV development.\nA grid of camera images from a vehicle\u2019s point of view, each showing slight adjustments in camera pitch, depth, and height.\nFigure 4. Examples of NVS-transformed images, generating viewpoints with changes in pitch, depth and height", "Validating NVS and improving perception performance\nBefore incorporating NVS-generated data into the training dataset, first validate that it accurately represents the real world and is effective for perception training.\nTo do this, validate the NVS algorithm by training a perception algorithm on a combination of fleet data and NVS-transformed data. In the absence of real data to test the model\u2019s performance from multiple sensor viewpoints, generate synthetic data and GT labels in DRIVE Sim, similar to the sensitivity testing previously discussed.\nA grid of camera images from a vehicle\u2019s point of view, each showing slight adjustments in camera pitch, depth, and height.\nFigure 5. A set of camera images generated in DRIVE Sim with varied pitch, depth, and height for perception validation Running inference on these synthetic datasets shows that using NVS-generated data for training can improve perception performance. Specifically:\nNVS-generated data quality is best for changes in sensor pitch and lowest for large changes in height.\nNVS-transformed data for training enables recovering valuable perception performance that would only have been possible by collecting new data for each new sensor configuration.\nThis approach unlocks a new approach to AV development, where data only needs to be collected once, then repurposed for multiple vehicle types\u2014significantly reducing cost and time to deployment.", "Conclusion\nDeveloping a perception stack that works robustly across different vehicle types is a massive data challenge. However, synthetic data generation and AI techniques for novel view synthesis enable the systematic measurement of perception sensitivity. This significantly multiplies the value of existing datasets and reduces the time to deploy a perception stack for any vehicle.\nWe invite the research community to add to this body of work. Accordingly, we are releasing the synthetic data from DRIVE Sim as reported in Towards Viewpoint Robustness in Bird\u2019s Eye View Segmentation. Explore this data and learn more."], "document_title": "Using Synthetic Data to Address Novel Viewpoints for Autonomous Vehicle Perception", "document_url": "https://developer.nvidia.com/blog/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/", "document_date": "2023-11-13T16:58:27", "document_date_modified": "2024-01-10T18:06:50", "document_full_text": "Using Synthetic Data to Address Novel Viewpoints for Autonomous Vehicle Perception\nAutonomous vehicles (AV) come in all shapes and sizes, ranging from small passenger cars to multi-axle semi-trucks. However, a perception algorithm deployed on these vehicles must be trained to handle similar situations, like avoiding an obstacle or a pedestrian.\nThe datasets used to develop and validate these algorithms are typically collected by one type of vehicle\u2014 for example sedans outfitted with cameras, radars, lidars, and ultrasonic sensors.\nPerception algorithms trained on fleet sensor data can perform reliably on similar sensor configurations. However, when deploying the same algorithm on a vehicle with a different sensor configuration, perception performance can degrade, as it is seeing the world from a new point of view.\nAddressing any loss in perception accuracy requires measuring the sensitivity of the deep neural network (DNN) to new sensor positions. Using the sensitivity analysis, it is possible to retrain the perception algorithm with data from multiple points of view to improve robustness in a targeted manner.\nHowever, performing a sensitivity analysis and retraining perception both require the collection and annotation of datasets across a variety of sensor configurations. This is a time- and cost-prohibitive process.\nThis post shows how synthetic datasets in NVIDIA DRIVE Sim and the latest NVIDIA research in novel view synthesis (NVS) fill these data gaps and help recover perception accuracy otherwise lost when deploying to new sensor configurations.\nMeasuring DNN sensitivity\nBefore creating synthetic datasets for different sensor viewpoints, the first step is to create a digital twin of the test fleet vehicle in NVIDIA DRIVE Sim, along with a sensor rig of simulated cameras that are calibrated to match real-world sensor configurations.\nSynthetic data is generated by driving the ego-vehicle through a predefined scenario where it follows a specific trajectory, and saving the simulated camera data. For each run of the scenario, aspects are varied, such as sensor rig height, pitch, and mount position to emulate other vehicle types.\nUsing the capabilities of NVIDIA Omniverse Replicator, generate the ground truth (GT) labels, such as 3D bounding boxes, and object classes needed to evaluate perception algorithms. This entire workflow is repeatable and enables running well-defined experiments to quickly measure perception sensitivity.\nAfter running DNN inference on the generated datasets, compare the network\u2019s predictions with the GT labels to measure the network\u2019s accuracy for each sensor configuration for different camera heights, as shown in Figures 1 and 2. Each dataset is the same scenario but from different sensor view points. In Figure 1, blue boxes represent GT labels while green boxes show the network\u2019s predictions. In Figure 2, blue boxes represent GT labels while red boxes show the network\u2019s predictions.\nSide-by-side images from four sensor configurations showing a simulated driving scene with bounding boxes around other cars and objects in each frame.\nFigure 1. An example of an object detection DNN running on four different synthetic datasets, focusing on the vehicle object class\nSide-by-side images from four sensor configurations showing a simulated driving scene with bounding boxes around other cars and objects in each frame.\nFigure 2. Example of an object detection DNN running on four different synthetic datasets, focusing on the pedestrian object class\nGiven that the network was trained on data from one vehicle type, detections are more accurate for similar camera positions, and degrade as camera positions change significantly.\nAddressing these gaps in perception and deploying on a new vehicle type requires a targeted dataset for viewpoints that differ from the original data. While existing fleet data can be used with traditional augmentations, this approach does not fully satisfy the need for datasets captured from new points of view.\nNovel view synthesis\nNVS is a computer vision method for generating new, unseen views of a scene from a set of existing images. This capability makes it possible to create images of a scene from different viewpoints or angles not originally captured by the vehicle\u2019s camera.\nFive side-by-side images illustrating the NVS pipeline. It starts with the source image, then depth estimation, followed by mesh creation. Finally, a diagram showing sensors being adjusted to a new viewpoint, resulting in the final novel view image.\nFigure 3. The complete novel view synthesis pipeline\nThe NVIDIA research team recently presented an NVS method that can transform dynamic driving data from one sensor position to new viewpoints emulating other camera heights, pitches, and angles. For details, see Towards Viewpoint Robustness in Bird\u2019s Eye View Segmentation.\nVideo 1. NVIDIA DRIVE Labs video series walks through the latest research on transforming existing driving data to new sensor viewpoints Our approach builds on Worldsheet, a method for using depth estimation and 3D meshing to synthesize new viewpoints of a static scene. A 3D scene mesh is created by warping a lattice grid onto a scene based on predicted depth values. Then, a texture sampler is used to \u201csplat,\u201d or project, the RGB pixel intensities from the original image onto the texture map of the 3D mesh. This approach expands on prior work in this area by using lidar-based depth supervision and automasking to improve the quality of the depth estimation and handle occlusions.\nThe NVS model can now be used to generate data as if it was acquired from different vehicle types, unblocking existing fleet data for use in all future AV development.\nA grid of camera images from a vehicle\u2019s point of view, each showing slight adjustments in camera pitch, depth, and height.\nFigure 4. Examples of NVS-transformed images, generating viewpoints with changes in pitch, depth and height\nValidating NVS and improving perception performance\nBefore incorporating NVS-generated data into the training dataset, first validate that it accurately represents the real world and is effective for perception training.\nTo do this, validate the NVS algorithm by training a perception algorithm on a combination of fleet data and NVS-transformed data. In the absence of real data to test the model\u2019s performance from multiple sensor viewpoints, generate synthetic data and GT labels in DRIVE Sim, similar to the sensitivity testing previously discussed.\nA grid of camera images from a vehicle\u2019s point of view, each showing slight adjustments in camera pitch, depth, and height.\nFigure 5. A set of camera images generated in DRIVE Sim with varied pitch, depth, and height for perception validation Running inference on these synthetic datasets shows that using NVS-generated data for training can improve perception performance. Specifically:\nNVS-generated data quality is best for changes in sensor pitch and lowest for large changes in height.\nNVS-transformed data for training enables recovering valuable perception performance that would only have been possible by collecting new data for each new sensor configuration.\nThis approach unlocks a new approach to AV development, where data only needs to be collected once, then repurposed for multiple vehicle types\u2014significantly reducing cost and time to deployment.\nConclusion\nDeveloping a perception stack that works robustly across different vehicle types is a massive data challenge. However, synthetic data generation and AI techniques for novel view synthesis enable the systematic measurement of perception sensitivity. This significantly multiplies the value of existing datasets and reduces the time to deploy a perception stack for any vehicle.\nWe invite the research community to add to this body of work. Accordingly, we are releasing the synthetic data from DRIVE Sim as reported in Towards Viewpoint Robustness in Bird\u2019s Eye View Segmentation. Explore this data and learn more."}], "https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/": [{"text": "The article discusses the importance of optimizing energy efficiency in supercomputing centers, focusing on maximizing workload per watt. It explores measuring and optimizing energy usage of multi-node VASP simulations, a program for atomic-scale materials modeling, with NVIDIA products. The study investigates the energy usage of hybrid density functional theory calculations for different system sizes of hafnia compound. The results show that running simulations on the NVIDIA accelerated platform using Magnum IO NCCL can significantly reduce energy consumption and enable more work per unit of time. The study recommends running VASP on the NVIDIA GPU platform, using NCCL to maximize parallel efficiency, and running simulations at the optimal GPU clock frequency for the best energy savings. The article emphasizes the importance of balancing speed and energy efficiency in HPC applications and provides insights into optimizing energy usage for large-scale molecular simulations.", "text_components": ["Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO\nComputational energy efficiency has become a primary decision criterion for most supercomputing centers. Data centers, once built, are capped in terms of the amount of power they can use without expensive and time-consuming retrofits. Maximizing insight in the form of workload throughput then means maximizing workload per watt. NVIDIA products have, for several generations, focused on maximizing real application performance per kilowatt hour (kWh) used.\nThis post explores measuring and optimizing energy usage of multi-node hybrid density functional theory-(DFT) calculations with the Vienna Ab initio Simulation Package (VASP). VASP is a computer program for atomic-scale materials modeling, such as electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.\nMaterial property research is an active area for researchers using supercomputing facilities for cases as broad as high-temperature, low-pressure superconductors, to the next generation of solar cells. VASP is a primary tool in these digital investigations.\nThis post follows our 2022 investigation on multi-node VASP scalability for varying system sizes of a simple compound hafnia (HfO 2 ). For details, see Scaling VASP with NVIDIA Magnum IO.", "Experiment setup\nThe environment and setup used for this energy-focused extension of our previous work is largely the same. This section provides details about our experiment setup for reproducing our results.", "Hardware: NVIDIA GPU system\nNVIDIA Selene cluster\nNVIDIA DGX A100\nAMD EPYC 7742 64C 2.25 GHz\nNVIDIA A100 Tensor Core GPUs (80 GB) (eight per node)\nNVIDIA HDR InfiniBand (eight per node)", "Hardware: CPU system\nDual-socket Intel 8280 CPUs (28 cores per socket)\n192 GB per node\nNVIDIA HDR InfiniBand per node", "Software\nUpdated versions for all the following components of the NVIDIA GPU software stack are available. However, we intentionally used the same components as for our previous work to retain comparability. VASP was updated to version 6.4 released in 2023.\nNVIDIA HPC SDK 22.5 (formerly known as PGI)\nNVIDIA Magnum IO NCCL 2.12.9\nCUDA 11.7\nFFT\nGPU: FFT lib \u2013 cuFFT 10.7.2 (GPU side)\nCPU: FFTW interface comes from Intel MKL 2020.0.166\nMPI: open MPI 4.1.4rc2 compiled with PGI\nUCX\nVASP 6.4.0\nFor runs on an Intel CPU-only cluster, we employed the accordingly optimal toolchain consisting of the most recent version available at the time of writing:\nRocky Linux release 9.2 (Blue Onyx)\nIntel oneAPI HPC Toolkit 2022.3.1\nMPI: hpcx-2.15", "Extrapolating runtime and energy\nAs detailed in Scaling VASP with NVIDIA Magnum IO, we shortened the benchmarking runs and extrapolated to the full results to save resources. In other words, we only used a fraction of the energy presented in the following formula:\nt_{total} = t_{init} + 19 t_{iter} + t_{post}\nThe method was extended for energy by assuming the energy used for one iteration was similarly constant:\nE_{total} = E_{init} + 19 E_{iter} + E_{post}", "Chemistry and models\nChemistry: hafnia (HfO 2 )\nModels:\n3x3x2: 216 atoms, 1,280 orbitals\n3x3x3: 324 atoms, 1,792 orbitals\n4x4x3: 576 atoms, 3,072 orbitals\n4x4x4: 768 atoms, 3,840 orbitals", "Capturing energy usage\nThe GPU benchmarks were done on the NVIDIA Selene supercomputer, which is equipped with smart PDUs that can provide information on currently used power through the SNMP protocol. We collected the data using a simple Python script launched in the background of each node before starting the application.\nWe collected \u200cpower usage with a frequency of 1 Hz combined with timestamps. Given that, for GPU runs at hybrid-DFT level, VASP leaves the CPU mostly idle, this logging comes at almost no overhead. Based on the information in the files and timestamps included in the outputs from VASP, we calculated the energy usage for each part of the code and project as described above.", "Optimizing for best energy efficiency (MaxQ)\nBy default, NVIDIA GPUs operate at their maximum clock frequency with sufficient load to ensure best possible performance, and hence time to solution. However, the performance of certain parts of an application might not be primarily limited by clock frequency to begin with.\nHigher clock frequencies require higher voltages, and this in turn leads to a higher energy uptake. Hence, the sweet spot for the maximum GPU clock frequency for solving a problem in the shortest time to solution might be different from those necessary to achieve the lowest energy to solution.\nLooking at hypothetical applications that are entirely limited by memory loads and stores, one would expect the lowest frequency that suffices to still saturate memory bandwidth should give a better energy to solution while not impairing performance.\nGiven that real applications have mixed computational profiles and the dependence on frequency varies with the workload, the ideal frequency can be determined on a case-by-case basis. This was done for the VASP hafnia workloads presented here. However, we \u200chave observed that our findings also work well for other high-performance computing (HPC) applications.\nThe frequencies can be controlled through the NVIDIA System Management Interface (SMI), as shown in the code snippets below:\n```\n-lgc --lock-gpu-clocks= Specifies <minGpuClock,maxGpuClock> clocks as a pair (1500,1500) that defines\nthe range of desired locked GPU clock speed in MHz. \nSetting this will supersede application clocks and take effect regardless if an app is running. \nInput can also be a singular desired clock value (<GpuClockValue>).\n\nFor example:\n# nvidia-smi -pm 1\n# nvidia-smi -i 0 -pl 250\n# nvidia-smi -i 0 -lgc 1090,1355\n```\nAdditional data collected includes:\nCPU multi-node performance\nSingle-node, SM frequency sweep for MaxQ", "Results\nThis section showcases the influence of GPU clock frequency on energy usage in VASP simulations, emphasizing the trade-offs between computational speed and energy usage. It also explores the complexities of optimizing the balance between performance and energy in HPC by analyzing data and heatmaps to minimize both time to solution and energy usage.", "GPU clock frequency for efficiency\nIn the pursuit of a maximum rate of scientific insight for minimum energy cost, the GPU clock frequency can be set dynamically at a rate less than maximum. For NVIDIA A100 GPUs, the maximum is 1,410 MHz.\nLowering the GPU clock frequency has two effects: it lowers the maximum theoretical computational rate the GPU can achieve, which reduces \u200cenergy usage by the GPU. But it also reduces the amount of heat the GPU generates as it performs computations.\nIn Figures 1 and 2, the data are normalized to the energy used by one node with NCCL enabled, running at the maximum frequency of 1,410 MHz. All the data shown are for the 216-atom case of hafnia. The vertical axes are matched, so relative energy usage can be seen between NCCL enabled and disabled.\nA chart showing energy use relative to default GPU clock of 1,410 MHz as the GPU clock is changed from less than 800 MHz to 1,410. The plots show a smooth curve, and minimum at 1,250 MHz for both NCCL enabled and disabled. NCCL enabled at 128 nodes shows a 60% increase over the baseline, NCCL disabled shows a 200+% increase over the baseline.\nFigure 1. Relative energy use at different GPU clocks (x-axis) for different node counts (separate lines) for NCCL enabled and the 216-atom case Chart showing energy use relative to default GPU clock of 1,410 MHz as the GPU clock is changed from less than 800 MHz to 1,410. The plots show a smooth curve, and minimum at 1,250 MHz for both NCCL enabled and disabled. NCCL enabled at 128 nodes shows a 60% increase over the baseline, NCCL disabled shows a 200+% increase over the baseline.\nFigure 2. Relative energy use at different GPU clocks (x-axis) for different node counts (separate lines) for NCCL disabled and the 216-atom case For both the NCCL enabled and disabled cases, reducing the GPU clock offers, at best, a 10% reduction in energy usage compared to the single-node, maximum frequency energy usage. In both cases, the minimum energy usage for most runs is close to the 1,250 MHz GPU clock.", "Scalability and energy usage\nOur previous investigation showed the significant performance available to NVIDIA GPU users when calculating large atomic systems in VASP within the hybrid DFT level of theory. Though the focus of this work is energy usage and efficiency, performance remains a crucial concern.\nChart showing the near-linear scaling for the three models: 96, 216, and 423 atoms.\nFigure 3. CPU scalability compared to ideal for 96, 216, and 324 atoms\nChart showing the energy usage of the three models at about 10 kWh for 96 atoms, 100 for 216 atoms, about 700 kWh for 324 atoms.\nFigure 4. CPU energy usage scaling for 96, 216, and 324 atoms To view the energy use trends, we began with a comparison of CPU-only and GPU performance, and energy. Though the CPUs for these systems are two generations behind current state-of-the-art Intel CPUs, they exhibit excellent scalability and a roughly linear trend for energy use. For the range of a single node up to 32 nodes, the energy usage for the 96-atoms case increases by 24%, for 216 atoms it increases by 13%, and for the 324-atom case it increases by 12%.\nBy comparison, for the GPU runs at the same scales with NCCL enabled, energy increases by 10% at 216 atoms, and 3% at 324 atoms. Though it is not plotted, parallel efficiency for all three CPU-based runs stays above 80%, so the scaling is very good.\nIn Figures 5 and 6, GPU performance data is shown for the maximum available GPU frequency of 1,410 MHz with NCCL enabled on four A100 GPUs per node. Note that the GPU system is more than an order of magnitude faster than the CPU system, and the scalability (slope of the lines) are essentially parallel, so both are scaling at the same rate.\nLog-log plot of elapsed time compared to number of nodes, where GPUs are more than an order of magnitude faster than CPUs for both the 216 and 324 atom cases, and both scale roughly linearly\nFigure 5. CPU elapsed time compared to GPU-based systems for 216 and 324 atoms A semi-log plot of energy where GPUs use more than a factor of two less energy for both 216 and 324 atom cases.\nFigure 6. CPU energy compared to GPU-based systems for 216 and 324 atoms Figure 5 shows that the performance achieved for the 324-atom case on 32 nodes of CPUs is about the same speed as for a single node of A100 GPUs. GPU systems, even though they run at higher power, use less than 20% of the energy compared to CPU-based simulations.\nThe other item of note is the near invariance in energy use between one and 32 nodes for the GPU system. For the case of hafnia at these sizes, the A100 GPU systems are 5x as energy efficient while delivering more than 32x the throughput in the same elapsed time.\nFigures 7 and 8 show the scalability of VASP with NCCL enabled and disabled. NCCL offers about twice the performance at 128 nodes (or 1,024 GPUs). Our previous work showed that the performance for VASP 6.4.0 is slightly better than 6.3.2.\nBoth figures are plotted relative to the single-node VASP 6.3.2 results. They show a 128-node speedup of 107x compared to 114x for the 576-atom case; and 113x compared to 115x for the 768-atom case.\nSpeedup plot showing good scaling for 216, 324 atom cases up to 64 nodes (1,024 GPUs) and almost a factor of 2 in performance at 64 nodes between NCCL enabled and disabled.\nFigure 7. Strong scaling speedup relative to single-node performance at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes for 216 and 324 atoms\nNear-perfect scaling to 128 nodes (2,048 GPUs) for the 576 and 768 atom cases for NCCL enabled, with less scalability for NCCL disabled, and a factor of 2 in performance difference.\nFigure 8. Strong scaling speedup relative to single-node performance at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes for 576 and 768 atom s\nThe most important effect to note is the performance of the models ranging from 216 atoms to 768. This enables using NCCL instead of only MPI, offering the end user more than 2x the performance for a given number of GPUs. Or similarly, at a given parallel efficiency, an NCCL-enabled VASP hybrid-DFT calculation can be scaled to much higher numbers of GPUs to compress the elapsed time.\nOccasionally concerns are expressed that GPU-enabled servers require 4x to 8x the power that a CPU-based server requires. While it is true that GPU servers do require more power, accelerated applications always use far less energy to complete a task compared to a CPU. Energy is power multiplied by time. So, though the GPU server may draw more power (watts) while it is running, it runs for less time and so uses less total energy.\nFigure 9 shows a comparison of a GPU and CPU workload, where the CPU workload runs for a long time at low power and the GPU workload finishes much earlier though at a higher power, which enables GPUs to use less energy. The GPU workload runs very quickly at high power. Energy is the area of each of the time histories.\nPower compared to time plot showing that GPUs run at high power for short time, CPUs run at low power for a long time. Energy is the area under the curve for each.\nFigure 9. Comparison of a GPU and CPU workload\nFigure 10 shows the energy advantage that using NCCL over MPI-only can provide. Both MPI and NCCL have the GPU server running at roughly the same power level, but because using NCCL scales better, the runtimes are shorter, and thereby less energy is used.\nThe gap in energy between the two grows with node count simply because the scalability of MPI-only is significantly worse. As the parallel efficiency drops, the simulation runs for a longer time while not doing more productive work, and thereby uses more energy.\nQuantitatively, a hafnia hybrid-DFT calculation shows up to 1.8x reduction in energy usage for models between 216 and 768 atoms running at maximum GPU frequency at 128 nodes for using NCCL (Figures 10 and 11).\nUsing lower numbers of nodes reduces the energy difference because the MPI-only simulations have a relatively higher parallel efficiency when running on fewer nodes. The tradeoff is that the runtime per simulation is extended.\nEnergy use for 216 and 324 atom cases from one to 128 nodes where NCCL enabled and disabled start at the same energy, and diverge with NCCL disabled using 1.5-2.0x more energy at 128 nodes.\nFigure 10. Energy used at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes from 216 atoms and 324 atoms\nEnergy use for 576 and 768 atom cases from one to 128 nodes where NCCL enabled / disabled start at the same energy, and diverge with NCCL disabled using 1.5-2.0x more energy at 128 nodes.\nFigure 11. Energy used at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes from for 576 and 768 atoms\nAs a VASP user or an HPC center manager, you may be asking yourself, \u201cFor a given large system of atoms, what is the most efficient point, or how many nodes are needed per simulation?\u201d This is a very good question\u2014one that we expect more and more people will be asking in the near future.\nGenerally speaking, the fewer the nodes, the closer to 100% parallel efficiency the run will be, and will thereby use less energy. Figure 7 of Scaling VASP with NVIDIA Magnum IO shows the parallel efficiency, and Figures 10 and 11 of this post show the energy.\nHowever, other factors such as researcher time, publishing deadlines, and external influences can increase the value of having simulation results sooner. In these cases, maximizing the number of runs completed in a given time or minimizing the end-to-end wait time for results suggests less energy will be used if the algorithm is configured to obtain the best parallel efficiency. For example, a run with NCCL enabled.\nMaximizing application performance against energy usage constraints also helps optimize the return on investment for the HPC center manager.", "Balancing speed and energy\nFigure 10 or 11 might be interpreted as a recommendation to run a VASP simulation on as few nodes as possible. For HPC centers more focused on maximum efficiency than scientific output, that may be the right decision. We do not anticipate such an attitude to be common.\nDoing so, however, could easily ignore costs, which are not captured in a single metric like energy per simulation. For instance, researcher time is arguably the most precious resource in the scientific toolchain. As such, most HPC centers will want to explore a more balanced approach of reducing energy usage while minimizing the performance impact to their users.\nThis is a multi-objective optimization problem with a family of solutions depending on the relative weight of the time-to-solution and energy-to-solution objectives. We therefore expect a rigorous analysis to produce a pareto front of optimal solutions.\nA quick approach, however, is to plot energy to solution on the vertical axis, and time to solution on the horizontal axis. By visualizing the data this way, the best compromise between these two is the datapoint closest to the origin.\nFigure 12 shows the separation between NCCL enabled and NCCL disabled as the cluster of dotted lines and the cluster of solid lines, where the solid lines reach an area much closer to the optimum. It also shows some difference between the maximum performance line (blue) and maximum efficiency line (green) for both NCCL enabled and disabled.\nPlot of energy to solution compared to time to solution where lower of both parameters is labeled \u201cbetter\u201d for NCCL enabled and disabled for four different GPU clocks: 1,410, 1,350, 1,250, 1,005.\nFigure 12. Energy per jobs compared to runtime used on a 216-atom hafnia study It is difficult, however, to draw a conclusion about the optimum point to run. Is it 32 or 64 nodes? To help answer this question, Figure 13 shows a heat map calculating the distance to the origin, where green is closest to the optimum.\nRed to green heat map of distance from the origin, where the minimum distance represents the best compromise between time to solution and energy to solution. Best are 1,350 MHz and 1,250 MHz frequency, NCCL-enabled, 16, 32 nodes. Worst is NCCL enabled or disabled for one node at 510 MHz.\nFigure 13. Heat map of distance from the origin, where green is closest to the optimum", "Summary\nEnergy will continue to be a precious commodity for the foreseeable future. We have shown here and in our previous post that the NVIDIA accelerated computing platform of hardware and software is vastly more performant for multi-node simulations using VASP for medium and large-sized molecular hybrid-DFT calculations compared to both a CPU-only platform as well as an accelerated platform that uses MPI only.\nBased on these results, we urge researchers using VASP for anything but the smallest systems to do so using the NVIDIA accelerated platform. This approach uses less energy per expended kWh and enables more work per unit of time.\nThe results of this investigation show that the energy use of simulations varies by more than two orders of magnitude depending on the atom count. However, the optimization opportunity is not as large as the total use.\nThe energy savings opportunity associated with using NVIDIA Magnum IO NCCL ranges from 41 kWh in the 216-atom case at 128 nodes at best time to solution (1,410 MHz for the A100 GPU) to 724 kWh per simulation for the 768-atom case at 128 nodes. Running for best energy to solution (1,250 MHz) doesn\u2019t change the number materially for 216 atoms, and drops the difference between NCCL enabled and disabled to 709 kWh per simulation for 768 atoms.\nIn order of most energy savings for minimum impact to runtime, our recommendation for running multi-node, multi-GPU simulations for large VASP systems follows:\nRun VASP on the NVIDIA accelerated GPU platform rather than on CPUs only.\nUse NVIDIA Magnum IO NCCL to maximize parallel efficiency.\nRun a 216-atom hybridDFT calculation between 16 and 64 nodes (128 to 512 A100 GPUs); more for larger systems, and less for smaller.\nRun at the MaxQ point of 1,250 MHz (GPU clock) to get the last 5-10% energy savings.\nLooking beyond the hybrid DFT in VASP analyzed in this post, software developers can also conserve energy by (in descending order of impact):\nAccelerating applications with GPUs\nOptimizing as much as possible, including hiding unproductive, but necessary parts\nHaving users run at optimized frequencies\nFor more information about NCCL and NVIDIA Magnum IO, watch the GTC sessions, Scaling Deep Learning Training: Fast Inter-GPU Communication with NCCL and Optimizing Energy Efficiency for Applications on NVIDIA GPUs."], "document_title": "Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO", "document_url": "https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/", "document_date": "2023-11-13T16:00:00", "document_date_modified": "2023-11-20T18:42:51", "document_full_text": "Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO\nComputational energy efficiency has become a primary decision criterion for most supercomputing centers. Data centers, once built, are capped in terms of the amount of power they can use without expensive and time-consuming retrofits. Maximizing insight in the form of workload throughput then means maximizing workload per watt. NVIDIA products have, for several generations, focused on maximizing real application performance per kilowatt hour (kWh) used.\nThis post explores measuring and optimizing energy usage of multi-node hybrid density functional theory-(DFT) calculations with the Vienna Ab initio Simulation Package (VASP). VASP is a computer program for atomic-scale materials modeling, such as electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.\nMaterial property research is an active area for researchers using supercomputing facilities for cases as broad as high-temperature, low-pressure superconductors, to the next generation of solar cells. VASP is a primary tool in these digital investigations.\nThis post follows our 2022 investigation on multi-node VASP scalability for varying system sizes of a simple compound hafnia (HfO 2 ). For details, see Scaling VASP with NVIDIA Magnum IO.\nExperiment setup\nThe environment and setup used for this energy-focused extension of our previous work is largely the same. This section provides details about our experiment setup for reproducing our results.\nHardware: NVIDIA GPU system\nNVIDIA Selene cluster\nNVIDIA DGX A100\nAMD EPYC 7742 64C 2.25 GHz\nNVIDIA A100 Tensor Core GPUs (80 GB) (eight per node)\nNVIDIA HDR InfiniBand (eight per node)\nHardware: CPU system\nDual-socket Intel 8280 CPUs (28 cores per socket)\n192 GB per node\nNVIDIA HDR InfiniBand per node\nSoftware\nUpdated versions for all the following components of the NVIDIA GPU software stack are available. However, we intentionally used the same components as for our previous work to retain comparability. VASP was updated to version 6.4 released in 2023.\nNVIDIA HPC SDK 22.5 (formerly known as PGI)\nNVIDIA Magnum IO NCCL 2.12.9\nCUDA 11.7\nFFT\nGPU: FFT lib \u2013 cuFFT 10.7.2 (GPU side)\nCPU: FFTW interface comes from Intel MKL 2020.0.166\nMPI: open MPI 4.1.4rc2 compiled with PGI\nUCX\nVASP 6.4.0\nFor runs on an Intel CPU-only cluster, we employed the accordingly optimal toolchain consisting of the most recent version available at the time of writing:\nRocky Linux release 9.2 (Blue Onyx)\nIntel oneAPI HPC Toolkit 2022.3.1\nMPI: hpcx-2.15\nExtrapolating runtime and energy\nAs detailed in Scaling VASP with NVIDIA Magnum IO, we shortened the benchmarking runs and extrapolated to the full results to save resources. In other words, we only used a fraction of the energy presented in the following formula:\nt_{total} = t_{init} + 19 t_{iter} + t_{post}\nThe method was extended for energy by assuming the energy used for one iteration was similarly constant:\nE_{total} = E_{init} + 19 E_{iter} + E_{post}\nChemistry and models\nChemistry: hafnia (HfO 2 )\nModels:\n3x3x2: 216 atoms, 1,280 orbitals\n3x3x3: 324 atoms, 1,792 orbitals\n4x4x3: 576 atoms, 3,072 orbitals\n4x4x4: 768 atoms, 3,840 orbitals\nCapturing energy usage\nThe GPU benchmarks were done on the NVIDIA Selene supercomputer, which is equipped with smart PDUs that can provide information on currently used power through the SNMP protocol. We collected the data using a simple Python script launched in the background of each node before starting the application.\nWe collected \u200cpower usage with a frequency of 1 Hz combined with timestamps. Given that, for GPU runs at hybrid-DFT level, VASP leaves the CPU mostly idle, this logging comes at almost no overhead. Based on the information in the files and timestamps included in the outputs from VASP, we calculated the energy usage for each part of the code and project as described above.\nOptimizing for best energy efficiency (MaxQ)\nBy default, NVIDIA GPUs operate at their maximum clock frequency with sufficient load to ensure best possible performance, and hence time to solution. However, the performance of certain parts of an application might not be primarily limited by clock frequency to begin with.\nHigher clock frequencies require higher voltages, and this in turn leads to a higher energy uptake. Hence, the sweet spot for the maximum GPU clock frequency for solving a problem in the shortest time to solution might be different from those necessary to achieve the lowest energy to solution.\nLooking at hypothetical applications that are entirely limited by memory loads and stores, one would expect the lowest frequency that suffices to still saturate memory bandwidth should give a better energy to solution while not impairing performance.\nGiven that real applications have mixed computational profiles and the dependence on frequency varies with the workload, the ideal frequency can be determined on a case-by-case basis. This was done for the VASP hafnia workloads presented here. However, we \u200chave observed that our findings also work well for other high-performance computing (HPC) applications.\nThe frequencies can be controlled through the NVIDIA System Management Interface (SMI), as shown in the code snippets below:\n```\n-lgc --lock-gpu-clocks= Specifies <minGpuClock,maxGpuClock> clocks as a pair (1500,1500) that defines\nthe range of desired locked GPU clock speed in MHz. \nSetting this will supersede application clocks and take effect regardless if an app is running. \nInput can also be a singular desired clock value (<GpuClockValue>).\n\nFor example:\n# nvidia-smi -pm 1\n# nvidia-smi -i 0 -pl 250\n# nvidia-smi -i 0 -lgc 1090,1355\n```\nAdditional data collected includes:\nCPU multi-node performance\nSingle-node, SM frequency sweep for MaxQ\nResults\nThis section showcases the influence of GPU clock frequency on energy usage in VASP simulations, emphasizing the trade-offs between computational speed and energy usage. It also explores the complexities of optimizing the balance between performance and energy in HPC by analyzing data and heatmaps to minimize both time to solution and energy usage.\nGPU clock frequency for efficiency\nIn the pursuit of a maximum rate of scientific insight for minimum energy cost, the GPU clock frequency can be set dynamically at a rate less than maximum. For NVIDIA A100 GPUs, the maximum is 1,410 MHz.\nLowering the GPU clock frequency has two effects: it lowers the maximum theoretical computational rate the GPU can achieve, which reduces \u200cenergy usage by the GPU. But it also reduces the amount of heat the GPU generates as it performs computations.\nIn Figures 1 and 2, the data are normalized to the energy used by one node with NCCL enabled, running at the maximum frequency of 1,410 MHz. All the data shown are for the 216-atom case of hafnia. The vertical axes are matched, so relative energy usage can be seen between NCCL enabled and disabled.\nA chart showing energy use relative to default GPU clock of 1,410 MHz as the GPU clock is changed from less than 800 MHz to 1,410. The plots show a smooth curve, and minimum at 1,250 MHz for both NCCL enabled and disabled. NCCL enabled at 128 nodes shows a 60% increase over the baseline, NCCL disabled shows a 200+% increase over the baseline.\nFigure 1. Relative energy use at different GPU clocks (x-axis) for different node counts (separate lines) for NCCL enabled and the 216-atom case Chart showing energy use relative to default GPU clock of 1,410 MHz as the GPU clock is changed from less than 800 MHz to 1,410. The plots show a smooth curve, and minimum at 1,250 MHz for both NCCL enabled and disabled. NCCL enabled at 128 nodes shows a 60% increase over the baseline, NCCL disabled shows a 200+% increase over the baseline.\nFigure 2. Relative energy use at different GPU clocks (x-axis) for different node counts (separate lines) for NCCL disabled and the 216-atom case For both the NCCL enabled and disabled cases, reducing the GPU clock offers, at best, a 10% reduction in energy usage compared to the single-node, maximum frequency energy usage. In both cases, the minimum energy usage for most runs is close to the 1,250 MHz GPU clock.\nScalability and energy usage\nOur previous investigation showed the significant performance available to NVIDIA GPU users when calculating large atomic systems in VASP within the hybrid DFT level of theory. Though the focus of this work is energy usage and efficiency, performance remains a crucial concern.\nChart showing the near-linear scaling for the three models: 96, 216, and 423 atoms.\nFigure 3. CPU scalability compared to ideal for 96, 216, and 324 atoms\nChart showing the energy usage of the three models at about 10 kWh for 96 atoms, 100 for 216 atoms, about 700 kWh for 324 atoms.\nFigure 4. CPU energy usage scaling for 96, 216, and 324 atoms To view the energy use trends, we began with a comparison of CPU-only and GPU performance, and energy. Though the CPUs for these systems are two generations behind current state-of-the-art Intel CPUs, they exhibit excellent scalability and a roughly linear trend for energy use. For the range of a single node up to 32 nodes, the energy usage for the 96-atoms case increases by 24%, for 216 atoms it increases by 13%, and for the 324-atom case it increases by 12%.\nBy comparison, for the GPU runs at the same scales with NCCL enabled, energy increases by 10% at 216 atoms, and 3% at 324 atoms. Though it is not plotted, parallel efficiency for all three CPU-based runs stays above 80%, so the scaling is very good.\nIn Figures 5 and 6, GPU performance data is shown for the maximum available GPU frequency of 1,410 MHz with NCCL enabled on four A100 GPUs per node. Note that the GPU system is more than an order of magnitude faster than the CPU system, and the scalability (slope of the lines) are essentially parallel, so both are scaling at the same rate.\nLog-log plot of elapsed time compared to number of nodes, where GPUs are more than an order of magnitude faster than CPUs for both the 216 and 324 atom cases, and both scale roughly linearly\nFigure 5. CPU elapsed time compared to GPU-based systems for 216 and 324 atoms A semi-log plot of energy where GPUs use more than a factor of two less energy for both 216 and 324 atom cases.\nFigure 6. CPU energy compared to GPU-based systems for 216 and 324 atoms Figure 5 shows that the performance achieved for the 324-atom case on 32 nodes of CPUs is about the same speed as for a single node of A100 GPUs. GPU systems, even though they run at higher power, use less than 20% of the energy compared to CPU-based simulations.\nThe other item of note is the near invariance in energy use between one and 32 nodes for the GPU system. For the case of hafnia at these sizes, the A100 GPU systems are 5x as energy efficient while delivering more than 32x the throughput in the same elapsed time.\nFigures 7 and 8 show the scalability of VASP with NCCL enabled and disabled. NCCL offers about twice the performance at 128 nodes (or 1,024 GPUs). Our previous work showed that the performance for VASP 6.4.0 is slightly better than 6.3.2.\nBoth figures are plotted relative to the single-node VASP 6.3.2 results. They show a 128-node speedup of 107x compared to 114x for the 576-atom case; and 113x compared to 115x for the 768-atom case.\nSpeedup plot showing good scaling for 216, 324 atom cases up to 64 nodes (1,024 GPUs) and almost a factor of 2 in performance at 64 nodes between NCCL enabled and disabled.\nFigure 7. Strong scaling speedup relative to single-node performance at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes for 216 and 324 atoms\nNear-perfect scaling to 128 nodes (2,048 GPUs) for the 576 and 768 atom cases for NCCL enabled, with less scalability for NCCL disabled, and a factor of 2 in performance difference.\nFigure 8. Strong scaling speedup relative to single-node performance at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes for 576 and 768 atom s\nThe most important effect to note is the performance of the models ranging from 216 atoms to 768. This enables using NCCL instead of only MPI, offering the end user more than 2x the performance for a given number of GPUs. Or similarly, at a given parallel efficiency, an NCCL-enabled VASP hybrid-DFT calculation can be scaled to much higher numbers of GPUs to compress the elapsed time.\nOccasionally concerns are expressed that GPU-enabled servers require 4x to 8x the power that a CPU-based server requires. While it is true that GPU servers do require more power, accelerated applications always use far less energy to complete a task compared to a CPU. Energy is power multiplied by time. So, though the GPU server may draw more power (watts) while it is running, it runs for less time and so uses less total energy.\nFigure 9 shows a comparison of a GPU and CPU workload, where the CPU workload runs for a long time at low power and the GPU workload finishes much earlier though at a higher power, which enables GPUs to use less energy. The GPU workload runs very quickly at high power. Energy is the area of each of the time histories.\nPower compared to time plot showing that GPUs run at high power for short time, CPUs run at low power for a long time. Energy is the area under the curve for each.\nFigure 9. Comparison of a GPU and CPU workload\nFigure 10 shows the energy advantage that using NCCL over MPI-only can provide. Both MPI and NCCL have the GPU server running at roughly the same power level, but because using NCCL scales better, the runtimes are shorter, and thereby less energy is used.\nThe gap in energy between the two grows with node count simply because the scalability of MPI-only is significantly worse. As the parallel efficiency drops, the simulation runs for a longer time while not doing more productive work, and thereby uses more energy.\nQuantitatively, a hafnia hybrid-DFT calculation shows up to 1.8x reduction in energy usage for models between 216 and 768 atoms running at maximum GPU frequency at 128 nodes for using NCCL (Figures 10 and 11).\nUsing lower numbers of nodes reduces the energy difference because the MPI-only simulations have a relatively higher parallel efficiency when running on fewer nodes. The tradeoff is that the runtime per simulation is extended.\nEnergy use for 216 and 324 atom cases from one to 128 nodes where NCCL enabled and disabled start at the same energy, and diverge with NCCL disabled using 1.5-2.0x more energy at 128 nodes.\nFigure 10. Energy used at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes from 216 atoms and 324 atoms\nEnergy use for 576 and 768 atom cases from one to 128 nodes where NCCL enabled / disabled start at the same energy, and diverge with NCCL disabled using 1.5-2.0x more energy at 128 nodes.\nFigure 11. Energy used at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes from for 576 and 768 atoms\nAs a VASP user or an HPC center manager, you may be asking yourself, \u201cFor a given large system of atoms, what is the most efficient point, or how many nodes are needed per simulation?\u201d This is a very good question\u2014one that we expect more and more people will be asking in the near future.\nGenerally speaking, the fewer the nodes, the closer to 100% parallel efficiency the run will be, and will thereby use less energy. Figure 7 of Scaling VASP with NVIDIA Magnum IO shows the parallel efficiency, and Figures 10 and 11 of this post show the energy.\nHowever, other factors such as researcher time, publishing deadlines, and external influences can increase the value of having simulation results sooner. In these cases, maximizing the number of runs completed in a given time or minimizing the end-to-end wait time for results suggests less energy will be used if the algorithm is configured to obtain the best parallel efficiency. For example, a run with NCCL enabled.\nMaximizing application performance against energy usage constraints also helps optimize the return on investment for the HPC center manager.\nBalancing speed and energy\nFigure 10 or 11 might be interpreted as a recommendation to run a VASP simulation on as few nodes as possible. For HPC centers more focused on maximum efficiency than scientific output, that may be the right decision. We do not anticipate such an attitude to be common.\nDoing so, however, could easily ignore costs, which are not captured in a single metric like energy per simulation. For instance, researcher time is arguably the most precious resource in the scientific toolchain. As such, most HPC centers will want to explore a more balanced approach of reducing energy usage while minimizing the performance impact to their users.\nThis is a multi-objective optimization problem with a family of solutions depending on the relative weight of the time-to-solution and energy-to-solution objectives. We therefore expect a rigorous analysis to produce a pareto front of optimal solutions.\nA quick approach, however, is to plot energy to solution on the vertical axis, and time to solution on the horizontal axis. By visualizing the data this way, the best compromise between these two is the datapoint closest to the origin.\nFigure 12 shows the separation between NCCL enabled and NCCL disabled as the cluster of dotted lines and the cluster of solid lines, where the solid lines reach an area much closer to the optimum. It also shows some difference between the maximum performance line (blue) and maximum efficiency line (green) for both NCCL enabled and disabled.\nPlot of energy to solution compared to time to solution where lower of both parameters is labeled \u201cbetter\u201d for NCCL enabled and disabled for four different GPU clocks: 1,410, 1,350, 1,250, 1,005.\nFigure 12. Energy per jobs compared to runtime used on a 216-atom hafnia study It is difficult, however, to draw a conclusion about the optimum point to run. Is it 32 or 64 nodes? To help answer this question, Figure 13 shows a heat map calculating the distance to the origin, where green is closest to the optimum.\nRed to green heat map of distance from the origin, where the minimum distance represents the best compromise between time to solution and energy to solution. Best are 1,350 MHz and 1,250 MHz frequency, NCCL-enabled, 16, 32 nodes. Worst is NCCL enabled or disabled for one node at 510 MHz.\nFigure 13. Heat map of distance from the origin, where green is closest to the optimum\nSummary\nEnergy will continue to be a precious commodity for the foreseeable future. We have shown here and in our previous post that the NVIDIA accelerated computing platform of hardware and software is vastly more performant for multi-node simulations using VASP for medium and large-sized molecular hybrid-DFT calculations compared to both a CPU-only platform as well as an accelerated platform that uses MPI only.\nBased on these results, we urge researchers using VASP for anything but the smallest systems to do so using the NVIDIA accelerated platform. This approach uses less energy per expended kWh and enables more work per unit of time.\nThe results of this investigation show that the energy use of simulations varies by more than two orders of magnitude depending on the atom count. However, the optimization opportunity is not as large as the total use.\nThe energy savings opportunity associated with using NVIDIA Magnum IO NCCL ranges from 41 kWh in the 216-atom case at 128 nodes at best time to solution (1,410 MHz for the A100 GPU) to 724 kWh per simulation for the 768-atom case at 128 nodes. Running for best energy to solution (1,250 MHz) doesn\u2019t change the number materially for 216 atoms, and drops the difference between NCCL enabled and disabled to 709 kWh per simulation for 768 atoms.\nIn order of most energy savings for minimum impact to runtime, our recommendation for running multi-node, multi-GPU simulations for large VASP systems follows:\nRun VASP on the NVIDIA accelerated GPU platform rather than on CPUs only.\nUse NVIDIA Magnum IO NCCL to maximize parallel efficiency.\nRun a 216-atom hybridDFT calculation between 16 and 64 nodes (128 to 512 A100 GPUs); more for larger systems, and less for smaller.\nRun at the MaxQ point of 1,250 MHz (GPU clock) to get the last 5-10% energy savings.\nLooking beyond the hybrid DFT in VASP analyzed in this post, software developers can also conserve energy by (in descending order of impact):\nAccelerating applications with GPUs\nOptimizing as much as possible, including hiding unproductive, but necessary parts\nHaving users run at optimized frequencies\nFor more information about NCCL and NVIDIA Magnum IO, watch the GTC sessions, Scaling Deep Learning Training: Fast Inter-GPU Communication with NCCL and Optimizing Energy Efficiency for Applications on NVIDIA GPUs."}], "https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/": [{"text": "A Stanford University team is utilizing AI to create patient-specific cardiovascular simulations in near real-time, offering insights into heart health. By employing physics-informed machine learning surrogate models, they can visualize blood flow accurately and enhance medical device efficacy. The team leveraged MeshGraphNet, a graph neural network-based architecture, to develop a one-dimensional Reduced Order Model for simulating blood flow. They used models from the Vascular Model Repository to generate data for training the AI surrogate. The experiments showed that the GNN model outperformed traditional physics-driven models, especially in handling complex geometries. The team plans to further explore generalizing the GNN to more geometries and optimizing feature sets. NVIDIA Modulus, an open-source project, supports physics-ML researchers and provides resources for building and training physics-ML models. Researchers interested in utilizing NVIDIA Modulus can access the project on GitHub and contribute to the community effort. This innovative work aims to advance cardiovascular medicine and accelerate simulations in the field.", "text_components": ["Enabling Greater Patient-Specific Cardiovascular Care with AI Surrogates\nA Stanford University team is transforming heart healthcare with near real-time cardiovascular simulations driven by the power of AI. Harnessing physics-informed machine learning surrogate models, the researchers are generating accurate and patient-specific blood flow visualizations for a non-invasive window into cardiac studies. The technology has far-reaching scope, from evaluating coronary artery aneurysms to pioneering new surgical methods for congenital heart disease and enhancing medical device efficacy. With enormous potential in advancing cardiovascular medicine, the work could offer innovative methods for combating the leading cause of death in the US.\nCardiovascular simulations are important enablers for patient-specific treatment for several heart-related ailments. 3D computational fluid dynamics (CFD) simulations of the blood flow using finite-element methods are a computationally challenging task, especially in clinical practice. As an alternative, physics-based reduced-order models (ROMs) are often employed due to their increased efficiency.\nHowever, such ROMs rely on simplified assumptions of vessel geometry, complexity, or simplified mathematical models. They often fail to model the quantities of interest, such as pressure losses at vascular junctions accurately, which would otherwise require full 3D simulations. Conventional data-driven reduced-order approaches, such as projection-based methods, don\u2019t offer much flexibility with respect to changes to the domain geometry, which is crucial for patient-specific cardiovascular simulations.\nAs an alternative, employing deep learning-based surrogates can model these complex physical processes. Physics-informed machine learning (physics-ML) enables training deep learning models that can offer both computational efficiency as well as flexibility, through parameterizable models. Recently, graph neural network (GNN) based architectures have been proposed to build physics-ML models for emulating mesh-based simulations. This approach offers generalization over different meshes, boundary conditions, and other input parameters, which makes it a perfect candidate for patient-specific cardiovascular simulations.\nThe research team from Stanford University leveraged MeshGraphNet, a graph neural network (GNN)-based architecture, to devise a one-dimensional Reduced Order Model (1D ROM) for simulating blood flow. The team implemented this approach into NVIDIA Modulus, a platform equipped with an optimized implementation of MeshGraphNet. The reference MeshGraphnet implementation in NVIDIA Modulus brings several code optimizations such as data parallelism, model parallelism, gradient checkpointing, cuGraphs, and multi-GPU and multi-node training. All of which are useful for the development of GNNs for cardiovascular simulation.\nNVIDIA Modulus is an open-source framework geared towards the development of physics-informed machine learning models. It enhances the efficacy of high-fidelity, complex multi-physics simulations by enabling the exploration and development of surrogate models. This framework enables seamless integration of datasets with first principles, whether described by the governing partial differential equations or other system attributes, such as physical geometry and boundary conditions. Furthermore, it provides the capability to parameterize the input space, fostering the development of parameterized surrogate models critical for applications like digital twins.\nNVIDIA Modulus provides a variety of reference applications across domains from CFD, thermal, structural, and electromagnetic simulations, and can be used for numerous industry solutions ranging across climate modeling, manufacturing, automotive, healthcare, and more. These examples can serve as a foundation for the work of researchers, such as vortex shedding, in this case for the Stanford research team.", "Modeling cardiovascular simulation as an AI problem\nIn this research study, the Stanford team\u2019s objective was to develop a one-dimensional physics-ML model. They chose GNNs for developing a surrogate that infers the pressure and flow rate along the centerline of compliant vessels. The team used \u200cgeometry from 3D vascular models and generated a directed graph consisting of a set of nodes along the centerline of the geometry.\nThe nodes and edges of the graph capture the state of the system at a given time. For instance, the cross-sectional average pressure, flow rate, and area of the vessel\u2019s lumen along the centerlines of the vessel are included as \u200cnode features. The GNN takes the state of the system at time t and infers the state of the system at the next time step. This can be iteratively applied to a rollout for simulating the cardiovascular cycle as shown in Figure 2.\nThe dataset preparation includes 3D simulation data from the Finite Element solver, averaged over the cross-section slice for 1D centerline representation, and the generation of a directed graph.\nFigure 1. The steps involved in modeling the simulation problem as a graph representation\nThe boundary conditions at the inlet (red) and outlet (yellow) are required for determining the hemodynamics in the vessels. These are modeled as special edges of the graph to account for the effect and the complexity of these boundary conditions. Boundary condition parameters, one-hot vector encoding for differentiating between nodes (branch, junction, inlet, outlet), and minimum diastolic and maximum systolic pressure in the cardiac cycle are also included as node features.\nFor the details on the modeling of the physics, the mapping of graph features, and boundary conditions, refer to the paper Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks.\nSchematics of MeshGraphNet, rollout phase to update the state of the system and the encoder, processor, and decoder of the MeshGraphnet architecture.\nFigure 2. Three steps of MeshGraphNet computation\u2014encode, process, and decode that are iteratively executed to the rollout to get simulation results", "AI surrogate: Dataset, architecture, and experiments\nThe researchers used models from the Vascular model repository, containing about 252 cardiovascular models. They picked eight models, capturing typically challenging scenarios. Figure 3 shows three of the eight models: an aortofemoral model featuring an aneurysm (Model 1), a healthy pulmonary model (Model 2), and an aorta model affected by coarctation (Model 3). These include features such as multiple junctions (Model 2) or stenoses (Model 3), which can be challenging to handle using 1D physics-based models.\nData was generated using 3D finite-element simulations of the unsteady Navier-Stokes flow. This was done using the SimVascular software suite. The simulation data was then transformed into a 1D centerline representation by averaging the quantities of interest along orthogonal sections at each node.\nModel 1 shows aortofemoral model affected by an aneurysm. Model 2 shows a healthy pulmonary model. Model 3 shows an aorta model affected by coarctation.\nFigure 3. Cardiovascular models from the Vascular Model Repository that were considered as part of the dataset\nSimulations were done with different boundary conditions and for each geometry. Two cardiac cycles were simulated using 50 random configurations of boundary conditions to generate the training dataset. You can download the training dataset using the scripts from the NVIDIA Modulus GitHub repo.\nThe MeshGraphNet architecture was chosen to model the system and was modified to suit the cardiovascular simulations. Like the base architecture, the model consists of three components: an encoder, a processor, and a decoder (Figure 2). The encoder transforms the node and edge features into latent representations using a fully connected neural network (FCNN).\nThen the processor performs several rounds of message passing along edges, updating both node and edge embeddings. The decoder extracts nodal pressure and flow rate at each node, which is used to update the mesh in an auto-regressive manner. For the exact details of the Meshgraphnet architecture, you can refer to the Learning Mesh-Based Simulation with Graph Networks.\nThe team ran various experiments to analyze:\nThe convergence of the rollout error as a function of the dataset size.\nSensitivity analysis to evaluate which node and edge features were more important to the accuracy of predicting the flow rate and pressure\nDirect comparisons with physics-driven one-dimensional models \u200cshowed superior performance of the GNN, especially when handling complex geometries such as those with many junctions (Figure 4).\nDifferent approaches to train the algorithm: training networks specific to different cardiovascular regions instead of a single network able to handle different geometries (GNN-A vs GNN-Bg in Figure 4).\nThe figure plots the pressure and flow rate predicted by two of the GNN models, the 1D physics-based ROM, and how they compare to the ground truth for a complex model with stenosis.\nFigure 4. Comparison between the 1D physics-based ROM and two GNN models against the ground truth\nThe research team is continuing to explore further work related to the generalization of the GNN to more geometries as well as exploring the optimal feature set for improving the performance. Furthermore, they are looking to extend this work to 3D models but also to leverage such physics-ML models into their Simvascular software suite to accelerate simulations.", "Using NVIDIA Modulus for your research\nNVIDIA Modulus is an open-source project under the Apache 2.0 license to support the growing physics-ML community. If you are an AI researcher working in the field of physics-informed machine learning and want to get started with NVIDIA Modulus go to the Modulus GitHub repo.\nIf you would like to contribute your work to the project, follow the contribution guidelines in the project or reach out to the NVIDIA Modulus team.\nNVIDIA is celebrating developer contributions across use cases, demonstrating how to build and train physics-ML models using the NVIDIA Modulus framework. Equally important is the effort to systematically organize such innovative work in the Modulus open-source project for the community and the ecosystem to leverage for their engineering and science surrogate modeling problems.\nTo learn more about how Modulus is being used by the industry, you can refer to the Modulus resources page."], "document_title": "Enabling Greater Patient-Specific Cardiovascular Care with AI Surrogates", "document_url": "https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/", "document_date": "2023-11-10T00:16:46", "document_date_modified": "2023-11-16T19:16:41", "document_full_text": "Enabling Greater Patient-Specific Cardiovascular Care with AI Surrogates\nA Stanford University team is transforming heart healthcare with near real-time cardiovascular simulations driven by the power of AI. Harnessing physics-informed machine learning surrogate models, the researchers are generating accurate and patient-specific blood flow visualizations for a non-invasive window into cardiac studies. The technology has far-reaching scope, from evaluating coronary artery aneurysms to pioneering new surgical methods for congenital heart disease and enhancing medical device efficacy. With enormous potential in advancing cardiovascular medicine, the work could offer innovative methods for combating the leading cause of death in the US.\nCardiovascular simulations are important enablers for patient-specific treatment for several heart-related ailments. 3D computational fluid dynamics (CFD) simulations of the blood flow using finite-element methods are a computationally challenging task, especially in clinical practice. As an alternative, physics-based reduced-order models (ROMs) are often employed due to their increased efficiency.\nHowever, such ROMs rely on simplified assumptions of vessel geometry, complexity, or simplified mathematical models. They often fail to model the quantities of interest, such as pressure losses at vascular junctions accurately, which would otherwise require full 3D simulations. Conventional data-driven reduced-order approaches, such as projection-based methods, don\u2019t offer much flexibility with respect to changes to the domain geometry, which is crucial for patient-specific cardiovascular simulations.\nAs an alternative, employing deep learning-based surrogates can model these complex physical processes. Physics-informed machine learning (physics-ML) enables training deep learning models that can offer both computational efficiency as well as flexibility, through parameterizable models. Recently, graph neural network (GNN) based architectures have been proposed to build physics-ML models for emulating mesh-based simulations. This approach offers generalization over different meshes, boundary conditions, and other input parameters, which makes it a perfect candidate for patient-specific cardiovascular simulations.\nThe research team from Stanford University leveraged MeshGraphNet, a graph neural network (GNN)-based architecture, to devise a one-dimensional Reduced Order Model (1D ROM) for simulating blood flow. The team implemented this approach into NVIDIA Modulus, a platform equipped with an optimized implementation of MeshGraphNet. The reference MeshGraphnet implementation in NVIDIA Modulus brings several code optimizations such as data parallelism, model parallelism, gradient checkpointing, cuGraphs, and multi-GPU and multi-node training. All of which are useful for the development of GNNs for cardiovascular simulation.\nNVIDIA Modulus is an open-source framework geared towards the development of physics-informed machine learning models. It enhances the efficacy of high-fidelity, complex multi-physics simulations by enabling the exploration and development of surrogate models. This framework enables seamless integration of datasets with first principles, whether described by the governing partial differential equations or other system attributes, such as physical geometry and boundary conditions. Furthermore, it provides the capability to parameterize the input space, fostering the development of parameterized surrogate models critical for applications like digital twins.\nNVIDIA Modulus provides a variety of reference applications across domains from CFD, thermal, structural, and electromagnetic simulations, and can be used for numerous industry solutions ranging across climate modeling, manufacturing, automotive, healthcare, and more. These examples can serve as a foundation for the work of researchers, such as vortex shedding, in this case for the Stanford research team.\nModeling cardiovascular simulation as an AI problem\nIn this research study, the Stanford team\u2019s objective was to develop a one-dimensional physics-ML model. They chose GNNs for developing a surrogate that infers the pressure and flow rate along the centerline of compliant vessels. The team used \u200cgeometry from 3D vascular models and generated a directed graph consisting of a set of nodes along the centerline of the geometry.\nThe nodes and edges of the graph capture the state of the system at a given time. For instance, the cross-sectional average pressure, flow rate, and area of the vessel\u2019s lumen along the centerlines of the vessel are included as \u200cnode features. The GNN takes the state of the system at time t and infers the state of the system at the next time step. This can be iteratively applied to a rollout for simulating the cardiovascular cycle as shown in Figure 2.\nThe dataset preparation includes 3D simulation data from the Finite Element solver, averaged over the cross-section slice for 1D centerline representation, and the generation of a directed graph.\nFigure 1. The steps involved in modeling the simulation problem as a graph representation\nThe boundary conditions at the inlet (red) and outlet (yellow) are required for determining the hemodynamics in the vessels. These are modeled as special edges of the graph to account for the effect and the complexity of these boundary conditions. Boundary condition parameters, one-hot vector encoding for differentiating between nodes (branch, junction, inlet, outlet), and minimum diastolic and maximum systolic pressure in the cardiac cycle are also included as node features.\nFor the details on the modeling of the physics, the mapping of graph features, and boundary conditions, refer to the paper Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks.\nSchematics of MeshGraphNet, rollout phase to update the state of the system and the encoder, processor, and decoder of the MeshGraphnet architecture.\nFigure 2. Three steps of MeshGraphNet computation\u2014encode, process, and decode that are iteratively executed to the rollout to get simulation results\nAI surrogate: Dataset, architecture, and experiments\nThe researchers used models from the Vascular model repository, containing about 252 cardiovascular models. They picked eight models, capturing typically challenging scenarios. Figure 3 shows three of the eight models: an aortofemoral model featuring an aneurysm (Model 1), a healthy pulmonary model (Model 2), and an aorta model affected by coarctation (Model 3). These include features such as multiple junctions (Model 2) or stenoses (Model 3), which can be challenging to handle using 1D physics-based models.\nData was generated using 3D finite-element simulations of the unsteady Navier-Stokes flow. This was done using the SimVascular software suite. The simulation data was then transformed into a 1D centerline representation by averaging the quantities of interest along orthogonal sections at each node.\nModel 1 shows aortofemoral model affected by an aneurysm. Model 2 shows a healthy pulmonary model. Model 3 shows an aorta model affected by coarctation.\nFigure 3. Cardiovascular models from the Vascular Model Repository that were considered as part of the dataset\nSimulations were done with different boundary conditions and for each geometry. Two cardiac cycles were simulated using 50 random configurations of boundary conditions to generate the training dataset. You can download the training dataset using the scripts from the NVIDIA Modulus GitHub repo.\nThe MeshGraphNet architecture was chosen to model the system and was modified to suit the cardiovascular simulations. Like the base architecture, the model consists of three components: an encoder, a processor, and a decoder (Figure 2). The encoder transforms the node and edge features into latent representations using a fully connected neural network (FCNN).\nThen the processor performs several rounds of message passing along edges, updating both node and edge embeddings. The decoder extracts nodal pressure and flow rate at each node, which is used to update the mesh in an auto-regressive manner. For the exact details of the Meshgraphnet architecture, you can refer to the Learning Mesh-Based Simulation with Graph Networks.\nThe team ran various experiments to analyze:\nThe convergence of the rollout error as a function of the dataset size.\nSensitivity analysis to evaluate which node and edge features were more important to the accuracy of predicting the flow rate and pressure\nDirect comparisons with physics-driven one-dimensional models \u200cshowed superior performance of the GNN, especially when handling complex geometries such as those with many junctions (Figure 4).\nDifferent approaches to train the algorithm: training networks specific to different cardiovascular regions instead of a single network able to handle different geometries (GNN-A vs GNN-Bg in Figure 4).\nThe figure plots the pressure and flow rate predicted by two of the GNN models, the 1D physics-based ROM, and how they compare to the ground truth for a complex model with stenosis.\nFigure 4. Comparison between the 1D physics-based ROM and two GNN models against the ground truth\nThe research team is continuing to explore further work related to the generalization of the GNN to more geometries as well as exploring the optimal feature set for improving the performance. Furthermore, they are looking to extend this work to 3D models but also to leverage such physics-ML models into their Simvascular software suite to accelerate simulations.\nUsing NVIDIA Modulus for your research\nNVIDIA Modulus is an open-source project under the Apache 2.0 license to support the growing physics-ML community. If you are an AI researcher working in the field of physics-informed machine learning and want to get started with NVIDIA Modulus go to the Modulus GitHub repo.\nIf you would like to contribute your work to the project, follow the contribution guidelines in the project or reach out to the NVIDIA Modulus team.\nNVIDIA is celebrating developer contributions across use cases, demonstrating how to build and train physics-ML models using the NVIDIA Modulus framework. Equally important is the effort to systematically organize such innovative work in the Modulus open-source project for the community and the ecosystem to leverage for their engineering and science surrogate modeling problems.\nTo learn more about how Modulus is being used by the industry, you can refer to the Modulus resources page."}], "https://developer.nvidia.com/blog/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/": [{"text": "The article discusses the need for scalable and intelligent data processing systems to handle large amounts of data in fields such as life sciences and finance. Prometheux, an AI company, builds technology capable of explaining its logical processes, leveraging the RAPIDS accelerator for Apache Spark. Their system, Vadalog Parallel, combines data with domain logic for scalable and transparent reasoning over large knowledge graphs. Vadalog Parallel finds applications in computational biology, enabling indication expansion and drug repurposing, as well as in finance, aiding in banking supervision and fraud detection. The integration of RAPIDS with Vadalog Parallel provides significant speedups and cost savings when processing massive knowledge graphs. Experimental results show up to 9x speedups with Spark RAPIDS for reasoning tasks. The article also discusses the hardware and software configurations used for testing, showcasing the performance improvements achieved with NVIDIA GPUs. Overall, Vadalog Parallel offers a powerful framework for combining data with logic-based reasoning to advance towards neurosymbolic AI.", "text_components": ["Accelerating Neurosymbolic AI with RAPIDS and Prometheux Vadalog Parallel\nAs the scale of available data continues to grow, so does the need for scalable and intelligent data processing systems to swiftly harness useful knowledge. Especially in high-stakes domains such as life sciences and finance, alongside scalability, transparency of data-driven processes becomes paramount to ensure the utmost trustworthiness.\nStarted by scientists coming from the Knowledge Graph labs across the University of Oxford and TU Wien, Prometheux, an NVIDIA Inception company, builds AI capable of explaining its exact logical process. From drug repurposing for AstraZeneca to financial data processing with the Applied Research Team of the Central Bank of Italy, Prometheux technology powers highly scalable and explainable reasoning over some of the world\u2019s largest knowledge graphs.\nPrometheux has seamlessly integrated the RAPIDS accelerator for Apache Spark into their proprietary knowledge graph management system, Vadalog Parallel, to leverage NVIDIA GPUs. They achieve significant speedups and cost savings for their clients when processing large knowledge graphs with hundreds of millions of entities and billions of relations.", "Knowledge graphs and reasoning\nOver the last decades, due to the ever-growing scale of available data, there has been a rapid rise in the popularity of large enterprise knowledge graphs. And a corresponding rise in the scalable and intelligent processing systems with which to exploit them.\nKnowledge graphs serve as a backbone for data integration and provide a common representation structure to enable query answering across large data sources. A knowledge graph can be defined as a semi-structured data model composed of the following:\nAn extensional component: Existing entities and relations integrating knowledge from heterogeneous data sources.\nAn intensional component: Domain knowledge, either in the form of statistical and ML models or defined declaratively with logical rules.\nA derived extensional component: Produced through the application of domain knowledge to the extensional component in the so-called reasoning process.\nFrom intricate company ownership graphs to protein interaction networks, knowledge graphs provide a concise and intuitive abstraction for a variety of domains and can be used to model real-world entities and their interrelations.", "Vadalog Parallel\nResembling the interplay of intuition and logical thinking in human intelligence, AI is also moving towards neurosymbolic, a synergistic combination of ML and logic-based reasoning.\nBuilding towards neurosymbolic AI, Prometheux provides Vadalog Parallel, its knowledge graph management system (KGMS) that offers a deductive fully explainable framework. This KGMS combines data with domain logic and automates complex reasoning tasks over large knowledge graphs with high scalability and transparency at its core.\nVadalog Parallel acts as middleware between heterogeneous enterprise data sources and applications on top of them. It serves as a backbone for data integration without data migration. Encoding domain logic at a high level enables the rapid development of entirely new solutions without having to program extensive code or design algorithms.\nVadalog Parallel offers database-agnostic compatibility, seamlessly connecting to all major databases (RDBMS, RDF, and NoSQL, for example, Neo4j and Mongo, and so on), as well as diverse data sources (CSV, Parquet, JSON, and so on). It enables modeling and reasoning over graphs, hypergraphs, and simplicial complexes without any constraints on the arity, whether dealing with tuples, triples, or n-tuples.\nDomain logic is encoded declaratively at a high level, enabling domain scientists to directly extract insights from large knowledge graphs in an automated, explainable, and intuitive logical manner, saving time and computational resources.\nGraph shows a portion of the billions of logical steps that led to an indication expansion prediction.\nFigure 1. Portion of the billions of logical steps that led to an indication expansion prediction\nThe graph shows a human-understandable explanation of an indication expansion prediction. Other nodes and edges represent the logical steps that brought Vadalog Parallel to the prediction, for example, the potential treatment was deduced using compound similarity, disease similarities and known treatments. Compound similarity, in turn, was deduced using paths and genes.\nFigure 2. Compact logical explanation of an indication expansion prediction\nFor each task, Vadalog Parallel natively provides the step-by-step, logical full explanation of the reasoning process. Up to billions of logical steps are computed at lightning speed, accompanied by a compact visual explanation to enable faster interactions with domain experts.\nThanks to its expressive framework and its distributed processing, Vadalog Parallel achieves low computational complexity and scalability in practice and enables modeling complex domains with advanced data analysis features.\nIndeed, it enables efficient graph traversal and captures both regular path queries (for navigating graphs using pattern matching, for example, Cypher), and SPARQL under the OWL2 QL regime (for querying the semantic web). It supports counterfactual and temporal reasoning, full recursion, and existential quantification, unlike other state-of-the-art systems, and outperforms existing big data analytics tools.\nWith the unprecedented scale of datasets that we encounter today, new requirements on the scalability and flexibility of reasoning engines have emerged so that AI techniques can execute sufficiently fast. To guarantee such desiderata, Prometheux precisely studies and develops compilation techniques that shift classical reasoning, inference methodologies, and tools onto Big Data platforms.\nVadalog Parallel already achieves unmatched scalability even when operating solely on CPUs. However, when reasoning over some of the world\u2019s largest knowledge graphs (with hundreds of millions of entities and billions of relations), GPUs become an essential asset.", "Explainable drug repurposing and beyond\nVadalog Parallel finds diverse applications in the field of computational biology, including its proven tangible impact on indication expansion.\nPrometheux enables life sciences organizations to scalably reason over large biological (and other kinds of) knowledge graphs in an automated and logically explainable manner. This enables more informed decisions and faster interactions with domain experts, resulting in accelerated pipeline development globally towards trustworthy precision medicine.\nThrough the automated analysis of both proprietary and client datasets, Prometheux unravels hidden insights through logical reasoning and unlocks the potential of existing drugs for new therapeutic purposes. By computing billions of logical steps at lightning speed, Vadalog Parallel produces hundreds of successfully validated indications (and many more to study), effectively providing a dynamic and explainable recommendation system for indication expansion.\nThe compact visual explanation of the recommendations enables faster interactions with domain experts. It leads to rapid adaptation to feedback bringing everyone from initial knowledge to novel indications in a fraction of the time of traditional techniques.\nIn the following sections, we also provide an experimental analysis of Vadalog Parallel with NVIDIA GPUs for a series of reasoning tasks. We use Prometheux\u2019s internal pilot biological knowledge graph (BIO KG), a starting point for life sciences organizations that have yet to build their own knowledge graphs.\nBIO KG has ~4.7M data points spanning compounds, diseases, genes, biological pathways, symptoms, and more. Vadalog Parallel achieves up to 9x speedups with Spark RAPIDS.", "Financial institutions\nAnother compelling application of Vadalog Parallel unfolds within the dynamic realms of finance and economics. Prometheux\u2019s knowledge graph-aided approach empowers an enhanced understanding of intricate interconnections between financial entities, whether they are institutions, companies, financial intermediaries, other types of shareholders, or transactions.\nIt can be employed for FinTech, RegTech, SupTech, and InsurTech applications encoding international regulations, as well as other domain logic. The goal is to automatically reason over knowledge graphs and achieve AI-aided banking supervision, compliance checks, creditworthiness evaluation, anti-money laundering, fraud detection, shock propagation, company control, detection of takeovers, and more.\nThis comprehensive approach empowers analysts to proactively manage risks, respond to challenges, optimize strategies, and foster financial stability.\nAlso for this domain, we show Vadalog Parallel performances with NVIDIA GPUs when reasoning over a company ownership graph (Company KG).\nCompany KG is a synthetically constructed knowledge graph reflecting the well-known topology of Italian companies, counting 8M ownership edges between companies and shareholders. On such graphs, Vadalog Parallel achieves up to 3x speedups with Spark RAPIDS.", "Strategy and solution design\nThe Vadalog Parallel architecture has the following key components to execute reasoning tasks efficiently on top of distributed frameworks such as Spark, Flink, GraphX, and more:\nRule parsing\nLogic optimization\nQuery planning\nPlanner optimization\nQuery compilation\nOverall, Vadalog Parallel exposes a reasoning API with the following interface:\n```reason(kg_ref,domain_logic)```\nA client application issues calls to the reasoning API specifying a reference to a knowledge graph ```kg_ref``` in which it activates the reasoning process. Vadalog Parallel connects to and handles repositories of knowledge graphs, each with unique identifiers ```kg_ref```. The reasoning engine encodes ```domain_logic``` into a set of distributed operations (narrow, wide transformations, that is, shuffling), and computes the answer to the reasoning task. It either expands the knowledge graph with new knowledge or materializes the output in the specified output data sources.\nTo ensure even faster and reliable processing, Prometheux has seamlessly integrated Spark-RAPIDS into Vadalog Parallel going beyond traditional Spark capabilities.\nThe RAPIDS Shuffle Manager, an integral part of Spark-rapids, provides a significant advantage by introducing custom mechanisms for exchanging shuffle data. This innovation offers two distinct modes of operation: Multi-Threaded and UCX, which can be configured to leverage GPU-to-GPU communication and RDMA capabilities. It unlocks unmatched levels of performance and efficiency for the reasoning tasks with Vadalog Parallel.", "Scenarios and data\nNavigating graphs can be very challenging without powerful recursion. Standard SQL, for instance, lacks native support for recursion.\nThe Vadalog Parallel framework enables full recursion. In this post, we show how both graph traversal and graph analytics tasks are significantly sped up by integrating GPUs through Spark-RAPIDS.\nWe categorized the tasks into the following distinct types:\nNon-recursive: The execution plan for the set of operations is structured as a tree and each distributed operation is executed one time.\nRecursive: The execution plan for the set of operations is structured as a graph and the set of distributed operations is applied until the knowledge graph has been exhaustively explored.\nIn doing so, we provide the first assessment of NVIDIA Spark-RAPIDS with recursive operations.", "Experimental results\nFigure 3 shows the four knowledge graph analytics tasks containing recursive and non-recursive operations, two over the Bio KG and two over the Company KG. For all measurements, we ran each single experiment 10 times and averaged the results.\nFour charts compare CPU and GPU performance on knowledge graphs, with GPUs achieving between 2.6-9x speedups.\nFigure 3. Experimental analysis of Vadalog Parallel reasoning with GPUs using Spark-RAPIDS\nFigure 3 shows four reasoning tasks over the knowledge graphs introduced earlier. We provide the relevant experimental evaluation with particular attention to the speedups achieved with NVIDIA GPUs and Spark RAPIDS.", "Hardware and software configuration\nIn all the experimental analyses, Vadalog Parallel was executed on top of a Spark 3.3.2 standalone cluster integrated with Spark-RAPIDS v23.08.1, with CUDA v12.0 and Java v8 as Spark language. The cluster was locally installed on an Amazon EC2 AMI p3.16xlarge with 64 vCPU 8 GPU, 480 GB RAM, and eight NVIDIA V100 Tensor Cores, each having 16 GB of GPU core memory.", "Test setup\nAll the tasks were required to perform the following steps:\nCall the Vadalog Parallel reasoning API passing in input the domain logic describing the task and the reference to one of the two knowledge graphs.\nEstablish the connection with the Spark cluster.\nExtract the input subgraph from the specific knowledge graph.\nExecute the task.\nWrite the output in the Parquet file.\nKG\nEdges\nTask\nDescription\nOperations\nReasoning times (sec)\nBIO KG\n~4.7M\nCompound/Gene/Disease Similarity\nDetermine the pairwise similarity for genes, compounds, and diseases based on common features\nNon-recursive Wide Transformations: 3 Joins, 3 Aggregations;\nNon-recursive Narrow Transformations: 3 Maps, 3 Filters;\nCPU: 386\nGPU: 43\nBIO KG\n~4.7M\nGuilt by Association\nRecommend new indications for each compound based on similar ones treating a set of similar diseases\nNon-recursive Wide Transformations: 11 Joins, 11 Aggregations;\nNon-recursive Narrow Transformations: 52 Maps, 26 Filters;\nCPU: 520\nGPU: 220\nCompany KG\n~8M\nAll Company-To-Company Links\nDetermine the pairwise connectivity between all nodes\nNon-recursive Wide Transformations: 1 Aggregation;\nNon-recursive Narrow Transformations: 2 Maps, 1 Filters;\nRecursive Wide Transformations: 1 Join, 2 Aggregations;\nRecursive Narrow Transformations: 4 Maps, 4 Filters\nCPU: 141\nGPU: 46\nCompany KG\n~8M\nCompany Control\nFind all pairs of controllers for each company\nNon-recursive Wide Transformations: 2 Aggregations;\nNon-recursive Narrow Transformations: 2 Maps;\nRecursive Wide Transformations: 1 Join, 1 Aggregation;\nRecursive Narrow Transformations: 1 Map\nCPU: 94\nGPU: 36\nTable 1. Comparisons of CPU and GPU performance on knowledge graph-based predictions", "Conclusion\nIn this post, we discussed the rapid rise in the popularity of reasoning with logic over large enterprise knowledge graphs as well as the scalable and intelligent processing systems to exploit them. We showcased Vadalog Parallel, Prometheux\u2019s knowledge graph management system. Vadalog Parallel is a powerful framework to combine data with domain logic and automate complex reasoning tasks. It\u2019s a solution advancing us toward neurosymbolic AI, a synergistic combination of ML and logic-based reasoning.\nWe also discussed the application of Vadalog Parallel in the financial and life sciences realm. The integration of RAPIDS enables significant speedups and cost savings leveraging NVIDIA GPUs when processing some of the world\u2019s largest knowledge graphs.\nFor more information about Prometheux, contact them via email. For Spark 3.0 and RAPIDS, see the RAPIDS developer forum."], "document_title": "Accelerating Neurosymbolic AI with RAPIDS and Prometheux Vadalog Parallel", "document_url": "https://developer.nvidia.com/blog/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/", "document_date": "2023-11-09T20:17:05", "document_date_modified": "2023-12-05T18:56:29", "document_full_text": "Accelerating Neurosymbolic AI with RAPIDS and Prometheux Vadalog Parallel\nAs the scale of available data continues to grow, so does the need for scalable and intelligent data processing systems to swiftly harness useful knowledge. Especially in high-stakes domains such as life sciences and finance, alongside scalability, transparency of data-driven processes becomes paramount to ensure the utmost trustworthiness.\nStarted by scientists coming from the Knowledge Graph labs across the University of Oxford and TU Wien, Prometheux, an NVIDIA Inception company, builds AI capable of explaining its exact logical process. From drug repurposing for AstraZeneca to financial data processing with the Applied Research Team of the Central Bank of Italy, Prometheux technology powers highly scalable and explainable reasoning over some of the world\u2019s largest knowledge graphs.\nPrometheux has seamlessly integrated the RAPIDS accelerator for Apache Spark into their proprietary knowledge graph management system, Vadalog Parallel, to leverage NVIDIA GPUs. They achieve significant speedups and cost savings for their clients when processing large knowledge graphs with hundreds of millions of entities and billions of relations.\nKnowledge graphs and reasoning\nOver the last decades, due to the ever-growing scale of available data, there has been a rapid rise in the popularity of large enterprise knowledge graphs. And a corresponding rise in the scalable and intelligent processing systems with which to exploit them.\nKnowledge graphs serve as a backbone for data integration and provide a common representation structure to enable query answering across large data sources. A knowledge graph can be defined as a semi-structured data model composed of the following:\nAn extensional component: Existing entities and relations integrating knowledge from heterogeneous data sources.\nAn intensional component: Domain knowledge, either in the form of statistical and ML models or defined declaratively with logical rules.\nA derived extensional component: Produced through the application of domain knowledge to the extensional component in the so-called reasoning process.\nFrom intricate company ownership graphs to protein interaction networks, knowledge graphs provide a concise and intuitive abstraction for a variety of domains and can be used to model real-world entities and their interrelations.\nVadalog Parallel\nResembling the interplay of intuition and logical thinking in human intelligence, AI is also moving towards neurosymbolic, a synergistic combination of ML and logic-based reasoning.\nBuilding towards neurosymbolic AI, Prometheux provides Vadalog Parallel, its knowledge graph management system (KGMS) that offers a deductive fully explainable framework. This KGMS combines data with domain logic and automates complex reasoning tasks over large knowledge graphs with high scalability and transparency at its core.\nVadalog Parallel acts as middleware between heterogeneous enterprise data sources and applications on top of them. It serves as a backbone for data integration without data migration. Encoding domain logic at a high level enables the rapid development of entirely new solutions without having to program extensive code or design algorithms.\nVadalog Parallel offers database-agnostic compatibility, seamlessly connecting to all major databases (RDBMS, RDF, and NoSQL, for example, Neo4j and Mongo, and so on), as well as diverse data sources (CSV, Parquet, JSON, and so on). It enables modeling and reasoning over graphs, hypergraphs, and simplicial complexes without any constraints on the arity, whether dealing with tuples, triples, or n-tuples.\nDomain logic is encoded declaratively at a high level, enabling domain scientists to directly extract insights from large knowledge graphs in an automated, explainable, and intuitive logical manner, saving time and computational resources.\nGraph shows a portion of the billions of logical steps that led to an indication expansion prediction.\nFigure 1. Portion of the billions of logical steps that led to an indication expansion prediction\nThe graph shows a human-understandable explanation of an indication expansion prediction. Other nodes and edges represent the logical steps that brought Vadalog Parallel to the prediction, for example, the potential treatment was deduced using compound similarity, disease similarities and known treatments. Compound similarity, in turn, was deduced using paths and genes.\nFigure 2. Compact logical explanation of an indication expansion prediction\nFor each task, Vadalog Parallel natively provides the step-by-step, logical full explanation of the reasoning process. Up to billions of logical steps are computed at lightning speed, accompanied by a compact visual explanation to enable faster interactions with domain experts.\nThanks to its expressive framework and its distributed processing, Vadalog Parallel achieves low computational complexity and scalability in practice and enables modeling complex domains with advanced data analysis features.\nIndeed, it enables efficient graph traversal and captures both regular path queries (for navigating graphs using pattern matching, for example, Cypher), and SPARQL under the OWL2 QL regime (for querying the semantic web). It supports counterfactual and temporal reasoning, full recursion, and existential quantification, unlike other state-of-the-art systems, and outperforms existing big data analytics tools.\nWith the unprecedented scale of datasets that we encounter today, new requirements on the scalability and flexibility of reasoning engines have emerged so that AI techniques can execute sufficiently fast. To guarantee such desiderata, Prometheux precisely studies and develops compilation techniques that shift classical reasoning, inference methodologies, and tools onto Big Data platforms.\nVadalog Parallel already achieves unmatched scalability even when operating solely on CPUs. However, when reasoning over some of the world\u2019s largest knowledge graphs (with hundreds of millions of entities and billions of relations), GPUs become an essential asset.\nExplainable drug repurposing and beyond\nVadalog Parallel finds diverse applications in the field of computational biology, including its proven tangible impact on indication expansion.\nPrometheux enables life sciences organizations to scalably reason over large biological (and other kinds of) knowledge graphs in an automated and logically explainable manner. This enables more informed decisions and faster interactions with domain experts, resulting in accelerated pipeline development globally towards trustworthy precision medicine.\nThrough the automated analysis of both proprietary and client datasets, Prometheux unravels hidden insights through logical reasoning and unlocks the potential of existing drugs for new therapeutic purposes. By computing billions of logical steps at lightning speed, Vadalog Parallel produces hundreds of successfully validated indications (and many more to study), effectively providing a dynamic and explainable recommendation system for indication expansion.\nThe compact visual explanation of the recommendations enables faster interactions with domain experts. It leads to rapid adaptation to feedback bringing everyone from initial knowledge to novel indications in a fraction of the time of traditional techniques.\nIn the following sections, we also provide an experimental analysis of Vadalog Parallel with NVIDIA GPUs for a series of reasoning tasks. We use Prometheux\u2019s internal pilot biological knowledge graph (BIO KG), a starting point for life sciences organizations that have yet to build their own knowledge graphs.\nBIO KG has ~4.7M data points spanning compounds, diseases, genes, biological pathways, symptoms, and more. Vadalog Parallel achieves up to 9x speedups with Spark RAPIDS.\nFinancial institutions\nAnother compelling application of Vadalog Parallel unfolds within the dynamic realms of finance and economics. Prometheux\u2019s knowledge graph-aided approach empowers an enhanced understanding of intricate interconnections between financial entities, whether they are institutions, companies, financial intermediaries, other types of shareholders, or transactions.\nIt can be employed for FinTech, RegTech, SupTech, and InsurTech applications encoding international regulations, as well as other domain logic. The goal is to automatically reason over knowledge graphs and achieve AI-aided banking supervision, compliance checks, creditworthiness evaluation, anti-money laundering, fraud detection, shock propagation, company control, detection of takeovers, and more.\nThis comprehensive approach empowers analysts to proactively manage risks, respond to challenges, optimize strategies, and foster financial stability.\nAlso for this domain, we show Vadalog Parallel performances with NVIDIA GPUs when reasoning over a company ownership graph (Company KG).\nCompany KG is a synthetically constructed knowledge graph reflecting the well-known topology of Italian companies, counting 8M ownership edges between companies and shareholders. On such graphs, Vadalog Parallel achieves up to 3x speedups with Spark RAPIDS.\nStrategy and solution design\nThe Vadalog Parallel architecture has the following key components to execute reasoning tasks efficiently on top of distributed frameworks such as Spark, Flink, GraphX, and more:\nRule parsing\nLogic optimization\nQuery planning\nPlanner optimization\nQuery compilation\nOverall, Vadalog Parallel exposes a reasoning API with the following interface:\n```reason(kg_ref,domain_logic)```\nA client application issues calls to the reasoning API specifying a reference to a knowledge graph ```kg_ref``` in which it activates the reasoning process. Vadalog Parallel connects to and handles repositories of knowledge graphs, each with unique identifiers ```kg_ref```. The reasoning engine encodes ```domain_logic``` into a set of distributed operations (narrow, wide transformations, that is, shuffling), and computes the answer to the reasoning task. It either expands the knowledge graph with new knowledge or materializes the output in the specified output data sources.\nTo ensure even faster and reliable processing, Prometheux has seamlessly integrated Spark-RAPIDS into Vadalog Parallel going beyond traditional Spark capabilities.\nThe RAPIDS Shuffle Manager, an integral part of Spark-rapids, provides a significant advantage by introducing custom mechanisms for exchanging shuffle data. This innovation offers two distinct modes of operation: Multi-Threaded and UCX, which can be configured to leverage GPU-to-GPU communication and RDMA capabilities. It unlocks unmatched levels of performance and efficiency for the reasoning tasks with Vadalog Parallel.\nScenarios and data\nNavigating graphs can be very challenging without powerful recursion. Standard SQL, for instance, lacks native support for recursion.\nThe Vadalog Parallel framework enables full recursion. In this post, we show how both graph traversal and graph analytics tasks are significantly sped up by integrating GPUs through Spark-RAPIDS.\nWe categorized the tasks into the following distinct types:\nNon-recursive: The execution plan for the set of operations is structured as a tree and each distributed operation is executed one time.\nRecursive: The execution plan for the set of operations is structured as a graph and the set of distributed operations is applied until the knowledge graph has been exhaustively explored.\nIn doing so, we provide the first assessment of NVIDIA Spark-RAPIDS with recursive operations.\nExperimental results\nFigure 3 shows the four knowledge graph analytics tasks containing recursive and non-recursive operations, two over the Bio KG and two over the Company KG. For all measurements, we ran each single experiment 10 times and averaged the results.\nFour charts compare CPU and GPU performance on knowledge graphs, with GPUs achieving between 2.6-9x speedups.\nFigure 3. Experimental analysis of Vadalog Parallel reasoning with GPUs using Spark-RAPIDS\nFigure 3 shows four reasoning tasks over the knowledge graphs introduced earlier. We provide the relevant experimental evaluation with particular attention to the speedups achieved with NVIDIA GPUs and Spark RAPIDS.\nHardware and software configuration\nIn all the experimental analyses, Vadalog Parallel was executed on top of a Spark 3.3.2 standalone cluster integrated with Spark-RAPIDS v23.08.1, with CUDA v12.0 and Java v8 as Spark language. The cluster was locally installed on an Amazon EC2 AMI p3.16xlarge with 64 vCPU 8 GPU, 480 GB RAM, and eight NVIDIA V100 Tensor Cores, each having 16 GB of GPU core memory.\nTest setup\nAll the tasks were required to perform the following steps:\nCall the Vadalog Parallel reasoning API passing in input the domain logic describing the task and the reference to one of the two knowledge graphs.\nEstablish the connection with the Spark cluster.\nExtract the input subgraph from the specific knowledge graph.\nExecute the task.\nWrite the output in the Parquet file.\nKG\nEdges\nTask\nDescription\nOperations\nReasoning times (sec)\nBIO KG\n~4.7M\nCompound/Gene/Disease Similarity\nDetermine the pairwise similarity for genes, compounds, and diseases based on common features\nNon-recursive Wide Transformations: 3 Joins, 3 Aggregations;\nNon-recursive Narrow Transformations: 3 Maps, 3 Filters;\nCPU: 386\nGPU: 43\nBIO KG\n~4.7M\nGuilt by Association\nRecommend new indications for each compound based on similar ones treating a set of similar diseases\nNon-recursive Wide Transformations: 11 Joins, 11 Aggregations;\nNon-recursive Narrow Transformations: 52 Maps, 26 Filters;\nCPU: 520\nGPU: 220\nCompany KG\n~8M\nAll Company-To-Company Links\nDetermine the pairwise connectivity between all nodes\nNon-recursive Wide Transformations: 1 Aggregation;\nNon-recursive Narrow Transformations: 2 Maps, 1 Filters;\nRecursive Wide Transformations: 1 Join, 2 Aggregations;\nRecursive Narrow Transformations: 4 Maps, 4 Filters\nCPU: 141\nGPU: 46\nCompany KG\n~8M\nCompany Control\nFind all pairs of controllers for each company\nNon-recursive Wide Transformations: 2 Aggregations;\nNon-recursive Narrow Transformations: 2 Maps;\nRecursive Wide Transformations: 1 Join, 1 Aggregation;\nRecursive Narrow Transformations: 1 Map\nCPU: 94\nGPU: 36\nTable 1. Comparisons of CPU and GPU performance on knowledge graph-based predictions\nConclusion\nIn this post, we discussed the rapid rise in the popularity of reasoning with logic over large enterprise knowledge graphs as well as the scalable and intelligent processing systems to exploit them. We showcased Vadalog Parallel, Prometheux\u2019s knowledge graph management system. Vadalog Parallel is a powerful framework to combine data with domain logic and automate complex reasoning tasks. It\u2019s a solution advancing us toward neurosymbolic AI, a synergistic combination of ML and logic-based reasoning.\nWe also discussed the application of Vadalog Parallel in the financial and life sciences realm. The integration of RAPIDS enables significant speedups and cost savings leveraging NVIDIA GPUs when processing some of the world\u2019s largest knowledge graphs.\nFor more information about Prometheux, contact them via email. For Spark 3.0 and RAPIDS, see the RAPIDS developer forum."}], "https://developer.nvidia.com/blog/whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx/": [{"text": "The IIT Madras Brain Centre is embarking on an ambitious project to map over 100 human brains at a cellular level using whole human brain imaging. This requires advanced computational tools and supercomputing capabilities, which are provided by NVIDIA technologies. The center is generating massive amounts of high-resolution images that need to be processed and analyzed to map every imaged cell. NVIDIA's GPU-based HPC offerings, particularly the DGX systems, are being used to handle the immense computational demands of this project. The NVIDIA technology stack includes tools and optimized libraries for every step of the process, making it easier to adopt and ensuring best practices and automated operation. By leveraging NVIDIA technologies, the IIT Madras Brain Centre aims to transform the landscape of deep learning in medical imaging and make significant breakthroughs in neuroscience research on a global scale. This project has the potential to revolutionize brain science, guide brain surgery and therapy, and enhance our understanding of the complexities of the human brain.", "text_components": ["Whole Human Brain Neuro-Mapping at Cellular Resolution on NVIDIA DGX\nWhole human brain imaging of 100 brains at a cellular level within a 2-year timespan, and subsequent analysis and mapping, requires accelerated supercomputing and computational tools. This need is well matched by NVIDIA technologies, which range across hardware, computational systems, high-bandwidth interconnects, domain-specific libraries, accelerated toolboxes, curated deep-learning models, and container runtimes. NVIDIA accelerated computing spans the IIT Madras Brain Centre\u2019s technology journey from solution building, rollout, optimization, management, and scaling.\nVideo 1. The Sudha Gopalakrishnan Brain Center Cellular resolution imaging is readily possible today for smaller brains of insects like flies and mammals like mice and small monkeys. However, the process of acquiring, converting, processing, analyzing, and interpreting, turns into an even more arduous, skill\u2013 and time-intensive activity for whole human brains, which are orders of magnitude larger and more complex.\nThe key big data characteristics of IITM Brain Centre\u2019s imaging pipeline are volume and velocity. At a scanning rate of 250 GB per hour per scanner, operating multiple scanners simultaneously, the center generates 2 TB per hour of high-resolution uncompressed images. All the images must be processed to map every imaged cell. For a computer vision object detection model, the equivalent incidence rate is approximately 10K objects per second.\nHandling such large-scale primary neuroanatomy data needs mathematical and computational methods to reveal the complex biological principles governing brain structure, organization, and developmental evolution, at multiple spatio-temporal scales.\nThis important and challenging scientific pursuit involves the analysis of carefully acquired cellular-resolution brain images for establishing quantitative descriptions of the following:\nSpatial layout\nCellular composition\nNeuronal pathways\nCompartmental organization\nWhole-brain architecture\nIt extends to studying inter-brain similarities and relationships at all these levels.\nThe new Brain Centre of IIT Madras has taken up this challenge and is powering a large-scale, multi-disciplinary effort to map more than 100 human brains at a cellular level. Using their proprietary technology platform, the center is imaging post-mortem human brains of different types and ages.\nTheir goal is to create an unprecedented cell resolution, uniformly studied, digital collection of multiple types of human brains, which will be queryable from cell level to whole brain level. This requires the capability to enumerate 100B neurons per brain, over 100 brains, and the connectivity across different brain regions.", "Meeting the computational demands of cellular resolution brain imaging\nThe center has developed a world-class computing platform to store, address, access, process, and visualize such high-resolution digital human brain data at the scale of 100+ petabytes, through nothing more than a web browser interface.\nThis endeavor can be related to the mapping of multiple whole planets to uniformly study patterns, trends, and differences through high-resolution, cross-sectional imaging data through the planet\u2019s volume. Satellite images of the earth\u2019s surface would reach terabytes, a manageable size in today\u2019s computers and web browsers. These geospatial rendering techniques power the likes of Google Maps and others.\nHowever, volumetric imaging of whole brains at cellular resolution yields petabytes of digital data per brain, a challenge to visualize, process, analyze, and query over a web interface.\nThe computational challenges behind the scenes are equally tremendous:\nA human-in-the-loop large image data pipeline\nAutomation right from indexing the data from multiple concurrent imaging systems\nTransporting the images to a central uniform parallel file storage cluster\nEncoding in a format convenient for random access at an arbitrary scale\nMachine learning models for multiple automated tasks like the detection of tissue outlines\nQuality control for imaging\nDeep learning tasks for large image normalization\nCellular-level object detection, classification, and region delineation\nAdvanced mathematical models for the geometric alignment of images across modalities and resolutions, followed by computational geometry for deriving quantitative informatics\nFormatting into a high-volume rapid retriable metadata and informatics data store that can transform the prohibitive to the possible: performing single-cell to whole-brain queries on demand and reverting with timely answers for a web-based interaction\nTo contemplate the scale of the challenge, look at just one of these tasks, a seemingly well-posed task of object detection. This is a task for which modern deep learning convolutional neural networks are known to perform quite well, achieving or in some cases exceeding human-level abilities in identifying and labeling objects.\nHowever, these models are trained to work on megapixel images, containing a few tens of objects. A single-cell resolution image is multi-gigapixel and can contain millions of distinct objects. Processing such large data requires specialized computational workhorses.\nThis is why the Brain Centre has turned to NVIDIA, a leader in GPU-based HPC offerings, and has operationalized a cluster of DGX A100 systems to do the complete processing of 10 to 20 brains. As the center scales to 100 brains and more, they look to a DGX SuperPOD to provide scalable performance, with industry-leading computing, storage, networking, and infrastructure management.\nWith eight NVIDIA A100 Tensor Core GPUs per DGX node, the same data that requires a minimum of 1 hour to detect cells has been reduced to less than 10 minutes on an NVIDIA DGX. This enables whole-brain analysis in a month\u2019s time frame, and scaling to 100 brains becomes practical.\n\u201cI am delighted to see IITM Brain Centre collaborating with NVIDIA in tackling this challenge of analyzing the very large and complex cellular-level human brain data we are generating,\u201d said Kris Gopalakrishan, a distinguished alumnus of IIT Madras and co-founder of Infosys. He played a pivotal role in setting up and supporting the IITM Brain Centre. \u201cBy working with an industry leader like NVIDIA, we look forward to creating breakthroughs in this area leading to global impact.\u201d", "Solving the computation challenges\nA gigapixel whole slide image making up 0.25 million tiles of 256 X 256 images takes just 420 seconds for inferencing on an A100 GPU. It was possible through end-to-end pipeline optimization using NVIDIA libraries and application frameworks:\nAccelerated tile creation and batching are performed using NVIDIA DALI.\nThe MONAI Core model is optimized by TensorRT.\nThe TensorRT plan file selects a mix of INT8, FP16, and TF32 at different parts of the network, and produces the engine.\nThree engines are put in one A100 GPU for distributed inferencing.\nThe NVIDIA accelerated image processing library cuCIM is used for accelerated image registration.\nNVIDIA IndeX is used for multi-GPU volume visualization for various zoom levels. Soon, MONAI Label\u2019s AI-assisted annotation, along with NVIDIA federated learning SDK-Flare, will be used to further refine various other MONAI core models and the pipeline will be deployed using MONAI Deploy.\n\u201cThe NVIDIA technology stack empowers the pioneers at the IIT Madras Brain Centre to effectively address the computational needs for high-resolution brain imaging at the cellular level, thereby propelling forward neuroscience research on a national and global scale,\u201d said Vishal Dhupar, managing director of South Asia at NVIDIA.\nMONAI and TensorRT are available with NVIDIA AI Enterprise, which is included with the NVIDIA DGX infrastructure.\nDiagram shows different stages of the NVIDIA accelerated computing workflow for medical imaging, from digital pathology scanner to MONAI Deploy.\nFigure 1. NVIDIA accelerated computing workflow for medical imaging\nThe compute capability of NVIDIA DGX systems, with dual 64-core CPUs and eight NVIDIA A100 GPUs with 640 GB of GPU memory, along with 2 TB of RAM and 30 TB of flash storage, represent the highest class of server compute available in a single 4U chassis.\nFurther, the DGX is scalable. NVIDIA offers an ecosystem of software and networking to interconnect multiple DGX systems and match the scale and performance requirements of IITM\u2019s Brain Centre for data processing of pipelined batch jobs, as well as on-demand burst computations.\nThe effective processing rate of a single NVIDIA A100 GPU for CNN inference as benchmarked on the Brain Centre data is 60 GB per hour (data in uint8, inference in FP16 precision), or 2.4 TB per hour over five DGX servers (40 A100 GPUs), which matches the current imaging rate. This makes pipelining of imaging and computation bottleneck-free. Owing to the scalability of the DGX compute nodes, any surge in data inflow rate can also be matched through scale-out growth.\nThe A100 GPU is mostly targeted for deep learning training for large datasets and large models that might not fit in smaller GPU vRAM. In the context of the IITM Brain Centre, the A100 GPUs within the DGX systems are used in CNN inference, in a multi-engine per A100 GPU fashion, with the data being mapped out across multi-GPU and multi-node, for scaling from 1\u201340x across five DGX servers.\nThis enables the handling of variable image sizes, corresponding to the physical size variability in human brains across ages from fetal to adult (1\u201332x scale variation). Also, the CPU compute capabilities and the storage type of the DGX A100 systems are well-used in the Brain Centre\u2019s compute pipeline for workloads that are CPU-intensive or data access or movement-intensive, as well as for remote visualization.\nThe NVIDIA technology stack offers tools and optimized libraries for every step in the process, in a uniform format as container runtimes, facilitating the adoption with minimal effort and ensuring best practices and automated operation.", "Transforming the landscape of deep learning in medical imaging\nDeep learning technology used to focus on engineering the best methods or tuning at training time for incremental boosts in performance. It has now shifted to inference with proven foundational models across domains like computer vision (object detection, semantic and panoptic segmentation, DL-based image registration) and natural language. The results are realizing applications previously held as non-amenable for computational automation.\nThe emphasis now is on implementing software guardrails around new applications powered by deep learning inference. Integrated hardware systems and software stacks are not just a convenience in this new direction but are a vehicle of scale and simplification. The NVIDIA technology stack is a way to leapfrog solution building, deployment, and scaling.\nA detailed map of the earth is today accessible for everyone and has emerged as a platform enabling new applications, and businesses. It now guides and shapes globally how we move about. The efforts of the IIT Madras Brain Centre are targeted to produce a similar, transformative platform that will yield new outcomes in brain science, shape and guide brain surgery and therapy, and expand our understanding of medicine\u2019s last frontier, the human brain.\nFor more information, see the /HTIC-Medical-Imaging GitHub repo."], "document_title": "Whole Human Brain Neuro-Mapping at Cellular Resolution on NVIDIA DGX", "document_url": "https://developer.nvidia.com/blog/whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx/", "document_date": "2023-11-08T17:55:53", "document_date_modified": "2023-11-16T19:16:42", "document_full_text": "Whole Human Brain Neuro-Mapping at Cellular Resolution on NVIDIA DGX\nWhole human brain imaging of 100 brains at a cellular level within a 2-year timespan, and subsequent analysis and mapping, requires accelerated supercomputing and computational tools. This need is well matched by NVIDIA technologies, which range across hardware, computational systems, high-bandwidth interconnects, domain-specific libraries, accelerated toolboxes, curated deep-learning models, and container runtimes. NVIDIA accelerated computing spans the IIT Madras Brain Centre\u2019s technology journey from solution building, rollout, optimization, management, and scaling.\nVideo 1. The Sudha Gopalakrishnan Brain Center Cellular resolution imaging is readily possible today for smaller brains of insects like flies and mammals like mice and small monkeys. However, the process of acquiring, converting, processing, analyzing, and interpreting, turns into an even more arduous, skill\u2013 and time-intensive activity for whole human brains, which are orders of magnitude larger and more complex.\nThe key big data characteristics of IITM Brain Centre\u2019s imaging pipeline are volume and velocity. At a scanning rate of 250 GB per hour per scanner, operating multiple scanners simultaneously, the center generates 2 TB per hour of high-resolution uncompressed images. All the images must be processed to map every imaged cell. For a computer vision object detection model, the equivalent incidence rate is approximately 10K objects per second.\nHandling such large-scale primary neuroanatomy data needs mathematical and computational methods to reveal the complex biological principles governing brain structure, organization, and developmental evolution, at multiple spatio-temporal scales.\nThis important and challenging scientific pursuit involves the analysis of carefully acquired cellular-resolution brain images for establishing quantitative descriptions of the following:\nSpatial layout\nCellular composition\nNeuronal pathways\nCompartmental organization\nWhole-brain architecture\nIt extends to studying inter-brain similarities and relationships at all these levels.\nThe new Brain Centre of IIT Madras has taken up this challenge and is powering a large-scale, multi-disciplinary effort to map more than 100 human brains at a cellular level. Using their proprietary technology platform, the center is imaging post-mortem human brains of different types and ages.\nTheir goal is to create an unprecedented cell resolution, uniformly studied, digital collection of multiple types of human brains, which will be queryable from cell level to whole brain level. This requires the capability to enumerate 100B neurons per brain, over 100 brains, and the connectivity across different brain regions.\nMeeting the computational demands of cellular resolution brain imaging\nThe center has developed a world-class computing platform to store, address, access, process, and visualize such high-resolution digital human brain data at the scale of 100+ petabytes, through nothing more than a web browser interface.\nThis endeavor can be related to the mapping of multiple whole planets to uniformly study patterns, trends, and differences through high-resolution, cross-sectional imaging data through the planet\u2019s volume. Satellite images of the earth\u2019s surface would reach terabytes, a manageable size in today\u2019s computers and web browsers. These geospatial rendering techniques power the likes of Google Maps and others.\nHowever, volumetric imaging of whole brains at cellular resolution yields petabytes of digital data per brain, a challenge to visualize, process, analyze, and query over a web interface.\nThe computational challenges behind the scenes are equally tremendous:\nA human-in-the-loop large image data pipeline\nAutomation right from indexing the data from multiple concurrent imaging systems\nTransporting the images to a central uniform parallel file storage cluster\nEncoding in a format convenient for random access at an arbitrary scale\nMachine learning models for multiple automated tasks like the detection of tissue outlines\nQuality control for imaging\nDeep learning tasks for large image normalization\nCellular-level object detection, classification, and region delineation\nAdvanced mathematical models for the geometric alignment of images across modalities and resolutions, followed by computational geometry for deriving quantitative informatics\nFormatting into a high-volume rapid retriable metadata and informatics data store that can transform the prohibitive to the possible: performing single-cell to whole-brain queries on demand and reverting with timely answers for a web-based interaction\nTo contemplate the scale of the challenge, look at just one of these tasks, a seemingly well-posed task of object detection. This is a task for which modern deep learning convolutional neural networks are known to perform quite well, achieving or in some cases exceeding human-level abilities in identifying and labeling objects.\nHowever, these models are trained to work on megapixel images, containing a few tens of objects. A single-cell resolution image is multi-gigapixel and can contain millions of distinct objects. Processing such large data requires specialized computational workhorses.\nThis is why the Brain Centre has turned to NVIDIA, a leader in GPU-based HPC offerings, and has operationalized a cluster of DGX A100 systems to do the complete processing of 10 to 20 brains. As the center scales to 100 brains and more, they look to a DGX SuperPOD to provide scalable performance, with industry-leading computing, storage, networking, and infrastructure management.\nWith eight NVIDIA A100 Tensor Core GPUs per DGX node, the same data that requires a minimum of 1 hour to detect cells has been reduced to less than 10 minutes on an NVIDIA DGX. This enables whole-brain analysis in a month\u2019s time frame, and scaling to 100 brains becomes practical.\n\u201cI am delighted to see IITM Brain Centre collaborating with NVIDIA in tackling this challenge of analyzing the very large and complex cellular-level human brain data we are generating,\u201d said Kris Gopalakrishan, a distinguished alumnus of IIT Madras and co-founder of Infosys. He played a pivotal role in setting up and supporting the IITM Brain Centre. \u201cBy working with an industry leader like NVIDIA, we look forward to creating breakthroughs in this area leading to global impact.\u201d\nSolving the computation challenges\nA gigapixel whole slide image making up 0.25 million tiles of 256 X 256 images takes just 420 seconds for inferencing on an A100 GPU. It was possible through end-to-end pipeline optimization using NVIDIA libraries and application frameworks:\nAccelerated tile creation and batching are performed using NVIDIA DALI.\nThe MONAI Core model is optimized by TensorRT.\nThe TensorRT plan file selects a mix of INT8, FP16, and TF32 at different parts of the network, and produces the engine.\nThree engines are put in one A100 GPU for distributed inferencing.\nThe NVIDIA accelerated image processing library cuCIM is used for accelerated image registration.\nNVIDIA IndeX is used for multi-GPU volume visualization for various zoom levels. Soon, MONAI Label\u2019s AI-assisted annotation, along with NVIDIA federated learning SDK-Flare, will be used to further refine various other MONAI core models and the pipeline will be deployed using MONAI Deploy.\n\u201cThe NVIDIA technology stack empowers the pioneers at the IIT Madras Brain Centre to effectively address the computational needs for high-resolution brain imaging at the cellular level, thereby propelling forward neuroscience research on a national and global scale,\u201d said Vishal Dhupar, managing director of South Asia at NVIDIA.\nMONAI and TensorRT are available with NVIDIA AI Enterprise, which is included with the NVIDIA DGX infrastructure.\nDiagram shows different stages of the NVIDIA accelerated computing workflow for medical imaging, from digital pathology scanner to MONAI Deploy.\nFigure 1. NVIDIA accelerated computing workflow for medical imaging\nThe compute capability of NVIDIA DGX systems, with dual 64-core CPUs and eight NVIDIA A100 GPUs with 640 GB of GPU memory, along with 2 TB of RAM and 30 TB of flash storage, represent the highest class of server compute available in a single 4U chassis.\nFurther, the DGX is scalable. NVIDIA offers an ecosystem of software and networking to interconnect multiple DGX systems and match the scale and performance requirements of IITM\u2019s Brain Centre for data processing of pipelined batch jobs, as well as on-demand burst computations.\nThe effective processing rate of a single NVIDIA A100 GPU for CNN inference as benchmarked on the Brain Centre data is 60 GB per hour (data in uint8, inference in FP16 precision), or 2.4 TB per hour over five DGX servers (40 A100 GPUs), which matches the current imaging rate. This makes pipelining of imaging and computation bottleneck-free. Owing to the scalability of the DGX compute nodes, any surge in data inflow rate can also be matched through scale-out growth.\nThe A100 GPU is mostly targeted for deep learning training for large datasets and large models that might not fit in smaller GPU vRAM. In the context of the IITM Brain Centre, the A100 GPUs within the DGX systems are used in CNN inference, in a multi-engine per A100 GPU fashion, with the data being mapped out across multi-GPU and multi-node, for scaling from 1\u201340x across five DGX servers.\nThis enables the handling of variable image sizes, corresponding to the physical size variability in human brains across ages from fetal to adult (1\u201332x scale variation). Also, the CPU compute capabilities and the storage type of the DGX A100 systems are well-used in the Brain Centre\u2019s compute pipeline for workloads that are CPU-intensive or data access or movement-intensive, as well as for remote visualization.\nThe NVIDIA technology stack offers tools and optimized libraries for every step in the process, in a uniform format as container runtimes, facilitating the adoption with minimal effort and ensuring best practices and automated operation.\nTransforming the landscape of deep learning in medical imaging\nDeep learning technology used to focus on engineering the best methods or tuning at training time for incremental boosts in performance. It has now shifted to inference with proven foundational models across domains like computer vision (object detection, semantic and panoptic segmentation, DL-based image registration) and natural language. The results are realizing applications previously held as non-amenable for computational automation.\nThe emphasis now is on implementing software guardrails around new applications powered by deep learning inference. Integrated hardware systems and software stacks are not just a convenience in this new direction but are a vehicle of scale and simplification. The NVIDIA technology stack is a way to leapfrog solution building, deployment, and scaling.\nA detailed map of the earth is today accessible for everyone and has emerged as a platform enabling new applications, and businesses. It now guides and shapes globally how we move about. The efforts of the IIT Madras Brain Centre are targeted to produce a similar, transformative platform that will yield new outcomes in brain science, shape and guide brain surgery and therapy, and expand our understanding of medicine\u2019s last frontier, the human brain.\nFor more information, see the /HTIC-Medical-Imaging GitHub repo."}], "https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/": [{"text": "NVIDIA achieved record-breaking performance in AI training with its H100 GPUs and Quantum-2 InfiniBand in MLPerf Training v3.1 and HPC v3.0. The platform demonstrated unprecedented scalability and efficiency, setting new records for LLM training, text-to-image generation, and classic MLPerf workloads. By leveraging software optimizations such as CUDA graphs, FlashAttention-2, and GroupNorm with Channels Last support, NVIDIA improved performance per GPU and achieved near-linear scaling. Additionally, the platform excelled in Stable Diffusion training, DLRMv2, RetinaNet, 3D U-Net, BERT-large, and other benchmarks, showcasing unmatched performance and versatility. Notably, NVIDIA's H100 GPUs outperformed the previous generation A100 GPUs in MLPerf HPC, delivering significant speedups and demonstrating superiority across all tested scenarios. The NVIDIA platform's exceptional results, combined with its availability from various providers, highlight its leadership in AI training and computational performance. All software used in NVIDIA's submissions is open-source, enabling reproducibility and accessibility for users.", "text_components": ["Setting New Records at Data Center Scale Using NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand\nGenerative AI is rapidly transforming computing, unlocking new use cases and turbocharging existing ones. Large language models (LLMs), such as OpenAI\u2019s GPT models and Meta\u2019s Llama 2, skillfully perform a variety of tasks on text-based content. These tasks include summarization, translation, classification, and generation of new content such as computer code, marketing copy, poetry, and much more.\nIn addition, diffusion models, such as Stable Diffusion developed by Stability.ai, enable users to generate incredible images using simple text prompts.\nThe capabilities enabled by state-of-the-art generative AI are considerable\u2014but so, too, is the amount of compute performance required to train them. LLMs, for example, have grown into hundreds of billions of parameters, with each model being trained on enormous amounts of data. The large and growing compute demands of training state-of-the-art LLMs are beyond the scope of a single GPU or even a single node packed with GPUs.\nInstead, training these LLMs requires accelerated computing at the scale of an entire data center. This is why the NVIDIA accelerated computing platform scales to many thousands of high-performance GPUs, interconnected with the highest-bandwidth networking fabrics, all efficiently orchestrated by carefully crafted software.\nMLPerf Training v3.1 is the latest edition of the long-running suite of AI training benchmarks, and serves as a trusted, peer-reviewed measure of AI training performance. After adding an LLM training benchmark in the prior round based on OpenAI\u2019s GPT-3 175B parameter model, this round sees the addition of a text-to-image generative AI training benchmark based on Stable Diffusion.\nReflecting the rapid convergence of traditional HPC workloads with AI, MLPerf HPC v3.0 measures the training performance of AI models that are used for scientific computing applications. In this round, a protein structure prediction test using OpenFold was added, complementing the existing tests which cover climate atmospheric river identification, cosmology parameter prediction, and quantum molecular modeling.\nIn just one MLPerf Training round, NVIDIA demonstrated unprecedented performance and scalability for LLM training, tripling the submission scale and nearly tripling performance. This shattered the performance record previously set by the NVIDIA platform and NVIDIA H100 Tensor Core GPUs just 6 months ago. With continued software improvements, the NVIDIA MLPerf submissions also boosted the per-accelerator performance of the H100 GPU, translating into faster time to train and lower cost to train.\nNVIDIA also submitted results on the newly added text-to-image training benchmark, achieving \u200crecord performance both on a per-accelerator basis as well as at scale.\nIn addition, NVIDIA set new performance records at scale on the DLRM-dcnv2, BERT-large, RetinaNet, and 3D U-Net workloads, extending the record-setting performance achieved by the NVIDIA platform and H100 GPUs in the prior round. For these benchmarks, collective operations were accelerated using NVIDIA Quantum-2 InfiniBand switches and in-network computing with NVIDIA SHARP to help achieve record performance at scale.\nFinally, NVIDIA also made its first MLPerf HPC submissions with H100 GPUs and ran all workloads and scenarios.\nThe following sections take a closer look at these incredible results.", "Supercharging GPT-3 175B training performance\nIn MLPerf Training v3.1, NVIDIA raised the bar for LLM training performance through both dramatically greater submission scale as well as software enhancements that achieved greater performance per GPU.", "Record-setting performance and excellent scaling efficiency\nNVIDIA made several large-scale LLM submissions at a maximum submission scale of 10,752 H100 GPUs. This tripled the maximum scale submitted in the prior round, representing the largest number of accelerators ever used in an MLPerf submission. NVIDIA achieved a time-to-train score of 3.92 minutes in its largest-scale submission, a 2.8x performance boost and a new LLM benchmark record.\nIn addition, NVIDIA partnered closely with Microsoft Azure on a joint LLM submission, also using 10,752 H100 GPUs and Quantum-2 InfiniBand networking, achieving a time to train of 4.01 minutes, nearly identical to that of the NVIDIA submission. Achieving this level of performance on two entirely different, giant-scale systems in the same MLPerf round is a singular technical achievement.\nA chart showing that the NVIDIA and Microsoft Azure MLPerf Training v3.1 LLM submissions this round were approximately 2.8x that of the MLPerf Training v3.0 submission using 3,584 H100 GPUs.\nFigure 1. Relative performance of the NVIDIA and Microsoft Azure 10,752 H100 GPU joint submission on the MLPerf Training LLM test\nMLPerf Training v3.0 and v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.0-2003, 3.1-2002, 3.1-2007. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nThe NVIDIA submissions also demonstrated near-linear performance scaling, as submission sizes scaled from 4,096 H100 GPUs to 10,752 H100 GPUs.\nA chart that shows that, compared to the baseline MLPerf Training v3.0 LLM submission using 3,584 H100 GPUs, NVIDIA\u2019s MLPerf Training v3.1 submissions achieve up to 2.8x more LLM training performance with up to 3x more H100 GPUs for near-linear performance scaling.\nFigure 2. MLPerf Training LLM performance compared to H100 GPU count used to achieve that performance MLPerf Training v3.0 and v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.0-2003, 3.1-2005, 3.1-2007, 3.1-2008, 3.1-2009. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nTo keep the benchmark duration within a reasonable time, MLPerf benchmarks represent a fraction of whole end-to-end workloads. As reported in Training Compute-Optimal Large Language Models, a GPT-3 175B model requires 3.7T tokens to train on to be compute-optimal. Projecting our MLPerf 10,752-GPU record of 3.92 minutes, the compute-optimal training on 3.7T tokens would complete in 8 days.\nThis result was achieved through the full array of NVIDIA technologies, including the latest, fourth-generation NVLink interconnect combined with the latest third-generation NVSwitch chip to enable 900 GB/s all-to-all communication between H100 GPUs, NVIDIA Quantum-2 InfiniBand networking, as well as NVIDIA\u2019s exceptional software stack, including the NVIDIA NeMo framework, NVIDIA Transformer Engine library, NVIDIA cuBLAS library, and the NVIDIA Magnum IO NCCL documentation.", "Increasing performance per H100 GPU\nNVIDIA made significant software improvements that yielded about a 10% faster time to train in both 512-GPU and 768-GPU submissions compared to the corresponding submissions in the prior round, achieving 797 TFLOPS of training throughput per H100 GPU. These improvements were seen in the larger-scale NVIDIA submissions as well, with the 4,096 H100 GPU submission demonstrating nearly 28% more performance with only 14% more GPUs.\nBy continuing to deliver more performance from NVIDIA GPUs with ongoing software enhancements, customers benefit from greater productivity through shorter model training times, faster time to deployment, and lower costs, particularly in the cloud. This is achieved by requiring fewer GPU hours to perform the same work, and the ability to train increasingly complex models on the same hardware.", "Setting the standard for Stable Diffusion training\nThe NVIDIA platform and H100 GPUs submitted record-setting results for the newly added Stable Diffusion workloads. The NVIDIA submission using 64 H100 GPUs completed the benchmark in just 10.02 minutes, and that time to train was reduced to just 2.47 minutes using 1,024 H100 GPUs.\nFigure 3 shows some of the optimizations used in the NVIDIA Stable Diffusion submission. All the optimizations are available in NeMo Multimodal release 23.09 (EA).\nA chart that shows the absolute time-to-train values for the 64 and 1,024 NVIDIA H100 MLPerf Stable Diffusion submissions at 10.02 minutes and 2.47 minutes, respectively.\nFigure 3. Time to train for the 64 and 1,024 NVIDIA H100 submissions on the MLPerf text-to-image (Stable Diffusion) benchmark MLPerf Training v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.1-2050, 3.1-2060. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.", "GroupNorm with Channels Last support\nMost diffusion models are composed of two basic building blocks, ```ResBlock``` and ```SpatialTransformer```. For ```ResBlock``` and other convolutional neural network (CNN)-based networks, using a channels-last memory format is preferred for performance. To avoid layout transformations around ```GroupNorm```, which would incur additional memory accesses, the NVIDIA Stable Diffusion submission added support for a ```GroupNorm``` block that employs a channels-last memory format. Using a channels-last layout in combination with APEX GroupNorm led to a 14% performance improvement.", "FlashAttention-2\nEach ```SpatialTransformer``` module contains a self-attention and a cross-attention block. Using FlashAttention-2 for these blocks achieved a 21% speedup.", "Host-device synchronization removals\nPyTorch models can suffer from significant CPU overheads. To reduce this overhead, several synchronization points between the CPU and the GPU were eliminated. This removes the host from the critical path and enables end-to-end application performance to correlate much better with GPU performance.\nHost execution also results in significant execution time variability, causing variations in iteration times across different workers. This is further exacerbated in multi-GPU settings, where all workers need to synchronize one time at each iteration. These optimizations also reduce runtime variations across each worker, improving multi-GPU training performance. Overall, these optimizations boosted performance by 11%.", "CUDA graphs\nThe NVIDIA submission also uses CUDA graphs to further reduce runtime overhead, boosting GPU utilization. The use of CUDA graphs for the U-Net model that is part of Stable Diffusion delivered a 4% performance increase. Similar to the previous optimization, this helps with reducing CPU overhead, as well as multi-GPU performance by reducing runtime variability across workers.", "Reduced kernel launch overheads\nDiffusion models tend to include many small CUDA kernels, and they can occur at many points across the model. To both reduce kernel launch overheads and minimize the trips to GPU memory, we applied fusion engines such as ```TorchInductor```, yielding a 6% speedup.", "Removing type-casting overheads\nTo make full use of NVIDIA Tensor Cores, modern diffusion models adopt Automatic Mixed Precision (AMP) to enable training using lower precision data formats, such as FP16. The PyTorch implementation of AMP stores model parameters in FP32 precision, which means that dynamic type-casting operations between FP16 and FP32 need to be performed. To avoid such overhead, the model is first cast to FP16 precision, resulting in FP16 model parameters and gradients that can be directly used without the need for type-casting operations.\nThe optimizer is also modified to maintain an FP32 copy of the model parameters, which are used to update the model instead of the FP16 model parameters. This enables faster model training while maintaining accuracy.", "Unlocking faster DLRMv2 training\nAs in the prior round, NVIDIA made submissions using 8, 64, and 128 H100 GPUs. Due to software improvements, the performance of each of these submissions increased round-over-round, with the largest improvement of 57% achieved in the 128 H100 GPU submission.\nKey to our DLRMv2 submissions this round was an extensive focus on removing host CPU bottlenecks. This is mostly achieved by using statically linked CUDART instead of dynamically linking it at runtime. This is because dynamically linked CUDART has additional locking overhead due to its requirement of allowing loading and unloading of modules at any point.\nA chart that shows the relative speedup at each of 8-, 64-, and 128-GPU scales on the for the DLRMv2 workload in MLPerf Training v3.1 compared to results submitted in MLPerf Training v3.0.\nFigure 4. DLRMv2 performance increases in MLPerf Training v3.1 compared to the prior MLPerf Training v3.0 round MLPerf Training v3.1. Results retrieved from www.mlperf.org on November 8, 2023 from entries: 128x 3.0-2065, 128x 3.1-2051. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nBy making use of data-parallel embeddings for small embedding tables, the NVIDIA submission this round also reduced the all-to-all traffic associated with embedding operations. Although data-parallel embedding naturally avoids all-to-all operations, it still requires an ```AllReduce``` operation to accumulate gradients across GPUs. So, to further enhance performance, a grouped ```AllReduce``` technique that fuses the ```AllReduce``` operation with the ```AllReduce``` operation from the data-parallel dense network is used.", "Boosting RetinaNet performance\nIn this round, the NVIDIA submissions demonstrated both performance increases at the same scales submitted in the prior round, as well as a dramatic boost in maximum scale to 2,048 H100 GPUs (compared to 768 H100 GPUs in the previous round). As a result, NVIDIA set a new record time to train of 0.92 minutes for the MLPerf RetinaNet benchmark, a 64% performance boost compared to the previous NVIDIA submission.\nThese improvements were fueled by several optimizations, described in the following sections.", "Avoiding gradient copies in PyTorch DDP\nIn the NVIDIA maximum-scale configuration with 2,048 H100 GPUs, device-to-device memory copies that were exposed between the end of backpropagation and the start of the optimizer step were observed. These were found to be copies between the model gradients and the ```AllReduce``` communication buckets used by PyTorch ```DistributedDataParallel```.\nSetting ```gradient_as_bucket_view=True``` for PyTorch ```DistributedDataParallel``` turns gradients into views (pointers) into the existing ```AllReduce``` buckets and avoids creating an extra buffer, reducing peak memory usage and avoiding the overhead of memory copies. This optimization improved training throughput by 3% in the largest-scale NVIDIA submission.", "Improved evaluation performance\nAt large scales, evaluation time increasingly becomes an important factor in time to train. Evaluation in RetinaNet consists of two phases: inference, which is performed on GPUs, and asynchronous scoring, which is performed on CPUs. To improve inference performance at max-scale, the NVIDIA submission this round uses a specifically optimized batch size rather than the one used for training. It also binds processes to CPU cores and increases the number of CPU threads to improve scoring performance.", "cuDNN optimizations\nThe cuDNN library has made improvements to computational kernels that use the fourth-generation Tensor Cores of the H100 GPU. This led to increased performance of convolutions across all sizes used in RetinaNet, delivering up to 3% higher training throughput compared to the previous NVIDIA submission. Improved convolution kernel performance in cuDNN has also led to up to 7% throughput improvement in other benchmarks, such as ResNet-50.", "Greater spatial parallelism for 3D U-Net\nFor the maximum-scale NVIDIA submission on the 3D U-Net workload in this round, the system configuration grew from 54 nodes (consisting of 42 training nodes and 12 evaluation nodes in the prior round) to 96 nodes (consisting of 84 training nodes and 12 training nodes).\nBy leveraging the capabilities of H100 GPUs and the latest version of NVLink, the NVIDIA submission in this round increased the level of spatial parallelism to 8x from a prior 4x. This means that instead of training one image across a set of four H100 GPUs, the latest NVIDIA submission trains one image across eight H100 GPUs, resulting in an end-to-end throughput improvement of over 6.2%.", "Improving BERT score at-scale\nThe max-scale NVIDIA submission using 434 nodes achieved a 1.12x speedup over our previous submission using 384 nodes. This round, we also introduced a sample re-ordering optimization to improve load balancing for small per-GPU batch sizes, as is the case for max-scale. This resulted in a 6% throughput improvement for the 434-node scale submission.", "Sample reordering\nPrior large-scale NVIDIA submissions used a packed dataset format where every sample had one to three actual sequences, so that the combined length is the same or slightly smaller than the maximum sequence length.\nAlthough this optimization minimizes the load imbalance among GPUs for large enough per-GPU batch sizes, the number of actual sequences processed by GPUs is different. This causes jitter at large-scale runs using small per-GPU batch size. Sample reordering optimization bucketizes the packed samples such that the number of actual sequences processed in each iteration remains constant across GPUs, thus minimizing jitter. We have verified that this optimization does not impact convergence for this benchmark.", "Bringing H100 to MLPerf HPC v3.0\nIn MLPerf HPC v3.0, NVIDIA made its first submissions using H100 GPUs. The newer GPUs delivered a significant boost compared to the NVIDIA MLPerf HPC v2.0 submissions, which used prior-generation NVIDIA A100 GPUs.\nThe following sections cover some of the software work that was done to make optimized submissions using H100 on all MLPerf HPC v3.0 tests, including the new test based on OpenFold.", "OpenFold\nA common method for reducing training time is data parallelism, which equally distributes the global batch across GPUs. The degree of parallelism of data-parallel task partitioning is limited by the global batch size. Larger batch sizes, however, result in lower accuracy, limiting the global batch size for OpenFold and preventing the use of data parallelism.\nFastFold introduces dynamic axial parallelism (DAP), a way to increase the degree of parallelism. With DAP, the model can continue scaling up, but the scaling efficiency of DAP is low. In the initial phase, increasing GPU count by 4x yields only a 1.7x speedup, with no additional performance increase when the GPU count is scaled to 8x.\nIn addition to scaling efficiency challenges, OpenFold contains about 150K operators, most of which are memory bandwidth\u2013bound, leading to suboptimal GPU utilization and high CPU launch overhead. Moreover, in the data load stage, all samples need to be cropped or padded into a fixed size. The time required to prepare these batches depends on both sequence length and the number of multiple sequence alignments (MSAs), which vary significantly. We observed that 0.1% batches are slow to prepare, blocking the training stage.\nTo resolve these problems, the NVIDIA OpenFold submission employs the following optimizations:\nIncorporate DAP into the submission, enabling continued GPU scaling to 8x.\nCreate a new data loader with a non-blocking priority queue, which enables slow batches to be skipped and processed later when they are ready.\nImplement three efficient NVIDIA Triton kernels for critical patterns, multi-head attention, ```LayerNorm```, and ```fusedAdam``` with stochastic weight averaging (SWA).\nApply the torch compiler to automatically fuse memory-bound operations.\nEnable BF16 training and CUDA graphs.\nUse asynchronous evaluation to free training nodes from evaluation and implement an evaluation dataset cache to speed up the performance of evaluation.", "OpenCatalyst20\nThe NVIDIA OpenCatalyst20 submissions incorporate several optimizations to maximize performance. First, the submissions use FP16 precision. Next, the submissions perform evaluation asynchronously, which frees the training nodes from needing to perform evaluation. Finally, by disabling PyTorch garbage collection, CPU overhead was reduced.", "DeepCAM\nFor the time-to-train metric, the NVIDIA DeepCAM submission on 2,048 GPUs uses a GPU local batch size of one. In the previous NVIDIA DeepCAM submissions, the PyTorch ```SyncBN``` kernel was used for distributed batch normalization. In the NVIDIA MLPerf HPC v3.0 submission, this kernel was replaced with a more efficient single-node group batch normalization kernel from APEX, boosting end-to-end performance.\nFor the throughput test, the NVIDIA DeepCAM submission ran on two nodes with a GPU local batch size of eight. In this scenario, a large portion of the dataset resides on the NVMe storage, and reading a large portion of this dataset during training thrashes the node\u2019s page cache frequently. This resulted in stalls in the input pipeline, an issue that manifested itself when running on H100 GPUs, which feature much more compute performance than A100 GPUs.\nTo overcome this issue, the NVIDIA submission implements ```O_DIRECT``` in the data reader, bypassing the page cache entirely. This ensured that I/O operations can be hidden almost entirely behind the computations, leading to a 1.8x performance increase compared to the unimproved code.\nFinally, enhancements to group convolution kernels designed for the NVIDIA Hopper architecture contributed another 2% speedup.", "CosmoFlow\nFor the CosmoFlow throughput benchmark, the NVIDIA submission runs on 32 GPUs using a global batch size of 32 and a local batch size of one. As the compute performance of H100 GPUs is much greater than A100 GPUs, the training pipeline began to stall, waiting for additional data to arrive.\nTo fix this, the NVIDIA submission implements support for ```O_DIRECT``` in the data reader. Previously, the GPU would make requests to the CPU, the CPU would perform the data read access, and then pass the data back to the GPU. With ```O_DIRECT```, the GPU makes the data requests directly and receives the data back, bypassing the CPU. This decreased \u200cdata latency and enabled the data input pipeline to remain mostly hidden behind the math computation.\nThe CosmoFlow network is primarily a series of 3D convolutions and ```MaxPool``` layers. For both the time-to-train and throughput benchmark runs, the individual kernels were slower than their maximum theoretical speed. Several low-level optimizations were made to the kernels to increase their performance in the forward and backward passes. In the time-to-train configuration, CosmoFlow ran on 512 GPUs with a global batch size of 512 and a local batch size of one. Kernel optimizations also improved the time to train by a few percentage points.", "MLPerf Training v3.1 and HPC v3.0 takeaways\nThe NVIDIA platform, powered by NVIDIA H100 GPUs and Quantum-2 InfiniBand, yet again raised the bar for AI training. From powering two LLM submissions at an unprecedented scale of 10,752 H100 GPUs, to unmatched performance on the newly added text-to-image test, to continued performance boosts for classic MLPerf workloads, the NVIDIA platform continues to demonstrate the highest performance and the greatest versatility for the most demanding AI training challenges.\nThe NVIDIA platform also demonstrated how it is accelerating the convergence of HPC and AI with the latest MLPerf HPC results, with the H100 GPU enabling substantial speedups compared to prior NVIDIA submissions using the A100 GPU. In addition to delivering the highest performance across all MLPerf HPC workloads and scenarios, only the NVIDIA platform ran every test, demonstrating remarkable versatility.\nIn addition to exceptional performance and versatility, the NVIDIA platform is broadly available from both cloud service providers and system makers worldwide. All software used for NVIDIA MLPerf submissions is available from the MLPerf repository, enabling users to reproduce results. All NVIDIA AI software used to achieve these results is also available in the enterprise-grade software suite, NVIDIA AI Enterprise."], "document_title": "Setting New Records at Data Center Scale Using NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand", "document_url": "https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/", "document_date": "2023-11-08T17:00:00", "document_date_modified": "2023-11-24T18:36:30", "document_full_text": "Setting New Records at Data Center Scale Using NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand\nGenerative AI is rapidly transforming computing, unlocking new use cases and turbocharging existing ones. Large language models (LLMs), such as OpenAI\u2019s GPT models and Meta\u2019s Llama 2, skillfully perform a variety of tasks on text-based content. These tasks include summarization, translation, classification, and generation of new content such as computer code, marketing copy, poetry, and much more.\nIn addition, diffusion models, such as Stable Diffusion developed by Stability.ai, enable users to generate incredible images using simple text prompts.\nThe capabilities enabled by state-of-the-art generative AI are considerable\u2014but so, too, is the amount of compute performance required to train them. LLMs, for example, have grown into hundreds of billions of parameters, with each model being trained on enormous amounts of data. The large and growing compute demands of training state-of-the-art LLMs are beyond the scope of a single GPU or even a single node packed with GPUs.\nInstead, training these LLMs requires accelerated computing at the scale of an entire data center. This is why the NVIDIA accelerated computing platform scales to many thousands of high-performance GPUs, interconnected with the highest-bandwidth networking fabrics, all efficiently orchestrated by carefully crafted software.\nMLPerf Training v3.1 is the latest edition of the long-running suite of AI training benchmarks, and serves as a trusted, peer-reviewed measure of AI training performance. After adding an LLM training benchmark in the prior round based on OpenAI\u2019s GPT-3 175B parameter model, this round sees the addition of a text-to-image generative AI training benchmark based on Stable Diffusion.\nReflecting the rapid convergence of traditional HPC workloads with AI, MLPerf HPC v3.0 measures the training performance of AI models that are used for scientific computing applications. In this round, a protein structure prediction test using OpenFold was added, complementing the existing tests which cover climate atmospheric river identification, cosmology parameter prediction, and quantum molecular modeling.\nIn just one MLPerf Training round, NVIDIA demonstrated unprecedented performance and scalability for LLM training, tripling the submission scale and nearly tripling performance. This shattered the performance record previously set by the NVIDIA platform and NVIDIA H100 Tensor Core GPUs just 6 months ago. With continued software improvements, the NVIDIA MLPerf submissions also boosted the per-accelerator performance of the H100 GPU, translating into faster time to train and lower cost to train.\nNVIDIA also submitted results on the newly added text-to-image training benchmark, achieving \u200crecord performance both on a per-accelerator basis as well as at scale.\nIn addition, NVIDIA set new performance records at scale on the DLRM-dcnv2, BERT-large, RetinaNet, and 3D U-Net workloads, extending the record-setting performance achieved by the NVIDIA platform and H100 GPUs in the prior round. For these benchmarks, collective operations were accelerated using NVIDIA Quantum-2 InfiniBand switches and in-network computing with NVIDIA SHARP to help achieve record performance at scale.\nFinally, NVIDIA also made its first MLPerf HPC submissions with H100 GPUs and ran all workloads and scenarios.\nThe following sections take a closer look at these incredible results.\nSupercharging GPT-3 175B training performance\nIn MLPerf Training v3.1, NVIDIA raised the bar for LLM training performance through both dramatically greater submission scale as well as software enhancements that achieved greater performance per GPU.\nRecord-setting performance and excellent scaling efficiency\nNVIDIA made several large-scale LLM submissions at a maximum submission scale of 10,752 H100 GPUs. This tripled the maximum scale submitted in the prior round, representing the largest number of accelerators ever used in an MLPerf submission. NVIDIA achieved a time-to-train score of 3.92 minutes in its largest-scale submission, a 2.8x performance boost and a new LLM benchmark record.\nIn addition, NVIDIA partnered closely with Microsoft Azure on a joint LLM submission, also using 10,752 H100 GPUs and Quantum-2 InfiniBand networking, achieving a time to train of 4.01 minutes, nearly identical to that of the NVIDIA submission. Achieving this level of performance on two entirely different, giant-scale systems in the same MLPerf round is a singular technical achievement.\nA chart showing that the NVIDIA and Microsoft Azure MLPerf Training v3.1 LLM submissions this round were approximately 2.8x that of the MLPerf Training v3.0 submission using 3,584 H100 GPUs.\nFigure 1. Relative performance of the NVIDIA and Microsoft Azure 10,752 H100 GPU joint submission on the MLPerf Training LLM test\nMLPerf Training v3.0 and v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.0-2003, 3.1-2002, 3.1-2007. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nThe NVIDIA submissions also demonstrated near-linear performance scaling, as submission sizes scaled from 4,096 H100 GPUs to 10,752 H100 GPUs.\nA chart that shows that, compared to the baseline MLPerf Training v3.0 LLM submission using 3,584 H100 GPUs, NVIDIA\u2019s MLPerf Training v3.1 submissions achieve up to 2.8x more LLM training performance with up to 3x more H100 GPUs for near-linear performance scaling.\nFigure 2. MLPerf Training LLM performance compared to H100 GPU count used to achieve that performance MLPerf Training v3.0 and v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.0-2003, 3.1-2005, 3.1-2007, 3.1-2008, 3.1-2009. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nTo keep the benchmark duration within a reasonable time, MLPerf benchmarks represent a fraction of whole end-to-end workloads. As reported in Training Compute-Optimal Large Language Models, a GPT-3 175B model requires 3.7T tokens to train on to be compute-optimal. Projecting our MLPerf 10,752-GPU record of 3.92 minutes, the compute-optimal training on 3.7T tokens would complete in 8 days.\nThis result was achieved through the full array of NVIDIA technologies, including the latest, fourth-generation NVLink interconnect combined with the latest third-generation NVSwitch chip to enable 900 GB/s all-to-all communication between H100 GPUs, NVIDIA Quantum-2 InfiniBand networking, as well as NVIDIA\u2019s exceptional software stack, including the NVIDIA NeMo framework, NVIDIA Transformer Engine library, NVIDIA cuBLAS library, and the NVIDIA Magnum IO NCCL documentation.\nIncreasing performance per H100 GPU\nNVIDIA made significant software improvements that yielded about a 10% faster time to train in both 512-GPU and 768-GPU submissions compared to the corresponding submissions in the prior round, achieving 797 TFLOPS of training throughput per H100 GPU. These improvements were seen in the larger-scale NVIDIA submissions as well, with the 4,096 H100 GPU submission demonstrating nearly 28% more performance with only 14% more GPUs.\nBy continuing to deliver more performance from NVIDIA GPUs with ongoing software enhancements, customers benefit from greater productivity through shorter model training times, faster time to deployment, and lower costs, particularly in the cloud. This is achieved by requiring fewer GPU hours to perform the same work, and the ability to train increasingly complex models on the same hardware.\nSetting the standard for Stable Diffusion training\nThe NVIDIA platform and H100 GPUs submitted record-setting results for the newly added Stable Diffusion workloads. The NVIDIA submission using 64 H100 GPUs completed the benchmark in just 10.02 minutes, and that time to train was reduced to just 2.47 minutes using 1,024 H100 GPUs.\nFigure 3 shows some of the optimizations used in the NVIDIA Stable Diffusion submission. All the optimizations are available in NeMo Multimodal release 23.09 (EA).\nA chart that shows the absolute time-to-train values for the 64 and 1,024 NVIDIA H100 MLPerf Stable Diffusion submissions at 10.02 minutes and 2.47 minutes, respectively.\nFigure 3. Time to train for the 64 and 1,024 NVIDIA H100 submissions on the MLPerf text-to-image (Stable Diffusion) benchmark MLPerf Training v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.1-2050, 3.1-2060. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nGroupNorm with Channels Last support\nMost diffusion models are composed of two basic building blocks, ```ResBlock``` and ```SpatialTransformer```. For ```ResBlock``` and other convolutional neural network (CNN)-based networks, using a channels-last memory format is preferred for performance. To avoid layout transformations around ```GroupNorm```, which would incur additional memory accesses, the NVIDIA Stable Diffusion submission added support for a ```GroupNorm``` block that employs a channels-last memory format. Using a channels-last layout in combination with APEX GroupNorm led to a 14% performance improvement.\nFlashAttention-2\nEach ```SpatialTransformer``` module contains a self-attention and a cross-attention block. Using FlashAttention-2 for these blocks achieved a 21% speedup.\nHost-device synchronization removals\nPyTorch models can suffer from significant CPU overheads. To reduce this overhead, several synchronization points between the CPU and the GPU were eliminated. This removes the host from the critical path and enables end-to-end application performance to correlate much better with GPU performance.\nHost execution also results in significant execution time variability, causing variations in iteration times across different workers. This is further exacerbated in multi-GPU settings, where all workers need to synchronize one time at each iteration. These optimizations also reduce runtime variations across each worker, improving multi-GPU training performance. Overall, these optimizations boosted performance by 11%.\nCUDA graphs\nThe NVIDIA submission also uses CUDA graphs to further reduce runtime overhead, boosting GPU utilization. The use of CUDA graphs for the U-Net model that is part of Stable Diffusion delivered a 4% performance increase. Similar to the previous optimization, this helps with reducing CPU overhead, as well as multi-GPU performance by reducing runtime variability across workers.\nReduced kernel launch overheads\nDiffusion models tend to include many small CUDA kernels, and they can occur at many points across the model. To both reduce kernel launch overheads and minimize the trips to GPU memory, we applied fusion engines such as ```TorchInductor```, yielding a 6% speedup.\nRemoving type-casting overheads\nTo make full use of NVIDIA Tensor Cores, modern diffusion models adopt Automatic Mixed Precision (AMP) to enable training using lower precision data formats, such as FP16. The PyTorch implementation of AMP stores model parameters in FP32 precision, which means that dynamic type-casting operations between FP16 and FP32 need to be performed. To avoid such overhead, the model is first cast to FP16 precision, resulting in FP16 model parameters and gradients that can be directly used without the need for type-casting operations.\nThe optimizer is also modified to maintain an FP32 copy of the model parameters, which are used to update the model instead of the FP16 model parameters. This enables faster model training while maintaining accuracy.\nUnlocking faster DLRMv2 training\nAs in the prior round, NVIDIA made submissions using 8, 64, and 128 H100 GPUs. Due to software improvements, the performance of each of these submissions increased round-over-round, with the largest improvement of 57% achieved in the 128 H100 GPU submission.\nKey to our DLRMv2 submissions this round was an extensive focus on removing host CPU bottlenecks. This is mostly achieved by using statically linked CUDART instead of dynamically linking it at runtime. This is because dynamically linked CUDART has additional locking overhead due to its requirement of allowing loading and unloading of modules at any point.\nA chart that shows the relative speedup at each of 8-, 64-, and 128-GPU scales on the for the DLRMv2 workload in MLPerf Training v3.1 compared to results submitted in MLPerf Training v3.0.\nFigure 4. DLRMv2 performance increases in MLPerf Training v3.1 compared to the prior MLPerf Training v3.0 round MLPerf Training v3.1. Results retrieved from www.mlperf.org on November 8, 2023 from entries: 128x 3.0-2065, 128x 3.1-2051. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information.\nBy making use of data-parallel embeddings for small embedding tables, the NVIDIA submission this round also reduced the all-to-all traffic associated with embedding operations. Although data-parallel embedding naturally avoids all-to-all operations, it still requires an ```AllReduce``` operation to accumulate gradients across GPUs. So, to further enhance performance, a grouped ```AllReduce``` technique that fuses the ```AllReduce``` operation with the ```AllReduce``` operation from the data-parallel dense network is used.\nBoosting RetinaNet performance\nIn this round, the NVIDIA submissions demonstrated both performance increases at the same scales submitted in the prior round, as well as a dramatic boost in maximum scale to 2,048 H100 GPUs (compared to 768 H100 GPUs in the previous round). As a result, NVIDIA set a new record time to train of 0.92 minutes for the MLPerf RetinaNet benchmark, a 64% performance boost compared to the previous NVIDIA submission.\nThese improvements were fueled by several optimizations, described in the following sections.\nAvoiding gradient copies in PyTorch DDP\nIn the NVIDIA maximum-scale configuration with 2,048 H100 GPUs, device-to-device memory copies that were exposed between the end of backpropagation and the start of the optimizer step were observed. These were found to be copies between the model gradients and the ```AllReduce``` communication buckets used by PyTorch ```DistributedDataParallel```.\nSetting ```gradient_as_bucket_view=True``` for PyTorch ```DistributedDataParallel``` turns gradients into views (pointers) into the existing ```AllReduce``` buckets and avoids creating an extra buffer, reducing peak memory usage and avoiding the overhead of memory copies. This optimization improved training throughput by 3% in the largest-scale NVIDIA submission.\nImproved evaluation performance\nAt large scales, evaluation time increasingly becomes an important factor in time to train. Evaluation in RetinaNet consists of two phases: inference, which is performed on GPUs, and asynchronous scoring, which is performed on CPUs. To improve inference performance at max-scale, the NVIDIA submission this round uses a specifically optimized batch size rather than the one used for training. It also binds processes to CPU cores and increases the number of CPU threads to improve scoring performance.\ncuDNN optimizations\nThe cuDNN library has made improvements to computational kernels that use the fourth-generation Tensor Cores of the H100 GPU. This led to increased performance of convolutions across all sizes used in RetinaNet, delivering up to 3% higher training throughput compared to the previous NVIDIA submission. Improved convolution kernel performance in cuDNN has also led to up to 7% throughput improvement in other benchmarks, such as ResNet-50.\nGreater spatial parallelism for 3D U-Net\nFor the maximum-scale NVIDIA submission on the 3D U-Net workload in this round, the system configuration grew from 54 nodes (consisting of 42 training nodes and 12 evaluation nodes in the prior round) to 96 nodes (consisting of 84 training nodes and 12 training nodes).\nBy leveraging the capabilities of H100 GPUs and the latest version of NVLink, the NVIDIA submission in this round increased the level of spatial parallelism to 8x from a prior 4x. This means that instead of training one image across a set of four H100 GPUs, the latest NVIDIA submission trains one image across eight H100 GPUs, resulting in an end-to-end throughput improvement of over 6.2%.\nImproving BERT score at-scale\nThe max-scale NVIDIA submission using 434 nodes achieved a 1.12x speedup over our previous submission using 384 nodes. This round, we also introduced a sample re-ordering optimization to improve load balancing for small per-GPU batch sizes, as is the case for max-scale. This resulted in a 6% throughput improvement for the 434-node scale submission.\nSample reordering\nPrior large-scale NVIDIA submissions used a packed dataset format where every sample had one to three actual sequences, so that the combined length is the same or slightly smaller than the maximum sequence length.\nAlthough this optimization minimizes the load imbalance among GPUs for large enough per-GPU batch sizes, the number of actual sequences processed by GPUs is different. This causes jitter at large-scale runs using small per-GPU batch size. Sample reordering optimization bucketizes the packed samples such that the number of actual sequences processed in each iteration remains constant across GPUs, thus minimizing jitter. We have verified that this optimization does not impact convergence for this benchmark.\nBringing H100 to MLPerf HPC v3.0\nIn MLPerf HPC v3.0, NVIDIA made its first submissions using H100 GPUs. The newer GPUs delivered a significant boost compared to the NVIDIA MLPerf HPC v2.0 submissions, which used prior-generation NVIDIA A100 GPUs.\nThe following sections cover some of the software work that was done to make optimized submissions using H100 on all MLPerf HPC v3.0 tests, including the new test based on OpenFold.\nOpenFold\nA common method for reducing training time is data parallelism, which equally distributes the global batch across GPUs. The degree of parallelism of data-parallel task partitioning is limited by the global batch size. Larger batch sizes, however, result in lower accuracy, limiting the global batch size for OpenFold and preventing the use of data parallelism.\nFastFold introduces dynamic axial parallelism (DAP), a way to increase the degree of parallelism. With DAP, the model can continue scaling up, but the scaling efficiency of DAP is low. In the initial phase, increasing GPU count by 4x yields only a 1.7x speedup, with no additional performance increase when the GPU count is scaled to 8x.\nIn addition to scaling efficiency challenges, OpenFold contains about 150K operators, most of which are memory bandwidth\u2013bound, leading to suboptimal GPU utilization and high CPU launch overhead. Moreover, in the data load stage, all samples need to be cropped or padded into a fixed size. The time required to prepare these batches depends on both sequence length and the number of multiple sequence alignments (MSAs), which vary significantly. We observed that 0.1% batches are slow to prepare, blocking the training stage.\nTo resolve these problems, the NVIDIA OpenFold submission employs the following optimizations:\nIncorporate DAP into the submission, enabling continued GPU scaling to 8x.\nCreate a new data loader with a non-blocking priority queue, which enables slow batches to be skipped and processed later when they are ready.\nImplement three efficient NVIDIA Triton kernels for critical patterns, multi-head attention, ```LayerNorm```, and ```fusedAdam``` with stochastic weight averaging (SWA).\nApply the torch compiler to automatically fuse memory-bound operations.\nEnable BF16 training and CUDA graphs.\nUse asynchronous evaluation to free training nodes from evaluation and implement an evaluation dataset cache to speed up the performance of evaluation.\nOpenCatalyst20\nThe NVIDIA OpenCatalyst20 submissions incorporate several optimizations to maximize performance. First, the submissions use FP16 precision. Next, the submissions perform evaluation asynchronously, which frees the training nodes from needing to perform evaluation. Finally, by disabling PyTorch garbage collection, CPU overhead was reduced.\nDeepCAM\nFor the time-to-train metric, the NVIDIA DeepCAM submission on 2,048 GPUs uses a GPU local batch size of one. In the previous NVIDIA DeepCAM submissions, the PyTorch ```SyncBN``` kernel was used for distributed batch normalization. In the NVIDIA MLPerf HPC v3.0 submission, this kernel was replaced with a more efficient single-node group batch normalization kernel from APEX, boosting end-to-end performance.\nFor the throughput test, the NVIDIA DeepCAM submission ran on two nodes with a GPU local batch size of eight. In this scenario, a large portion of the dataset resides on the NVMe storage, and reading a large portion of this dataset during training thrashes the node\u2019s page cache frequently. This resulted in stalls in the input pipeline, an issue that manifested itself when running on H100 GPUs, which feature much more compute performance than A100 GPUs.\nTo overcome this issue, the NVIDIA submission implements ```O_DIRECT``` in the data reader, bypassing the page cache entirely. This ensured that I/O operations can be hidden almost entirely behind the computations, leading to a 1.8x performance increase compared to the unimproved code.\nFinally, enhancements to group convolution kernels designed for the NVIDIA Hopper architecture contributed another 2% speedup.\nCosmoFlow\nFor the CosmoFlow throughput benchmark, the NVIDIA submission runs on 32 GPUs using a global batch size of 32 and a local batch size of one. As the compute performance of H100 GPUs is much greater than A100 GPUs, the training pipeline began to stall, waiting for additional data to arrive.\nTo fix this, the NVIDIA submission implements support for ```O_DIRECT``` in the data reader. Previously, the GPU would make requests to the CPU, the CPU would perform the data read access, and then pass the data back to the GPU. With ```O_DIRECT```, the GPU makes the data requests directly and receives the data back, bypassing the CPU. This decreased \u200cdata latency and enabled the data input pipeline to remain mostly hidden behind the math computation.\nThe CosmoFlow network is primarily a series of 3D convolutions and ```MaxPool``` layers. For both the time-to-train and throughput benchmark runs, the individual kernels were slower than their maximum theoretical speed. Several low-level optimizations were made to the kernels to increase their performance in the forward and backward passes. In the time-to-train configuration, CosmoFlow ran on 512 GPUs with a global batch size of 512 and a local batch size of one. Kernel optimizations also improved the time to train by a few percentage points.\nMLPerf Training v3.1 and HPC v3.0 takeaways\nThe NVIDIA platform, powered by NVIDIA H100 GPUs and Quantum-2 InfiniBand, yet again raised the bar for AI training. From powering two LLM submissions at an unprecedented scale of 10,752 H100 GPUs, to unmatched performance on the newly added text-to-image test, to continued performance boosts for classic MLPerf workloads, the NVIDIA platform continues to demonstrate the highest performance and the greatest versatility for the most demanding AI training challenges.\nThe NVIDIA platform also demonstrated how it is accelerating the convergence of HPC and AI with the latest MLPerf HPC results, with the H100 GPU enabling substantial speedups compared to prior NVIDIA submissions using the A100 GPU. In addition to delivering the highest performance across all MLPerf HPC workloads and scenarios, only the NVIDIA platform ran every test, demonstrating remarkable versatility.\nIn addition to exceptional performance and versatility, the NVIDIA platform is broadly available from both cloud service providers and system makers worldwide. All software used for NVIDIA MLPerf submissions is available from the MLPerf repository, enabling users to reproduce results. All NVIDIA AI software used to achieve these results is also available in the enterprise-grade software suite, NVIDIA AI Enterprise."}], "https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/": [{"text": "NetworkX is a popular Python graph analytics library known for its ease of use and wide range of algorithms. However, it falls short in terms of performance and scalability, particularly for medium-to-large networks. To address this, the RAPIDS cuGraph project developed nx-cugraph, a backend that adds GPU acceleration to NetworkX without requiring code changes. By using nx-cugraph, users can achieve significant speedups in graph analytics tasks, such as betweenness centrality calculations. Benchmarks show that nx-cugraph outperforms the default NetworkX implementation by factors ranging from 5.72x to over 600x, depending on the algorithm and parameters used. NetworkX dispatching allows for the integration of third-party backends, making it a standardized frontend for various graph analytic engines. The combination of NetworkX dispatching and nx-cugraph provides users with the best of both worlds: the ease of use of NetworkX and the performance of GPU acceleration. Feedback and contributions to both projects are welcomed to further enhance their capabilities.", "text_components": ["Accelerating NetworkX on NVIDIA GPUs for High Performance Graph Analytics\nNetworkX states in its documentation that it is \u201c\u2026a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\u201d Since its first public release in 2005, it\u2019s become the most popular Python graph analytics library available. This may explain why NetworkX amassed 27M PyPI downloads just in September of 2023.\nHow is NetworkX able to achieve such mass appeal? Are there use cases where NetworkX falls short, and if so, what can be done to address those? I examine these questions and more in this post.\nimg\nWatch the Accelerating NetworkX: the Future of Easy Graph Analytics session replay from the recent AI and Data Science Virtual Summit hosted by NVIDIA.", "NetworkX: Easy graph analytics\nThere are several reasons NetworkX is so popular among data scientists, students, and many others interested in graph analytics. NetworkX is open-source and backed by a large and friendly community, eager to answer questions and help. The code is mature and well-documented, and the package itself is easy to install and requires no additional dependencies. But most of all, NetworkX has a plethora of algorithms that cover something for everyone (including plotting!) with an easy-to-use API.\nWith just a few lines of simple code, you can load and analyze graph data using any of the algorithms provided. Here is an example of finding the shortest weighted path of a simple four-node graph:\nExample of finding the shortest weighted path of a simple four-node graph\nFigure 1. A simple weighted graph with four nodes and four edges\n```\n>>> import networkx as nx\n>>> G = nx.Graph()\n>>> G.add_edge(\"A\", \"B\", weight=4)\n>>> G.add_edge(\"B\", \"D\", weight=2)\n>>> G.add_edge(\"A\", \"C\", weight=3)\n>>> G.add_edge(\"C\", \"D\", weight=4)\n>>> nx.shortest_path(G, \"A\", \"D\", weight=\"weight\")\n['A', 'B', 'D']\n```\nIn just a few lines, easily typed at a Python prompt, you can interactively explore your graph data.", "So what\u2019s missing?\nWhile NetworkX provides a tremendous amount of usability right out of the box, performance and scalability for medium-to-large-sized networks are far from best-in-class and can significantly limit a data scientist\u2019s productivity.\nTo get an idea of how graph size and algorithm options impact runtime, here\u2019s an interesting analytic that answers questions about a real-world dataset.", "Examining influential U.S. patents using betweenness centrality\nThe , provided by the Stanford Network Analysis Platform (SNAP), is a citation graph of patents granted between 1975 and 1999, totaling 16,522,438 citations. If you know which patents are more central than others, you may get an idea of their relative importance.\nThe citation graph can be processed using the pandas library to create a DataFrame containing graph edges. The DataFrame has two columns: one for the source node and another for the destination node for each edge. NetworkX can then take this DataFrame and create a graph object, which can then be used to run betweenness centrality.\nBetweenness centrality is a metric that quantifies the extent to which nodes act as intermediaries between other nodes, as determined by the number of shortest paths they are a part of. In the context of patent citations, it may be used to measure the extent to which a patent connects other patents.\nUsing NetworkX, you can run ```betweenness_centrality``` to find these central patents. NetworkX selects ```k``` nodes at random for the shortest path analysis used by the betweenness centrality computation. A higher value of ```k``` leads to more accurate results at the cost of increased computation time.\nThe following code example loads the citation graph data, creates a NetworkX graph object, and runs betweenness_centrality.\n```\n###############################################################################\n# Run Betweenness Centrality on a large citation graph using NetworkX\nimport sys\nimport time\n\nimport networkx as nx\nimport pandas as pd\n\nk = int(sys.argv[1])\n\n# Dataset from https://snap.stanford.edu/data/cit-Patents.txt.gz\nprint(\"Reading dataset into Pandas DataFrame as an edgelist...\", flush=True,\n      end=\"\")\npandas_edgelist = pd.read_csv(\n    \"cit-Patents.txt\",\n    skiprows=4,\n    delimiter=\"\\t\",\n    names=[\"src\", \"dst\"],\n    dtype={\"src\": \"int32\", \"dst\": \"int32\"},\n)\nprint(\"done.\", flush=True)\nprint(\"Creating Graph from Pandas DataFrame edgelist...\", flush=True, end=\"\")\nG = nx.from_pandas_edgelist(\n    pandas_edgelist, source=\"src\", target=\"dst\", create_using=nx.DiGraph\n)\nprint(\"done.\", flush=True)\n\nprint(\"Running betweenness_centrality...\", flush=True, end=\"\")\nst = time.time()\nbc_result = nx.betweenness_centrality(G, k=k)\nprint(f\"done, BC time with {k=} was: {(time.time() - st):.6f} s\")\n```\nPass a ```k``` value of 10 when you run the code:\n```\nbash:~$ python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 97.553809 s\n```\nAs you can see, running ```betweenness_centrality``` on a moderately large graph with just a value of ```k=10``` and a fast, modern CPU (Intel Xeon Platinum 8480CL) takes almost 98 seconds. Achieving a higher level of accuracy that aligns with your expectations for a graph of this scale would necessitate significantly increasing the value of ```k```. However, this would result in considerably longer runtimes, as highlighted in the benchmark results later in this post, where execution times extend to several hours.", "RAPIDS cuGraph: Speed and NetworkX interoperability\nThe RAPIDS cuGraph project was created to bridge the gap between fast, scalable, GPU-based graph analytics and NetworkX ease-of-use. For more information, see RAPIDS cuGraph adds NetworkX and DiGraph Compatibility.\ncuGraph was designed with NetworkX interoperability in mind, which can be seen when you replace only the ```betweenness_centrality``` call from the prior example with cuGraph\u2019s ```betweenness_centrality``` and leave the rest of the code as-is.\nThe result is a greater-than-12x speedup with only a few lines of code changed:\n```\n###############################################################################\n# Run Betweenness Centrality on a large citation graph using NetworkX\n# and RAPIDS cuGraph.\n# NOTE: This demonstrates legacy RAPIDS cuGraph/NetworkX interop. THIS CODE IS\n# NOT PORTABLE TO NON-GPU ENVIRONMENTS! Use nx-cugraph to GPU-accelerate\n# NetworkX with no code changes and configurable CPU fallback.\nimport sys\nimport time\n\nimport cugraph as cg\nimport pandas as pd\n\nk = int(sys.argv[1])\n\n# Dataset from https://snap.stanford.edu/data/cit-Patents.txt.gz\nprint(\"Reading dataset into Pandas DataFrame as an edgelist...\", flush=True,\n      end=\"\")\npandas_edgelist = pd.read_csv(\n    \"cit-Patents.txt\",\n    skiprows=4,\n    delimiter=\"\\t\",\n    names=[\"src\", \"dst\"],\n    dtype={\"src\": \"int32\", \"dst\": \"int32\"},\n)\nprint(\"done.\", flush=True)\nprint(\"Creating Graph from Pandas DataFrame edgelist...\", flush=True, end=\"\")\nG = cg.from_pandas_edgelist(\n    pandas_edgelist, source=\"src\", destination=\"dst\", create_using=cg.Graph(directed=True)\n)\nprint(\"done.\", flush=True)\n\nprint(\"Running betweenness_centrality...\", flush=True, end=\"\")\nst = time.time()\nbc_result = cg.betweenness_centrality(G, k=k)\nprint(f\"done, BC time with {k=} was: {(time.time() - st):.6f} s\")\n```\nWhen you run the new code on the same machine with the same ```k``` value, you can see that it\u2019s over 12x faster:\n```\nbash:~$ python cg_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 7.770531 s\n```\nThis example showcases cuGraph interoperability with NetworkX quite well. However, there are instances that require you to make more significant changes when adding cuGraph to your code.\nMany differences are intentional (different options to better map to GPU implementations for performance reasons, options not supported, and so on), while others are unavoidable (cuGraph has fewer algorithms implemented, cuGraph requires a GPU, and so on). These differences require you to add special case code to convert options or check if the code is running on a cuGraph-compatible system and call the equivalent NetworkX API if they intend to support environments without GPUs or cuGraph.\ncuGraph is an easy-to-use Python library on its own, but it\u2019s not intended to be a drop-in replacement for NetworkX.", "Meanwhile, NetworkX adds dispatching\u2026\nNetworkX has recently added the ability to dispatch API calls to different analytic backends provided by third parties. These backends can provide alternate implementations for various NetworkX APIs that can greatly improve performance.\nBackends can be specified by either an additional ```backend=keyword``` argument on supported APIs or by setting the ```NETWORKX_AUTOMATIC_BACKENDS``` environment variable.\nNetworkX can be configured to raise an informative error or automatically fall back to its default implementation to satisfy the call.\nFigure 2. NetworkX dispatching can call alternate backends or use the default implementation based on user configuration\nIf a NetworkX API call is made and a backend isn\u2019t available to support that call, NetworkX can be configured to raise an informative error or automatically fall back to its default implementation to satisfy the call (Figure 2).\nDispatching is opt-in. Even when backends are installed and available, NetworkX uses the default implementation if you don\u2019t specify one or more backends to use.\nBy enabling other graph libraries to easily extend NetworkX through backends, NetworkX becomes a standard graph analytics frontend. This means more users can use the capabilities of other graph libraries without the learning curve and integration time associated with a new library.\nLibrary maintainers also benefit from NetworkX dispatching because they can reach more users without the overhead of maintaining a user-facing API. Instead, they can just focus on delivering a backend.", "GPU-accelerated NetworkX using nx-cugraph\nNetworkX dispatching opened the door for the RAPIDS cuGraph team to create nx-cugraph, a new project that adds a backend for NetworkX based on the graph analytic engine provided by RAPIDS cuGraph.\nThis approach also enables nx-cugraph to have fewer dependencies and avoid code paths that the cuGraph Python library adds for efficient integration with RAPIDS cuDF, which is not needed for NetworkX.\nWith nx-cugraph, NetworkX users can finally have everything: ease of use, portability between GPU and non-GPU environments, and performance, all without code changes.\nBut maybe best of all, you can now unlock use cases that were not practical before due to excessive runtime, just by adding GPUs and nx-cugraph. For more information, see the benchmark section later in this post.", "Installing nx-cugraph\nAssuming NetworkX version 3.2 or later has been installed, nx-cugraph can be installed using either conda or pip.\nconda\n```\nconda install -c rapidsai-nightly -c conda-forge -c nvidia nx-cugraph\n```\npip\n```\npython -m pip install nx-cugraph-cu11 --extra-index-url https://pypi.nvidia.com\n```\nNightly wheel builds are not available until the 23.12 release, therefore the index URL for the stable release version is being used in the ```pip install``` command.\nFor more information about installing any RAPIDS package, see Quick Local Install.", "Revisiting NetworkX betweenness centrality with nx-cugraph\nWhen you install nx-cugraph and specify the cugraph backend, NetworkX dispatches the ```betweenness_centrality``` call to nx-cugraph. You don\u2019t have to change your code to see the benefits of GPU acceleration.\nThe following runs were done on the same system used in the benchmark section later in this post. These demonstrations also did not include a warmup run, which can improve performance, but the benchmarks shown later did.\nHere\u2019s the initial NetworkX run on the U.S. Patent dataset with k=10:\n```\nbash:~$ python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 97.553809 s\n```\nWith no changes to the code, set the environment variable ```NETWORKX_AUTOMATIC_BACKENDS``` to cugraph to use nx-cugraph for the ```betweenness_centrality``` run and observe a 6.8x speedup:\n```\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 14.286906 s\n```\nLarger ```k``` values result in a significant slowdown for the default NetworkX implementation:\n```\nbash:~$ python nx_bc_demo.py 50\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=50 was: 513.636750 s\n```\nUsing the cugraph backend on the same ```k``` value results in a 31x speedup:\n```\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 50\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=50 was: 16.389574 s\n```\nAs you can see, when you increase ```k```, you see the speedup increase. The larger ```k``` value has little impact on the runtime when using the cugraph backend due to the high parallel processing capability of the GPU.\nIn fact, you can go much higher with ```k``` to increase accuracy with little difference to the overall runtime:\n```\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 500\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=500 was: 18.673590 s\n```\nSetting ```k``` to 500 when using the default NetworkX implementation takes over an hour but adds only a few seconds when using the cugraph backend. For more information, see the next section on benchmarks.", "Benchmarks\nBenchmark results for NetworkX with and without nx-cugraph are shown in Tables 1-3 using the following dataset and system hardware configuration:\nDataset: directed graph, 3.7M nodes, 16.5M edges\nCPU: Intel Xeon Platinum 8480CL, 2TB\nGPU: NVIDIA H100, 80 GB\nThese benchmarks were run using pytest with the pytest-benchmark plugin. Each run includes a warmup step for both NetworkX and nx-cugraph, which improves performance for the measured run.\nThe benchmark code is available in the cuGraph Github repo.\n```nx.betweenness_centrality(G, k=k)``` k=10\nk=20\nk=50\nk=100\nk=500\nk=1000\nNetworkX\n97.28 s\n184.77 s\n463.15 s\n915.84 s\n4,585.96 s\n9,125.48 s\nnx-cugraph\n8.71 s\n8.26 s\n8.91 s\n8.67 s\n11.31 s\n14.37 s\nspeedup\n11.17 X\n22.37 X\n51.96 X\n105.58 X\n405.59 X\n634.99 X\nTable 1. nx.betweenness_centrality: default implementation (NetworkX) vs. cugraph backend (nx-cugraph) ```nx.edge_betweenness_centrality(G, k=k)``` k=10\nk=20\nk=50\nk=100\nk=500\nk=1000\nNetworkX\n112.22 s\n211.52 s\n503.57 s\n993.15 s\n4,937.70 s\n9,858.11 s\nnx-cugraph\n19.62 s\n19.93 s\n21.26 s\n22.48 s\n41.65 s\n57.79 s\nspeedup\n5.72 X\n10.61 X\n23.69 X\n44.19 X\n118.55 X\n170.59 X\nTable 2. nx.edge_betweenness_centrality: default implementation (NetworkX) vs. cugraph backend (nx-cugraph) ```nx.community.louvain_communities(G)``` NetworkX\n2834.86 s\nnx-cugraph\n21.59 s\nspeedup\n131.3 X\nTable 3. nx.community.louvain_communities: default implementation (NetworkX) vs. cugraph backend (nx-cugraph)", "Conclusion\nNetworkX dispatching is a new chapter in the evolution of NetworkX, which will result in the adoption of NetworkX by even more users for use cases not previously feasible.\nInterchangeable, third-party backends enable NetworkX to become a standardized frontend, where you no longer have to rewrite your Python code to use different graph analytic engines. nx-cugraph adds cuGraph-based GPU acceleration and scalability directly to NetworkX, so you can finally have the speed and scalability missing from NetworkX without code changes.\nBecause both NetworkX and nx-cugraph are open-source projects, feedback, suggestions, and contributions are welcome. If there\u2019s something you\u2019d like to see, such as specific algorithms in nx-cugraph or additional dispatchable NetworkX APIs, leave a suggestion with the appropriate GitHub project:\nNetworkX\ncuGraph\nTo learn more about accelerating NetworkX on GPUs with nx-cugraph, register for the AI and Data Science Virtual Summit."], "document_title": "Accelerating NetworkX on NVIDIA GPUs for High Performance Graph Analytics", "document_url": "https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/", "document_date": "2023-11-08T14:00:00", "document_date_modified": "2023-12-07T17:01:54", "document_full_text": "Accelerating NetworkX on NVIDIA GPUs for High Performance Graph Analytics\nNetworkX states in its documentation that it is \u201c\u2026a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\u201d Since its first public release in 2005, it\u2019s become the most popular Python graph analytics library available. This may explain why NetworkX amassed 27M PyPI downloads just in September of 2023.\nHow is NetworkX able to achieve such mass appeal? Are there use cases where NetworkX falls short, and if so, what can be done to address those? I examine these questions and more in this post.\nimg\nWatch the Accelerating NetworkX: the Future of Easy Graph Analytics session replay from the recent AI and Data Science Virtual Summit hosted by NVIDIA.\nNetworkX: Easy graph analytics\nThere are several reasons NetworkX is so popular among data scientists, students, and many others interested in graph analytics. NetworkX is open-source and backed by a large and friendly community, eager to answer questions and help. The code is mature and well-documented, and the package itself is easy to install and requires no additional dependencies. But most of all, NetworkX has a plethora of algorithms that cover something for everyone (including plotting!) with an easy-to-use API.\nWith just a few lines of simple code, you can load and analyze graph data using any of the algorithms provided. Here is an example of finding the shortest weighted path of a simple four-node graph:\nExample of finding the shortest weighted path of a simple four-node graph\nFigure 1. A simple weighted graph with four nodes and four edges\n```\n>>> import networkx as nx\n>>> G = nx.Graph()\n>>> G.add_edge(\"A\", \"B\", weight=4)\n>>> G.add_edge(\"B\", \"D\", weight=2)\n>>> G.add_edge(\"A\", \"C\", weight=3)\n>>> G.add_edge(\"C\", \"D\", weight=4)\n>>> nx.shortest_path(G, \"A\", \"D\", weight=\"weight\")\n['A', 'B', 'D']\n```\nIn just a few lines, easily typed at a Python prompt, you can interactively explore your graph data.\nSo what\u2019s missing?\nWhile NetworkX provides a tremendous amount of usability right out of the box, performance and scalability for medium-to-large-sized networks are far from best-in-class and can significantly limit a data scientist\u2019s productivity.\nTo get an idea of how graph size and algorithm options impact runtime, here\u2019s an interesting analytic that answers questions about a real-world dataset.\nExamining influential U.S. patents using betweenness centrality\nThe , provided by the Stanford Network Analysis Platform (SNAP), is a citation graph of patents granted between 1975 and 1999, totaling 16,522,438 citations. If you know which patents are more central than others, you may get an idea of their relative importance.\nThe citation graph can be processed using the pandas library to create a DataFrame containing graph edges. The DataFrame has two columns: one for the source node and another for the destination node for each edge. NetworkX can then take this DataFrame and create a graph object, which can then be used to run betweenness centrality.\nBetweenness centrality is a metric that quantifies the extent to which nodes act as intermediaries between other nodes, as determined by the number of shortest paths they are a part of. In the context of patent citations, it may be used to measure the extent to which a patent connects other patents.\nUsing NetworkX, you can run ```betweenness_centrality``` to find these central patents. NetworkX selects ```k``` nodes at random for the shortest path analysis used by the betweenness centrality computation. A higher value of ```k``` leads to more accurate results at the cost of increased computation time.\nThe following code example loads the citation graph data, creates a NetworkX graph object, and runs betweenness_centrality.\n```\n###############################################################################\n# Run Betweenness Centrality on a large citation graph using NetworkX\nimport sys\nimport time\n\nimport networkx as nx\nimport pandas as pd\n\nk = int(sys.argv[1])\n\n# Dataset from https://snap.stanford.edu/data/cit-Patents.txt.gz\nprint(\"Reading dataset into Pandas DataFrame as an edgelist...\", flush=True,\n      end=\"\")\npandas_edgelist = pd.read_csv(\n    \"cit-Patents.txt\",\n    skiprows=4,\n    delimiter=\"\\t\",\n    names=[\"src\", \"dst\"],\n    dtype={\"src\": \"int32\", \"dst\": \"int32\"},\n)\nprint(\"done.\", flush=True)\nprint(\"Creating Graph from Pandas DataFrame edgelist...\", flush=True, end=\"\")\nG = nx.from_pandas_edgelist(\n    pandas_edgelist, source=\"src\", target=\"dst\", create_using=nx.DiGraph\n)\nprint(\"done.\", flush=True)\n\nprint(\"Running betweenness_centrality...\", flush=True, end=\"\")\nst = time.time()\nbc_result = nx.betweenness_centrality(G, k=k)\nprint(f\"done, BC time with {k=} was: {(time.time() - st):.6f} s\")\n```\nPass a ```k``` value of 10 when you run the code:\n```\nbash:~$ python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 97.553809 s\n```\nAs you can see, running ```betweenness_centrality``` on a moderately large graph with just a value of ```k=10``` and a fast, modern CPU (Intel Xeon Platinum 8480CL) takes almost 98 seconds. Achieving a higher level of accuracy that aligns with your expectations for a graph of this scale would necessitate significantly increasing the value of ```k```. However, this would result in considerably longer runtimes, as highlighted in the benchmark results later in this post, where execution times extend to several hours.\nRAPIDS cuGraph: Speed and NetworkX interoperability\nThe RAPIDS cuGraph project was created to bridge the gap between fast, scalable, GPU-based graph analytics and NetworkX ease-of-use. For more information, see RAPIDS cuGraph adds NetworkX and DiGraph Compatibility.\ncuGraph was designed with NetworkX interoperability in mind, which can be seen when you replace only the ```betweenness_centrality``` call from the prior example with cuGraph\u2019s ```betweenness_centrality``` and leave the rest of the code as-is.\nThe result is a greater-than-12x speedup with only a few lines of code changed:\n```\n###############################################################################\n# Run Betweenness Centrality on a large citation graph using NetworkX\n# and RAPIDS cuGraph.\n# NOTE: This demonstrates legacy RAPIDS cuGraph/NetworkX interop. THIS CODE IS\n# NOT PORTABLE TO NON-GPU ENVIRONMENTS! Use nx-cugraph to GPU-accelerate\n# NetworkX with no code changes and configurable CPU fallback.\nimport sys\nimport time\n\nimport cugraph as cg\nimport pandas as pd\n\nk = int(sys.argv[1])\n\n# Dataset from https://snap.stanford.edu/data/cit-Patents.txt.gz\nprint(\"Reading dataset into Pandas DataFrame as an edgelist...\", flush=True,\n      end=\"\")\npandas_edgelist = pd.read_csv(\n    \"cit-Patents.txt\",\n    skiprows=4,\n    delimiter=\"\\t\",\n    names=[\"src\", \"dst\"],\n    dtype={\"src\": \"int32\", \"dst\": \"int32\"},\n)\nprint(\"done.\", flush=True)\nprint(\"Creating Graph from Pandas DataFrame edgelist...\", flush=True, end=\"\")\nG = cg.from_pandas_edgelist(\n    pandas_edgelist, source=\"src\", destination=\"dst\", create_using=cg.Graph(directed=True)\n)\nprint(\"done.\", flush=True)\n\nprint(\"Running betweenness_centrality...\", flush=True, end=\"\")\nst = time.time()\nbc_result = cg.betweenness_centrality(G, k=k)\nprint(f\"done, BC time with {k=} was: {(time.time() - st):.6f} s\")\n```\nWhen you run the new code on the same machine with the same ```k``` value, you can see that it\u2019s over 12x faster:\n```\nbash:~$ python cg_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 7.770531 s\n```\nThis example showcases cuGraph interoperability with NetworkX quite well. However, there are instances that require you to make more significant changes when adding cuGraph to your code.\nMany differences are intentional (different options to better map to GPU implementations for performance reasons, options not supported, and so on), while others are unavoidable (cuGraph has fewer algorithms implemented, cuGraph requires a GPU, and so on). These differences require you to add special case code to convert options or check if the code is running on a cuGraph-compatible system and call the equivalent NetworkX API if they intend to support environments without GPUs or cuGraph.\ncuGraph is an easy-to-use Python library on its own, but it\u2019s not intended to be a drop-in replacement for NetworkX.\nMeanwhile, NetworkX adds dispatching\u2026\nNetworkX has recently added the ability to dispatch API calls to different analytic backends provided by third parties. These backends can provide alternate implementations for various NetworkX APIs that can greatly improve performance.\nBackends can be specified by either an additional ```backend=keyword``` argument on supported APIs or by setting the ```NETWORKX_AUTOMATIC_BACKENDS``` environment variable.\nNetworkX can be configured to raise an informative error or automatically fall back to its default implementation to satisfy the call.\nFigure 2. NetworkX dispatching can call alternate backends or use the default implementation based on user configuration\nIf a NetworkX API call is made and a backend isn\u2019t available to support that call, NetworkX can be configured to raise an informative error or automatically fall back to its default implementation to satisfy the call (Figure 2).\nDispatching is opt-in. Even when backends are installed and available, NetworkX uses the default implementation if you don\u2019t specify one or more backends to use.\nBy enabling other graph libraries to easily extend NetworkX through backends, NetworkX becomes a standard graph analytics frontend. This means more users can use the capabilities of other graph libraries without the learning curve and integration time associated with a new library.\nLibrary maintainers also benefit from NetworkX dispatching because they can reach more users without the overhead of maintaining a user-facing API. Instead, they can just focus on delivering a backend.\nGPU-accelerated NetworkX using nx-cugraph\nNetworkX dispatching opened the door for the RAPIDS cuGraph team to create nx-cugraph, a new project that adds a backend for NetworkX based on the graph analytic engine provided by RAPIDS cuGraph.\nThis approach also enables nx-cugraph to have fewer dependencies and avoid code paths that the cuGraph Python library adds for efficient integration with RAPIDS cuDF, which is not needed for NetworkX.\nWith nx-cugraph, NetworkX users can finally have everything: ease of use, portability between GPU and non-GPU environments, and performance, all without code changes.\nBut maybe best of all, you can now unlock use cases that were not practical before due to excessive runtime, just by adding GPUs and nx-cugraph. For more information, see the benchmark section later in this post.\nInstalling nx-cugraph\nAssuming NetworkX version 3.2 or later has been installed, nx-cugraph can be installed using either conda or pip.\nconda\n```\nconda install -c rapidsai-nightly -c conda-forge -c nvidia nx-cugraph\n```\npip\n```\npython -m pip install nx-cugraph-cu11 --extra-index-url https://pypi.nvidia.com\n```\nNightly wheel builds are not available until the 23.12 release, therefore the index URL for the stable release version is being used in the ```pip install``` command.\nFor more information about installing any RAPIDS package, see Quick Local Install.\nRevisiting NetworkX betweenness centrality with nx-cugraph\nWhen you install nx-cugraph and specify the cugraph backend, NetworkX dispatches the ```betweenness_centrality``` call to nx-cugraph. You don\u2019t have to change your code to see the benefits of GPU acceleration.\nThe following runs were done on the same system used in the benchmark section later in this post. These demonstrations also did not include a warmup run, which can improve performance, but the benchmarks shown later did.\nHere\u2019s the initial NetworkX run on the U.S. Patent dataset with k=10:\n```\nbash:~$ python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 97.553809 s\n```\nWith no changes to the code, set the environment variable ```NETWORKX_AUTOMATIC_BACKENDS``` to cugraph to use nx-cugraph for the ```betweenness_centrality``` run and observe a 6.8x speedup:\n```\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 14.286906 s\n```\nLarger ```k``` values result in a significant slowdown for the default NetworkX implementation:\n```\nbash:~$ python nx_bc_demo.py 50\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=50 was: 513.636750 s\n```\nUsing the cugraph backend on the same ```k``` value results in a 31x speedup:\n```\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 50\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=50 was: 16.389574 s\n```\nAs you can see, when you increase ```k```, you see the speedup increase. The larger ```k``` value has little impact on the runtime when using the cugraph backend due to the high parallel processing capability of the GPU.\nIn fact, you can go much higher with ```k``` to increase accuracy with little difference to the overall runtime:\n```\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 500\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=500 was: 18.673590 s\n```\nSetting ```k``` to 500 when using the default NetworkX implementation takes over an hour but adds only a few seconds when using the cugraph backend. For more information, see the next section on benchmarks.\nBenchmarks\nBenchmark results for NetworkX with and without nx-cugraph are shown in Tables 1-3 using the following dataset and system hardware configuration:\nDataset: directed graph, 3.7M nodes, 16.5M edges\nCPU: Intel Xeon Platinum 8480CL, 2TB\nGPU: NVIDIA H100, 80 GB\nThese benchmarks were run using pytest with the pytest-benchmark plugin. Each run includes a warmup step for both NetworkX and nx-cugraph, which improves performance for the measured run.\nThe benchmark code is available in the cuGraph Github repo.\n```nx.betweenness_centrality(G, k=k)``` k=10\nk=20\nk=50\nk=100\nk=500\nk=1000\nNetworkX\n97.28 s\n184.77 s\n463.15 s\n915.84 s\n4,585.96 s\n9,125.48 s\nnx-cugraph\n8.71 s\n8.26 s\n8.91 s\n8.67 s\n11.31 s\n14.37 s\nspeedup\n11.17 X\n22.37 X\n51.96 X\n105.58 X\n405.59 X\n634.99 X\nTable 1. nx.betweenness_centrality: default implementation (NetworkX) vs. cugraph backend (nx-cugraph) ```nx.edge_betweenness_centrality(G, k=k)``` k=10\nk=20\nk=50\nk=100\nk=500\nk=1000\nNetworkX\n112.22 s\n211.52 s\n503.57 s\n993.15 s\n4,937.70 s\n9,858.11 s\nnx-cugraph\n19.62 s\n19.93 s\n21.26 s\n22.48 s\n41.65 s\n57.79 s\nspeedup\n5.72 X\n10.61 X\n23.69 X\n44.19 X\n118.55 X\n170.59 X\nTable 2. nx.edge_betweenness_centrality: default implementation (NetworkX) vs. cugraph backend (nx-cugraph) ```nx.community.louvain_communities(G)``` NetworkX\n2834.86 s\nnx-cugraph\n21.59 s\nspeedup\n131.3 X\nTable 3. nx.community.louvain_communities: default implementation (NetworkX) vs. cugraph backend (nx-cugraph)\nConclusion\nNetworkX dispatching is a new chapter in the evolution of NetworkX, which will result in the adoption of NetworkX by even more users for use cases not previously feasible.\nInterchangeable, third-party backends enable NetworkX to become a standardized frontend, where you no longer have to rewrite your Python code to use different graph analytic engines. nx-cugraph adds cuGraph-based GPU acceleration and scalability directly to NetworkX, so you can finally have the speed and scalability missing from NetworkX without code changes.\nBecause both NetworkX and nx-cugraph are open-source projects, feedback, suggestions, and contributions are welcome. If there\u2019s something you\u2019d like to see, such as specific algorithms in nx-cugraph or additional dispatchable NetworkX APIs, leave a suggestion with the appropriate GitHub project:\nNetworkX\ncuGraph\nTo learn more about accelerating NetworkX on GPUs with nx-cugraph, register for the AI and Data Science Virtual Summit."}], "https://developer.nvidia.com/blog/cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo/": [{"text": "The article discusses the importance of real-time autonomous robot navigation in various industries and the challenges involved in motion generation for manipulators. It introduces cuRobo, a CUDA-accelerated robot motion generation algorithm developed by NVIDIA. cuRobo formulates the motion generation problem as a global optimization problem and leverages GPUs for parallel processing to solve it efficiently. The algorithm first performs collision-free inverse kinematics to find final joint configurations and then optimizes trajectories based on these configurations. cuRobo also includes a GPU-accelerated graph planner for extreme cases and is implemented in PyTorch, allowing for easy customization of cost terms. The results demonstrate that cuRobo can generate motion plans within milliseconds, outperforming existing approaches. The article provides information on cuRobo's implementation, integration with NVIDIA technologies, and the availability of the code for further exploration. Overall, cuRobo offers a fast and efficient solution for complex robot motion generation problems.", "text_components": ["CUDA-Accelerated Robot Motion Generation in Milliseconds with NVIDIA cuRobo\nReal-time autonomous robot navigation powered by a fast motion-generation algorithm can enable applications in several industries such as food and services, warehouse automation, and machine tending. Motion generation for manipulators is extremely challenging, as it requires satisfying complex constraints and minimizing several cost terms.\nIn addition, manipulators can have many articulations, complex link geometries, entire goal regions beyond a single configuration, task constraints, and nontrivial kinematic and torque limitations. Prior approaches mitigated this complexity by first planning collision-free geometric paths and then optimizing them locally for smoother plans.\nHowever, research has increasingly shown that trajectory optimization can be a powerful tool to address problems larger than just trajectory smoothing. Our modern understanding of this robot navigation problem is that it is a large global motion optimization problem.\nFigure 1. cuRobo\u2019s approach to motion generation In the video, the steps in motion generation with cuRobo are visualized, starting with inverse kinematics iterations, followed by iterations of trajectory optimization.\nNVIDIA cuRobo formulates the motion generation problem as a global optimization problem and leverages GPUs to solve it with many parallel seeds. cuRobo first performs collision-free inverse kinematics (IK) to find collision-free final joint configurations, followed by trajectory optimization leveraging the final joint configurations as seeds (Figure 1).\ncuRobo also implements a GPU-accelerated, fast graph planner to use as a seed for trajectory optimization for use in extreme cases.\nFigure 2. cuRobo\u2019s solution to motion generation problems from the motion policy networks and motionbenchmaker datasets In the video, a robot manipulator moves through space avoiding obstacles to reach targets.\ncuRobo is implemented in PyTorch, enabling you to easily implement your own cost terms for motion generation. cuRobo comes with a library of custom robotics CUDA kernels for common and time-consuming tasks. It uses several NVIDIA technologies:\nNVIDIA Warp for mesh distance queries.\nNVIDIA nvblox for signed distance from depth images.\nCUDA Graphs for reducing kernel launch overheads.\nNVIDIA Isaac Sim for rendering and examples.\ncuRobo also runs on the NVIDIA Jetson enabling embedded applications.\nResults show that cuRobo can generate motion plans within 100 ms (median) on NVIDIA AGX Orin. Figure 3 shows an example integration of cuRobo running on an NVIDIA Jetson AGX Orin on a UR10.\nFigure 3. cuRobo generates collision-free minimum-jerk motions In the video, cuRobo generates smooth motion for a UR10 robot rapidly for dynamically appearing targets.\ncuRobo provides CUDA-accelerated implementations of several motion generation components including kinematics, collision checking, inverse kinematics, numerical optimization solvers, trajectory optimization, and motion generation. Results show that cuRobo solves complex problems in milliseconds, significantly faster than existing approaches (Figure 4).\nBar chart shows compute time for Forward Kinematics, Collision Checking, Inverse Kinematics, Collision-Free Inverse Kinematics, Geometric Planning, Motion Generation on NVIDIA RTX 4090, and Motion Generation on NVIDIA AGX Orin.\nFigure 4. Median compute time across different modules", "More resources\ncuRobo code is available at /NVlabs/curobo. For more information, see CuRobo: CUDA Accelerated Robot Library."], "document_title": "CUDA-Accelerated Robot Motion Generation in Milliseconds with NVIDIA cuRobo", "document_url": "https://developer.nvidia.com/blog/cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo/", "document_date": "2023-11-07T22:22:37", "document_date_modified": "2023-12-05T19:04:45", "document_full_text": "CUDA-Accelerated Robot Motion Generation in Milliseconds with NVIDIA cuRobo\nReal-time autonomous robot navigation powered by a fast motion-generation algorithm can enable applications in several industries such as food and services, warehouse automation, and machine tending. Motion generation for manipulators is extremely challenging, as it requires satisfying complex constraints and minimizing several cost terms.\nIn addition, manipulators can have many articulations, complex link geometries, entire goal regions beyond a single configuration, task constraints, and nontrivial kinematic and torque limitations. Prior approaches mitigated this complexity by first planning collision-free geometric paths and then optimizing them locally for smoother plans.\nHowever, research has increasingly shown that trajectory optimization can be a powerful tool to address problems larger than just trajectory smoothing. Our modern understanding of this robot navigation problem is that it is a large global motion optimization problem.\nFigure 1. cuRobo\u2019s approach to motion generation In the video, the steps in motion generation with cuRobo are visualized, starting with inverse kinematics iterations, followed by iterations of trajectory optimization.\nNVIDIA cuRobo formulates the motion generation problem as a global optimization problem and leverages GPUs to solve it with many parallel seeds. cuRobo first performs collision-free inverse kinematics (IK) to find collision-free final joint configurations, followed by trajectory optimization leveraging the final joint configurations as seeds (Figure 1).\ncuRobo also implements a GPU-accelerated, fast graph planner to use as a seed for trajectory optimization for use in extreme cases.\nFigure 2. cuRobo\u2019s solution to motion generation problems from the motion policy networks and motionbenchmaker datasets In the video, a robot manipulator moves through space avoiding obstacles to reach targets.\ncuRobo is implemented in PyTorch, enabling you to easily implement your own cost terms for motion generation. cuRobo comes with a library of custom robotics CUDA kernels for common and time-consuming tasks. It uses several NVIDIA technologies:\nNVIDIA Warp for mesh distance queries.\nNVIDIA nvblox for signed distance from depth images.\nCUDA Graphs for reducing kernel launch overheads.\nNVIDIA Isaac Sim for rendering and examples.\ncuRobo also runs on the NVIDIA Jetson enabling embedded applications.\nResults show that cuRobo can generate motion plans within 100 ms (median) on NVIDIA AGX Orin. Figure 3 shows an example integration of cuRobo running on an NVIDIA Jetson AGX Orin on a UR10.\nFigure 3. cuRobo generates collision-free minimum-jerk motions In the video, cuRobo generates smooth motion for a UR10 robot rapidly for dynamically appearing targets.\ncuRobo provides CUDA-accelerated implementations of several motion generation components including kinematics, collision checking, inverse kinematics, numerical optimization solvers, trajectory optimization, and motion generation. Results show that cuRobo solves complex problems in milliseconds, significantly faster than existing approaches (Figure 4).\nBar chart shows compute time for Forward Kinematics, Collision Checking, Inverse Kinematics, Collision-Free Inverse Kinematics, Geometric Planning, Motion Generation on NVIDIA RTX 4090, and Motion Generation on NVIDIA AGX Orin.\nFigure 4. Median compute time across different modules\nMore resources\ncuRobo code is available at /NVlabs/curobo. For more information, see CuRobo: CUDA Accelerated Robot Library."}], "https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/": [{"text": "NVIDIA's RAPIDS cuDF now allows GPU acceleration for pandas users without requiring any code changes. This new feature, available in the RAPIDS v23.10 release, brings a unified CPU/GPU experience to pandas workflows, addressing challenges previously faced when using cuDF with pandas. By enabling operations to execute on the GPU when possible and on the CPU (using pandas) when needed, cuDF's pandas accelerator mode significantly accelerates processing times, as shown in a benchmark comparing pandas on CPU with cuDF on GPU. This new feature allows data scientists to continue using pandas as their primary tool while accessing top performance for processing large datasets. The pandas accelerator mode is compatible with most third-party libraries and provides a seamless experience for developing, testing, and running code on heterogeneous hardware. With a 150x speed increase demonstrated in benchmarks, data scientists can now enjoy the benefits of GPU acceleration without any code changes required.", "text_components": ["RAPIDS cuDF Accelerates pandas Nearly 150x with Zero Code Changes\nNVIDIA announced that RAPIDS cuDF can now bring GPU acceleration to 9.5M million pandas users without requiring them to change their code.\nimg\nWatch the keynote replay from the recent AI and Data Science Virtual Summit hosted by NVIDIA.\npandas, a flexible and powerful data analysis and manipulation library for Python, is a top choice for data scientists because of its easy-to-use API. However, as dataset sizes grow, it struggles with processing speed and efficiency in CPU-only systems.\nRAPIDS is an open-source suite of GPU-accelerated Python libraries designed to improve data science and analytics pipelines. RAPIDS cuDF is a GPU DataFrame library that provides a pandas-like API for loading, filtering, and manipulating data. In earlier releases of cuDF, it was meant for GPU-only development workflows.\nWith the latest release of RAPIDS v23.10, cuDF now brings accelerated computing to pandas workflows with no code changes through a unified CPU/GPU user experience with its new pandas accelerator mode. It\u2019s available today in the open-source RAPIDS v23.10 release as an open beta and will be supported in NVIDIA AI Enterprise soon.\nVideo 1. Accelerate Pandas by Nearly 150X with RAPIDS cuDF In the video, you can see identical pandas workflows running side-by-side: one uses pandas with CPU-only and the other uses pandas accelerator mode in RAPIDS cuDF.", "Bringing a unified CPU/GPU experience to pandas workflows\ncuDF has always provided users with top DataFrame library performance using a pandas-like API. However, adopting cuDF has sometimes required workarounds:\nWorking around any pandas functionality not yet implemented or supported in cuDF.\nDesigning separate code paths for CPU and GPU execution in codebases that require running on heterogeneous hardware.\nManually switching between cuDF and pandas when interacting with other PyData libraries or organization-specific tooling designed for pandas.\nStarting with the RAPIDS v23.10 release, cuDF now provides a pandas accelerator mode to address these challenges, in addition to the existing GPU-only experience.\nThis feature was built for data scientists who want to continue using pandas as data sizes grow into the gigabytes and pandas performance slows. In cuDF\u2019s pandas accelerator mode, operations execute on the GPU where possible and on the CPU (using pandas) otherwise, synchronizing under the hood as needed. This enables a unified CPU/GPU experience that brings best-in-class performance to your pandas workflows.\nWith the latest release, cuDF now provides the following features:\nZero code change acceleration: Just load the cuDF Jupyter Notebook extension or use the cuDF Python module option.\nThird-party library compatibility: pandas accelerator mode is compatible with most third-party libraries that operate on pandas objects. It will even accelerate pandas operations within these libraries.\nUnified CPU/GPU workflows: Develop, test, and run in production with a single code path, regardless of hardware.\nTo bring GPU acceleration into your pandas workflows in a Jupyter notebook, load the ```cudf.pandas``` extension:\n```\n%load_ext cudf.pandas\nimport pandas as pd\n```\nTo access it when running Python scripts, use the ```cudf.pandas``` module option:\n```\npython -m cudf.pandas script.py\n```", "Bringing top performance to pandas workflows\nAs data sizes scale into the gigabytes, using pandas often becomes challenging due to slower performance, causing some data scientists to grudgingly give up the pandas API they love. With the new RAPIDS cuDF, you can keep using pandas as your primary tool and access the highest performance.\nYou can see this in action by running the pandas portion of the popular DuckDB Database-like Ops Benchmark originally developed by H2o.ai. DuckDB\u2019s benchmark setup compares popular CPU-based DataFrame and SQL engines on a series of common analytics tasks such as joining data together or computing statistical measures on a per-group basis.\nWith 5 GB of data, pandas performance slows to a crawl, taking minutes to perform the series of join and advanced groupby operations.\nHistorically, running this benchmark with cuDF rather than pandas has required changing the code and working around missing functionality. With cuDF\u2019s new pandas accelerator mode, this is no longer an issue. You can run the pandas benchmark code unchanged and achieve significant speedups, using the GPU for most of the operations and the CPU for a small portion to ensure that the workflow succeeds.\nThe results are excellent. The cuDF unified CPU/GPU experience turns minutes of processing into just 1 or 2 seconds with no code change required (Figure 1).\nBar chart shows a 150x speed increase using pandas with RAPIDS cuDF on NVIDIA Grace Hopper. The 10 join operations take only 1 second with RAPIDS cuDF as opposed to 336 seconds with pandas on CPU, while the 10 groupby advanced operations take 2 seconds with RAPIDS cuDF compared to 288 seconds with pandas on CPU.\nFigure 1. Performance comparison between Traditional pandas v1.5 on Intel Xeon Platinum 8480CL CPU and pandas v1.5 with RAPIDS cuDF on NVIDIA Grace Hopper\nFor more information about these benchmark results and how to reproduce them, see the cuDF documentation.", "Conclusion\npandas is the most popular DataFrame library in the Python ecosystem, but it slows down as data sizes grow on CPUs.\nWith cuDF\u2019s pandas accelerator mode now available in open beta as part of the RAPIDS v23.10 release, you can now bring accelerated computing to your pandas workflows without needing to change your code. Based on an analytics benchmark processing a 5 GB dataset, you can achieve 150x faster processing times.\nTake cuDF\u2019s new pandas accelerator mode for a test drive with this detailed walkthrough notebook in a free GPU-enabled environment on Google Colab.\nFor more information, see the cuDF pandas accelerator mode page on the RAPIDS website."], "document_title": "RAPIDS cuDF Accelerates pandas Nearly 150x with Zero Code Changes", "document_url": "https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/", "document_date": "2023-11-08T14:00:00", "document_date_modified": "2023-11-16T22:44:24", "document_full_text": "RAPIDS cuDF Accelerates pandas Nearly 150x with Zero Code Changes\nNVIDIA announced that RAPIDS cuDF can now bring GPU acceleration to 9.5M million pandas users without requiring them to change their code.\nimg\nWatch the keynote replay from the recent AI and Data Science Virtual Summit hosted by NVIDIA.\npandas, a flexible and powerful data analysis and manipulation library for Python, is a top choice for data scientists because of its easy-to-use API. However, as dataset sizes grow, it struggles with processing speed and efficiency in CPU-only systems.\nRAPIDS is an open-source suite of GPU-accelerated Python libraries designed to improve data science and analytics pipelines. RAPIDS cuDF is a GPU DataFrame library that provides a pandas-like API for loading, filtering, and manipulating data. In earlier releases of cuDF, it was meant for GPU-only development workflows.\nWith the latest release of RAPIDS v23.10, cuDF now brings accelerated computing to pandas workflows with no code changes through a unified CPU/GPU user experience with its new pandas accelerator mode. It\u2019s available today in the open-source RAPIDS v23.10 release as an open beta and will be supported in NVIDIA AI Enterprise soon.\nVideo 1. Accelerate Pandas by Nearly 150X with RAPIDS cuDF In the video, you can see identical pandas workflows running side-by-side: one uses pandas with CPU-only and the other uses pandas accelerator mode in RAPIDS cuDF.\nBringing a unified CPU/GPU experience to pandas workflows\ncuDF has always provided users with top DataFrame library performance using a pandas-like API. However, adopting cuDF has sometimes required workarounds:\nWorking around any pandas functionality not yet implemented or supported in cuDF.\nDesigning separate code paths for CPU and GPU execution in codebases that require running on heterogeneous hardware.\nManually switching between cuDF and pandas when interacting with other PyData libraries or organization-specific tooling designed for pandas.\nStarting with the RAPIDS v23.10 release, cuDF now provides a pandas accelerator mode to address these challenges, in addition to the existing GPU-only experience.\nThis feature was built for data scientists who want to continue using pandas as data sizes grow into the gigabytes and pandas performance slows. In cuDF\u2019s pandas accelerator mode, operations execute on the GPU where possible and on the CPU (using pandas) otherwise, synchronizing under the hood as needed. This enables a unified CPU/GPU experience that brings best-in-class performance to your pandas workflows.\nWith the latest release, cuDF now provides the following features:\nZero code change acceleration: Just load the cuDF Jupyter Notebook extension or use the cuDF Python module option.\nThird-party library compatibility: pandas accelerator mode is compatible with most third-party libraries that operate on pandas objects. It will even accelerate pandas operations within these libraries.\nUnified CPU/GPU workflows: Develop, test, and run in production with a single code path, regardless of hardware.\nTo bring GPU acceleration into your pandas workflows in a Jupyter notebook, load the ```cudf.pandas``` extension:\n```\n%load_ext cudf.pandas\nimport pandas as pd\n```\nTo access it when running Python scripts, use the ```cudf.pandas``` module option:\n```\npython -m cudf.pandas script.py\n```\nBringing top performance to pandas workflows\nAs data sizes scale into the gigabytes, using pandas often becomes challenging due to slower performance, causing some data scientists to grudgingly give up the pandas API they love. With the new RAPIDS cuDF, you can keep using pandas as your primary tool and access the highest performance.\nYou can see this in action by running the pandas portion of the popular DuckDB Database-like Ops Benchmark originally developed by H2o.ai. DuckDB\u2019s benchmark setup compares popular CPU-based DataFrame and SQL engines on a series of common analytics tasks such as joining data together or computing statistical measures on a per-group basis.\nWith 5 GB of data, pandas performance slows to a crawl, taking minutes to perform the series of join and advanced groupby operations.\nHistorically, running this benchmark with cuDF rather than pandas has required changing the code and working around missing functionality. With cuDF\u2019s new pandas accelerator mode, this is no longer an issue. You can run the pandas benchmark code unchanged and achieve significant speedups, using the GPU for most of the operations and the CPU for a small portion to ensure that the workflow succeeds.\nThe results are excellent. The cuDF unified CPU/GPU experience turns minutes of processing into just 1 or 2 seconds with no code change required (Figure 1).\nBar chart shows a 150x speed increase using pandas with RAPIDS cuDF on NVIDIA Grace Hopper. The 10 join operations take only 1 second with RAPIDS cuDF as opposed to 336 seconds with pandas on CPU, while the 10 groupby advanced operations take 2 seconds with RAPIDS cuDF compared to 288 seconds with pandas on CPU.\nFigure 1. Performance comparison between Traditional pandas v1.5 on Intel Xeon Platinum 8480CL CPU and pandas v1.5 with RAPIDS cuDF on NVIDIA Grace Hopper\nFor more information about these benchmark results and how to reproduce them, see the cuDF documentation.\nConclusion\npandas is the most popular DataFrame library in the Python ecosystem, but it slows down as data sizes grow on CPUs.\nWith cuDF\u2019s pandas accelerator mode now available in open beta as part of the RAPIDS v23.10 release, you can now bring accelerated computing to your pandas workflows without needing to change your code. Based on an analytics benchmark processing a 5 GB dataset, you can achieve 150x faster processing times.\nTake cuDF\u2019s new pandas accelerator mode for a test drive with this detailed walkthrough notebook in a free GPU-enabled environment on Google Colab.\nFor more information, see the cuDF pandas accelerator mode page on the RAPIDS website."}], "https://developer.nvidia.com/blog/video-exploring-speech-ai-from-research-to-practical-production-applications/": [{"text": "The integration of speech and translation AI is rapidly transforming our daily interactions, from virtual assistants to call centers and augmented reality experiences. Speech AI Day highlighted the latest advancements in speech AI, emphasizing the importance of a unified compatible framework, efficiency through MLOps, rigorous reliability testing, and versatility in handling audio. These key takeaways underscore the need for standardized development frameworks, streamlined model management, thorough testing and validation processes, and the capability to process both verbal and non-verbal audio. The advancements in speech AI research enable the development of multilingual applications that transcend cultural and national boundaries, providing superior user experiences. Resources such as Speech AI Day sessions, an ebook on speech AI, and NVIDIA Riva offer in-depth insights into the latest trends and techniques in speech and translation AI, including ASR, TTS, and NMT. NVIDIA Riva, a GPU-accelerated speech and translation AI, provides automatic speech recognition, text-to-speech, and neural machine translation skills for conversational applications across various platforms.", "text_components": ["Video: Exploring Speech AI from Research to Practical Production Applications\nThe integration of speech and translation AI into our daily lives is rapidly reshaping our interactions, from virtual assistants to call centers and augmented reality experiences. Speech AI Day provided valuable insights into the latest advancements in speech AI, showcasing how this technology addresses real-world challenges.\nIn this first of three Speech AI Day sessions, experts from Carnegie Mellon University, Hippocratic AI, Suno, and Wipro discussed deploying speech AI to maximize business investment.", "Key takeaways\nUnified compatible framework: Establishing a standardized speech AI development framework ensures seamless compatibility between different components. This fosters easier speech AI solutions development and deployment and ultimately enhances the overall quality of speech AI services.\nEfficiency through MLOps: Implementing MLOps streamlines model management from research to production, enabling companies to overcome the challenges associated with transitioning from proof-of-concepts to full-scale production implementations.\nRigorous reliability testing: A thorough testing and validation process is vital for ensuring the accuracy and reliability of speech AI solutions. This involves evaluating the solution\u2019s understanding of various speech types and its ability to handle errors and unexpected inputs effectively.\nVersatility in handling audio: Speech AI\u2019s capability to process both verbal and non-verbal audio expands its utility across diverse applications, enhancing its practicality and applicability.\nVideo: Exploring Speech AI From Research to Practical Production Applications", "Summary\nThe advancements in speech AI research are revolutionizing the development of multilingual applications, allowing concurrent understanding of different languages. Cutting-edge multilingual speech technologies empower you to create applications and deliver superior user experiences transcending cultural and national boundaries.\nFor in-depth insights into the latest trends and techniques in speech and translation AI, including automatic speech recognition (ASR), text-to-speech (TTS), and neural machine translation (NMT), see the following resources:\nSpeech AI Day: Access all three Speech AI Day sessions on-demand, featuring presentations from leading companies such as Motorola and Deloitte.\nSpeech AI Ebook: Get a comprehensive overview of the speech AI landscape, understanding its functionalities and significance across various industries.\nNVIDIA Riva: Dive into NVIDIA Riva, a GPU-accelerated speech and translation AI with automatic speech recognition, text-to-speech, and neural machine translation skills ideal for conversational applications across cloud platforms, on-premises, at the edge, and embedded devices."], "document_title": "Video: Exploring Speech AI from Research to Practical Production Applications", "document_url": "https://developer.nvidia.com/blog/video-exploring-speech-ai-from-research-to-practical-production-applications/", "document_date": "2023-11-07T16:07:22", "document_date_modified": "2023-11-16T19:16:46", "document_full_text": "Video: Exploring Speech AI from Research to Practical Production Applications\nThe integration of speech and translation AI into our daily lives is rapidly reshaping our interactions, from virtual assistants to call centers and augmented reality experiences. Speech AI Day provided valuable insights into the latest advancements in speech AI, showcasing how this technology addresses real-world challenges.\nIn this first of three Speech AI Day sessions, experts from Carnegie Mellon University, Hippocratic AI, Suno, and Wipro discussed deploying speech AI to maximize business investment.\nKey takeaways\nUnified compatible framework: Establishing a standardized speech AI development framework ensures seamless compatibility between different components. This fosters easier speech AI solutions development and deployment and ultimately enhances the overall quality of speech AI services.\nEfficiency through MLOps: Implementing MLOps streamlines model management from research to production, enabling companies to overcome the challenges associated with transitioning from proof-of-concepts to full-scale production implementations.\nRigorous reliability testing: A thorough testing and validation process is vital for ensuring the accuracy and reliability of speech AI solutions. This involves evaluating the solution\u2019s understanding of various speech types and its ability to handle errors and unexpected inputs effectively.\nVersatility in handling audio: Speech AI\u2019s capability to process both verbal and non-verbal audio expands its utility across diverse applications, enhancing its practicality and applicability.\nVideo: Exploring Speech AI From Research to Practical Production Applications\nSummary\nThe advancements in speech AI research are revolutionizing the development of multilingual applications, allowing concurrent understanding of different languages. Cutting-edge multilingual speech technologies empower you to create applications and deliver superior user experiences transcending cultural and national boundaries.\nFor in-depth insights into the latest trends and techniques in speech and translation AI, including automatic speech recognition (ASR), text-to-speech (TTS), and neural machine translation (NMT), see the following resources:\nSpeech AI Day: Access all three Speech AI Day sessions on-demand, featuring presentations from leading companies such as Motorola and Deloitte.\nSpeech AI Ebook: Get a comprehensive overview of the speech AI landscape, understanding its functionalities and significance across various industries.\nNVIDIA Riva: Dive into NVIDIA Riva, a GPU-accelerated speech and translation AI with automatic speech recognition, text-to-speech, and neural machine translation skills ideal for conversational applications across cloud platforms, on-premises, at the edge, and embedded devices."}], "https://developer.nvidia.com/blog/level-up-your-lighting-qa-with-lighting-artist-ted-mebratu/": [{"text": "Ted Mebratu, a veteran-level lighting artist, recently discussed his work on the Rainy Neon Lights scene with NVIDIA, where he pushed real-time lighting to its limits using an NVIDIA RTX 3090Ti and the NVIDIA RTX Branch of Unreal Engine. Mebratu aimed to explore the future of real-time lighting for the next generation of games and graphics by incorporating technologies like RTXDI and DLSS to narrow the gap between artistic control and performance efficiency. He chose RTXDI for this project due to its unlimited runtime light budget and real-time global illumination capabilities, which allowed him to focus on aesthetics without technical limitations. Mebratu found the process of installing and getting up to speed with RTXDI through NvRTX to be smooth, drawing on his experience with building engines from sources like VXGI. Interested individuals can view more of Ted Mebratu's artwork on his ArtStation page and access NVIDIA resources for Unreal Engine developers to enhance their own projects.", "text_components": ["Level Up Your Lighting: Q&A with Lighting Artist Ted Mebratu\nNVIDIA recently caught up with veteran-level lighting artist Ted Mebratu to find out how he pushed real-time lighting to its limits with the Rainy Neon Lights scene created by environment artist Maarten Hof. Using an NVIDIA RTX 3090Ti and the NVIDIA RTX Branch of Unreal Engine (NvRTX ), Mebratu spoke to NVIDIA about what his aspirations were for the scene and pushing the limits of real-time lighting.\nVideo 1. A dynamically lit scene with hundreds of shadow-casting lights What do you want to achieve with this scene?\nMebratu: Ultimately, I wanted to explore the potential future of real-time lighting for the next generation of games and real-time graphics. Most current and upcoming games still need to rely on a prebaked global illumination system to some extent. This is because ray tracing and real-time global illumination remain quite resource-intensive. There\u2019s a significant concern regarding artistic freedom when working with a fully dynamic GI system.\nWith a prebaked lighting setup, you can incorporate thousands of baked lights without impacting performance, except for the bake time. This enables significant artistic control over where your light goes, what it affects, and how far and intensely indirect bounces travel across the scene. However, when using real-time global illumination, your options become constrained because each additional light that you introduce comes with a performance cost.\nTechnologies like RTXDI and DLSS have the potential to narrow this gap, enabling much greater artistic control and capability while maintaining performance efficiency.\nWhy did you choose RTXDI for this project?\nMebratu: I\u2019m always looking to push the limits of real-time lighting with Unreal Engine 5 in my personal projects. When I came across RTXDI, the decision was a no-brainer for me. Having an unlimited runtime light budget combined with a real-time global illumination technique like Lumen creates an incredible combination for achieving results that are near to path-traced quality. For the scene, there were a total of 141 dynamic shadow-casting lights with the attenuation radius set to maximum. This enabled me to focus on the aesthetics and not worry about technical limitations.\nWas installing and getting up to speed with RTXDI through NvRTX a smooth process?\nMebratu: I have experience building engines from sources like voxel-based GI (VXGI), so compiling NvRTX was a smooth process. I compiled the UE5.2 version of the NvRTX branch and did a series of extensive lighting tests and scenarios. Specifically, I selected scenes that were densely populated with numerous light sources and approached the scene lighting from a real-world perspective. Since I didn\u2019t have to worry about faking lighting effects for the sake of performance, I placed point, spot, and area lights with infinite radii and adjusted the sizes and shapes of the lights to closely match the actual shapes of the light sources.", "More resources\nFor more artwork by Ted, check out his ArtStation page. Learn more about NVIDIA resources for Unreal Engine developers and NvRTX training resources, and join the Level Up with NVIDIA webinar series to ask questions directly about your NVIDIA RTX integrations in Unreal Engine."], "document_title": "Level Up Your Lighting: Q&A with Lighting Artist Ted Mebratu", "document_url": "https://developer.nvidia.com/blog/level-up-your-lighting-qa-with-lighting-artist-ted-mebratu/", "document_date": "2023-11-06T22:20:18", "document_date_modified": "2023-11-16T19:16:46", "document_full_text": "Level Up Your Lighting: Q&A with Lighting Artist Ted Mebratu\nNVIDIA recently caught up with veteran-level lighting artist Ted Mebratu to find out how he pushed real-time lighting to its limits with the Rainy Neon Lights scene created by environment artist Maarten Hof. Using an NVIDIA RTX 3090Ti and the NVIDIA RTX Branch of Unreal Engine (NvRTX ), Mebratu spoke to NVIDIA about what his aspirations were for the scene and pushing the limits of real-time lighting.\nVideo 1. A dynamically lit scene with hundreds of shadow-casting lights What do you want to achieve with this scene?\nMebratu: Ultimately, I wanted to explore the potential future of real-time lighting for the next generation of games and real-time graphics. Most current and upcoming games still need to rely on a prebaked global illumination system to some extent. This is because ray tracing and real-time global illumination remain quite resource-intensive. There\u2019s a significant concern regarding artistic freedom when working with a fully dynamic GI system.\nWith a prebaked lighting setup, you can incorporate thousands of baked lights without impacting performance, except for the bake time. This enables significant artistic control over where your light goes, what it affects, and how far and intensely indirect bounces travel across the scene. However, when using real-time global illumination, your options become constrained because each additional light that you introduce comes with a performance cost.\nTechnologies like RTXDI and DLSS have the potential to narrow this gap, enabling much greater artistic control and capability while maintaining performance efficiency.\nWhy did you choose RTXDI for this project?\nMebratu: I\u2019m always looking to push the limits of real-time lighting with Unreal Engine 5 in my personal projects. When I came across RTXDI, the decision was a no-brainer for me. Having an unlimited runtime light budget combined with a real-time global illumination technique like Lumen creates an incredible combination for achieving results that are near to path-traced quality. For the scene, there were a total of 141 dynamic shadow-casting lights with the attenuation radius set to maximum. This enabled me to focus on the aesthetics and not worry about technical limitations.\nWas installing and getting up to speed with RTXDI through NvRTX a smooth process?\nMebratu: I have experience building engines from sources like voxel-based GI (VXGI), so compiling NvRTX was a smooth process. I compiled the UE5.2 version of the NvRTX branch and did a series of extensive lighting tests and scenarios. Specifically, I selected scenes that were densely populated with numerous light sources and approached the scene lighting from a real-world perspective. Since I didn\u2019t have to worry about faking lighting effects for the sake of performance, I placed point, spot, and area lights with infinite radii and adjusted the sizes and shapes of the lights to closely match the actual shapes of the light sources.\nMore resources\nFor more artwork by Ted, check out his ArtStation page. Learn more about NVIDIA resources for Unreal Engine developers and NvRTX training resources, and join the Level Up with NVIDIA webinar series to ask questions directly about your NVIDIA RTX integrations in Unreal Engine."}], "https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/": [{"text": "Large language models (LLMs) are deep learning algorithms trained on massive datasets that can read, write, code, draw, and enhance human creativity across industries. LLMs are used in various sectors, from healthcare to retail, to generate compounds, fix bugs, and provide productivity assistance. Organizations can choose to use existing LLMs, customize pretrained models, or build custom models based on their specific needs. NVIDIA NeMo is a powerful framework that supports building and training custom LLMs with various customization techniques. Connecting LLMs to external data enhances their capabilities, and techniques like retrieval augmented generation improve accuracy and reliability. Keeping LLMs secure and on track is crucial, and tools like NeMo Guardrails help ensure ethical behavior and mitigate biases. Optimizing LLM inference involves techniques like model quantization and hardware acceleration to improve performance. Getting started with LLMs requires evaluating cost, effort, and business objectives and choosing tools and frameworks that align with specific use cases. NVIDIA AI Enterprise and NeMo provide cloud-native solutions for developing, customizing, and deploying LLMs efficiently. NVIDIA Training offers courses to help organizations train their workforce on LLM technology. Overall, effectively leveraging LLMs in enterprise solutions can drive productivity, innovation, and competitive advantage.", "text_components": ["Getting Started with Large Language Models for Enterprise Solutions\nLarge language models (LLMs) are deep learning algorithms that are trained on Internet-scale datasets with hundreds of billions of parameters. LLMs can read, write, code, draw, and augment human creativity to improve productivity across industries and solve the world\u2019s toughest problems.\nLLMs are used in a wide range of industries, from retail to healthcare, and for a wide range of tasks. They learn the language of protein sequences to generate new, viable compounds that can help scientists develop groundbreaking, life-saving vaccines. They help software programmers generate code and fix bugs based on natural language descriptions. And they provide productivity co-pilots so humans can do what they do best\u2014create, question, and understand.\nA prompt is submitted into a large language model and can be leveraged for many different use cases, from content generation to summarization, translation, classification, or chatbots.\nFigure 1. LLMs are used to generate content, summarize, translate, classify, answer questions, and much more\nEffectively leveraging LLMs in enterprise applications and workflows requires understanding key topics such as model selection, customization, optimization, and deployment. This post explores the following enterprise LLM topics:\nHow organizations are using LLMs\nUse, customize, or build an LLM?\nBegin with foundation models\nBuild a custom language model\nConnect an LLM to external data\nKeep LLMs secure and on track\nOptimize LLM inference in production\nGet started using LLMs\nWhether you are a data scientist looking to build custom models or a chief data officer exploring the potential of LLMs for your organization, read on for valuable insights and guidance.", "How organizations are using LLMs\nLLMs are used in a wide variety of applications across industries to efficiently recognize, summarize, translate, predict, and generate text and other forms of content based on knowledge gained from massive datasets. For example, companies are leveraging LLMs to develop chatbot-like interfaces that can support users with customer inquiries, provide personalized recommendations, and assist with internal knowledge management.\nLLMs also have the potential to broaden the reach of AI across industries and enterprises and enable a new wave of research, creativity, and productivity. They can help generate complex solutions to challenging problems in fields such as healthcare and chemistry. LLMs are also used to create reimagined search engines, tutoring chatbots, composition tools, marketing materials, and more.", "Intelligent AI assistants and agents\nCollaboration between ServiceNow and NVIDIA will help drive new levels of automation to fuel productivity and maximize business impact. Generative AI use cases being explored include developing intelligent virtual assistants and agents to help answer user questions and resolve support requests and using generative AI for automatic issue resolution, knowledge-base article generation, and chat summarization.", "Language accessibility\nA consortium in Sweden is developing a state-of-the-art language model with NVIDIA NeMo Megatron and will make it available to any user in the Nordic region. The team aims to train an LLM with a whopping 175 billion parameters that can handle all sorts of language tasks in the Nordic languages of Swedish, Danish, Norwegian, and potentially Icelandic.\nThe project is seen as a strategic asset, a keystone of digital sovereignty in a world that speaks thousands of languages across nearly 200 countries. To learn more, see The King\u2019s Swedish: AI Rewrites the Book in Scandinavia.", "Smarter contact centers\nThe leading mobile operator in South Korea, KT, has developed a billion-parameter LLM using the NVIDIA DGX SuperPOD platform and NVIDIA NeMo framework. NeMo is an end-to-end, cloud-native enterprise framework that provides prebuilt components for building, training, and running custom LLMs.\nKT\u2019s LLM has been used to improve the understanding of the company\u2019s AI-powered speaker, GiGA Genie, which can control TVs, offer real-time traffic updates, and complete other home-assistance tasks based on voice commands. For details, see No Hang Ups With Hangul: KT Trains Smart Speakers, Customer Call Centers With NVIDIA AI.", "Key resources\nWhat Are Large Language Models Used For?\nWhat AI Teams Need to Know About Generative AI\nIntroduction to LLM Agents\nNYU, NVIDIA Collaborate on Large Language Model to Predict Patient Readmission\nStartup \u201cWriter\u201d Pens Generative AI Success Story With NVIDIA NeMo", "Use, customize, or build an LLM?\nOrganizations can choose to use an existing LLM, customize a pretrained LLM, or build a custom LLM from scratch. Using an existing LLM provides a quick and cost-effective solution, while customizing a pretrained LLM enables organizations to tune the model for specific tasks and embed proprietary knowledge. Building an LLM from scratch offers the most flexibility but requires significant expertise and resources.\nNeMo offers a choice of several customization techniques and is optimized for at-scale inference of models for language and image applications, with multi-GPU and multi-node configurations. For more details, see Unlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo.\nNeMo makes generative AI model development easy, cost-effective, and fast for enterprises. It is available across all major clouds, including Google Cloud as part of their A3 instances powered by NVIDIA H100 Tensor Core GPUs to build, customize, and deploy LLMs at scale. To learn more, see Streamline Generative AI Development with NVIDIA NeMo on GPU-Accelerated Google Cloud.\nTo quickly try generative AI models such as Llama 2, Mistral 7B, and Nemotron-3 directly from your browser with an easy-to-use interface, see NVIDIA AI Foundation Models.", "Key resources\nLarge Language Models Explained\nBuilding Generative AI Applications for Enterprise Demands\nBuilding Your First LLM Agent Application\nUnlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo\nNVIDIA AI Foundation Models", "Begin with foundation models\nFoundation models are large AI models trained on enormous quantities of unlabeled data through self-supervised learning. Examples include Llama 2, GPT-3, and Stable Diffusion.\nThe models can handle a wide variety of tasks, such as image classification, natural language processing, and question-answering, with remarkable accuracy.\nThese foundation models are the starting point for building more specialized and sophisticated custom models. Organizations can customize foundation models using domain-specific labeled data to create more accurate and context-aware models for specific use cases.\nFoundation models generate an enormous number of unique responses from a single prompt by generating a probability distribution over all items that could follow the input and then choosing the next output randomly from that distribution. The randomization is amplified by the model\u2019s use of context. Each time the model generates a probability distribution, it considers the last generated item, which means each prediction impacts every prediction that follows.\nNeMo supports NVIDIA-trained foundation models, like Nemotron-3, as well as community models such as Llama 2, Falcon LLM, Mistral 7B, and MPT. You can experience a variety of optimized community and NVIDIA-built foundation models directly from your browser for free on NVIDIA NGC. You can then customize the foundation model using your proprietary enterprise data. This results in a model that is an expert in your business and domain.", "Key resources\nWhat Are Foundation Models?\nNVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs\nBuild Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\nMind the Gap: Large Language Models Get Smarter With Enterprise Data\nNVIDIA AI Foundation Models\nRinging in the Future: NVIDIA and Amdocs Bring Custom Generative AI to Global Telco Industry", "Build a custom language model\nEnterprises will often need custom models to tailor \u200clanguage processing capabilities to their specific use cases and domain knowledge. Custom LLMs enable a business to generate and understand text more efficiently and accurately within a certain industry or organizational context. They empower enterprises to create personalized solutions that align with their brand voice, optimize workflows, provide more precise insights, and deliver enhanced user experiences, ultimately driving a competitive edge in the market.\nNVIDIA NeMo is a powerful framework that provides components for building and training custom LLMs on-premises, across all leading cloud service providers, or in NVIDIA DGX Cloud. It includes a suite of customization techniques from prompt learning to parameter-efficient fine-tuning, to reinforcement learning through human feedback (RLHF). NVIDIA also released a new, open customization technique called SteerLM that allows for tuning during inference.\nWhen training an LLM, there is always the risk of it becoming \u201cgarbage in, garbage out.\u201d A large percentage of the effort is acquiring and curating the data that will be used to train or customize the LLM.\nNeMo Data Curator is a scalable data-curation tool that enables you to curate trillion-token multilingual datasets for pretraining LLMs. The tool allows you to preprocess and deduplicate datasets with exact or fuzzy deduplication, so you can ensure that models are trained on unique documents, potentially leading to greatly reduced training costs.", "Key resources\nNVIDIA NeMo\nNVIDIA DGX Cloud\nMastering LLM Techniques: Customization\nNVIDIA Fast-Tracks Custom Generative AI Model Development for Enterprises\nSilicon Volley: Designers Tap Generative AI for a Chip Assist\nTake the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model\u2019s Responses During Inference\nCurating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator", "Connect an LLM to external data\nConnecting an LLM to external enterprise data sources enhances its capabilities. This enables the LLM to perform more complex tasks and leverage data that has been created since it was last trained.\nRetrieval augmented generation (RAG) is an architecture that provides an LLM with the ability to use current, curated, domain-specific data sources that are easy to add, delete, and update. With RAG, external data sources are processed into vectors (using an embedding model) and placed into a vector database for fast retrieval at inference time. For more information about how to build a production-grade RAG pipeline, see the /GenerativeAIExamples GitHub repo.\nIn addition to reducing computational and financial costs, RAG increases accuracy and enables more reliable and trustworthy AI-powered applications. Accelerating vector search is one of the hottest topics in the AI landscape due to its applications in LLMs and generative AI.\nNVIDIA NeMo Retriever is a semantic-retrieval microservice to help organizations enhance their generative AI applications with enterprise-grade RAG capabilities.", "Key resources\nWhat Is Retrieval-Augmented Generation, aka RAG?\nRAG 101: Demystifying Retrieval-Augmented Generation Pipelines\nRAG 101: Retrieval-Augmented Generation Questions Answered\nBuild Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model\nAccelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT", "Keep LLMs on track and secure\nTo ensure an LLM\u2019s behavior aligns with desired outcomes, it\u2019s important to establish guidelines, monitor its performance, and customize as needed. This involves defining ethical boundaries, addressing biases in training data, and regularly evaluating the model\u2019s outputs against predefined metrics, often in concert with a guardrails capability. For more information, see NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems.\nTo address this need, NVIDIA has developed NeMo Guardrails, an open-source toolkit that helps developers ensure their generative AI applications are accurate, appropriate, and safe. It provides a framework that works with all LLMs, including OpenAI\u2019s ChatGPT, to make it easier for developers to build safe and trustworthy LLM conversational systems that leverage foundation models.\nKeeping LLMs secure is of paramount importance for generative AI-powered applications. NVIDIA has also introduced accelerated Confidential Computing, a groundbreaking security feature that mitigates threats while providing access to the unprecedented acceleration of NVIDIA H100 Tensor Core GPUs for AI workloads. This feature ensures that sensitive data remains secure and protected, even during processing.", "Key resources\nBest Practices for Securing LLM-enabled Applications\nNVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems\nRight on Track\u2014NVIDIA Open-Source Software Helps Developers Add Guardrails to AI Chatbots\nConfidential Compute on NVIDIA Hopper H100", "Optimize LLM inference in production\nOptimizing LLM inference involves techniques such as model quantization, hardware acceleration, and efficient deployment strategies. Model quantization reduces the memory footprint of the model, while hardware acceleration leverages specialized hardware like GPUs for faster inference. Efficient deployment strategies ensure scalability and reliability in production environments.\nNVIDIA TensorRT-LLM is an open-source software library that supercharges large LLM inference on NVIDIA accelerated computing. It enables users to convert their model weights into a new FP8 format and compile their models to take advantage of optimized FP8 kernels with NVIDIA H100 GPUs. TensorRT-LLM can accelerate inference performance by 4.6x compared to NVIDIA A100 GPUs. It provides a faster and more efficient way to run LLMs, making them more accessible and cost-effective.\nThese custom generative AI processes involve pulling together models, frameworks, toolkits, and more. Many of these tools are open source, requiring time and energy to maintain development projects. The process can become incredibly complex and time-consuming, especially when trying to collaborate and deploy across multiple environments and platforms.\nNVIDIA AI Workbench helps simplify this process by providing a single platform for managing data, models, resources, and compute needs. This enables seamless collaboration and deployment for developers to create cost-effective, scalable generative AI models quickly.\nNVIDIA and VMware are working together to transform the modern data center built on VMware Cloud Foundation and bring AI to every enterprise. Using the NVIDIA AI Enterprise suite and NVIDIA\u2019s most advanced GPUs and data processing units (DPUs), VMware customers can securely run modern, accelerated workloads alongside existing enterprise applications on NVIDIA-Certified Systems.", "Key resources\nMastering LLM Techniques: Inference Optimization\nNVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\nNew NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility\nOptimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available\nNew Models, Tools and Resources for Windows Development on RTX PCs\nDevelop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench\nModernizing the Data Center with VMware and NVIDIA", "Get started using LLMs\nGetting started with LLMs requires weighing factors such as cost, effort, training data availability, and business objectives. Organizations should evaluate the trade-offs between using existing models and customizing them with domain-specific knowledge versus building custom models from scratch in most circumstances. Choosing tools and frameworks that align with specific use cases and technical requirements is important, including those listed below.\nThe Generative AI Knowledge Base Chatbot lab \u200cshows you how to adapt an existing AI foundational model to accurately generate responses for your specific use case. This free lab provides hands-on experience with customizing a model using prompt learning, ingesting data into a vector database, and chaining all components to create a chatbot.\nNVIDIA AI Enterprise, available on all major cloud and data center platforms, is a cloud-native suite of AI and data analytics software that provides over 50 frameworks, including the NeMo framework, pretrained models, and development tools optimized for accelerated GPU infrastructures. You can try this end-to-end enterprise-ready software suite is with a free 90-day trial.\nNeMo is an end-to-end, cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. It is optimized for at-scale inference of models with multi-GPU and multi-node configurations. The framework makes generative AI model development easy, cost-effective, and fast for enterprises. Explore the NeMo tutorials to get started.\nNVIDIA Training helps organizations train their workforce on the latest technology and bridge the skills gap by offering comprehensive technical hands-on workshops and courses. The LLM learning path developed by NVIDIA subject matter experts spans fundamental to advanced topics that are relevant to software engineering and IT operations teams. NVIDIA Training Advisors are available to help develop customized training plans and offer team pricing.", "Key resources\nGenerative AI Knowledge Base Chatbot\nAI Chatbot with Retrieval Augmented Generation\nBuilding Intelligent AI Chatbots Using RAG\nNVIDIA AI Enterprise Free Trial\nNVIDIA NeMo Tutorials\nNVIDIA Training", "Summary\nAs enterprises race to keep pace with AI advancements, identifying the best approach for adopting LLMs is essential. Foundation models help jumpstart the development process. Using key tools and environments to efficiently process and store data and customize models can significantly accelerate productivity and advance business goals."], "document_title": "Getting Started with Large Language Models for Enterprise Solutions", "document_url": "https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/", "document_date": "2023-11-07T21:30:00", "document_date_modified": "2024-01-12T21:02:18", "document_full_text": "Getting Started with Large Language Models for Enterprise Solutions\nLarge language models (LLMs) are deep learning algorithms that are trained on Internet-scale datasets with hundreds of billions of parameters. LLMs can read, write, code, draw, and augment human creativity to improve productivity across industries and solve the world\u2019s toughest problems.\nLLMs are used in a wide range of industries, from retail to healthcare, and for a wide range of tasks. They learn the language of protein sequences to generate new, viable compounds that can help scientists develop groundbreaking, life-saving vaccines. They help software programmers generate code and fix bugs based on natural language descriptions. And they provide productivity co-pilots so humans can do what they do best\u2014create, question, and understand.\nA prompt is submitted into a large language model and can be leveraged for many different use cases, from content generation to summarization, translation, classification, or chatbots.\nFigure 1. LLMs are used to generate content, summarize, translate, classify, answer questions, and much more\nEffectively leveraging LLMs in enterprise applications and workflows requires understanding key topics such as model selection, customization, optimization, and deployment. This post explores the following enterprise LLM topics:\nHow organizations are using LLMs\nUse, customize, or build an LLM?\nBegin with foundation models\nBuild a custom language model\nConnect an LLM to external data\nKeep LLMs secure and on track\nOptimize LLM inference in production\nGet started using LLMs\nWhether you are a data scientist looking to build custom models or a chief data officer exploring the potential of LLMs for your organization, read on for valuable insights and guidance.\nHow organizations are using LLMs\nLLMs are used in a wide variety of applications across industries to efficiently recognize, summarize, translate, predict, and generate text and other forms of content based on knowledge gained from massive datasets. For example, companies are leveraging LLMs to develop chatbot-like interfaces that can support users with customer inquiries, provide personalized recommendations, and assist with internal knowledge management.\nLLMs also have the potential to broaden the reach of AI across industries and enterprises and enable a new wave of research, creativity, and productivity. They can help generate complex solutions to challenging problems in fields such as healthcare and chemistry. LLMs are also used to create reimagined search engines, tutoring chatbots, composition tools, marketing materials, and more.\nIntelligent AI assistants and agents\nCollaboration between ServiceNow and NVIDIA will help drive new levels of automation to fuel productivity and maximize business impact. Generative AI use cases being explored include developing intelligent virtual assistants and agents to help answer user questions and resolve support requests and using generative AI for automatic issue resolution, knowledge-base article generation, and chat summarization.\nLanguage accessibility\nA consortium in Sweden is developing a state-of-the-art language model with NVIDIA NeMo Megatron and will make it available to any user in the Nordic region. The team aims to train an LLM with a whopping 175 billion parameters that can handle all sorts of language tasks in the Nordic languages of Swedish, Danish, Norwegian, and potentially Icelandic.\nThe project is seen as a strategic asset, a keystone of digital sovereignty in a world that speaks thousands of languages across nearly 200 countries. To learn more, see The King\u2019s Swedish: AI Rewrites the Book in Scandinavia.\nSmarter contact centers\nThe leading mobile operator in South Korea, KT, has developed a billion-parameter LLM using the NVIDIA DGX SuperPOD platform and NVIDIA NeMo framework. NeMo is an end-to-end, cloud-native enterprise framework that provides prebuilt components for building, training, and running custom LLMs.\nKT\u2019s LLM has been used to improve the understanding of the company\u2019s AI-powered speaker, GiGA Genie, which can control TVs, offer real-time traffic updates, and complete other home-assistance tasks based on voice commands. For details, see No Hang Ups With Hangul: KT Trains Smart Speakers, Customer Call Centers With NVIDIA AI.\nKey resources\nWhat Are Large Language Models Used For?\nWhat AI Teams Need to Know About Generative AI\nIntroduction to LLM Agents\nNYU, NVIDIA Collaborate on Large Language Model to Predict Patient Readmission\nStartup \u201cWriter\u201d Pens Generative AI Success Story With NVIDIA NeMo\nUse, customize, or build an LLM?\nOrganizations can choose to use an existing LLM, customize a pretrained LLM, or build a custom LLM from scratch. Using an existing LLM provides a quick and cost-effective solution, while customizing a pretrained LLM enables organizations to tune the model for specific tasks and embed proprietary knowledge. Building an LLM from scratch offers the most flexibility but requires significant expertise and resources.\nNeMo offers a choice of several customization techniques and is optimized for at-scale inference of models for language and image applications, with multi-GPU and multi-node configurations. For more details, see Unlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo.\nNeMo makes generative AI model development easy, cost-effective, and fast for enterprises. It is available across all major clouds, including Google Cloud as part of their A3 instances powered by NVIDIA H100 Tensor Core GPUs to build, customize, and deploy LLMs at scale. To learn more, see Streamline Generative AI Development with NVIDIA NeMo on GPU-Accelerated Google Cloud.\nTo quickly try generative AI models such as Llama 2, Mistral 7B, and Nemotron-3 directly from your browser with an easy-to-use interface, see NVIDIA AI Foundation Models.\nKey resources\nLarge Language Models Explained\nBuilding Generative AI Applications for Enterprise Demands\nBuilding Your First LLM Agent Application\nUnlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo\nNVIDIA AI Foundation Models\nBegin with foundation models\nFoundation models are large AI models trained on enormous quantities of unlabeled data through self-supervised learning. Examples include Llama 2, GPT-3, and Stable Diffusion.\nThe models can handle a wide variety of tasks, such as image classification, natural language processing, and question-answering, with remarkable accuracy.\nThese foundation models are the starting point for building more specialized and sophisticated custom models. Organizations can customize foundation models using domain-specific labeled data to create more accurate and context-aware models for specific use cases.\nFoundation models generate an enormous number of unique responses from a single prompt by generating a probability distribution over all items that could follow the input and then choosing the next output randomly from that distribution. The randomization is amplified by the model\u2019s use of context. Each time the model generates a probability distribution, it considers the last generated item, which means each prediction impacts every prediction that follows.\nNeMo supports NVIDIA-trained foundation models, like Nemotron-3, as well as community models such as Llama 2, Falcon LLM, Mistral 7B, and MPT. You can experience a variety of optimized community and NVIDIA-built foundation models directly from your browser for free on NVIDIA NGC. You can then customize the foundation model using your proprietary enterprise data. This results in a model that is an expert in your business and domain.\nKey resources\nWhat Are Foundation Models?\nNVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs\nBuild Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\nMind the Gap: Large Language Models Get Smarter With Enterprise Data\nNVIDIA AI Foundation Models\nRinging in the Future: NVIDIA and Amdocs Bring Custom Generative AI to Global Telco Industry\nBuild a custom language model\nEnterprises will often need custom models to tailor \u200clanguage processing capabilities to their specific use cases and domain knowledge. Custom LLMs enable a business to generate and understand text more efficiently and accurately within a certain industry or organizational context. They empower enterprises to create personalized solutions that align with their brand voice, optimize workflows, provide more precise insights, and deliver enhanced user experiences, ultimately driving a competitive edge in the market.\nNVIDIA NeMo is a powerful framework that provides components for building and training custom LLMs on-premises, across all leading cloud service providers, or in NVIDIA DGX Cloud. It includes a suite of customization techniques from prompt learning to parameter-efficient fine-tuning, to reinforcement learning through human feedback (RLHF). NVIDIA also released a new, open customization technique called SteerLM that allows for tuning during inference.\nWhen training an LLM, there is always the risk of it becoming \u201cgarbage in, garbage out.\u201d A large percentage of the effort is acquiring and curating the data that will be used to train or customize the LLM.\nNeMo Data Curator is a scalable data-curation tool that enables you to curate trillion-token multilingual datasets for pretraining LLMs. The tool allows you to preprocess and deduplicate datasets with exact or fuzzy deduplication, so you can ensure that models are trained on unique documents, potentially leading to greatly reduced training costs.\nKey resources\nNVIDIA NeMo\nNVIDIA DGX Cloud\nMastering LLM Techniques: Customization\nNVIDIA Fast-Tracks Custom Generative AI Model Development for Enterprises\nSilicon Volley: Designers Tap Generative AI for a Chip Assist\nTake the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model\u2019s Responses During Inference\nCurating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator\nConnect an LLM to external data\nConnecting an LLM to external enterprise data sources enhances its capabilities. This enables the LLM to perform more complex tasks and leverage data that has been created since it was last trained.\nRetrieval augmented generation (RAG) is an architecture that provides an LLM with the ability to use current, curated, domain-specific data sources that are easy to add, delete, and update. With RAG, external data sources are processed into vectors (using an embedding model) and placed into a vector database for fast retrieval at inference time. For more information about how to build a production-grade RAG pipeline, see the /GenerativeAIExamples GitHub repo.\nIn addition to reducing computational and financial costs, RAG increases accuracy and enables more reliable and trustworthy AI-powered applications. Accelerating vector search is one of the hottest topics in the AI landscape due to its applications in LLMs and generative AI.\nNVIDIA NeMo Retriever is a semantic-retrieval microservice to help organizations enhance their generative AI applications with enterprise-grade RAG capabilities.\nKey resources\nWhat Is Retrieval-Augmented Generation, aka RAG?\nRAG 101: Demystifying Retrieval-Augmented Generation Pipelines\nRAG 101: Retrieval-Augmented Generation Questions Answered\nBuild Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model\nAccelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT\nKeep LLMs on track and secure\nTo ensure an LLM\u2019s behavior aligns with desired outcomes, it\u2019s important to establish guidelines, monitor its performance, and customize as needed. This involves defining ethical boundaries, addressing biases in training data, and regularly evaluating the model\u2019s outputs against predefined metrics, often in concert with a guardrails capability. For more information, see NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems.\nTo address this need, NVIDIA has developed NeMo Guardrails, an open-source toolkit that helps developers ensure their generative AI applications are accurate, appropriate, and safe. It provides a framework that works with all LLMs, including OpenAI\u2019s ChatGPT, to make it easier for developers to build safe and trustworthy LLM conversational systems that leverage foundation models.\nKeeping LLMs secure is of paramount importance for generative AI-powered applications. NVIDIA has also introduced accelerated Confidential Computing, a groundbreaking security feature that mitigates threats while providing access to the unprecedented acceleration of NVIDIA H100 Tensor Core GPUs for AI workloads. This feature ensures that sensitive data remains secure and protected, even during processing.\nKey resources\nBest Practices for Securing LLM-enabled Applications\nNVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems\nRight on Track\u2014NVIDIA Open-Source Software Helps Developers Add Guardrails to AI Chatbots\nConfidential Compute on NVIDIA Hopper H100\nOptimize LLM inference in production\nOptimizing LLM inference involves techniques such as model quantization, hardware acceleration, and efficient deployment strategies. Model quantization reduces the memory footprint of the model, while hardware acceleration leverages specialized hardware like GPUs for faster inference. Efficient deployment strategies ensure scalability and reliability in production environments.\nNVIDIA TensorRT-LLM is an open-source software library that supercharges large LLM inference on NVIDIA accelerated computing. It enables users to convert their model weights into a new FP8 format and compile their models to take advantage of optimized FP8 kernels with NVIDIA H100 GPUs. TensorRT-LLM can accelerate inference performance by 4.6x compared to NVIDIA A100 GPUs. It provides a faster and more efficient way to run LLMs, making them more accessible and cost-effective.\nThese custom generative AI processes involve pulling together models, frameworks, toolkits, and more. Many of these tools are open source, requiring time and energy to maintain development projects. The process can become incredibly complex and time-consuming, especially when trying to collaborate and deploy across multiple environments and platforms.\nNVIDIA AI Workbench helps simplify this process by providing a single platform for managing data, models, resources, and compute needs. This enables seamless collaboration and deployment for developers to create cost-effective, scalable generative AI models quickly.\nNVIDIA and VMware are working together to transform the modern data center built on VMware Cloud Foundation and bring AI to every enterprise. Using the NVIDIA AI Enterprise suite and NVIDIA\u2019s most advanced GPUs and data processing units (DPUs), VMware customers can securely run modern, accelerated workloads alongside existing enterprise applications on NVIDIA-Certified Systems.\nKey resources\nMastering LLM Techniques: Inference Optimization\nNVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\nNew NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility\nOptimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available\nNew Models, Tools and Resources for Windows Development on RTX PCs\nDevelop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench\nModernizing the Data Center with VMware and NVIDIA\nGet started using LLMs\nGetting started with LLMs requires weighing factors such as cost, effort, training data availability, and business objectives. Organizations should evaluate the trade-offs between using existing models and customizing them with domain-specific knowledge versus building custom models from scratch in most circumstances. Choosing tools and frameworks that align with specific use cases and technical requirements is important, including those listed below.\nThe Generative AI Knowledge Base Chatbot lab \u200cshows you how to adapt an existing AI foundational model to accurately generate responses for your specific use case. This free lab provides hands-on experience with customizing a model using prompt learning, ingesting data into a vector database, and chaining all components to create a chatbot.\nNVIDIA AI Enterprise, available on all major cloud and data center platforms, is a cloud-native suite of AI and data analytics software that provides over 50 frameworks, including the NeMo framework, pretrained models, and development tools optimized for accelerated GPU infrastructures. You can try this end-to-end enterprise-ready software suite is with a free 90-day trial.\nNeMo is an end-to-end, cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. It is optimized for at-scale inference of models with multi-GPU and multi-node configurations. The framework makes generative AI model development easy, cost-effective, and fast for enterprises. Explore the NeMo tutorials to get started.\nNVIDIA Training helps organizations train their workforce on the latest technology and bridge the skills gap by offering comprehensive technical hands-on workshops and courses. The LLM learning path developed by NVIDIA subject matter experts spans fundamental to advanced topics that are relevant to software engineering and IT operations teams. NVIDIA Training Advisors are available to help develop customized training plans and offer team pricing.\nKey resources\nGenerative AI Knowledge Base Chatbot\nAI Chatbot with Retrieval Augmented Generation\nBuilding Intelligent AI Chatbots Using RAG\nNVIDIA AI Enterprise Free Trial\nNVIDIA NeMo Tutorials\nNVIDIA Training\nSummary\nAs enterprises race to keep pace with AI advancements, identifying the best approach for adopting LLMs is essential. Foundation models help jumpstart the development process. Using key tools and environments to efficiently process and store data and customize models can significantly accelerate productivity and advance business goals."}], "https://developer.nvidia.com/blog/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/": [{"text": "NVIDIA is hosting the first LLM Developer Day, a virtual event aimed at helping developers enhance their skills in building LLM-based applications and services. The event will cover key technologies, customization techniques, and deployment options for LLMs, as well as provide hands-on guidance and examples. Sessions will explore APIs, self-managed LLMs, and Retrieval Augmented Generation. The event will take place on November 17 and is free to attend. Participants will learn about key LLM techniques, tools, and resources to navigate the rapidly growing generative AI technology ecosystem. Highlights include practical techniques for deploying LLM-powered systems, customizing models for specific applications, and leveraging open LLMs for lower-latency inference. Attendees can also connect with NVIDIA experts in live Q&A sessions and receive a special discount for further training through the NVIDIA Deep Learning Institute. By participating, developers can gain valuable insights and tools to stay ahead in the AI wave.", "text_components": ["Join the First NVIDIA LLM Developer Day: Elevate Your App-Building Skills\nNVIDIA LLM Developer Day is a virtual event providing hands-on guidance for developers exploring and building LLM-based applications and services. You can gain an understanding \u200cof key technologies, their pros and cons, and explore example applications. The sessions also cover how to create, customize, and deploy applications using managed APIs, self-managed LLMs, and Retrieval Augmented Generation.\nThe NVIDIA Deep Learning Institute is hosting the event and sessions on November 17, starting at 8 a.m. PT (5 p.m. CEST). Participation is free of charge.", "Learn about key LLM techniques, tools, and resources\nAs the generative AI technology ecosystem explodes, offering a myriad of choices across different levels of the stack, navigating where to begin is challenging.\nThe sessions at LLM Developer Day are designed to help developers evaluate the starting point for their use case, while providing resources and tools to begin.\nSession highlights include:\nThe Fast Path to Developing with LLMs: Explore practical techniques for deploying LLM-powered systems using popular APIs.\nTailoring LLMs to Your Use Case: Learn how to push the limits of off-the-shelf models and APIs by customizing your models for domain-specific applications.\nRunning Your Own LLM: Discover how to leverage open, commercially licensed LLMs running on commonly available hardware and optimizers for lower-latency and higher-throughput inference, reducing compute needs.\nLive Q&A: Connect with NVIDIA experts and get your questions answered.", "Attend LLM Developer Day\nSave the date, spread the word, and join us on November 17 as we unveil new tools and technologies, and share insights to help you lead the AI wave.\nAs an event participant, you will also receive a special discount for continuing your learning journey through the NVIDIA Deep Learning Institute. Check out our LLM Training paths, with instructor-led hands-on workshops, and earn a certificate upon successful completion of your training."], "document_title": "Join the First NVIDIA LLM Developer Day: Elevate Your App-Building Skills", "document_url": "https://developer.nvidia.com/blog/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/", "document_date": "2023-11-06T21:37:40", "document_date_modified": "2023-11-16T19:16:46", "document_full_text": "Join the First NVIDIA LLM Developer Day: Elevate Your App-Building Skills\nNVIDIA LLM Developer Day is a virtual event providing hands-on guidance for developers exploring and building LLM-based applications and services. You can gain an understanding \u200cof key technologies, their pros and cons, and explore example applications. The sessions also cover how to create, customize, and deploy applications using managed APIs, self-managed LLMs, and Retrieval Augmented Generation.\nThe NVIDIA Deep Learning Institute is hosting the event and sessions on November 17, starting at 8 a.m. PT (5 p.m. CEST). Participation is free of charge.\nLearn about key LLM techniques, tools, and resources\nAs the generative AI technology ecosystem explodes, offering a myriad of choices across different levels of the stack, navigating where to begin is challenging.\nThe sessions at LLM Developer Day are designed to help developers evaluate the starting point for their use case, while providing resources and tools to begin.\nSession highlights include:\nThe Fast Path to Developing with LLMs: Explore practical techniques for deploying LLM-powered systems using popular APIs.\nTailoring LLMs to Your Use Case: Learn how to push the limits of off-the-shelf models and APIs by customizing your models for domain-specific applications.\nRunning Your Own LLM: Discover how to leverage open, commercially licensed LLMs running on commonly available hardware and optimizers for lower-latency and higher-throughput inference, reducing compute needs.\nLive Q&A: Connect with NVIDIA experts and get your questions answered.\nAttend LLM Developer Day\nSave the date, spread the word, and join us on November 17 as we unveil new tools and technologies, and share insights to help you lead the AI wave.\nAs an event participant, you will also receive a special discount for continuing your learning journey through the NVIDIA Deep Learning Institute. Check out our LLM Training paths, with instructor-led hands-on workshops, and earn a certificate upon successful completion of your training."}], "https://developer.nvidia.com/blog/analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim/": [{"text": "FlexSim, a simulation modeling software provider, has developed an NVIDIA Omniverse Connector that allows users to convert models to OpenUSD format for advanced visualization. This collaboration enables engineers, designers, and simulation experts to visualize their models in real time with photoreal quality, aiding decision-making and improving understanding of complex systems. The incorporation of USD into FlexSim's workflow enhances collaboration, data consistency, and interoperability with other CAD packages. The development team used C++ and Python samples to create the Connector, enabling real-time bidirectional collaborative editing of USD Stages. Users can download FlexSim and Omniverse for free and use the Omniverse Connector Properties in FlexSim to export assets to USD. To learn more about developing extensions and applications on Omniverse, users can access resources such as online courses, forums, and validation services. Subscribing to the newsletter and following NVIDIA Omniverse on social media platforms can help users stay updated on the latest developments.", "text_components": ["Analyze, Visualize, and Optimize Real-World Processes with OpenUSD in FlexSim\nFor manufacturing and industrial enterprises, efficiency and precision are essential. To streamline operations, reduce costs, and enhance productivity, companies are turning to digital twins and discrete-event simulation.\nDiscrete-event simulation enables manufacturers to optimize processes by experimenting with different inputs and behaviors that can be modeled and tested step by step.\nFlexSim is a simulation modeling software provider that specializes in discrete event simulation and enables users to analyze, visualize, and optimize real-world processes across various industries. With a powerful toolset for creating and running simulations, it\u2019s being used in industries such as manufacturing, warehousing, and healthcare to improve complex systems and operations.\nRecently, FlexSim developed an NVIDIA Omniverse Connector, enabling engineers, designers, and simulation experts to seamlessly convert FlexSim models to Universal Scene Description (OpenUSD) format.\nNVIDIA Omniverse is a development computing platform that enables developers to build interoperable 3D workflows and tools based on OpenUSD. OpenUSD is an extensible open-source framework for describing, composing, and collaborating within 3D worlds, initially developed by Pixar Animation Studios.\nFlexSim leverages the Omniverse Connector to enable users to visualize their simulation models in real time, with photoreal quality, using the Omniverse platform\u2019s native RTX Renderer. This advanced visualization, now powered by DLSS 3.5, aids in decision-making, as stakeholders gain a better understanding of complex systems through interactive 3D models.\nWith the most recent update to FlexSim, users can now export 3D models and assets to USD, bridging the gap between their simulation data and real-time 3D visualization in Omniverse.\nVideo 1. Export FlexSim models in USD format for real-time, collaborative editing in NVIDIA Omniverse FlexSim has long been a favored tool among conveyor, automated guided vehicle (AGV), and wire-guided industrial robotics system planners. In recent years, the proliferation of free-roaming autonomous mobile robots (AMRs) occupying more manufacturing and warehouse space has spurred the development of FlexSim A* navigation.\nCoupling FlexSim\u2019s advanced material-handling toolkit with collaborative layout tools in Omniverse can bring increased flexibility to the simulation-planning phase and greater visual fidelity to the digital twin operational phase.", "Seamless digital twin collaboration with OpenUSD\nOpenUSD is becoming more widely accepted as a standard across various sectors, including visual effects, architecture, design, robotics, and manufacturing, due to its power and versatility. The incorporation of USD into FlexSim\u2019s workflow has unlocked a range of advantages, benefiting both the FlexSim team and users in various ways.\nOne significant advantage is the enhanced collaboration it enables. FlexSim\u2019s Omniverse Connector streamlines collaboration, offering a platform where multiple teams can work together live in a unified digital twin environment on various aspects of a project. This promotes cross-functional communication and synergy among engineers, designers, and simulation experts.\nData consistency is another critical benefit. FlexSim models often serve as the definitive reference for complex industrial systems. Building support for USD into FlexSim ensures the preservation of not just geometric data, but also essential simulation metadata. This data consistency maintains accuracy throughout the design and simulation phases, ultimately enhancing decision-making.\nIn addition, adopting USD aligns FlexSim with the broader 3D design and engineering community. CAD packages increasingly use USD as a standard exchange format. This interoperability simplifies data exchange and accelerates project workflows, improving collaboration with partners and clients who use these tools.\nFlexSim\u2019s OpenUSD support also includes the ability to use a USD Stage within FlexSim. Located in the 3D Object Library\u2019s Visual area, the USD Stage functions as a container for 3D objects that enables users to load OpenUSD assets into a FlexSim model.\nThe FlexSim user interface, showing a simulation for a conveyor belt in a factory setting\nFigure 1. A USD Stage in FlexSim with a conveyor belt that can be used in Omniverse", "Developing the Connector\nTo develop the Connector, FlexSim\u2019s development team started with the \u201cHello World\u201d C++ and Python sample, which serves as a comprehensive example for developers seeking to build Connectors. The sample program creates a USD Stage on an Omniverse Nucleus server and demonstrates various functionalities, such as:\nCreating a custom mesh and adding it to the stage\nUploading an MDL material and its textures to a Nucleus server\nTweaking skeletal mesh animation data with live updates\nThe team then built a Live Session experience using the Live Session sample to enable multiple FlexSim users to collaborate on the same scene in real time. The sample demonstrates how to implement numerous live session functionalities, including:\nSetting the edit target to the .live layer so changes replicate to other clients\nDisplaying the owner and connected users in a live session\nMerging changes from the .live session back to the root stage\nWith live sessions implemented, users can create or join live sessions with real-time, bidirectional collaborative editing of USD Stages. These stages can include assets from various 3D software tools, enabling cross-functional teams to collaborate on the same scene.\nThey also used Omni CLI to demonstrate how to use the Client Library API to interact with Nucleus.\nOmniverse Live connection of a simulation model in FlexSim and USD Composer.\nFigure 2. A live-sync simulation model in FlexSim and USD Composer", "Get started with FlexSim on Omniverse\nDownload both FlexSim and Omniverse for free. In FlexSim, find Omniverse Connector Properties in the Toolbox under Connectivity. You can choose which properties to export to USD, including meshes, camera, textures, and object property tables.\nScreenshot of the FlexSim UI with Omniverse Connector Properties window for exporting assets to USD.\nFigure 3. Select properties in FlexSim to export to USD To learn more about developing extensions, Connectors, and applications on Omniverse, see Get Started Building on Omniverse.\nTo learn about Universal Scene Description, see the OpenUSD resources. You can also take the self-paced online course, Getting Started with USD for Collaborative 3D Workflows. And join the conversation on the Alliance for OpenUSD (AOUSD) forums and Discord.\nTry the free RunUSD Validation Service to validate the compatibility of your OpenUSD assets and applications against a range of OpenUSD versions and configurations.\nStay up to date on the platform by subscribing to the newsletter and following NVIDIA Omniverse on Instagram, LinkedIn, Medium, Threads and Twitter. For more, check out our forums, Discord server, Twitch and YouTube channels."], "document_title": "Analyze, Visualize, and Optimize Real-World Processes with OpenUSD in FlexSim", "document_url": "https://developer.nvidia.com/blog/analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim/", "document_date": "2023-11-03T15:00:00", "document_date_modified": "2023-11-16T19:37:36", "document_full_text": "Analyze, Visualize, and Optimize Real-World Processes with OpenUSD in FlexSim\nFor manufacturing and industrial enterprises, efficiency and precision are essential. To streamline operations, reduce costs, and enhance productivity, companies are turning to digital twins and discrete-event simulation.\nDiscrete-event simulation enables manufacturers to optimize processes by experimenting with different inputs and behaviors that can be modeled and tested step by step.\nFlexSim is a simulation modeling software provider that specializes in discrete event simulation and enables users to analyze, visualize, and optimize real-world processes across various industries. With a powerful toolset for creating and running simulations, it\u2019s being used in industries such as manufacturing, warehousing, and healthcare to improve complex systems and operations.\nRecently, FlexSim developed an NVIDIA Omniverse Connector, enabling engineers, designers, and simulation experts to seamlessly convert FlexSim models to Universal Scene Description (OpenUSD) format.\nNVIDIA Omniverse is a development computing platform that enables developers to build interoperable 3D workflows and tools based on OpenUSD. OpenUSD is an extensible open-source framework for describing, composing, and collaborating within 3D worlds, initially developed by Pixar Animation Studios.\nFlexSim leverages the Omniverse Connector to enable users to visualize their simulation models in real time, with photoreal quality, using the Omniverse platform\u2019s native RTX Renderer. This advanced visualization, now powered by DLSS 3.5, aids in decision-making, as stakeholders gain a better understanding of complex systems through interactive 3D models.\nWith the most recent update to FlexSim, users can now export 3D models and assets to USD, bridging the gap between their simulation data and real-time 3D visualization in Omniverse.\nVideo 1. Export FlexSim models in USD format for real-time, collaborative editing in NVIDIA Omniverse FlexSim has long been a favored tool among conveyor, automated guided vehicle (AGV), and wire-guided industrial robotics system planners. In recent years, the proliferation of free-roaming autonomous mobile robots (AMRs) occupying more manufacturing and warehouse space has spurred the development of FlexSim A* navigation.\nCoupling FlexSim\u2019s advanced material-handling toolkit with collaborative layout tools in Omniverse can bring increased flexibility to the simulation-planning phase and greater visual fidelity to the digital twin operational phase.\nSeamless digital twin collaboration with OpenUSD\nOpenUSD is becoming more widely accepted as a standard across various sectors, including visual effects, architecture, design, robotics, and manufacturing, due to its power and versatility. The incorporation of USD into FlexSim\u2019s workflow has unlocked a range of advantages, benefiting both the FlexSim team and users in various ways.\nOne significant advantage is the enhanced collaboration it enables. FlexSim\u2019s Omniverse Connector streamlines collaboration, offering a platform where multiple teams can work together live in a unified digital twin environment on various aspects of a project. This promotes cross-functional communication and synergy among engineers, designers, and simulation experts.\nData consistency is another critical benefit. FlexSim models often serve as the definitive reference for complex industrial systems. Building support for USD into FlexSim ensures the preservation of not just geometric data, but also essential simulation metadata. This data consistency maintains accuracy throughout the design and simulation phases, ultimately enhancing decision-making.\nIn addition, adopting USD aligns FlexSim with the broader 3D design and engineering community. CAD packages increasingly use USD as a standard exchange format. This interoperability simplifies data exchange and accelerates project workflows, improving collaboration with partners and clients who use these tools.\nFlexSim\u2019s OpenUSD support also includes the ability to use a USD Stage within FlexSim. Located in the 3D Object Library\u2019s Visual area, the USD Stage functions as a container for 3D objects that enables users to load OpenUSD assets into a FlexSim model.\nThe FlexSim user interface, showing a simulation for a conveyor belt in a factory setting\nFigure 1. A USD Stage in FlexSim with a conveyor belt that can be used in Omniverse\nDeveloping the Connector\nTo develop the Connector, FlexSim\u2019s development team started with the \u201cHello World\u201d C++ and Python sample, which serves as a comprehensive example for developers seeking to build Connectors. The sample program creates a USD Stage on an Omniverse Nucleus server and demonstrates various functionalities, such as:\nCreating a custom mesh and adding it to the stage\nUploading an MDL material and its textures to a Nucleus server\nTweaking skeletal mesh animation data with live updates\nThe team then built a Live Session experience using the Live Session sample to enable multiple FlexSim users to collaborate on the same scene in real time. The sample demonstrates how to implement numerous live session functionalities, including:\nSetting the edit target to the .live layer so changes replicate to other clients\nDisplaying the owner and connected users in a live session\nMerging changes from the .live session back to the root stage\nWith live sessions implemented, users can create or join live sessions with real-time, bidirectional collaborative editing of USD Stages. These stages can include assets from various 3D software tools, enabling cross-functional teams to collaborate on the same scene.\nThey also used Omni CLI to demonstrate how to use the Client Library API to interact with Nucleus.\nOmniverse Live connection of a simulation model in FlexSim and USD Composer.\nFigure 2. A live-sync simulation model in FlexSim and USD Composer\nGet started with FlexSim on Omniverse\nDownload both FlexSim and Omniverse for free. In FlexSim, find Omniverse Connector Properties in the Toolbox under Connectivity. You can choose which properties to export to USD, including meshes, camera, textures, and object property tables.\nScreenshot of the FlexSim UI with Omniverse Connector Properties window for exporting assets to USD.\nFigure 3. Select properties in FlexSim to export to USD To learn more about developing extensions, Connectors, and applications on Omniverse, see Get Started Building on Omniverse.\nTo learn about Universal Scene Description, see the OpenUSD resources. You can also take the self-paced online course, Getting Started with USD for Collaborative 3D Workflows. And join the conversation on the Alliance for OpenUSD (AOUSD) forums and Discord.\nTry the free RunUSD Validation Service to validate the compatibility of your OpenUSD assets and applications against a range of OpenUSD versions and configurations.\nStay up to date on the platform by subscribing to the newsletter and following NVIDIA Omniverse on Instagram, LinkedIn, Medium, Threads and Twitter. For more, check out our forums, Discord server, Twitch and YouTube channels."}], "https://developer.nvidia.com/blog/cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing/": [{"text": "The article discusses the release of CUDA Toolkit 12.3, which offers new features for accelerated computing using NVIDIA GPUs. These features include lazy loading default on Windows, single-step CUDA uninstall on Windows, and enhanced developer tools like NVIDIA Nsight Compute and Nsight Systems. Nsight Compute provides detailed profiling and analysis for CUDA kernels, including a new PM Sampling feature for time-correlated kernel performance data. Nsight Systems offers performance tuning tools for hardware metrics and CUDA applications, with new features such as NIC profiling and support for NVIDIA Grace CPU. The release aims to enhance support for accelerated computing applications in various fields like data science, machine learning, graphics, and scientific computing. Users are encouraged to explore the CUDA documentation, NVIDIA Deep Learning Institute offerings, and NGC Catalog for more information and to engage in discussions in the CUDA Developer Forums. Overall, the CUDA Toolkit 12.3 release advances accelerated computing capabilities and developer tools for NVIDIA GPUs.", "text_components": ["CUDA Toolkit 12.3 Delivers New Features for Accelerated Computing\nThe latest release of CUDA Toolkit continues to push the envelope of accelerated computing performance using the latest NVIDIA GPUs. New features of this release, version 12.3, include:\nLazy loading default on Windows\nSingle-step CUDA uninstall on Windows\nEnhanced NVIDIA Nsight Compute and NVIDIA Nsight Systems developer tools\nCUDA and the CUDA Toolkit continue to provide the foundation for all accelerated computing applications in data science, machine learning and deep learning, generative AI with LLMs for both training and inference, graphics and simulation, and scientific computing. CUDA is fundamental to helping solve the world\u2019s most complex computing problems.", "NVIDIA Nsight Developer Tools\nThe latest versions of NVIDIA Nsight Developer Tools are included in the CUDA Toolkit to help you optimize and debug your CUDA applications on NVIDIA Grace Hopper platforms.", "Nsight Compute\nNsight Compute provides detailed profiling and analysis for CUDA kernels, and version 2023.3 debuts with CUDA Toolkit 12.3. This version includes features that improve performance and data collection and analysis capabilities.\nThe new PM Sampling feature adds time-correlated kernel performance data. Previously, most performance metrics were aggregated across an entire kernel. This frequently requested feature can help users uncover performance issues that occur in phases within a kernel and temporal effects such as the tail effect (Figure 1). It is included in the ```--full``` metric set. It can be added as the PM Sampling section in the GUI, or by adding the ```--section PmSampling``` flag to the CLI.\nScreenshot of Nsight Compute kernel profiler.\nFigure 1. Nsight Compute PM Sampling\nNsight Compute 2023.3 also introduces the ability to compare source code changes across profiles to see how modifications have impacted performance at the source level. To use this feature, set one report as a baseline, and click the Source Comparison button from another report to view highlighted source differences and the associated performance metrics.\nUse the ```\u2013-lineinfo``` flag when compiling the kernel to enable source resolution and if the source file is modified in place. Use the Import Source option or ```--import-source``` flag to preserve the original source code.\nTo learn more about Nsight Compute 2023.3 features, see Getting Started with Nsight Compute.", "Nsight Systems\nCUDA Toolkit 12.3 also includes Nsight Systems 2023.3, a performance tuning tool that profiles hardware metrics and CUDA apps, APIs, and libraries on a unified timeline.\nThe latest version of Nsight Systems introduces support for NVIDIA Grace CPU, enabling you to drill into Grace CPU cycles in the context of your application\u2019s performance. Nsight Systems 2023.3 also adds new features, including network interface card (NIC) profiling from the GUI.\nAs the primary way that data moves between hardware units on a server, understanding internode communication from the network will help diagnose bottlenecks. Nsight Systems monitors NIC throughput, charting the volume of bytes sent and received. Extended NIC wait times are a strong indication that the internode network needs optimization. Nsight Systems can also profile NVIDIA Quantum InfiniBand switch throughput.\nTo learn more about Nsight Systems 2023.3 features, see Getting Started with Nsight Systems. For a deeper dive into how Nsight Systems supports development at data center scale, see Accelerating Data Center and HPC Performance Analysis with NVIDIA Nsight Systems.", "Summary\nThe CUDA Toolkit 12.3 release enriches the foundational NVIDIA driver and runtime software for accelerated computing while continuing to provide enhanced support for the newest NVIDIA GPUs, accelerated libraries, compilers, and developer tools.\nTo learn more, see the CUDA documentation, check out the latest NVIDIA Deep Learning Institute offerings, and browse the NGC Catalog. Ask questions and join the conversation in the CUDA Developer Forums."], "document_title": "CUDA Toolkit 12.3 Delivers New Features for Accelerated Computing", "document_url": "https://developer.nvidia.com/blog/cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing/", "document_date": "2023-11-01T16:00:00", "document_date_modified": "2023-11-02T18:52:56", "document_full_text": "CUDA Toolkit 12.3 Delivers New Features for Accelerated Computing\nThe latest release of CUDA Toolkit continues to push the envelope of accelerated computing performance using the latest NVIDIA GPUs. New features of this release, version 12.3, include:\nLazy loading default on Windows\nSingle-step CUDA uninstall on Windows\nEnhanced NVIDIA Nsight Compute and NVIDIA Nsight Systems developer tools\nCUDA and the CUDA Toolkit continue to provide the foundation for all accelerated computing applications in data science, machine learning and deep learning, generative AI with LLMs for both training and inference, graphics and simulation, and scientific computing. CUDA is fundamental to helping solve the world\u2019s most complex computing problems.\nNVIDIA Nsight Developer Tools\nThe latest versions of NVIDIA Nsight Developer Tools are included in the CUDA Toolkit to help you optimize and debug your CUDA applications on NVIDIA Grace Hopper platforms.\nNsight Compute\nNsight Compute provides detailed profiling and analysis for CUDA kernels, and version 2023.3 debuts with CUDA Toolkit 12.3. This version includes features that improve performance and data collection and analysis capabilities.\nThe new PM Sampling feature adds time-correlated kernel performance data. Previously, most performance metrics were aggregated across an entire kernel. This frequently requested feature can help users uncover performance issues that occur in phases within a kernel and temporal effects such as the tail effect (Figure 1). It is included in the ```--full``` metric set. It can be added as the PM Sampling section in the GUI, or by adding the ```--section PmSampling``` flag to the CLI.\nScreenshot of Nsight Compute kernel profiler.\nFigure 1. Nsight Compute PM Sampling\nNsight Compute 2023.3 also introduces the ability to compare source code changes across profiles to see how modifications have impacted performance at the source level. To use this feature, set one report as a baseline, and click the Source Comparison button from another report to view highlighted source differences and the associated performance metrics.\nUse the ```\u2013-lineinfo``` flag when compiling the kernel to enable source resolution and if the source file is modified in place. Use the Import Source option or ```--import-source``` flag to preserve the original source code.\nTo learn more about Nsight Compute 2023.3 features, see Getting Started with Nsight Compute.\nNsight Systems\nCUDA Toolkit 12.3 also includes Nsight Systems 2023.3, a performance tuning tool that profiles hardware metrics and CUDA apps, APIs, and libraries on a unified timeline.\nThe latest version of Nsight Systems introduces support for NVIDIA Grace CPU, enabling you to drill into Grace CPU cycles in the context of your application\u2019s performance. Nsight Systems 2023.3 also adds new features, including network interface card (NIC) profiling from the GUI.\nAs the primary way that data moves between hardware units on a server, understanding internode communication from the network will help diagnose bottlenecks. Nsight Systems monitors NIC throughput, charting the volume of bytes sent and received. Extended NIC wait times are a strong indication that the internode network needs optimization. Nsight Systems can also profile NVIDIA Quantum InfiniBand switch throughput.\nTo learn more about Nsight Systems 2023.3 features, see Getting Started with Nsight Systems. For a deeper dive into how Nsight Systems supports development at data center scale, see Accelerating Data Center and HPC Performance Analysis with NVIDIA Nsight Systems.\nSummary\nThe CUDA Toolkit 12.3 release enriches the foundational NVIDIA driver and runtime software for accelerated computing while continuing to provide enhanced support for the newest NVIDIA GPUs, accelerated libraries, compilers, and developer tools.\nTo learn more, see the CUDA documentation, check out the latest NVIDIA Deep Learning Institute offerings, and browse the NGC Catalog. Ask questions and join the conversation in the CUDA Developer Forums."}], "https://developer.nvidia.com/blog/how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data/": [{"text": "The article discusses the use of synthetic data to train perception AI models for autonomous mobile robots (AMRs) in warehouse settings. It focuses on training AMRs to detect warehouse pallet jacks using synthetic data generated from a 3D simulation. By varying parameters such as color, texture, and lighting, developers can create diverse datasets to improve model performance. The article outlines a step-by-step process for iterating with synthetic data to enhance the real-world performance of the object detection model. Each iteration involves changing parameters, generating new training data, and validating the model against real data. The team uses NVIDIA Omniverse Replicator and TAO Toolkit for training and visualization. By adding distractors and increasing dataset diversity, the model's precision in detecting pallet jacks improves significantly over multiple iterations. The article also highlights the importance of domain randomization and provides resources for building custom synthetic data generation pipelines with Omniverse Replicator. The ultimate goal is to optimize and deploy the model on NVIDIA Jetson for real-world applications.", "text_components": ["How to Train Autonomous Mobile Robots to Detect Warehouse Pallet Jacks Using Synthetic Data\nSynthetic data can play a key role when training perception AI models that are deployed on autonomous mobile robots (AMRs). This process is becoming increasingly important in manufacturing. For an example of using synthetic data to generate a pretrained model that can detect pallets in a warehouse, see Developing a Pallet Detection Model Using OpenUSD and Synthetic Data.\nThis post explores how to train AMRs to detect warehouse pallet jacks using synthetic data. Pallet jacks are commonly used in warehouses to lift and transport heavy pallets. In a crowded warehouse, it\u2019s important for the AMR to detect and avoid colliding with a pallet jack.\nTo achieve this goal, it\u2019s necessary to train the AI model with a large and diverse set of data under varying lighting conditions and occlusions. Real data can rarely capture the full range of potential scenarios. Synthetic data generation (SDG), which is annotated data generated from a 3D simulation, enables developers to overcome the data gap and bootstrap the model training process.\nVideo 1. Synthetic data generation using NVIDIA Omniverse Replicator for NVIDIA Isaac Sim This use case will again take a data-centric approach by manipulating the data, as opposed to changing the model parameters to fit the data. The process begins by generating synthetic data using NVIDIA Omniverse Replicator in NVIDIA Isaac Sim. Next, train the model with synthetic data in NVIDIA TAO Toolkit. Finally, visualize the model\u2019s performance on real data, and modify the parameters to generate better synthetic data to reach the desired level of performance.\nOmniverse Replicator is a core extension of NVIDIA Omniverse, a computing platform that enables individuals and teams to develop workflows based on Universal Scene Description (OpenUSD). Replicator enables developers to build custom synthetic data generation pipelines to generate data to bootstrap the training of computer vision models.", "Iterating with synthetic data to improve model performance\nThe sections below explain how the team iterated with synthetic data to improve the real-world performance of our object detection model. It walks through the steps using Python scripts that work with the Omniverse Replicator APIs.\nFor each iteration, we incrementally changed various parameters in the model and generated new sets of training data. The model\u2019s performance was then validated against real data. We continued this process until we were able to close the sim-to-real gap.\nThe process of varying object or scene parameters is called domain randomization. You can randomize many parameters, including location, color, texture, background, lighting of objects and scene, allowing you to generate new data quickly for your model training.\nOpenUSD, an extensible framework, 3D scene description, and the foundation for NVIDIA Omniverse, makes it easy to experiment with different parameters of a scene. Parameters can be modified and tested in individual layers, and users can author non-destructive overrides on top of those layers.", "Preparation\nTo get started with this example, you\u2019ll need a system with NVIDIA RTX GPUs and the latest version of NVIDIA Isaac Sim installed. Isaac Sim is a scalable robotics simulation application that leverages the core functionality of Omniverse Replicator for generating synthetic data. For details on installation and configuration, see the documentation section.\nWhen Isaac Sim is up and running, you can then download all the assets from NVIDIA-AI-IOT/synthetic_data_generation_training_workflow on GitHub.", "Iteration 1: Changing color and camera position\nFor the first iteration, the team varied the color and pose of the pallet jack, along with the pose of the camera. Follow the steps below to replicate this scenario in your own session.\nStart by loading the stage:\n```\nENV_URL = \"/Isaac/Environments/Simple_Warehouse/warehouse.usd\"\nopen_stage(prefix_with_isaac_asset_server(ENV_URL))\n```\nThen add pallet jacks and a camera to the scene. The pallet jacks can be loaded from the SimReady Asset library.\n```\nPALLETJACKS = [\"http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Scale_A/PalletTruckScale_A01_PR_NVD_01.usd\",\n            \"http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Heavy_Duty_A/HeavyDutyPalletTruck_A01_PR_NVD_01.usd\",\n            \"http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Low_Profile_A/LowProfilePalletTruck_A01_PR_NVD_01.usd\"]\n\ncam = rep.create.camera(clipping_range=(0.1, 1000000))\n```\nSimReady, or simulation-ready, assets are physically-accurate 3D objects that encompass accurate physical properties and behavior. They are preloaded with the metadata and annotation required for model training.\nNext, add domain randomization for the pallet jacks and the camera:\n```\nwith cam:\n            \trep.modify.pose(position=rep.distribution.uniform((-9.2, -11.8,     0.4), (7.2, 15.8, 4)),look_at=(0, 0, 0))\n\n     # Get the Palletjack body mesh and modify its color\n     with rep.get.prims(path_pattern=\"SteerAxles\"):\n          \trep.randomizer.color(colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1)))\n\n   # Randomize the pose of all the added palletjacks\n   with rep_palletjack_group:\n      rep.modify.pose(\n   position=rep.distribution.uniform((-6, -6, 0), (6, 12, 0)),\n   rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\n   scale=rep.distribution.uniform((0.01, 0.01, 0.01), (0.01, 0.01, 0.01)))\n```\nComposite of three synthetic images showing pallet jacks of different colors and a randomized camera position.\nFigure 1. Synthetic images showing randomized color and location of the pallet jacks and randomized camera position\nFinally, configure writers for annotating data:\n```\nwriter = rep.WriterRegistry.get(\"KittiWriter\") \nwriter.initialize(output_dir=output_directory,\n                    omit_semantic_type=True,)\n```\nNote that this example uses the KittiWriter provided with Replicator to store the annotations in KITTI format for object detection labels. This will ensure easier compatibility with training pipelines.", "Results\nFor this first batch of synthetic data, the team used the LOCO dataset, which is a scene understanding dataset for logistics covering the problem of detecting logistics-specific objects to visualize the real-world model performance.\nThe resulting images show that the model is still trying to detect the pallet jack in a crowded warehouse (Figure 2). Many bounding boxes have been created around objects surrounding the pallet jack. This result is somewhat expected, given that it is the first training iteration. Reducing the domain gap will be a focus for subsequent iterations.\nComposite of real-world images showing many false positives.One image shows two bounding boxes with one pallet jack. Another shows many more objects in addition to the pallet jack.\nFigure 2. Real-world images showing many false positives after validating the model against real data", "Iteration 2: Adding textures and changing ambient lighting\nIn this iteration, the team randomized the texture and the ambient lighting, in addition to the pallet color and camera position from the first iteration.\nActivate the randomization for both textures and lighting:\n```\n# Randomize the lighting of the scene\n    with rep.get.prims(path_pattern=\"RectLight\"):\n     rep.modify.attribute(\"color\", rep.distribution.uniform((0, 0, 0), (1, 1, 1)))\n     rep.modify.attribute(\"intensity\", rep.distribution.normal(100000.0, 600000.0))\n     rep.modify.visibility(rep.distribution.choice([True, False, False, False, False, False, False]))\n\n# select floor material\nrandom_mat_floor = rep.create.material_omnipbr(diffuse_texture=rep.distribution.choice(textures),                                                    roughness=rep.distribution.uniform(0, 1),                                               metallic=rep.distribution.choice([0, 1]),                                                    emissive_texture=rep.distribution.choice(textures),           emissive_intensity=rep.distribution.uniform(0, 1000),)\n        \n        \n     with rep.get.prims(path_pattern=\"SM_Floor\"):\n          rep.randomizer.materials(random_mat_floor)\n```\nFigure 3 shows the resulting synthetic images. Notice the various textures that have been added to the background, along with different types of ambient light incident on the objects.\nComposite of three synthetic images showing pallet jacks with different texture backgrounds.\nFigure 3. Synthetic images of pallet jacks with backgrounds of different textures", "Results\nThis iteration shows a reduction in the number of false positives, with the addition of texture and lighting randomization. One crucial factor when generating synthetic data is to ensure a good diversity of data in the resulting dataset. Similar or repetitive data from the synthetic domain will likely not help to improve \u200creal-world model performance.\nTo improve the diversity of the dataset, add more objects in the scene with randomization. This is addressed in the third iteration and should help improve model robustness.\nComposite of three real-world images showing red bounding boxes around pallet jacks.\nFigure 4. Real-world images showing that the model detects pallet jacks with higher precision after training on randomized texture and lighting images", "Iteration 3: Adding distractors\nThis iteration introduces additional objects, called distractors, into the scene. These distractors add more diversity to the dataset. This iteration also includes all the changes shown in the first two iterations.\nAdd distractors to the scene:\n```\nDISTRACTORS_WAREHOUSE = [\"/Isaac/Environments/Simple_Warehouse/Props/S_TrafficCone.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/S_WetFloorSign.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_01.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_02.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_03.usd\"]\n\n# Modify the pose of all the distractors in the scene\n   with rep_distractor_group:\n        rep.modify.pose(\nposition=rep.distribution.uniform((-6, -6, 0), (6, 12, 0)),\n            rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\n            scale=rep.distribution.uniform(1, 1.5))\n```\nNote that all the assets used in this project are available with the default Isaac Sim installation. Load them by specifying their path on the nucleus server.\nComposite of synthetic images showing pallet jacks surrounded by common warehouse objects such as a caution sign and bottles.\nFigure 5. Synthetic images of pallet jacks surrounded by common warehouse objects (distractors)", "Results\nFigure 6 shows results from the third iteration. The model can accurately detect the pallet jacks, and there are fewer bounding boxes. The model performance has improved significantly compared to the first iteration.\nComposite of real-world images with red bounding boxes around pallet jacks.\nFigure 6. Real-world images showing that the model detects pallet jacks with high precision", "Continue iterating\nThe team used 5,000 images to train the model for each iteration. You can continue to iterate on this workflow by generating more variations, along with increasing the size of your synthetic data, to reach the desired level of accuracy.\nWe used NVIDIA TAO Toolkit to train a DetectNet_v2 model with a resnet18 backbone for these experiments. Using this model is not a workflow requirement. You can leverage the data generated with the annotations to train a model of your architecture and framework choice.\nWe leveraged the KITTI writer in our experiments. However, you can write your own custom writer with Omniverse Replicator to generate data in the correct annotations format. This enables seamless compatibility with your training workflows.\nYou can also experiment with mixing real and synthetic data during your training process. The final model can be optimized and deployed on NVIDIA Jetson in the real world after obtaining satisfactory evaluation metrics.", "Develop synthetic data pipelines with Omniverse Replicator\nWith Omniverse Replicator, you can build your own custom synthetic data generation pipeline or tools to programmatically generate large sets of diverse synthetic data to bootstrap your model, and iterate quickly. Introducing various types of randomizations adds the necessary diversity to the dataset, enabling the model to recognize the object or objects of interest in a variety of conditions.\nTo get started with the workflow featured in this post, visit NVIDIA-AI-IOT/synthetic_data_generation_training_workflow on GitHub. To see the full workflow in action, join Rishabh Chadha of NVIDIA and Jenny Plunkett of Edge Impulse as they showcase how to use Omniverse Replicator and synthetic data to train object detection models for manufacturing processes (Video 2).\nVideo 2. Learn how to train computer vision models with synthetic data To build your own custom synthetic data generation pipeline, download Omniverse free and follow the instructions for getting started with Replicator in Omniverse Code. You can also take the self-paced online course, Synthetic Data Generation for Training Computer Vision Models and watch the latest Omniverse Replicator tutorials.\nNVIDIA recently released Omniverse Replicator 1.10 with new support for developers building low-code SDG workflows. For details, see Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10.\nNVIDIA Isaac ROS 2.0 and NVIDIA Isaac Sim 2023.1 are also now available with major updates to performant perception and high-fidelity simulation. To learn more, see Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform.\nStay up to date with NVIDIA Omniverse by subscribing to the newsletter and following Omniverse on Instagram, LinkedIn, Medium, Threads, and Twitter. For more, check out our forums, Discord server, Twitch and YouTube channels."], "document_title": "How to Train Autonomous Mobile Robots to Detect Warehouse Pallet Jacks Using Synthetic Data", "document_url": "https://developer.nvidia.com/blog/how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data/", "document_date": "2023-10-29T16:55:33", "document_date_modified": "2023-11-02T20:22:40", "document_full_text": "How to Train Autonomous Mobile Robots to Detect Warehouse Pallet Jacks Using Synthetic Data\nSynthetic data can play a key role when training perception AI models that are deployed on autonomous mobile robots (AMRs). This process is becoming increasingly important in manufacturing. For an example of using synthetic data to generate a pretrained model that can detect pallets in a warehouse, see Developing a Pallet Detection Model Using OpenUSD and Synthetic Data.\nThis post explores how to train AMRs to detect warehouse pallet jacks using synthetic data. Pallet jacks are commonly used in warehouses to lift and transport heavy pallets. In a crowded warehouse, it\u2019s important for the AMR to detect and avoid colliding with a pallet jack.\nTo achieve this goal, it\u2019s necessary to train the AI model with a large and diverse set of data under varying lighting conditions and occlusions. Real data can rarely capture the full range of potential scenarios. Synthetic data generation (SDG), which is annotated data generated from a 3D simulation, enables developers to overcome the data gap and bootstrap the model training process.\nVideo 1. Synthetic data generation using NVIDIA Omniverse Replicator for NVIDIA Isaac Sim This use case will again take a data-centric approach by manipulating the data, as opposed to changing the model parameters to fit the data. The process begins by generating synthetic data using NVIDIA Omniverse Replicator in NVIDIA Isaac Sim. Next, train the model with synthetic data in NVIDIA TAO Toolkit. Finally, visualize the model\u2019s performance on real data, and modify the parameters to generate better synthetic data to reach the desired level of performance.\nOmniverse Replicator is a core extension of NVIDIA Omniverse, a computing platform that enables individuals and teams to develop workflows based on Universal Scene Description (OpenUSD). Replicator enables developers to build custom synthetic data generation pipelines to generate data to bootstrap the training of computer vision models.\nIterating with synthetic data to improve model performance\nThe sections below explain how the team iterated with synthetic data to improve the real-world performance of our object detection model. It walks through the steps using Python scripts that work with the Omniverse Replicator APIs.\nFor each iteration, we incrementally changed various parameters in the model and generated new sets of training data. The model\u2019s performance was then validated against real data. We continued this process until we were able to close the sim-to-real gap.\nThe process of varying object or scene parameters is called domain randomization. You can randomize many parameters, including location, color, texture, background, lighting of objects and scene, allowing you to generate new data quickly for your model training.\nOpenUSD, an extensible framework, 3D scene description, and the foundation for NVIDIA Omniverse, makes it easy to experiment with different parameters of a scene. Parameters can be modified and tested in individual layers, and users can author non-destructive overrides on top of those layers.\nPreparation\nTo get started with this example, you\u2019ll need a system with NVIDIA RTX GPUs and the latest version of NVIDIA Isaac Sim installed. Isaac Sim is a scalable robotics simulation application that leverages the core functionality of Omniverse Replicator for generating synthetic data. For details on installation and configuration, see the documentation section.\nWhen Isaac Sim is up and running, you can then download all the assets from NVIDIA-AI-IOT/synthetic_data_generation_training_workflow on GitHub.\nIteration 1: Changing color and camera position\nFor the first iteration, the team varied the color and pose of the pallet jack, along with the pose of the camera. Follow the steps below to replicate this scenario in your own session.\nStart by loading the stage:\n```\nENV_URL = \"/Isaac/Environments/Simple_Warehouse/warehouse.usd\"\nopen_stage(prefix_with_isaac_asset_server(ENV_URL))\n```\nThen add pallet jacks and a camera to the scene. The pallet jacks can be loaded from the SimReady Asset library.\n```\nPALLETJACKS = [\"http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Scale_A/PalletTruckScale_A01_PR_NVD_01.usd\",\n            \"http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Heavy_Duty_A/HeavyDutyPalletTruck_A01_PR_NVD_01.usd\",\n            \"http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Low_Profile_A/LowProfilePalletTruck_A01_PR_NVD_01.usd\"]\n\ncam = rep.create.camera(clipping_range=(0.1, 1000000))\n```\nSimReady, or simulation-ready, assets are physically-accurate 3D objects that encompass accurate physical properties and behavior. They are preloaded with the metadata and annotation required for model training.\nNext, add domain randomization for the pallet jacks and the camera:\n```\nwith cam:\n            \trep.modify.pose(position=rep.distribution.uniform((-9.2, -11.8,     0.4), (7.2, 15.8, 4)),look_at=(0, 0, 0))\n\n     # Get the Palletjack body mesh and modify its color\n     with rep.get.prims(path_pattern=\"SteerAxles\"):\n          \trep.randomizer.color(colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1)))\n\n   # Randomize the pose of all the added palletjacks\n   with rep_palletjack_group:\n      rep.modify.pose(\n   position=rep.distribution.uniform((-6, -6, 0), (6, 12, 0)),\n   rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\n   scale=rep.distribution.uniform((0.01, 0.01, 0.01), (0.01, 0.01, 0.01)))\n```\nComposite of three synthetic images showing pallet jacks of different colors and a randomized camera position.\nFigure 1. Synthetic images showing randomized color and location of the pallet jacks and randomized camera position\nFinally, configure writers for annotating data:\n```\nwriter = rep.WriterRegistry.get(\"KittiWriter\") \nwriter.initialize(output_dir=output_directory,\n                    omit_semantic_type=True,)\n```\nNote that this example uses the KittiWriter provided with Replicator to store the annotations in KITTI format for object detection labels. This will ensure easier compatibility with training pipelines.\nResults\nFor this first batch of synthetic data, the team used the LOCO dataset, which is a scene understanding dataset for logistics covering the problem of detecting logistics-specific objects to visualize the real-world model performance.\nThe resulting images show that the model is still trying to detect the pallet jack in a crowded warehouse (Figure 2). Many bounding boxes have been created around objects surrounding the pallet jack. This result is somewhat expected, given that it is the first training iteration. Reducing the domain gap will be a focus for subsequent iterations.\nComposite of real-world images showing many false positives.One image shows two bounding boxes with one pallet jack. Another shows many more objects in addition to the pallet jack.\nFigure 2. Real-world images showing many false positives after validating the model against real data\nIteration 2: Adding textures and changing ambient lighting\nIn this iteration, the team randomized the texture and the ambient lighting, in addition to the pallet color and camera position from the first iteration.\nActivate the randomization for both textures and lighting:\n```\n# Randomize the lighting of the scene\n    with rep.get.prims(path_pattern=\"RectLight\"):\n     rep.modify.attribute(\"color\", rep.distribution.uniform((0, 0, 0), (1, 1, 1)))\n     rep.modify.attribute(\"intensity\", rep.distribution.normal(100000.0, 600000.0))\n     rep.modify.visibility(rep.distribution.choice([True, False, False, False, False, False, False]))\n\n# select floor material\nrandom_mat_floor = rep.create.material_omnipbr(diffuse_texture=rep.distribution.choice(textures),                                                    roughness=rep.distribution.uniform(0, 1),                                               metallic=rep.distribution.choice([0, 1]),                                                    emissive_texture=rep.distribution.choice(textures),           emissive_intensity=rep.distribution.uniform(0, 1000),)\n        \n        \n     with rep.get.prims(path_pattern=\"SM_Floor\"):\n          rep.randomizer.materials(random_mat_floor)\n```\nFigure 3 shows the resulting synthetic images. Notice the various textures that have been added to the background, along with different types of ambient light incident on the objects.\nComposite of three synthetic images showing pallet jacks with different texture backgrounds.\nFigure 3. Synthetic images of pallet jacks with backgrounds of different textures\nResults\nThis iteration shows a reduction in the number of false positives, with the addition of texture and lighting randomization. One crucial factor when generating synthetic data is to ensure a good diversity of data in the resulting dataset. Similar or repetitive data from the synthetic domain will likely not help to improve \u200creal-world model performance.\nTo improve the diversity of the dataset, add more objects in the scene with randomization. This is addressed in the third iteration and should help improve model robustness.\nComposite of three real-world images showing red bounding boxes around pallet jacks.\nFigure 4. Real-world images showing that the model detects pallet jacks with higher precision after training on randomized texture and lighting images\nIteration 3: Adding distractors\nThis iteration introduces additional objects, called distractors, into the scene. These distractors add more diversity to the dataset. This iteration also includes all the changes shown in the first two iterations.\nAdd distractors to the scene:\n```\nDISTRACTORS_WAREHOUSE = [\"/Isaac/Environments/Simple_Warehouse/Props/S_TrafficCone.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/S_WetFloorSign.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_01.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_02.usd\",\n                            \"/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_03.usd\"]\n\n# Modify the pose of all the distractors in the scene\n   with rep_distractor_group:\n        rep.modify.pose(\nposition=rep.distribution.uniform((-6, -6, 0), (6, 12, 0)),\n            rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\n            scale=rep.distribution.uniform(1, 1.5))\n```\nNote that all the assets used in this project are available with the default Isaac Sim installation. Load them by specifying their path on the nucleus server.\nComposite of synthetic images showing pallet jacks surrounded by common warehouse objects such as a caution sign and bottles.\nFigure 5. Synthetic images of pallet jacks surrounded by common warehouse objects (distractors)\nResults\nFigure 6 shows results from the third iteration. The model can accurately detect the pallet jacks, and there are fewer bounding boxes. The model performance has improved significantly compared to the first iteration.\nComposite of real-world images with red bounding boxes around pallet jacks.\nFigure 6. Real-world images showing that the model detects pallet jacks with high precision\nContinue iterating\nThe team used 5,000 images to train the model for each iteration. You can continue to iterate on this workflow by generating more variations, along with increasing the size of your synthetic data, to reach the desired level of accuracy.\nWe used NVIDIA TAO Toolkit to train a DetectNet_v2 model with a resnet18 backbone for these experiments. Using this model is not a workflow requirement. You can leverage the data generated with the annotations to train a model of your architecture and framework choice.\nWe leveraged the KITTI writer in our experiments. However, you can write your own custom writer with Omniverse Replicator to generate data in the correct annotations format. This enables seamless compatibility with your training workflows.\nYou can also experiment with mixing real and synthetic data during your training process. The final model can be optimized and deployed on NVIDIA Jetson in the real world after obtaining satisfactory evaluation metrics.\nDevelop synthetic data pipelines with Omniverse Replicator\nWith Omniverse Replicator, you can build your own custom synthetic data generation pipeline or tools to programmatically generate large sets of diverse synthetic data to bootstrap your model, and iterate quickly. Introducing various types of randomizations adds the necessary diversity to the dataset, enabling the model to recognize the object or objects of interest in a variety of conditions.\nTo get started with the workflow featured in this post, visit NVIDIA-AI-IOT/synthetic_data_generation_training_workflow on GitHub. To see the full workflow in action, join Rishabh Chadha of NVIDIA and Jenny Plunkett of Edge Impulse as they showcase how to use Omniverse Replicator and synthetic data to train object detection models for manufacturing processes (Video 2).\nVideo 2. Learn how to train computer vision models with synthetic data To build your own custom synthetic data generation pipeline, download Omniverse free and follow the instructions for getting started with Replicator in Omniverse Code. You can also take the self-paced online course, Synthetic Data Generation for Training Computer Vision Models and watch the latest Omniverse Replicator tutorials.\nNVIDIA recently released Omniverse Replicator 1.10 with new support for developers building low-code SDG workflows. For details, see Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10.\nNVIDIA Isaac ROS 2.0 and NVIDIA Isaac Sim 2023.1 are also now available with major updates to performant perception and high-fidelity simulation. To learn more, see Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform.\nStay up to date with NVIDIA Omniverse by subscribing to the newsletter and following Omniverse on Instagram, LinkedIn, Medium, Threads, and Twitter. For more, check out our forums, Discord server, Twitch and YouTube channels."}], "https://developer.nvidia.com/blog/accelerate-genomic-analysis-for-any-sequencer-with-parabricks-v4-2/": [{"text": "NVIDIA has released Parabricks version 4.2, aiming to enhance speed, cost-effectiveness, and accuracy in genomics sequencing analysis. The new version includes accelerated workflows for Oxford Nanopore sequencing and supports the latest NVIDIA GPUs. It also features updated deep learning variant calling initiatives to accommodate data from major sequencer types. The latest release includes upgraded WDL and NextFlow workflows for short and long-read sequencing, with an emphasis on high-speed analysis on NVIDIA H100 GPUs. The new Oxford Nanopore workflow has been benchmarked to analyze a whole genome in under an hour. The DeepVariant variant caller in Parabricks has been optimized for accuracy with accelerated models pretrained for various sequencer data types, achieving over 80x acceleration on NVIDIA GPUs. Moving genomic analysis workflows to GPUs with Parabricks significantly reduces processing time, as shown in Cancer Research UK's TRACERx EVO project. Parabricks v4.2 is now available on NGC, offering support for GPU-accelerated workflows and custom model training. The platform aims to provide universal full-stack acceleration for genomics analysis on GPUs, with containers available for free on NGC.", "text_components": ["Accelerate Genomic Analysis for Any Sequencer with NVIDIA Parabricks v4.2\nParabricks version 4.2 has been released, furthering its mission to deliver unprecedented speed, cost-effectiveness, and accuracy in genomics sequencing analysis. The latest version delivers a newly accelerated workflow for Oxford Nanopore sequencing (in the featured image), enables Parabricks to be run on the latest NVIDIA GPUs, and furthers Parabricks\u2019 accelerated deep learning variant calling initiative to support data types from all major sequencer types.", "Analyzing a long-read whole genome in under an hour\nParabricks v4.2 includes upgraded WDL and NextFlow workflows, as best practices for deploying Parabricks tools, available on the Parabricks Workflows GitHub repo and including both short\u2013 and long-read workflows.\nThis latest release of Parabricks delivers an updated Oxford Nanopore germline workflow, delivering high-speed analysis on NVIDIA H100 GPUs.\nFollowing on from the success of the Ultrarapid Nanopore Analysis Pipeline (UNAP) released by NVIDIA in 2022, this new workflow includes the basecalling, alignment, and small and structural variant calling steps. It has updated software from Guppy to Dorado, and from PEPPER-MARGIN-DeepVariant to the newly integrated long-read variant calling of DeepVariant 1.5, deployed with Parabricks v4.2.\nFigure 1 shows the workflow for the Oxford Nanopore germline sequencing analysis.\nSchematic shows basecalling and integrated alignment with Dorado/Minimap2, small variant calling with DeepVariant in Parabricks, and structural variant calling with Sniffles2.\nFigure 1. Workflow schematic for analysis of Oxford Nanopore germline sequencing data\nThis latest Oxford Nanopore workflow was recently benchmarked by Oracle Cloud on eight NVIDIA H100 GPUs, achieving an end-to-end runtime of under an hour on a single 55x coverage whole genome.\nHigh-speed Oxford Nanopore sequencing and Parabricks workflows also have the potential to provide rapid turnaround times to clinical sequencing.\nIn terms of ongoing development, this workflow will be further benchmarked and optimized as part of a collaboration between NVIDIA and the Clinical Long-read Genome Initiative (lonGER) consortium. This consists of four institutes across Germany aimed at optimizing the analysis of nanopore data both in time to results and in clinical-grade accuracy of methods, to identify the most relevant clinical genomic alterations.\nThe National Institutes of Health Center for Alzheimer\u2019s and Related Dementias (CARD) has developed a protocol for highly accurate, whole-genome sequencing at scale. That example study, among others, shows how Oxford Nanopore sequencing and rapid analysis can provide a comprehensive view of haplotype-resolved variation and methylation.\nIn a recent Nature Methods paper, the CARD team described how this makes large-scale, long, native DNA sequencing projects feasible due to the lower cost and higher throughput of Oxford Nanopore\u2019s PromethION when compared with alternative sequencing methods.", "High-accuracy variant calling for all sequencers with optimized DeepVariant models\nDeepVariant, the CNN-based, high-accuracy germline variant caller, is accelerated on GPUs as part of Parabricks.\nRecently, Parabricks v4.1 introduced an accelerated framework for re-training the underlying CNN model, to more easily enable custom models, and bring more accurate variant calls to analysis workflows. This brings greater accuracy by learning the error profiles of different sequencers, or the unique artifacts introduced in different high-throughput labs.\nParabricks v4.2 now comes with accelerated models pretrained for a variety of sequencer data types, as part of DeepVariant in Parabricks:\nIllumina\nOxford Nanopore\nPacBio\nUltima\nSingular\n\u2026and more\nThe acceleration factors of these models can reach over 80x acceleration, from hours on CPU instances to under 4 minutes on NVIDIA GPUs.\nBenchmarks shown are for a single HG002 whole genome sequencing sample from different sequencer types. Oxford Nanopore reference sample was sequenced to a higher depth (~55x).\nFigure 2. Runtime performance of DeepVariant in Parabricks on the NVIDIA DGX A100 compared to a cpu-only M5.24xlarge (96 vCPU cores) instance", "Unprecedented speed on NVIDIA GPUs\nIn high-throughput settings, moving genomic analysis workflows to GPU with Parabricks results in hugely reduced processing time.\nOne example of this is in Cancer Research UK\u2019s TRACERx EVO, the latest project of TRACERx, which is the world\u2019s largest long-term lung cancer research program, and is driven by infrastructure at the Francis Crick Institute, University College London, and the University of Manchester.\nInitial results from the Francis Crick Institute show that the end-to-end analysis of whole human genomes (including FastQ alignment and deep variant calling) can be done in just over 2 hours with NVIDIA Parabricks, compared to approximately 13 hours on their NEMO CPU cluster. This performance gain is anticipated to be pushed even further on their latest GPU cluster.\nFor the TRACERx EVO project alone, they estimate this will save nearly 9 years of bioinformatics processing time, an improvement described as a \u201cgame-changer in terms of the feasibility of the analysis pipelines for the project,\u201d by Mark S. Hill, principal research fellow at TRACERx EVO.\nFor the latest GPU architectures, the newest NVIDIA Hopper architecture has been dubbed the engine of the world\u2019s AI infrastructure, with an order-of-magnitude performance leap for a diverse range of workloads.\nHigh-performance computing applications being run in data centers benefit from NVIDIA Hopper\u2019s multi-GPU scalability, and its advancements in tensor core technology, meaning impressive results such as 30x acceleration in AI inference over previous generations.\nFor genomics specifically, NVIDIA Hopper architectures include new dynamic programming instructions (DPX) designed to solve complex recursive problems. Dynamic programming is used commonly across multiple fields such as in graph analytics or in route optimizations. This includes in genomics with the Smith-Waterman algorithm, which underlies most aligners and multiple variant callers. The new DPX instructions accelerate these algorithms by 40x compared to CPU-only, and 7x compared to the previous NVIDIA Ampere architecture.\nCombining all these advances means that the latest NVIDIA GPU architectures are incredibly well-suited to accelerate bioinformatics tools like the BWA-MEM aligner, which can run in just 8 minutes on eight NVIDIA H100 GPUs, or the deep learning\u2013based DeepVariant variant caller, which can run in just 3 minutes on eight H100 GPUs. These runtimes mean an end-to-end germline workflow can be achieved in just 14 minutes with H100 GPUs and Parabricks.\nBenchmarks shown are for a single 30x HG002 whole genome Illumina sequencing sample, run with Parabricks DeepVariant germline pipeline.\nFigure 3. Runtime performance of the Parabricks germline workflow on 8 x NVIDIA H100 GPUs compared to a cpu-only M5.24xlarge (96 vCPU cores) instance", "NVIDIA Parabricks v4.2 now available on NGC\nParabricks v4.2 integrates seamlessly into genomics workflows, with continued support for GPU-accelerated versions of well-established workflows with tools like BWA-MEM, and GATK and the ability to quickly train custom models for DeepVariant variant calling. In providing these for new GPU architectures, and across both short\u2013 and long-read sequencing devices, Parabricks is a truly universal full-stack acceleration platform, for gold-standard genomics analysis on GPU.\nThe Parabricks v4.2 container is freely available now under the NVIDIA Parabricks Collection on NGC. For WDL and NextFlow reference workflows, see the Parabricks Workflows GitHub repo.\nFor more information about Parabricks, see Whole Genome Sequencing Analysis, which includes customer success stories with analysis at scale, deployment in sequencers and devices, and cutting-edge research.\nIf you require enterprise support, contact NVIDIA sales to access enterprise benefits, including access to NVIDIA experts to ensure optimization at scale, guaranteed critical support response times, and enterprise training services\nFor more information about what\u2019s new, tutorials, and deployment guides for cloud service providers, see the Parabricks documentation.\nFor more information about scaling sequencing analysis with Parabricks, see the NVIDIA DGX BasePOD solutions for genomic sequencing whitepaper."], "document_title": "Accelerate Genomic Analysis for Any Sequencer with NVIDIA Parabricks v4.2", "document_url": "https://developer.nvidia.com/blog/accelerate-genomic-analysis-for-any-sequencer-with-parabricks-v4-2/", "document_date": "2023-10-28T17:35:57", "document_date_modified": "2023-11-02T20:22:18", "document_full_text": "Accelerate Genomic Analysis for Any Sequencer with NVIDIA Parabricks v4.2\nParabricks version 4.2 has been released, furthering its mission to deliver unprecedented speed, cost-effectiveness, and accuracy in genomics sequencing analysis. The latest version delivers a newly accelerated workflow for Oxford Nanopore sequencing (in the featured image), enables Parabricks to be run on the latest NVIDIA GPUs, and furthers Parabricks\u2019 accelerated deep learning variant calling initiative to support data types from all major sequencer types.\nAnalyzing a long-read whole genome in under an hour\nParabricks v4.2 includes upgraded WDL and NextFlow workflows, as best practices for deploying Parabricks tools, available on the Parabricks Workflows GitHub repo and including both short\u2013 and long-read workflows.\nThis latest release of Parabricks delivers an updated Oxford Nanopore germline workflow, delivering high-speed analysis on NVIDIA H100 GPUs.\nFollowing on from the success of the Ultrarapid Nanopore Analysis Pipeline (UNAP) released by NVIDIA in 2022, this new workflow includes the basecalling, alignment, and small and structural variant calling steps. It has updated software from Guppy to Dorado, and from PEPPER-MARGIN-DeepVariant to the newly integrated long-read variant calling of DeepVariant 1.5, deployed with Parabricks v4.2.\nFigure 1 shows the workflow for the Oxford Nanopore germline sequencing analysis.\nSchematic shows basecalling and integrated alignment with Dorado/Minimap2, small variant calling with DeepVariant in Parabricks, and structural variant calling with Sniffles2.\nFigure 1. Workflow schematic for analysis of Oxford Nanopore germline sequencing data\nThis latest Oxford Nanopore workflow was recently benchmarked by Oracle Cloud on eight NVIDIA H100 GPUs, achieving an end-to-end runtime of under an hour on a single 55x coverage whole genome.\nHigh-speed Oxford Nanopore sequencing and Parabricks workflows also have the potential to provide rapid turnaround times to clinical sequencing.\nIn terms of ongoing development, this workflow will be further benchmarked and optimized as part of a collaboration between NVIDIA and the Clinical Long-read Genome Initiative (lonGER) consortium. This consists of four institutes across Germany aimed at optimizing the analysis of nanopore data both in time to results and in clinical-grade accuracy of methods, to identify the most relevant clinical genomic alterations.\nThe National Institutes of Health Center for Alzheimer\u2019s and Related Dementias (CARD) has developed a protocol for highly accurate, whole-genome sequencing at scale. That example study, among others, shows how Oxford Nanopore sequencing and rapid analysis can provide a comprehensive view of haplotype-resolved variation and methylation.\nIn a recent Nature Methods paper, the CARD team described how this makes large-scale, long, native DNA sequencing projects feasible due to the lower cost and higher throughput of Oxford Nanopore\u2019s PromethION when compared with alternative sequencing methods.\nHigh-accuracy variant calling for all sequencers with optimized DeepVariant models\nDeepVariant, the CNN-based, high-accuracy germline variant caller, is accelerated on GPUs as part of Parabricks.\nRecently, Parabricks v4.1 introduced an accelerated framework for re-training the underlying CNN model, to more easily enable custom models, and bring more accurate variant calls to analysis workflows. This brings greater accuracy by learning the error profiles of different sequencers, or the unique artifacts introduced in different high-throughput labs.\nParabricks v4.2 now comes with accelerated models pretrained for a variety of sequencer data types, as part of DeepVariant in Parabricks:\nIllumina\nOxford Nanopore\nPacBio\nUltima\nSingular\n\u2026and more\nThe acceleration factors of these models can reach over 80x acceleration, from hours on CPU instances to under 4 minutes on NVIDIA GPUs.\nBenchmarks shown are for a single HG002 whole genome sequencing sample from different sequencer types. Oxford Nanopore reference sample was sequenced to a higher depth (~55x).\nFigure 2. Runtime performance of DeepVariant in Parabricks on the NVIDIA DGX A100 compared to a cpu-only M5.24xlarge (96 vCPU cores) instance\nUnprecedented speed on NVIDIA GPUs\nIn high-throughput settings, moving genomic analysis workflows to GPU with Parabricks results in hugely reduced processing time.\nOne example of this is in Cancer Research UK\u2019s TRACERx EVO, the latest project of TRACERx, which is the world\u2019s largest long-term lung cancer research program, and is driven by infrastructure at the Francis Crick Institute, University College London, and the University of Manchester.\nInitial results from the Francis Crick Institute show that the end-to-end analysis of whole human genomes (including FastQ alignment and deep variant calling) can be done in just over 2 hours with NVIDIA Parabricks, compared to approximately 13 hours on their NEMO CPU cluster. This performance gain is anticipated to be pushed even further on their latest GPU cluster.\nFor the TRACERx EVO project alone, they estimate this will save nearly 9 years of bioinformatics processing time, an improvement described as a \u201cgame-changer in terms of the feasibility of the analysis pipelines for the project,\u201d by Mark S. Hill, principal research fellow at TRACERx EVO.\nFor the latest GPU architectures, the newest NVIDIA Hopper architecture has been dubbed the engine of the world\u2019s AI infrastructure, with an order-of-magnitude performance leap for a diverse range of workloads.\nHigh-performance computing applications being run in data centers benefit from NVIDIA Hopper\u2019s multi-GPU scalability, and its advancements in tensor core technology, meaning impressive results such as 30x acceleration in AI inference over previous generations.\nFor genomics specifically, NVIDIA Hopper architectures include new dynamic programming instructions (DPX) designed to solve complex recursive problems. Dynamic programming is used commonly across multiple fields such as in graph analytics or in route optimizations. This includes in genomics with the Smith-Waterman algorithm, which underlies most aligners and multiple variant callers. The new DPX instructions accelerate these algorithms by 40x compared to CPU-only, and 7x compared to the previous NVIDIA Ampere architecture.\nCombining all these advances means that the latest NVIDIA GPU architectures are incredibly well-suited to accelerate bioinformatics tools like the BWA-MEM aligner, which can run in just 8 minutes on eight NVIDIA H100 GPUs, or the deep learning\u2013based DeepVariant variant caller, which can run in just 3 minutes on eight H100 GPUs. These runtimes mean an end-to-end germline workflow can be achieved in just 14 minutes with H100 GPUs and Parabricks.\nBenchmarks shown are for a single 30x HG002 whole genome Illumina sequencing sample, run with Parabricks DeepVariant germline pipeline.\nFigure 3. Runtime performance of the Parabricks germline workflow on 8 x NVIDIA H100 GPUs compared to a cpu-only M5.24xlarge (96 vCPU cores) instance\nNVIDIA Parabricks v4.2 now available on NGC\nParabricks v4.2 integrates seamlessly into genomics workflows, with continued support for GPU-accelerated versions of well-established workflows with tools like BWA-MEM, and GATK and the ability to quickly train custom models for DeepVariant variant calling. In providing these for new GPU architectures, and across both short\u2013 and long-read sequencing devices, Parabricks is a truly universal full-stack acceleration platform, for gold-standard genomics analysis on GPU.\nThe Parabricks v4.2 container is freely available now under the NVIDIA Parabricks Collection on NGC. For WDL and NextFlow reference workflows, see the Parabricks Workflows GitHub repo.\nFor more information about Parabricks, see Whole Genome Sequencing Analysis, which includes customer success stories with analysis at scale, deployment in sequencers and devices, and cutting-edge research.\nIf you require enterprise support, contact NVIDIA sales to access enterprise benefits, including access to NVIDIA experts to ensure optimization at scale, guaranteed critical support response times, and enterprise training services\nFor more information about what\u2019s new, tutorials, and deployment guides for cloud service providers, see the Parabricks documentation.\nFor more information about scaling sequencing analysis with Parabricks, see the NVIDIA DGX BasePOD solutions for genomic sequencing whitepaper."}], "https://developer.nvidia.com/blog/advanced-api-performance-descriptors/": [{"text": "The article discusses the importance of using descriptor types to efficiently bind resources to shaders and improve communication between the CPU and GPU. It recommends a \"bindless\" design with unbounded array descriptors pointing to big descriptor tables, uploading data upfront, and caching descriptors on GPU-visible heaps or sets to lower CPU overhead. Root constants are suggested for transferring per-draw varying constants, and dynamic resource binding is advised for omitting some descriptor tables from the root signature. In DirectX 12, Root constants are the fastest, followed by Root CBV/SRV/UAV, while descriptor tables are the slowest. In Vulkan, it is recommended to keep the number of descriptor sets low and use dynamic uniform and storage buffers for per-draw call changes. Exceeding the limits of active descriptors and samplers could lead to pipeline stalls. It is advised to prevent excessive creation or copying of descriptors during the frame and reduce duplicate descriptors to the same resources. Keeping bindings tightly packed in a single descriptor set in Vulkan is also recommended to avoid wasting memory and reducing cache efficiency.", "text_components": ["Advanced API Performance: Descriptors\nBy using descriptor types, you can bind resources to shaders and specify how those resources are accessed. This creates efficient communication between the CPU and GPU and enables shaders to access the necessary data during rendering.", "Recommended\nPrefer a \u201cbindless\u201d design.\nUse unbounded array descriptors pointing to big descriptor tables or sets with all known textures, buffers, and acceleration structures needed for the frame.\nUpload as much data upfront as possible (textures, per-draw constants, and per-frame constants) and make them accessible through these descriptor arrays.\nThis design also makes it easier to implement ray tracing; that is, allowing access to every texture and buffers from each shader.\nCache descriptors on GPU-visible descriptor heaps (DirectX 12) or sets (Vulkan) with a known offset. This lowers the CPU overhead and virtually eliminates the need for copying descriptors.\nUse multiple copies of the heap to handle descriptor changes gracefully, such as streaming textures and buffers. But don\u2019t exceed the 1M and 2K limits. For more information, see the Not Recommended section later in this post.\nUse root (DirectX 12) or push (Vulkan) constants. They are the fastest way to transfer per-draw varying constants.\nOn Pascal: Prefer CBVs over SRVs for constant data.\nGenerally, SRV buffers are slower than CBV buffers on <= Pascal.\nPerformance is equivalent on Volta and up.\nBetter yet, try using root constants.\nThey can be faster, even for infrequently changing data (for example, material data, pass data, and per-frame data).", "DirectX 12\nFeel free to maximize the use of the full 64 ```DWORD``` data types available in the root signature.\nPerformance ranking on both GPU and CPU:\nRoot constants are the fastest with no indirections, and they are directly indexable.\nRoot CBV/SRV/UAV are the second fastest, with single indirection and no bounds checking.\nDescriptor tables are the slowest, with two indirections and bounds checking.\nUse dynamic resource binding, such as HLSL SM 6.6.\nThis enables you to omit some descriptor tables from the root signature, for more space for root constants and other data. For more information, see In the works: HLSL Shader Model 6.6.\nSwitching root signatures is a fast operation.\nThe usage of multiple root signatures to improve binding efficiency could be a valid strategy.\nThis especially holds true for a non-bindless design.\nIt could be inefficient when having to rebind a lot of data unnecessarily. Switching root signature causes existing bindings to be lost.\nUse Root Signature 1.1 to get slightly more performance in some cases.\nIn particular, using ```DATA_STATIC_SET_AT_EXECUTE``` where possible enables the driver to inline some data early.\nThis is not a high priority; only use it whenever it is convenient to do so.", "Vulkan\nTry to keep the number of descriptor sets in pipeline layouts as low as possible.\nUse dynamic uniform and storage buffers for per-draw call changes.\nPrefer using combined image and sampler descriptors.\nVulkan 1.2 enables passing device addresses of storage buffers as 64-bit values to shaders. This enables pointer-like workflows (such as casting) that are not available in DirectX or HLSL. GLSL exposes this through ```GL_EXT_buffer_reference(2)``` and uses ```SPV_EXT_physical_storage_buffer```. Try to make optimal usage of the ```buffer_reference_align``` information, as the hardware can leverage wider memory load operations accordingly.", "Not recommended\nDo not exceed 1M active descriptors and 2K samplers in total for the whole application (GPU-visible).\nOtherwise, pipeline stalls across the whole GPU could occur when switching descriptor heaps (DirectX 12).\nWhenever the limits are exceeded, it reduces the asynchronous execution efficiency of command lists.\nOn Vulkan, the deduplication of descriptors is automatically performed by the driver. The limits mentioned earlier only count towards unique variations.\nIn general, try to keep under the thresholds described in ```VkPhysicalDeviceLimits```.\nAvoid typed UAV loads or stores where possible.", "DirectX 12\nPrevent the excessive creation or copying of descriptors during the frame.\nKeep descriptors around persistently instead of re-allocating or copying them in each frame.\nUse root CBVs instead of CBVs in a descriptor table.\nThere\u2019s no need to call ```CreateConstantBufferView``` with a root CBV.\nThe careful selection of smaller descriptor tables could also improve the situation.\nReduce duplicate descriptors to the same resources as much as possible.\nExample: Texture 0 should not be referenced in Descriptor 0, 10, 20, 30, 40, 50, and so on.\nInstead, try changing the layout from the descriptor tables to be able to reuse the same descriptor multiple times.", "Vulkan\nDo not have excessively sparse binding offsets in a single descriptor set.\nKeep bindings as tightly packed as possible.\nUnused binding indices waste memory and reduce cache efficiency."], "document_title": "Advanced API Performance: Descriptors", "document_url": "https://developer.nvidia.com/blog/advanced-api-performance-descriptors/", "document_date": "2023-10-27T16:00:00", "document_date_modified": "2023-11-02T20:23:13", "document_full_text": "Advanced API Performance: Descriptors\nBy using descriptor types, you can bind resources to shaders and specify how those resources are accessed. This creates efficient communication between the CPU and GPU and enables shaders to access the necessary data during rendering.\nRecommended\nPrefer a \u201cbindless\u201d design.\nUse unbounded array descriptors pointing to big descriptor tables or sets with all known textures, buffers, and acceleration structures needed for the frame.\nUpload as much data upfront as possible (textures, per-draw constants, and per-frame constants) and make them accessible through these descriptor arrays.\nThis design also makes it easier to implement ray tracing; that is, allowing access to every texture and buffers from each shader.\nCache descriptors on GPU-visible descriptor heaps (DirectX 12) or sets (Vulkan) with a known offset. This lowers the CPU overhead and virtually eliminates the need for copying descriptors.\nUse multiple copies of the heap to handle descriptor changes gracefully, such as streaming textures and buffers. But don\u2019t exceed the 1M and 2K limits. For more information, see the Not Recommended section later in this post.\nUse root (DirectX 12) or push (Vulkan) constants. They are the fastest way to transfer per-draw varying constants.\nOn Pascal: Prefer CBVs over SRVs for constant data.\nGenerally, SRV buffers are slower than CBV buffers on <= Pascal.\nPerformance is equivalent on Volta and up.\nBetter yet, try using root constants.\nThey can be faster, even for infrequently changing data (for example, material data, pass data, and per-frame data).\nDirectX 12\nFeel free to maximize the use of the full 64 ```DWORD``` data types available in the root signature.\nPerformance ranking on both GPU and CPU:\nRoot constants are the fastest with no indirections, and they are directly indexable.\nRoot CBV/SRV/UAV are the second fastest, with single indirection and no bounds checking.\nDescriptor tables are the slowest, with two indirections and bounds checking.\nUse dynamic resource binding, such as HLSL SM 6.6.\nThis enables you to omit some descriptor tables from the root signature, for more space for root constants and other data. For more information, see In the works: HLSL Shader Model 6.6.\nSwitching root signatures is a fast operation.\nThe usage of multiple root signatures to improve binding efficiency could be a valid strategy.\nThis especially holds true for a non-bindless design.\nIt could be inefficient when having to rebind a lot of data unnecessarily. Switching root signature causes existing bindings to be lost.\nUse Root Signature 1.1 to get slightly more performance in some cases.\nIn particular, using ```DATA_STATIC_SET_AT_EXECUTE``` where possible enables the driver to inline some data early.\nThis is not a high priority; only use it whenever it is convenient to do so.\nVulkan\nTry to keep the number of descriptor sets in pipeline layouts as low as possible.\nUse dynamic uniform and storage buffers for per-draw call changes.\nPrefer using combined image and sampler descriptors.\nVulkan 1.2 enables passing device addresses of storage buffers as 64-bit values to shaders. This enables pointer-like workflows (such as casting) that are not available in DirectX or HLSL. GLSL exposes this through ```GL_EXT_buffer_reference(2)``` and uses ```SPV_EXT_physical_storage_buffer```. Try to make optimal usage of the ```buffer_reference_align``` information, as the hardware can leverage wider memory load operations accordingly.\nNot recommended\nDo not exceed 1M active descriptors and 2K samplers in total for the whole application (GPU-visible).\nOtherwise, pipeline stalls across the whole GPU could occur when switching descriptor heaps (DirectX 12).\nWhenever the limits are exceeded, it reduces the asynchronous execution efficiency of command lists.\nOn Vulkan, the deduplication of descriptors is automatically performed by the driver. The limits mentioned earlier only count towards unique variations.\nIn general, try to keep under the thresholds described in ```VkPhysicalDeviceLimits```.\nAvoid typed UAV loads or stores where possible.\nDirectX 12\nPrevent the excessive creation or copying of descriptors during the frame.\nKeep descriptors around persistently instead of re-allocating or copying them in each frame.\nUse root CBVs instead of CBVs in a descriptor table.\nThere\u2019s no need to call ```CreateConstantBufferView``` with a root CBV.\nThe careful selection of smaller descriptor tables could also improve the situation.\nReduce duplicate descriptors to the same resources as much as possible.\nExample: Texture 0 should not be referenced in Descriptor 0, 10, 20, 30, 40, 50, and so on.\nInstead, try changing the layout from the descriptor tables to be able to reuse the same descriptor multiple times.\nVulkan\nDo not have excessively sparse binding offsets in a single descriptor set.\nKeep bindings as tightly packed as possible.\nUnused binding indices waste memory and reduce cache efficiency."}], "https://developer.nvidia.com/blog/reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library/": [{"text": "Spark RAPIDS ML is an open-source Python package that enables NVIDIA GPU acceleration of PySpark MLlib, offering compatibility with DataFrame API and speedups with supported algorithms. The library has demonstrated significant time and cost advantages compared to CPU-based PySpark MLlib when training with supported algorithms in three-node Spark clusters. The latest release includes GPU-accelerated PySpark MLlib APIs for binomial logistic regression, cross validation, and UMAP, providing faster and more cost-efficient options for machine learning tasks. The specialized CrossValidator class in Spark RAPIDS ML eliminates redundant data copies and offers a 2x speedup over the baseline in cross validation. The UMAP algorithm, a state-of-the-art dimensionality reduction technique, is now available with GPU acceleration in Spark RAPIDS ML, providing faster processing and better performance for high-dimensional data analysis. Overall, the library aims to reduce compute costs and accelerate Apache Spark ML applications with minimal code changes. Users can access the source code and documentation on GitHub for more information and support.", "text_components": ["Reduce Apache Spark ML Compute Costs with New Algorithms in Spark RAPIDS ML Library\nSpark RAPIDS ML is an open-source Python package enabling NVIDIA GPU acceleration of PySpark MLlib. It offers PySpark MLlib DataFrame API compatibility and speedups when training with the supported algorithms. See New GPU Library Lowers Compute Costs for Apache Spark ML for more details.\nPySpark MLlib DataFrame API compatibility means easier incorporation into existing PySpark ML applications, with only a package import change (at most). An example of the K-means algorithm is shown below. The package import change is the only additional step necessary to enable GPU acceleration using this library.", "PySpark MLlib\n```\nfrom pyspark.ml.clustering import KMeans\n\nkmeans_estm = KMeans()\\\n.setK(100)\\\n.setFeaturesCol(\"features\")\\\n.setMaxIter(30)\n\nkmeans_model = kmeans_estm.fit(pyspark_data_frame)\n\nkmeans_model.write().save(\"saved-model\")\n\ntransformed = kmeans_model.transform(pyspark_data_frame)\n```", "Spark RAPIDS ML\n```\nfrom spark_rapids_ml.clustering import KMeans\n\nkmeans_estm = KMeans()\\\n.setK(100)\\\n.setFeaturesCol(\"features\")\\\n.setMaxIter(30)\n\nkmeans_model = kmeans_estm.fit(pyspark_data_frame)\n\nkmeans_model.write().save(\"saved-model\")\n\ntransformed = kmeans_model.transform(pyspark_data_frame)\n```\nTraining with supported algorithms in a benchmarking suite run in three-node Spark clusters on GPU-accelerated Databricks\u2019 AWS-hosted Spark service demonstrated significant time and cost advantages compared to CPU-based PySpark MLlib. Specifically, this achieved a 7x to 100x speedup (depending on the algorithm) and 3x to 50x more in cost savings. Moreover, the Spark RAPIDS ML library is built on top of the proven, highly optimized RAPIDS cuML GPU-accelerated ML library.\nThe initial release of Spark RAPIDS ML supported GPU acceleration of a subset of PySpark MLlib algorithms with readily available counterparts in RAPIDS cuML, namely linear regression, random forest classification, random forest regression, k-means, and pca. It also included a PySpark DataFrame API for the cuML distributed implementation of exact k-nearest neighbors (k-NN) for easy incorporation of this useful algorithm into Spark applications using a familiar API.\nThe Spark RAPIDS ML 23.08 release includes GPU-accelerated PySpark MLlib APIs for three new algorithms:\nBinomial logistic regression with L-BFGS optimization\nCross validation\nUniform manifold approximation and projection (UMAP)\nBanner for e-book: Accelerating Apache Spark 3", "Binomial logistic regression with L-BFGS\nLogistic regression is a well-known machine learning (ML) classification algorithm that models the conditional probability distribution of a finite valued class variable as a generalized linear function (softmax or sigmoid and linear, for example) of a feature vector.\nThe 23.08 release includes GPU-accelerated versions of the PySpark MLlib ``` classification.LogisticRegression``` and ```classification.LogisticRegressionModel``` supporting accelerated fit and transform. This is initially for binary classification (binomial logistic regression) and L2 regularization, with full support (elastic net regularization and multi-class classification, for example) planned for upcoming releases.\nSupporting accelerated logistic regression was more involved than the previously released algorithms. Unlike with the algorithms in previous releases, there was no readily available distributed implementation in cuML to leverage.\nThe first step was thus to contribute a multi-node multi-GPU (MNMG) extension of the cuML single GPU-accelerated L-BFGS-based logistic regression optimization algorithm (which is also used in Spark MLlib). To do this, the team followed the design pattern of the other cuML distributed implementations, with a design similar to a Message Passing Interface (MPI) centered around the GPU-optimized NVIDIA Collective Communication Library (NCCL).\nThe Spark RAPIDS ML bootstrapping of this implementation using the PySpark Barrier RDD and the MLlib API compatibility was then layered on top (Figure 1). As with the previously released algorithms, this design enables the GPU-accelerated distributed implementation to carry out communication in a manner that optimizes GPU utilization and over the best available interconnect between GPUs. These include Ethernet or higher-performance interconnects like NVLink and InfiniBand.\nStack diagram showing PySpark, Spark RAPIDS ML, cuML, GPU, and NCCL communication layers.\nFigure 1. Integration of Spark RAPIDS ML and the newly added cuML MNMG distributed logistic regression implementation", "Performance\nThe benchmarking setting used for the previously released algorithms was also used to compare GPU-accelerated Spark RAPIDS ML logistic regression with the baseline CPU-based Spark ML version. The PySpark RAPIDS MLlib implementation was 6x faster and 3x more cost-efficient than the PySpark MLlib CPU implementation.\nThese benchmarks were run in three-node Spark clusters (one driver, two executors) on Databricks\u2019 AWS-hosted Spark service with the hardware configurations listed below.\nIn the CPU cluster, the m5.2xlarge executor and driver nodes each have eight CPU cores and 32GB of RAM.\nIn the GPU cluster, the g5.2xlarge executor nodes each have the same CPU and RAM as the m5.2xlarge nodes, along with NVIDIA A10 24GB GPUs.\nThe benchmarks were run on a 3,000-feature 12GB synthetic dataset generated with the scikit-learn synthetic data-generating routines and stored in Parquet format on Amazon S3. Note that the runtimes are for end-to-end data loading from Amazon S3 plus fit method execution, and the spark-rapids plugin was used to accelerate data loading for GPU runs.\nFor more information including scripts related to this benchmark, visit NVIDIA/spark-rapids-ml on GitHub. See also a sample Jupyter Notebook demonstrating how to use the accelerated LogisticRegression APIs.", "Cross validation\nCross validation is a well-known algorithm for optimizing model or training algorithm hyperparameters that are not tuned directly by the core training algorithm itself, such as the regularization parameters in logistic regression. It has long been supported in PySpark MLlib through the tuning.CrossValidator class.\nThanks to the MLlib API compatibility of Spark RAPIDS ML, the supported accelerated algorithm Estimator classes can undergo PySpark CrossValidator hyperparameter tuning out of the box. It provides speedup and cost benefits over cross validation with CPU training, which is on par with a single training run GPU compared to CPU cases. However, it suffers from the inefficiency of repeatedly copying data from CPU to GPU for each change in hyperparameter values.\nSuch excessive copying is a known performance bottleneck in GPU computing. It is even more pronounced for Spark RAPIDS ML, as these copies also exist between the JVM executors and Python workers over a local socket connection.\nTo eliminate this inefficiency, Spark RAPIDS ML now includes a specialized variant of the PySpark native CrossValidator compatible with MLlib API. It copies data to Python workers and GPUs only once while hyperparameter values change for a given cross validation fold.\nThe Spark RAPIDS ML specialized CrossValidator trains and evaluates all the models for the hyperparameter values under test in single respective training and evaluation Spark stages, copying the data once per stage. Figure 2 illustrates timeline traces showing patterns of copies and training and evaluation compute steps for the baseline PySpark MLlib and Spark RAPIDS ML CrossValidator versions.\nAbove, a timeline for PySpark MLlib CrossValidator shows alternating copy, train, copy, and eval steps for each change of hyperparameter values. Below, a timeline for Spark RAPIDS ML CrossValidator shows a single copy for training followed by training on different values of hyperparameters and then a single copy for evaluation followed by evaluation on those same value of hyperparameters.\nFigure 2. The Spark RAPIDS ML CrossValidator eliminates redundant data copies", "Performance\nOur team benchmarked the new specialized CrossValidator class on three-fold cross validation with four hyperparameter values per fold for GPU-accelerated ```RandomForestClassifier```, ```RandomForestRegression```, and ```LinearRegression```. We observed a 2x speedup over the baseline CrossValidator operating on the GPU-accelerated implementations.\nNote that this speedup multiplies existing speedup factors due to GPU implementations of the core training algorithms when considering an overall comparison to pure CPU cross validation.\nVisit NVIDIA/spark-rapids-ml on GitHub for a sample Jupyter Notebook showing the Spark MLlib API-compatible accelerated CrossValidator.", "UMAP\nUMAP is a state-of-the-art non-linear dimensionality reduction algorithm that is highly effective in capturing structure from the high-dimensional data into the computed lower dimensional representations or embeddings. It can be used to simplify downstream ML tasks such as classification and clustering, or for visualization.\nThe algorithm involves compute-intensive steps to arrive at the lower dimensional embeddings\u2014such as k-nearest neighbors (k-NN)\u2014in the original high-dimensional space and an iterative cross-entropy optimization of a random graph over the embeddings. It is thus a natural candidate for GPU acceleration and has been implemented in the cuML library, offering significant speedups over the original CPU implementation.\nIn the latest Spark RAPIDS ML release, UMAP joins exact k-NN as a non-MLlib accelerated algorithm from cuML that is wrapped in a PySpark MLlib API for easy integration in Spark applications. The design is shown in Figure 3.\nThe UMAP estimator\u2019s fit method implementation is single-node and operates on a random sample of the full dataset to create a UMAP model comprising that random sample along with its embedding. The transform method of the UMAP model then extends the embedding to the rest of the dataset in a scalable, distributed manner.\nIt uses k-NN and cross-entropy optimization with respect to the original random sample and embedding captured in the model. The implementation overcomes serialization limitations in Spark to enable a large model size (many GBs).\nIn a left-to-right flow, a random sample of a dataset is read from persistent storage by a worker running on a single server and GPU to carry out the Fit or training step. The trained model is broadcast to a collection of workers running on multiple servers and GPUs for the Transform step operating on the full distributed dataset. The UMAP embeddings computed by each worker in the Transform step are then available for storage or other downstream tasks.\nFigure 3. Spark RAPIDS ML UMAP fit and transform implementations\nVisit NVIDIA/spark-rapids-ml on GitHub to see a sample Jupyter Notebook demonstrating GPU-accelerated UMAP on Spark. For more details about the API, see the UMAP documentation.", "Summary\nWith Spark RAPIDS ML and its growing capabilities, you can dramatically accelerate Spark ML applications with a one-line code change while reducing your computing cost. The latest release of Spark RAPIDS ML extends these benefits of GPU acceleration to logistic regression and cross validation. In addition, GPU-accelerated UMAP is now available with a PySpark MLlib API for easier adoption in Spark ML applications.\nVisit NVIDIA/spark-rapids-ml on GitHub to access Spark RAPIDS ML source code and documentation, and to provide feedback. You can also check out the resources for getting started with Spark RAPIDS ML."], "document_title": "Reduce Apache Spark ML Compute Costs with New Algorithms in Spark RAPIDS ML Library", "document_url": "https://developer.nvidia.com/blog/reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library/", "document_date": "2023-10-24T19:00:00", "document_date_modified": "2023-11-02T18:14:32", "document_full_text": "Reduce Apache Spark ML Compute Costs with New Algorithms in Spark RAPIDS ML Library\nSpark RAPIDS ML is an open-source Python package enabling NVIDIA GPU acceleration of PySpark MLlib. It offers PySpark MLlib DataFrame API compatibility and speedups when training with the supported algorithms. See New GPU Library Lowers Compute Costs for Apache Spark ML for more details.\nPySpark MLlib DataFrame API compatibility means easier incorporation into existing PySpark ML applications, with only a package import change (at most). An example of the K-means algorithm is shown below. The package import change is the only additional step necessary to enable GPU acceleration using this library.\nPySpark MLlib\n```\nfrom pyspark.ml.clustering import KMeans\n\nkmeans_estm = KMeans()\\\n.setK(100)\\\n.setFeaturesCol(\"features\")\\\n.setMaxIter(30)\n\nkmeans_model = kmeans_estm.fit(pyspark_data_frame)\n\nkmeans_model.write().save(\"saved-model\")\n\ntransformed = kmeans_model.transform(pyspark_data_frame)\n```\nSpark RAPIDS ML\n```\nfrom spark_rapids_ml.clustering import KMeans\n\nkmeans_estm = KMeans()\\\n.setK(100)\\\n.setFeaturesCol(\"features\")\\\n.setMaxIter(30)\n\nkmeans_model = kmeans_estm.fit(pyspark_data_frame)\n\nkmeans_model.write().save(\"saved-model\")\n\ntransformed = kmeans_model.transform(pyspark_data_frame)\n```\nTraining with supported algorithms in a benchmarking suite run in three-node Spark clusters on GPU-accelerated Databricks\u2019 AWS-hosted Spark service demonstrated significant time and cost advantages compared to CPU-based PySpark MLlib. Specifically, this achieved a 7x to 100x speedup (depending on the algorithm) and 3x to 50x more in cost savings. Moreover, the Spark RAPIDS ML library is built on top of the proven, highly optimized RAPIDS cuML GPU-accelerated ML library.\nThe initial release of Spark RAPIDS ML supported GPU acceleration of a subset of PySpark MLlib algorithms with readily available counterparts in RAPIDS cuML, namely linear regression, random forest classification, random forest regression, k-means, and pca. It also included a PySpark DataFrame API for the cuML distributed implementation of exact k-nearest neighbors (k-NN) for easy incorporation of this useful algorithm into Spark applications using a familiar API.\nThe Spark RAPIDS ML 23.08 release includes GPU-accelerated PySpark MLlib APIs for three new algorithms:\nBinomial logistic regression with L-BFGS optimization\nCross validation\nUniform manifold approximation and projection (UMAP)\nBanner for e-book: Accelerating Apache Spark 3\nBinomial logistic regression with L-BFGS\nLogistic regression is a well-known machine learning (ML) classification algorithm that models the conditional probability distribution of a finite valued class variable as a generalized linear function (softmax or sigmoid and linear, for example) of a feature vector.\nThe 23.08 release includes GPU-accelerated versions of the PySpark MLlib ``` classification.LogisticRegression``` and ```classification.LogisticRegressionModel``` supporting accelerated fit and transform. This is initially for binary classification (binomial logistic regression) and L2 regularization, with full support (elastic net regularization and multi-class classification, for example) planned for upcoming releases.\nSupporting accelerated logistic regression was more involved than the previously released algorithms. Unlike with the algorithms in previous releases, there was no readily available distributed implementation in cuML to leverage.\nThe first step was thus to contribute a multi-node multi-GPU (MNMG) extension of the cuML single GPU-accelerated L-BFGS-based logistic regression optimization algorithm (which is also used in Spark MLlib). To do this, the team followed the design pattern of the other cuML distributed implementations, with a design similar to a Message Passing Interface (MPI) centered around the GPU-optimized NVIDIA Collective Communication Library (NCCL).\nThe Spark RAPIDS ML bootstrapping of this implementation using the PySpark Barrier RDD and the MLlib API compatibility was then layered on top (Figure 1). As with the previously released algorithms, this design enables the GPU-accelerated distributed implementation to carry out communication in a manner that optimizes GPU utilization and over the best available interconnect between GPUs. These include Ethernet or higher-performance interconnects like NVLink and InfiniBand.\nStack diagram showing PySpark, Spark RAPIDS ML, cuML, GPU, and NCCL communication layers.\nFigure 1. Integration of Spark RAPIDS ML and the newly added cuML MNMG distributed logistic regression implementation\nPerformance\nThe benchmarking setting used for the previously released algorithms was also used to compare GPU-accelerated Spark RAPIDS ML logistic regression with the baseline CPU-based Spark ML version. The PySpark RAPIDS MLlib implementation was 6x faster and 3x more cost-efficient than the PySpark MLlib CPU implementation.\nThese benchmarks were run in three-node Spark clusters (one driver, two executors) on Databricks\u2019 AWS-hosted Spark service with the hardware configurations listed below.\nIn the CPU cluster, the m5.2xlarge executor and driver nodes each have eight CPU cores and 32GB of RAM.\nIn the GPU cluster, the g5.2xlarge executor nodes each have the same CPU and RAM as the m5.2xlarge nodes, along with NVIDIA A10 24GB GPUs.\nThe benchmarks were run on a 3,000-feature 12GB synthetic dataset generated with the scikit-learn synthetic data-generating routines and stored in Parquet format on Amazon S3. Note that the runtimes are for end-to-end data loading from Amazon S3 plus fit method execution, and the spark-rapids plugin was used to accelerate data loading for GPU runs.\nFor more information including scripts related to this benchmark, visit NVIDIA/spark-rapids-ml on GitHub. See also a sample Jupyter Notebook demonstrating how to use the accelerated LogisticRegression APIs.\nCross validation\nCross validation is a well-known algorithm for optimizing model or training algorithm hyperparameters that are not tuned directly by the core training algorithm itself, such as the regularization parameters in logistic regression. It has long been supported in PySpark MLlib through the tuning.CrossValidator class.\nThanks to the MLlib API compatibility of Spark RAPIDS ML, the supported accelerated algorithm Estimator classes can undergo PySpark CrossValidator hyperparameter tuning out of the box. It provides speedup and cost benefits over cross validation with CPU training, which is on par with a single training run GPU compared to CPU cases. However, it suffers from the inefficiency of repeatedly copying data from CPU to GPU for each change in hyperparameter values.\nSuch excessive copying is a known performance bottleneck in GPU computing. It is even more pronounced for Spark RAPIDS ML, as these copies also exist between the JVM executors and Python workers over a local socket connection.\nTo eliminate this inefficiency, Spark RAPIDS ML now includes a specialized variant of the PySpark native CrossValidator compatible with MLlib API. It copies data to Python workers and GPUs only once while hyperparameter values change for a given cross validation fold.\nThe Spark RAPIDS ML specialized CrossValidator trains and evaluates all the models for the hyperparameter values under test in single respective training and evaluation Spark stages, copying the data once per stage. Figure 2 illustrates timeline traces showing patterns of copies and training and evaluation compute steps for the baseline PySpark MLlib and Spark RAPIDS ML CrossValidator versions.\nAbove, a timeline for PySpark MLlib CrossValidator shows alternating copy, train, copy, and eval steps for each change of hyperparameter values. Below, a timeline for Spark RAPIDS ML CrossValidator shows a single copy for training followed by training on different values of hyperparameters and then a single copy for evaluation followed by evaluation on those same value of hyperparameters.\nFigure 2. The Spark RAPIDS ML CrossValidator eliminates redundant data copies\nPerformance\nOur team benchmarked the new specialized CrossValidator class on three-fold cross validation with four hyperparameter values per fold for GPU-accelerated ```RandomForestClassifier```, ```RandomForestRegression```, and ```LinearRegression```. We observed a 2x speedup over the baseline CrossValidator operating on the GPU-accelerated implementations.\nNote that this speedup multiplies existing speedup factors due to GPU implementations of the core training algorithms when considering an overall comparison to pure CPU cross validation.\nVisit NVIDIA/spark-rapids-ml on GitHub for a sample Jupyter Notebook showing the Spark MLlib API-compatible accelerated CrossValidator.\nUMAP\nUMAP is a state-of-the-art non-linear dimensionality reduction algorithm that is highly effective in capturing structure from the high-dimensional data into the computed lower dimensional representations or embeddings. It can be used to simplify downstream ML tasks such as classification and clustering, or for visualization.\nThe algorithm involves compute-intensive steps to arrive at the lower dimensional embeddings\u2014such as k-nearest neighbors (k-NN)\u2014in the original high-dimensional space and an iterative cross-entropy optimization of a random graph over the embeddings. It is thus a natural candidate for GPU acceleration and has been implemented in the cuML library, offering significant speedups over the original CPU implementation.\nIn the latest Spark RAPIDS ML release, UMAP joins exact k-NN as a non-MLlib accelerated algorithm from cuML that is wrapped in a PySpark MLlib API for easy integration in Spark applications. The design is shown in Figure 3.\nThe UMAP estimator\u2019s fit method implementation is single-node and operates on a random sample of the full dataset to create a UMAP model comprising that random sample along with its embedding. The transform method of the UMAP model then extends the embedding to the rest of the dataset in a scalable, distributed manner.\nIt uses k-NN and cross-entropy optimization with respect to the original random sample and embedding captured in the model. The implementation overcomes serialization limitations in Spark to enable a large model size (many GBs).\nIn a left-to-right flow, a random sample of a dataset is read from persistent storage by a worker running on a single server and GPU to carry out the Fit or training step. The trained model is broadcast to a collection of workers running on multiple servers and GPUs for the Transform step operating on the full distributed dataset. The UMAP embeddings computed by each worker in the Transform step are then available for storage or other downstream tasks.\nFigure 3. Spark RAPIDS ML UMAP fit and transform implementations\nVisit NVIDIA/spark-rapids-ml on GitHub to see a sample Jupyter Notebook demonstrating GPU-accelerated UMAP on Spark. For more details about the API, see the UMAP documentation.\nSummary\nWith Spark RAPIDS ML and its growing capabilities, you can dramatically accelerate Spark ML applications with a one-line code change while reducing your computing cost. The latest release of Spark RAPIDS ML extends these benefits of GPU acceleration to logistic regression and cross validation. In addition, GPU-accelerated UMAP is now available with a PySpark MLlib API for easier adoption in Spark ML applications.\nVisit NVIDIA/spark-rapids-ml on GitHub to access Spark RAPIDS ML source code and documentation, and to provide feedback. You can also check out the resources for getting started with Spark RAPIDS ML."}], "https://developer.nvidia.com/blog/differentiable-slang-example-applications/": [{"text": "Differentiable Slang is a powerful tool for integrating differential programming and automatic gradient computation into existing codebases for various computer graphics tasks. In this article, several example applications of Differentiable Slang are discussed, including appearance-based BRDF optimization, texture compression, NVDIFFREC inverse rendering, and differentiable path tracers. These applications demonstrate the potential of Differentiable Slang in optimizing rendering processes, improving compression techniques, reconstructing scene properties, and solving complex material parameter optimization problems. By leveraging Differentiable Slang's automatic differentiation capabilities, developers can achieve high-quality results with minimal code and enhanced compatibility with existing frameworks like PyTorch and CUDA. The article emphasizes the ease of integration and flexibility of Differentiable Slang, showcasing its potential for data-driven graphics development and rendering applications. For more in-depth information and code examples, readers are encouraged to explore the available resources on the GitHub repositories related to Differentiable Slang and its applications.", "text_components": ["Differentiable Slang: Example Applications\nDifferentiable Slang easily integrates with existing codebases\u2014from Python, PyTorch, and CUDA to HLSL\u2014to aid multiple computer graphics tasks and enable novel data-driven and neural research. In this post, we introduce several code examples using differentiable Slang to demonstrate the potential use across different rendering applications and the ease of integration.\nThis is part of a series on Differentiable Slang. For more information about differential programming and automatic gradient computation in the Slang language, see Differentiable Slang: A Shading Language for Renderers That Learn.", "Example application: Appearance-based BRDF optimization\nOne of the basic building blocks in computer graphics is BRDF texture maps representing multiple properties of materials and describing how the light interacts with the rendered surfaces. Artists author and preview textures, but then rendering algorithms transform them automatically, for instance, filtering, blending the BRDF properties, or creating mipmaps.\nRendering is highly nonlinear, so linear operations on texture maps do not produce the correct linearly changing appearance. Various models were proposed to preserve appearance in applications like mipmap chain creation. Those models are approximate and often created only for a specific BRDF; new ones must be designed when rendering changes.\nInstead of refining those models, we propose to use differentiable rendering and a data-driven approach to build appearance-preserving mipmaps. For more information and code examples, see the /shader-slang GitHub repo.\nThree columns of images showing the differences in rendering results of a surface with a brick-and-grass material.\nFigure 1. Inverse rendering enables appearance-preserving minification of a material: (left) naively downsampled material; (middle) low-resolution material optimized with Slang; (right) reference.\nIn Figure 1, the left column shows the surface rendered with a naively downsampled material. The middle column shows the same surface rendered with a low-resolution material obtained from an optimization algorithm implemented using Slang\u2019s automatic differentiation feature. The right column shows the rendering result using the reference material without downsampling. The rendering using the optimized material preserves more details than using the naively downsampled material and matches much closer to the reference material.\nTo demonstrate Slang\u2019s flexibility and compatibility with multiple existing frameworks, we write the optimization loop in PyTorch in a Jupyter notebook, enabling easy visualization, interactive debugging, and Markdown documentation of the code. The shading code is written in Slang, which will look familiar to graphics programmers. Easy Slang, Python, PyTorch, and Jupyter interoperability enable you to choose the best combination of languages for data-driven graphics development.", "Example application: Texture compression\nTexture compression is an optimization task that significantly reduces the texture file size and memory usage while trying to preserve the image quality. There are many approaches to texture compression and many different compressors available, with the most popular being hardware block compression (BC). We demonstrate how we can use gradient descent to find a close-to-optimal solution for BC7 Mode 6 texture compression automatically with Slang automatic differentiation capabilities.\nBy using gradient descent, we don\u2019t need to write the compression code explicitly. Slang automatically generates gradients of BC7 block color interpolation through backward differentiation of the Mode 6 decoder:\n```\n[Differentiable]\nfloat4 decodeTexel() {\nreturn weight * maxEndPoint + (1 - weight) * minEndPoint;\n}\n```\nTo facilitate compression, we provide an effective initial guess, with endpoints initialized to the color space box\u2019s corners enveloping a block and interpolation weights set to 0.5. We model the BC7 quantization and iteratively adjust endpoints and weights for each 4\u00d74 block, ensuring minimal difference between the original textures and its compressed version.\nThis simple approach achieves a high compression quality, and for the best computational performance, we merge the forward (decoding) and backward (encoding) passes into a single compute shader. Every thread works independently on a BC7 block, improving efficiency by retaining all data in registers and avoiding atomic operations to accumulate gradients. On an NVIDIA RTX 4090, this method can compress 400 4k textures every second, achieving a compression speed of 6.5 GTexel/s.\nThis example is written in Slang and the Python interface to the Falcor rendering infrastructure. For more information and code examples, see the /NVIDIAGameWorks/Falcor GitHub repo.", "Example application: NVDIFFREC\nFigure shows that the pipeline takes as input 2D images of an ancient artifact, and a randomly initialized soup of triangles, and outputs a triangle mesh whose shape and material matches the input images.\nFigure 2. The inverse rendering pipeline nvdiffrec, rewritten in Slang, running at equal performance to hand-differentiated CUDA kernels\nNvdiffrec is a large inverse rendering library for joint shape, material, and lighting optimization. Nvdiffrec allows the reconstruction of various scene properties from a series of 2D observations and can be used in various inverse rendering and appearance reconstruction applications.\nNvdiffrec is a large inverse rendering library for joint shape, material, and lighting optimization. Nvdiffrec reconstructs various scene properties from a series of 2D observations and can be used in various inverse rendering and appearance reconstruction applications.\nOriginally, Nvdiffrec\u2019s performance-critical operations were accelerated using PyTorch extensions built with hand-differentiated CUDA kernels. The CUDA kernels perform the following tasks:\nLoss computation (log-sRGB mapping and warp-wide reduction)\nTangent space normal mapping\nVertex transforms (multiplication of a vertex array with a batch of 4\u00d74 matrices)\nCube map pre-filtering (for the split-sum shading model)\nSlang generates automatically-differentiated CUDA kernels that achieve the same performance as the handwritten, manually-differentiated CUDA code. This reduces the number of lines of code considerably while staying compatible and interoperable with other CUDA kernels. Slang makes the code easier to maintain, extend, and connect to existing rendering pipelines and shading models.\nFor more information about the Slang version of nvdiffrec, see the /NVlabs/nvdiffrec GitHub repo.", "Example application: Differentiable path tracers\nWe converted a traditional, real-time path tracer into a differentiable path tracer, reusing over 5K lines of Slang code. The following are two different inverse path tracing examples in Slang:\nInverse-rendering optimization solving for material parameters via a differentiable path tracer\nDifferentiable path tracer with warped-area sampling for differential geometry\nPicture of 3D colored dots in a box.\nInitial set of material parameters\nPicture of 3D dots with a color translation.\nOptimized parameters\nPicture of 3D dots that is similar to to the color translation version.\nReference\nFigure 3. Inverse-rendering result that optimizes thousands of material parameters simultaneously", "Conclusion\nFor more information, see the SLANG.D: Fast, Modular and Differentiable Shader Programming paper and begin exploring differentiable rendering with Slang."], "document_title": "Differentiable Slang: Example Applications", "document_url": "https://developer.nvidia.com/blog/differentiable-slang-example-applications/", "document_date": "2023-10-23T04:03:00", "document_date_modified": "2023-11-02T20:23:30", "document_full_text": "Differentiable Slang: Example Applications\nDifferentiable Slang easily integrates with existing codebases\u2014from Python, PyTorch, and CUDA to HLSL\u2014to aid multiple computer graphics tasks and enable novel data-driven and neural research. In this post, we introduce several code examples using differentiable Slang to demonstrate the potential use across different rendering applications and the ease of integration.\nThis is part of a series on Differentiable Slang. For more information about differential programming and automatic gradient computation in the Slang language, see Differentiable Slang: A Shading Language for Renderers That Learn.\nExample application: Appearance-based BRDF optimization\nOne of the basic building blocks in computer graphics is BRDF texture maps representing multiple properties of materials and describing how the light interacts with the rendered surfaces. Artists author and preview textures, but then rendering algorithms transform them automatically, for instance, filtering, blending the BRDF properties, or creating mipmaps.\nRendering is highly nonlinear, so linear operations on texture maps do not produce the correct linearly changing appearance. Various models were proposed to preserve appearance in applications like mipmap chain creation. Those models are approximate and often created only for a specific BRDF; new ones must be designed when rendering changes.\nInstead of refining those models, we propose to use differentiable rendering and a data-driven approach to build appearance-preserving mipmaps. For more information and code examples, see the /shader-slang GitHub repo.\nThree columns of images showing the differences in rendering results of a surface with a brick-and-grass material.\nFigure 1. Inverse rendering enables appearance-preserving minification of a material: (left) naively downsampled material; (middle) low-resolution material optimized with Slang; (right) reference.\nIn Figure 1, the left column shows the surface rendered with a naively downsampled material. The middle column shows the same surface rendered with a low-resolution material obtained from an optimization algorithm implemented using Slang\u2019s automatic differentiation feature. The right column shows the rendering result using the reference material without downsampling. The rendering using the optimized material preserves more details than using the naively downsampled material and matches much closer to the reference material.\nTo demonstrate Slang\u2019s flexibility and compatibility with multiple existing frameworks, we write the optimization loop in PyTorch in a Jupyter notebook, enabling easy visualization, interactive debugging, and Markdown documentation of the code. The shading code is written in Slang, which will look familiar to graphics programmers. Easy Slang, Python, PyTorch, and Jupyter interoperability enable you to choose the best combination of languages for data-driven graphics development.\nExample application: Texture compression\nTexture compression is an optimization task that significantly reduces the texture file size and memory usage while trying to preserve the image quality. There are many approaches to texture compression and many different compressors available, with the most popular being hardware block compression (BC). We demonstrate how we can use gradient descent to find a close-to-optimal solution for BC7 Mode 6 texture compression automatically with Slang automatic differentiation capabilities.\nBy using gradient descent, we don\u2019t need to write the compression code explicitly. Slang automatically generates gradients of BC7 block color interpolation through backward differentiation of the Mode 6 decoder:\n```\n[Differentiable]\nfloat4 decodeTexel() {\nreturn weight * maxEndPoint + (1 - weight) * minEndPoint;\n}\n```\nTo facilitate compression, we provide an effective initial guess, with endpoints initialized to the color space box\u2019s corners enveloping a block and interpolation weights set to 0.5. We model the BC7 quantization and iteratively adjust endpoints and weights for each 4\u00d74 block, ensuring minimal difference between the original textures and its compressed version.\nThis simple approach achieves a high compression quality, and for the best computational performance, we merge the forward (decoding) and backward (encoding) passes into a single compute shader. Every thread works independently on a BC7 block, improving efficiency by retaining all data in registers and avoiding atomic operations to accumulate gradients. On an NVIDIA RTX 4090, this method can compress 400 4k textures every second, achieving a compression speed of 6.5 GTexel/s.\nThis example is written in Slang and the Python interface to the Falcor rendering infrastructure. For more information and code examples, see the /NVIDIAGameWorks/Falcor GitHub repo.\nExample application: NVDIFFREC\nFigure shows that the pipeline takes as input 2D images of an ancient artifact, and a randomly initialized soup of triangles, and outputs a triangle mesh whose shape and material matches the input images.\nFigure 2. The inverse rendering pipeline nvdiffrec, rewritten in Slang, running at equal performance to hand-differentiated CUDA kernels\nNvdiffrec is a large inverse rendering library for joint shape, material, and lighting optimization. Nvdiffrec allows the reconstruction of various scene properties from a series of 2D observations and can be used in various inverse rendering and appearance reconstruction applications.\nNvdiffrec is a large inverse rendering library for joint shape, material, and lighting optimization. Nvdiffrec reconstructs various scene properties from a series of 2D observations and can be used in various inverse rendering and appearance reconstruction applications.\nOriginally, Nvdiffrec\u2019s performance-critical operations were accelerated using PyTorch extensions built with hand-differentiated CUDA kernels. The CUDA kernels perform the following tasks:\nLoss computation (log-sRGB mapping and warp-wide reduction)\nTangent space normal mapping\nVertex transforms (multiplication of a vertex array with a batch of 4\u00d74 matrices)\nCube map pre-filtering (for the split-sum shading model)\nSlang generates automatically-differentiated CUDA kernels that achieve the same performance as the handwritten, manually-differentiated CUDA code. This reduces the number of lines of code considerably while staying compatible and interoperable with other CUDA kernels. Slang makes the code easier to maintain, extend, and connect to existing rendering pipelines and shading models.\nFor more information about the Slang version of nvdiffrec, see the /NVlabs/nvdiffrec GitHub repo.\nExample application: Differentiable path tracers\nWe converted a traditional, real-time path tracer into a differentiable path tracer, reusing over 5K lines of Slang code. The following are two different inverse path tracing examples in Slang:\nInverse-rendering optimization solving for material parameters via a differentiable path tracer\nDifferentiable path tracer with warped-area sampling for differential geometry\nPicture of 3D colored dots in a box.\nInitial set of material parameters\nPicture of 3D dots with a color translation.\nOptimized parameters\nPicture of 3D dots that is similar to to the color translation version.\nReference\nFigure 3. Inverse-rendering result that optimizes thousands of material parameters simultaneously\nConclusion\nFor more information, see the SLANG.D: Fast, Modular and Differentiable Shader Programming paper and begin exploring differentiable rendering with Slang."}], "https://developer.nvidia.com/blog/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/": [{"text": "NVIDIA Compute Sanitizer (NCS) is a valuable tool for debugging CUDA applications, offering tools like Memcheck, Racecheck, Initcheck, and Synccheck to improve code reliability and performance. Initcheck helps identify uninitialized memory access errors, while Synccheck aids in finding synchronization errors in CUDA code. By using NCS, developers can efficiently debug their code and catch bugs related to memory initialization and thread synchronization. For example, Initcheck can detect uninitialized memory accesses to global memory in device code, while Synccheck can identify errors in using synchronization primitives and Cooperative Groups API. Through examples like checking unused memory and detecting synchronization errors in code, developers can utilize NCS to improve the efficiency and reliability of their CUDA applications. By downloading the CUDA Toolkit and exploring the NVIDIA/compute-sanitizer-samples on GitHub, developers can leverage NCS to enhance their debugging process and improve the performance of their parallel programming projects.", "text_components": ["Efficient CUDA Debugging: Memory Initialization and Thread Synchronization with NVIDIA Compute Sanitizer\nNVIDIA Compute Sanitizer (NCS) is a powerful tool that can save you time and effort while improving the reliability and performance of your CUDA applications.\nIn our previous post, Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer, we explored efficient debugging in the realm of parallel programming. We discussed how debugging code in the CUDA environment can be both challenging and time-consuming, especially when dealing with thousands of threads, and how NCS can help with this process.\nThis post continues our exploration of efficient CUDA debugging. It highlights a few more NCS tools and walks through several examples.", "NVIDIA Compute Sanitizer\nNCS is a suite of tools that can perform different types of checks on the functional correctness of your code. There are four main tools in NCS:\nMemcheck for memory access error and leak detection\nRacecheck, a shared memory data access hazard detection tool\nInitcheck, an uninitialized device global memory access detection tool\nSynccheck for thread synchronization hazard detection\nIn addition to these tools, NCS capabilities include:\nAn API to enable the creation of sanitizing and tracing tools that target CUDA applications\nIntegration with NVIDIA Tools Extension (NVTX)\nCoredump support for use with CUDA-GDB\nSuppression features for managing the output of the tool\nThis post will focus on debugging code and catching bugs related to uninitialized device arrays using ```initcheck```, and synchronization using ```synccheck```. See Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer for details about using ```memcheck``` for discovering memory leaks and ```racecheck``` for finding race conditions.", "Initialization checking\nNCS Initcheck helps developers identify and resolve uninitialized memory access errors in CUDA code. Uninitialized memory access can lead to unpredictable behavior and incorrect results in CUDA applications.\nNCS Initcheck can detect uninitialized memory accesses to global memory in device code and provides detailed information about the location and timing of the access, as well as the stack trace of the accessing thread. This helps to reveal the root cause of the issue and resolve the problem.\nTo provide an example, the code below benefits from initialization checking.\n```\n#include <stdio.h>\n\n#define THREADS 32\n#define BLOCKS 2\n\n__global__ void addToVector(float *v) {\n  int tx = threadIdx.x + blockDim.x * blockIdx.x;\n  v[tx] += tx;\n}\n\nint main(int argc, char **argv) {\n  float *d_vec = NULL;\n  float *h_vec = NULL;\n\n  h_vec = (float *)malloc(BLOCKS*THREADS * sizeof(float));\n  cudaMalloc((void**)&d_vec, sizeof(float) * BLOCKS * THREADS);\n  cudaMemset(d_vec, 0, BLOCKS * THREADS); // Zero the array\n\n  addToVector<<<BLOCKS, THREADS>>>(d_vec);\n  cudaMemcpy(h_vec, d_vec, BLOCKS*THREADS * sizeof(float), cudaMemcpyDeviceToHost);\n  cudaDeviceSynchronize();\n  printf(\"After : Vector 0, 1 .. N-1: %f %f .. %f\\n\", h_vec[0], h_vec[1], h_vec[BLOCKS*THREADS-1]);\n\n  cudaFree(d_vec);\n  free(h_vec);\n  exit(0);\n}\n```\nThis code contains a CUDA kernel called ```addToVector``` that performs a simple add of a value to each element in a vector, with the results written back to the same element. At \u200cfirst glance, it looks fine: allocate the vector on the device with ```cudaMalloc```, then zero it with ```cudaMemset```, then perform calculations in the kernel. It even prints out the correct answer:\n```\n$ nvcc -lineinfo initcheck_example.cu -o initcheck_example\n$ ./initcheck_example\nAfter : Vector 0, 1 .. N-1: 0.000000 1.000000 .. 63.000000\n```\nBut the code contains a small mistake. (Twenty points if you can spot it.)\nUse the NCS ```initcheck``` tool to check whether any of the accesses to the vector in global memory on the device are trying to read uninitialized values.\n```\n$ compute-sanitizer --tool initcheck ./initcheck_example\n========= COMPUTE-SANITIZER\n========= Uninitialized __global__ memory read of size 4 bytes\n=========     at 0x70 in /home/pgraham/Code/BlogExamples/initcheck_example.cu:8:addToVector(float *)\n=========     by thread (16,0,0) in block (0,0,0)\n\n. . .\n\n========= Uninitialized __global__ memory read of size 4 bytes\n=========     at 0x70 in /home/pgraham/Code/BlogExamples/initcheck_example.cu:8:addToVector(float *)\n=========     by thread (17,0,0) in block (0,0,0)\n. . . \n=========\nAfter : Vector 0, 1 .. N-1: 0.000000 1.000000 .. 63.000000\n========= ERROR SUMMARY: 48 errors\n```\nThis should print a lot of information (the output shown has been edited for brevity), but something is not right. A large quantity of the output is backtrace information, which can be hidden using the ```--show-backtrace no``` option:\n```\n$ compute-sanitizer --tool initcheck --show-backtrace no ./initcheck_example\n```\nLooking at the output, you can see 48 errors in total. The report shows that they are all of the type, ```Uninitialized __global__ memory read of size 4 bytes```.\nEach message refers to an attempt to read something from global device memory, and that something had a size of 4 bytes. A reasonable guess would be that the errors refer to attempts to access elements of the vector, made up of floats that are 4 bytes each.\nLooking at the first error, the next part of the message indicates which thread, and which thread block, caused the error. In this case, it was thread 16 in block 0. As the kernel is set up so each thread accesses a different element of the vector, element 17 of the vector, ```d_vec[16]```, was uninitialized.\nIn your output, you may see a different thread as the first one causing an error. The GPU can schedule warps (groups of 32 threads) in whatever order it sees fit. But check through the rest of the output, and convince yourself that the lowest element in the vector causing an error was element 17 (thread 16 from block 0).\nNext, look at the line of code that initialized (or should have initialized) the array:\n```\ncudaMemset(d_vec, 0, BLOCKS * THREADS); // Zero the array\n```\nChecking the definition of ```cudaMemset```, it takes three arguments: the pointer to the device memory you want to set ( ```d_vec``` in this case), the value to which each byte in that memory region should be set (0 in this case), and the number of bytes to set ( ```BLOCKS * THREADS``` ).\nNow the problem begins to become more clear. The vector contains 64 elements determined by ```BLOCKS * THREADS```, but each element is a float, so the entire vector is 256 bytes long. ```cudaMemset``` was initializing only the first 64 bytes (the first 16 elements), which means the remaining 192 bytes (equivalent to 48 elements) are uninitialized. These 48 elements correspond to the 48 errors.\nThis ties in with the observation that element 17 (thread 16, block 0) was the first to cause an error. Bingo, problem found.\nTo fix the problem, change the ```cudaMemset``` call:\n```\ncudaMemset(d_vec, 0, sizeof(float) * BLOCKS * THREADS);\n```\nAnd check to make sure that the sanitizer is happy.", "Checking unused memory\nAnother feature of the ```initcheck``` tool is identifying allocated device memory that hasn\u2019t been accessed by the end of the application. In some programs, this may be deliberate\u2014using a large static buffer to handle a range of potential problem sizes, for example. But when this is more likely an error causing a bug, use ```initcheck```, as shown below.\n```\n#include <stdio.h>\n\n#define N 10\n\n__global__ void initArray(float* array, float value) {\n  int threadGlobalID = threadIdx.x + blockIdx.x * blockDim.x;\n  if (threadGlobalID < N)\n    array[threadGlobalID] = value;\n  return;\n}\n\nint main() {\n  float* array;\n\n  const int numThreadsPerBlock = 4;\n  const int numBlocks = 2;\n\n  cudaMalloc((void**)&array, sizeof(float) * N);\n\n  initArray<<<numBlocks, numThreadsPerBlock>>>(array, 3.0);\n  cudaDeviceSynchronize();\n\n  cudaFree(array);\n  exit(0);\n}\n```\nThis very basic code will reveal the potential error. It is initializing an array, but the number of threads and the number of blocks are hard-coded. The execution configuration ```<<< \u2026 >>>``` will launch a grid of eight threads while the dataset has 10 elements (the last two elements will go unused).\nCheck this using the track-unused-memory option. Note that the required syntax will depend on the CUDA version in use. For versions before 12.3, supply an argument \u201cyes\u201d using the following:\n```\n--track-unused-memory yes ;\n```\nBeginning with version 12.3, it is not necessary to supply an argument, as shown below:\n```\n$ nvcc -o unused -lineinfo unused.cu\n$ compute-sanitizer --tool initcheck --track-unused-memory ./unused\n========= COMPUTE-SANITIZER\n=========  Unused memory in allocation 0x7fe0a7200000 of size 40 bytes\n=========     Not written 8 bytes at offset 0x20 (0x7fe0a7200020)\n=========     20% of allocation were unused.\n=========\n========= ERROR SUMMARY: 1 error\n```\nClearly, ```track-unused-memory``` indicates that the array of 40 bytes (10 x 4 byte floats) includes 8 bytes that were not written to. Use the array address (the first long 0x\u2026 number) and the offset (0 x 20, which is 32 in decimal, so 32 bytes or 8 floats along) to see which bytes were unused. As expected, floats 9 and 10 in the array were not used.\nTo fix this, use ```N``` to define ```numBlocks```:\n```\nconst int numBlocks = (N + numThreadsPerBlock - 1) / numThreadsPerBlock;\n```\nNote that ```--track-unused-memory``` is designed to work for device memory assigned with ```cudaMalloc```. The feature doesn\u2019t work for unified memory ( ```cudaMallocManaged``` allocated memory, for example).", "Synchronization checking\nThe capability to synchronize threads at a variety of levels (beyond just block and warp) is a powerful CUDA feature, enabled by the Cooperative Groups programming model. Cooperative groups are device code APIs for defining, partitioning, and synchronizing groups of threads, giving much more flexibility and control compared to the standard ```syncthreads``` function, which synchronizes all the threads in a block. For more details, see Cooperative Groups: Flexible CUDA Thread Programming.\nYet, this capability comes with greater opportunities to introduce bugs. This is where NCS ```synccheck``` can help to identify and resolve synchronization errors in CUDA code. ```synccheck``` can identify whether a CUDA application is correctly using synchronization primitives and their Cooperative Groups API counterparts.\nOne interesting use of synchronization is the application of a mask to a warp of threads. Set up the warp so that some threads are true and others are false, enabling each thread to individually perform different operations depending on that property. For more details, see Using CUDA Warp-Level Primitives.\nA useful function to help with this is ```__ballot_sync``` defined as:\n```\nunsigned int __ballot_sync(unsigned int mask, int predicate);\n\nmask``` is an initial mask, typically created with all bits set to 1, representing that all threads in the warp are initially active. ```predicate``` is a condition evaluated by each thread, where predicate evaluates to either true (non-zero) or false (zero) for each thread.\nThe ballot function evaluates the predicate for each thread in the warp, and returns a mask representing the outcome for that thread. It also provides a synchronization point. All threads in the warp must reach this ```__ballot_sync``` before any of them can proceed further.\nFor example, set up a mask where even threads in the warp are true and odd threads are false:\n```\n__ballot_sync(0xffffffff, threadID % 2 == 0);\n```\nThe initial mask ```0xffffff``` is a hexadecimal representation, and evaluates to ```11111111111111111111111111111111``` in binary. This ensures that all 32 threads are involved in the ballot.\nThe outcome of the ballot is a mask, ```0xaaaaaaaa```, which in binary is ```10101010101010101010101010101010```. The even threads (thread ID 0, 2, 4 \u2026) are set to true, and odd threads are set to false.\nThe ballot is often used in conjunction with a ```__syncwarp```, which can synchronize threads in a warp, based on the mask provided.\nThe following example uses both ```_ballot_sync``` and ```_syncwarp```:\n```\nstatic constexpr int NumThreads = 32 ;\n\n__shared__ int smem[NumThreads];\n\n__global__ void sumValues(int *sum_out) {\n    int threadID = threadIdx.x;\n\n    unsigned int mask = __ballot_sync(0xffffffff, threadID < (NumThreads / 2));\n\n    if (threadId <= (NumThreads / 2)) {\n        smem[threadId] = threadId;\n\n        __syncwarp(mask);\n\n        if (threadID == 0) {\n          *sum_out = 0;\n          for (int i = 0; i < (NumThreads / 2); ++i)\n            *sum_out += smem[i];\n        }\n    }\n\n    __syncThreads();\n}\n\nint main(){\n    int *sum_out = nullptr;\n\n    cudaMallocManaged((void**)&sum_out, sizeof(int));\n\n    sumVaules<<<1, NumThreads>>>(sum_out);\n    cudaDeviceSynchronize();\n    \n    printf(\"Sum out = %d\\n\", *sum_out);\n    cudaFree(sum_out);\n    return 0;\n}\n```\nBefore reading further, take a look at the code and try and work out what it is doing given your understanding of the ballot and the ```syncwarp``` functionality. See if you can spot what\u2019s wrong. (Fifty points for this one\u2014it\u2019s more challenging.)\nThe purpose of this code is for each thread to assign a single value to shared memory, and then sum up all the values to get one answer. However, this is applied to only half the available threads. A single warp of 32 threads is set up through the execution configuration ```<<<1, numThreads>>>``` to execute the kernel ```sumValues```.\nIn that kernel, create a mask using ```__ballot_sync``` with ```threadID < NumThreads/2``` as the predicate, which will evaluate to true for the first half of the warp where ```threadID<16``` (threads 0, 1, .. 15).\nFor those 16 threads, assign a value (the threadID) to shared memory, and perform a ```__syncwarp(mask)``` synchronization on those threads to ensure they have all\u200c written to shared memory. Then update sum_out the global sum based on those values.\nNext, try compiling and running the following code:\n```\n$ nvcc -o ballot_example -lineinfo ballot_example.cu\n$ ./ballot_example\nSum out = 0\n```\nThe answer, zero, is not correct. It should be 120 (15 + 14 + 13 + \u2026 + 2 + 1 + 0).\nDid you spot the mistake? The conditional section of code was executed using if ```(threadId <= (NumThreads / 2))```. This code uses ```<=``` rather than ```<``` as the comparator, meaning that the first 17 threads will execute.\nWhat happens when thread 17 tries to call ```syncwarp``` when it is not included as true in the mask? It\u200c causes the whole kernel to stop running, so the sum calculation is never reached. Hence the output is zero.\nAll this fails silently, and only the incorrect output indicates a problem. In \u200cmore complicated code, this could be a nightmare to track down.\nUsing ```synccheck``` provides the following:\n```\n$ compute-sanitizer --tool synccheck --show-backtrace no ./ballot_example\n========= COMPUTE-SANITIZER\n========= Barrier error detected. Invalid arguments\n=========     at 0x220 in /home/pgraham/Code/devblog/NCS_Part2/ballot_example.cu:32:sumValues(int *)\n=========     by thread (0,0,0) in block (0,0,0)\n=========\n\n. . .\n\n========= Barrier error detected. Invalid arguments\n=========     at 0x220 in /home/pgraham/Code/devblog/NCS_Part2/ballot_example.cu:32:sumValues(int *)\n=========     by thread (16,0,0) in block (0,0,0)\n=========\nSum out = 0\n========= ERROR SUMMARY: 17 errors\n```\nRegarding these 17 errors, \u201cinvalid arguments,\u201d the ```synccheck``` documentation states that the invalid argument can occur if not all threads reaching a ```__syncwarp``` declare themselves in the mask parameter.\nIn this case, thread 17 or thread (16,0,0) is not active in the mask, so it shouldn\u2019t call the ```syncwarp```. Note that this causes all the other threads calling the ```syncwarp``` to also register an error. They are individually calling ```syncwarp```, but because one of them causes it to fail, all the other ```syncwarp``` calls must also fail. It is a collective operation that causes 17 errors in total.", "Conclusion\nThis post walked you through a few examples of how to debug code and catch bugs using the ```initcheck``` and ```synccheck``` features in NVIDIA Compute Sanitizer. To get started using NCS, download the CUDA Toolkit.\nTo learn more, visit NVIDIA/compute-sanitizer-samples on GitHub, and read the NCS documentation. Join the conversation in the NVIDIA Developer Forum dedicated to sanitizer tools. Good luck on your bug hunt!"], "document_title": "Efficient CUDA Debugging: Memory Initialization and Thread Synchronization with NVIDIA Compute Sanitizer", "document_url": "https://developer.nvidia.com/blog/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/", "document_date": "2023-10-24T16:00:00", "document_date_modified": "2023-11-02T18:14:33", "document_full_text": "Efficient CUDA Debugging: Memory Initialization and Thread Synchronization with NVIDIA Compute Sanitizer\nNVIDIA Compute Sanitizer (NCS) is a powerful tool that can save you time and effort while improving the reliability and performance of your CUDA applications.\nIn our previous post, Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer, we explored efficient debugging in the realm of parallel programming. We discussed how debugging code in the CUDA environment can be both challenging and time-consuming, especially when dealing with thousands of threads, and how NCS can help with this process.\nThis post continues our exploration of efficient CUDA debugging. It highlights a few more NCS tools and walks through several examples.\nNVIDIA Compute Sanitizer\nNCS is a suite of tools that can perform different types of checks on the functional correctness of your code. There are four main tools in NCS:\nMemcheck for memory access error and leak detection\nRacecheck, a shared memory data access hazard detection tool\nInitcheck, an uninitialized device global memory access detection tool\nSynccheck for thread synchronization hazard detection\nIn addition to these tools, NCS capabilities include:\nAn API to enable the creation of sanitizing and tracing tools that target CUDA applications\nIntegration with NVIDIA Tools Extension (NVTX)\nCoredump support for use with CUDA-GDB\nSuppression features for managing the output of the tool\nThis post will focus on debugging code and catching bugs related to uninitialized device arrays using ```initcheck```, and synchronization using ```synccheck```. See Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer for details about using ```memcheck``` for discovering memory leaks and ```racecheck``` for finding race conditions.\nInitialization checking\nNCS Initcheck helps developers identify and resolve uninitialized memory access errors in CUDA code. Uninitialized memory access can lead to unpredictable behavior and incorrect results in CUDA applications.\nNCS Initcheck can detect uninitialized memory accesses to global memory in device code and provides detailed information about the location and timing of the access, as well as the stack trace of the accessing thread. This helps to reveal the root cause of the issue and resolve the problem.\nTo provide an example, the code below benefits from initialization checking.\n```\n#include <stdio.h>\n\n#define THREADS 32\n#define BLOCKS 2\n\n__global__ void addToVector(float *v) {\n  int tx = threadIdx.x + blockDim.x * blockIdx.x;\n  v[tx] += tx;\n}\n\nint main(int argc, char **argv) {\n  float *d_vec = NULL;\n  float *h_vec = NULL;\n\n  h_vec = (float *)malloc(BLOCKS*THREADS * sizeof(float));\n  cudaMalloc((void**)&d_vec, sizeof(float) * BLOCKS * THREADS);\n  cudaMemset(d_vec, 0, BLOCKS * THREADS); // Zero the array\n\n  addToVector<<<BLOCKS, THREADS>>>(d_vec);\n  cudaMemcpy(h_vec, d_vec, BLOCKS*THREADS * sizeof(float), cudaMemcpyDeviceToHost);\n  cudaDeviceSynchronize();\n  printf(\"After : Vector 0, 1 .. N-1: %f %f .. %f\\n\", h_vec[0], h_vec[1], h_vec[BLOCKS*THREADS-1]);\n\n  cudaFree(d_vec);\n  free(h_vec);\n  exit(0);\n}\n```\nThis code contains a CUDA kernel called ```addToVector``` that performs a simple add of a value to each element in a vector, with the results written back to the same element. At \u200cfirst glance, it looks fine: allocate the vector on the device with ```cudaMalloc```, then zero it with ```cudaMemset```, then perform calculations in the kernel. It even prints out the correct answer:\n```\n$ nvcc -lineinfo initcheck_example.cu -o initcheck_example\n$ ./initcheck_example\nAfter : Vector 0, 1 .. N-1: 0.000000 1.000000 .. 63.000000\n```\nBut the code contains a small mistake. (Twenty points if you can spot it.)\nUse the NCS ```initcheck``` tool to check whether any of the accesses to the vector in global memory on the device are trying to read uninitialized values.\n```\n$ compute-sanitizer --tool initcheck ./initcheck_example\n========= COMPUTE-SANITIZER\n========= Uninitialized __global__ memory read of size 4 bytes\n=========     at 0x70 in /home/pgraham/Code/BlogExamples/initcheck_example.cu:8:addToVector(float *)\n=========     by thread (16,0,0) in block (0,0,0)\n\n. . .\n\n========= Uninitialized __global__ memory read of size 4 bytes\n=========     at 0x70 in /home/pgraham/Code/BlogExamples/initcheck_example.cu:8:addToVector(float *)\n=========     by thread (17,0,0) in block (0,0,0)\n. . . \n=========\nAfter : Vector 0, 1 .. N-1: 0.000000 1.000000 .. 63.000000\n========= ERROR SUMMARY: 48 errors\n```\nThis should print a lot of information (the output shown has been edited for brevity), but something is not right. A large quantity of the output is backtrace information, which can be hidden using the ```--show-backtrace no``` option:\n```\n$ compute-sanitizer --tool initcheck --show-backtrace no ./initcheck_example\n```\nLooking at the output, you can see 48 errors in total. The report shows that they are all of the type, ```Uninitialized __global__ memory read of size 4 bytes```.\nEach message refers to an attempt to read something from global device memory, and that something had a size of 4 bytes. A reasonable guess would be that the errors refer to attempts to access elements of the vector, made up of floats that are 4 bytes each.\nLooking at the first error, the next part of the message indicates which thread, and which thread block, caused the error. In this case, it was thread 16 in block 0. As the kernel is set up so each thread accesses a different element of the vector, element 17 of the vector, ```d_vec[16]```, was uninitialized.\nIn your output, you may see a different thread as the first one causing an error. The GPU can schedule warps (groups of 32 threads) in whatever order it sees fit. But check through the rest of the output, and convince yourself that the lowest element in the vector causing an error was element 17 (thread 16 from block 0).\nNext, look at the line of code that initialized (or should have initialized) the array:\n```\ncudaMemset(d_vec, 0, BLOCKS * THREADS); // Zero the array\n```\nChecking the definition of ```cudaMemset```, it takes three arguments: the pointer to the device memory you want to set ( ```d_vec``` in this case), the value to which each byte in that memory region should be set (0 in this case), and the number of bytes to set ( ```BLOCKS * THREADS``` ).\nNow the problem begins to become more clear. The vector contains 64 elements determined by ```BLOCKS * THREADS```, but each element is a float, so the entire vector is 256 bytes long. ```cudaMemset``` was initializing only the first 64 bytes (the first 16 elements), which means the remaining 192 bytes (equivalent to 48 elements) are uninitialized. These 48 elements correspond to the 48 errors.\nThis ties in with the observation that element 17 (thread 16, block 0) was the first to cause an error. Bingo, problem found.\nTo fix the problem, change the ```cudaMemset``` call:\n```\ncudaMemset(d_vec, 0, sizeof(float) * BLOCKS * THREADS);\n```\nAnd check to make sure that the sanitizer is happy.\nChecking unused memory\nAnother feature of the ```initcheck``` tool is identifying allocated device memory that hasn\u2019t been accessed by the end of the application. In some programs, this may be deliberate\u2014using a large static buffer to handle a range of potential problem sizes, for example. But when this is more likely an error causing a bug, use ```initcheck```, as shown below.\n```\n#include <stdio.h>\n\n#define N 10\n\n__global__ void initArray(float* array, float value) {\n  int threadGlobalID = threadIdx.x + blockIdx.x * blockDim.x;\n  if (threadGlobalID < N)\n    array[threadGlobalID] = value;\n  return;\n}\n\nint main() {\n  float* array;\n\n  const int numThreadsPerBlock = 4;\n  const int numBlocks = 2;\n\n  cudaMalloc((void**)&array, sizeof(float) * N);\n\n  initArray<<<numBlocks, numThreadsPerBlock>>>(array, 3.0);\n  cudaDeviceSynchronize();\n\n  cudaFree(array);\n  exit(0);\n}\n```\nThis very basic code will reveal the potential error. It is initializing an array, but the number of threads and the number of blocks are hard-coded. The execution configuration ```<<< \u2026 >>>``` will launch a grid of eight threads while the dataset has 10 elements (the last two elements will go unused).\nCheck this using the track-unused-memory option. Note that the required syntax will depend on the CUDA version in use. For versions before 12.3, supply an argument \u201cyes\u201d using the following:\n```\n--track-unused-memory yes ;\n```\nBeginning with version 12.3, it is not necessary to supply an argument, as shown below:\n```\n$ nvcc -o unused -lineinfo unused.cu\n$ compute-sanitizer --tool initcheck --track-unused-memory ./unused\n========= COMPUTE-SANITIZER\n=========  Unused memory in allocation 0x7fe0a7200000 of size 40 bytes\n=========     Not written 8 bytes at offset 0x20 (0x7fe0a7200020)\n=========     20% of allocation were unused.\n=========\n========= ERROR SUMMARY: 1 error\n```\nClearly, ```track-unused-memory``` indicates that the array of 40 bytes (10 x 4 byte floats) includes 8 bytes that were not written to. Use the array address (the first long 0x\u2026 number) and the offset (0 x 20, which is 32 in decimal, so 32 bytes or 8 floats along) to see which bytes were unused. As expected, floats 9 and 10 in the array were not used.\nTo fix this, use ```N``` to define ```numBlocks```:\n```\nconst int numBlocks = (N + numThreadsPerBlock - 1) / numThreadsPerBlock;\n```\nNote that ```--track-unused-memory``` is designed to work for device memory assigned with ```cudaMalloc```. The feature doesn\u2019t work for unified memory ( ```cudaMallocManaged``` allocated memory, for example).\nSynchronization checking\nThe capability to synchronize threads at a variety of levels (beyond just block and warp) is a powerful CUDA feature, enabled by the Cooperative Groups programming model. Cooperative groups are device code APIs for defining, partitioning, and synchronizing groups of threads, giving much more flexibility and control compared to the standard ```syncthreads``` function, which synchronizes all the threads in a block. For more details, see Cooperative Groups: Flexible CUDA Thread Programming.\nYet, this capability comes with greater opportunities to introduce bugs. This is where NCS ```synccheck``` can help to identify and resolve synchronization errors in CUDA code. ```synccheck``` can identify whether a CUDA application is correctly using synchronization primitives and their Cooperative Groups API counterparts.\nOne interesting use of synchronization is the application of a mask to a warp of threads. Set up the warp so that some threads are true and others are false, enabling each thread to individually perform different operations depending on that property. For more details, see Using CUDA Warp-Level Primitives.\nA useful function to help with this is ```__ballot_sync``` defined as:\n```\nunsigned int __ballot_sync(unsigned int mask, int predicate);\n\nmask``` is an initial mask, typically created with all bits set to 1, representing that all threads in the warp are initially active. ```predicate``` is a condition evaluated by each thread, where predicate evaluates to either true (non-zero) or false (zero) for each thread.\nThe ballot function evaluates the predicate for each thread in the warp, and returns a mask representing the outcome for that thread. It also provides a synchronization point. All threads in the warp must reach this ```__ballot_sync``` before any of them can proceed further.\nFor example, set up a mask where even threads in the warp are true and odd threads are false:\n```\n__ballot_sync(0xffffffff, threadID % 2 == 0);\n```\nThe initial mask ```0xffffff``` is a hexadecimal representation, and evaluates to ```11111111111111111111111111111111``` in binary. This ensures that all 32 threads are involved in the ballot.\nThe outcome of the ballot is a mask, ```0xaaaaaaaa```, which in binary is ```10101010101010101010101010101010```. The even threads (thread ID 0, 2, 4 \u2026) are set to true, and odd threads are set to false.\nThe ballot is often used in conjunction with a ```__syncwarp```, which can synchronize threads in a warp, based on the mask provided.\nThe following example uses both ```_ballot_sync``` and ```_syncwarp```:\n```\nstatic constexpr int NumThreads = 32 ;\n\n__shared__ int smem[NumThreads];\n\n__global__ void sumValues(int *sum_out) {\n    int threadID = threadIdx.x;\n\n    unsigned int mask = __ballot_sync(0xffffffff, threadID < (NumThreads / 2));\n\n    if (threadId <= (NumThreads / 2)) {\n        smem[threadId] = threadId;\n\n        __syncwarp(mask);\n\n        if (threadID == 0) {\n          *sum_out = 0;\n          for (int i = 0; i < (NumThreads / 2); ++i)\n            *sum_out += smem[i];\n        }\n    }\n\n    __syncThreads();\n}\n\nint main(){\n    int *sum_out = nullptr;\n\n    cudaMallocManaged((void**)&sum_out, sizeof(int));\n\n    sumVaules<<<1, NumThreads>>>(sum_out);\n    cudaDeviceSynchronize();\n    \n    printf(\"Sum out = %d\\n\", *sum_out);\n    cudaFree(sum_out);\n    return 0;\n}\n```\nBefore reading further, take a look at the code and try and work out what it is doing given your understanding of the ballot and the ```syncwarp``` functionality. See if you can spot what\u2019s wrong. (Fifty points for this one\u2014it\u2019s more challenging.)\nThe purpose of this code is for each thread to assign a single value to shared memory, and then sum up all the values to get one answer. However, this is applied to only half the available threads. A single warp of 32 threads is set up through the execution configuration ```<<<1, numThreads>>>``` to execute the kernel ```sumValues```.\nIn that kernel, create a mask using ```__ballot_sync``` with ```threadID < NumThreads/2``` as the predicate, which will evaluate to true for the first half of the warp where ```threadID<16``` (threads 0, 1, .. 15).\nFor those 16 threads, assign a value (the threadID) to shared memory, and perform a ```__syncwarp(mask)``` synchronization on those threads to ensure they have all\u200c written to shared memory. Then update sum_out the global sum based on those values.\nNext, try compiling and running the following code:\n```\n$ nvcc -o ballot_example -lineinfo ballot_example.cu\n$ ./ballot_example\nSum out = 0\n```\nThe answer, zero, is not correct. It should be 120 (15 + 14 + 13 + \u2026 + 2 + 1 + 0).\nDid you spot the mistake? The conditional section of code was executed using if ```(threadId <= (NumThreads / 2))```. This code uses ```<=``` rather than ```<``` as the comparator, meaning that the first 17 threads will execute.\nWhat happens when thread 17 tries to call ```syncwarp``` when it is not included as true in the mask? It\u200c causes the whole kernel to stop running, so the sum calculation is never reached. Hence the output is zero.\nAll this fails silently, and only the incorrect output indicates a problem. In \u200cmore complicated code, this could be a nightmare to track down.\nUsing ```synccheck``` provides the following:\n```\n$ compute-sanitizer --tool synccheck --show-backtrace no ./ballot_example\n========= COMPUTE-SANITIZER\n========= Barrier error detected. Invalid arguments\n=========     at 0x220 in /home/pgraham/Code/devblog/NCS_Part2/ballot_example.cu:32:sumValues(int *)\n=========     by thread (0,0,0) in block (0,0,0)\n=========\n\n. . .\n\n========= Barrier error detected. Invalid arguments\n=========     at 0x220 in /home/pgraham/Code/devblog/NCS_Part2/ballot_example.cu:32:sumValues(int *)\n=========     by thread (16,0,0) in block (0,0,0)\n=========\nSum out = 0\n========= ERROR SUMMARY: 17 errors\n```\nRegarding these 17 errors, \u201cinvalid arguments,\u201d the ```synccheck``` documentation states that the invalid argument can occur if not all threads reaching a ```__syncwarp``` declare themselves in the mask parameter.\nIn this case, thread 17 or thread (16,0,0) is not active in the mask, so it shouldn\u2019t call the ```syncwarp```. Note that this causes all the other threads calling the ```syncwarp``` to also register an error. They are individually calling ```syncwarp```, but because one of them causes it to fail, all the other ```syncwarp``` calls must also fail. It is a collective operation that causes 17 errors in total.\nConclusion\nThis post walked you through a few examples of how to debug code and catch bugs using the ```initcheck``` and ```synccheck``` features in NVIDIA Compute Sanitizer. To get started using NCS, download the CUDA Toolkit.\nTo learn more, visit NVIDIA/compute-sanitizer-samples on GitHub, and read the NCS documentation. Join the conversation in the NVIDIA Developer Forum dedicated to sanitizer tools. Good luck on your bug hunt!"}], "https://developer.nvidia.com/blog/differentiable-slang-a-shading-language-for-renderers-that-learn/": [{"text": "NVIDIA has released a research paper on SLANG.D, a shading language for renderers that learn, which allows for a unified platform for real-time, inverse, and differentiable rendering. The language, developed in collaboration with MIT, UCSD, and UW researchers, is open-source and supports modern language constructs for high-performance graphics codebases. It can generate code for various platforms and seamlessly integrates automatic differentiation with GPU graphics pipelines, Python, and PyTorch. Differentiable programming using SLANG enables developers to bring learning to rendering, build differentiable renderers from existing code, and bring graphics to ML training frameworks. The language offers a complete developer toolset and can emit derivative function code in multiple languages. By avoiding unnecessary synchronizations and global memory accesses, SLANG's automatic differentiation feature enables up to 10x training speedups compared to standard PyTorch operations, opening up new possibilities for inline neural networks in computer graphics research. The language's object-oriented differentiable programming model simplifies the integration of differentiable rendering in existing large codebases.", "text_components": ["Differentiable Slang: A Shading Language for Renderers That Learn\nNVIDIA just released a SIGGRAPH Asia 2023 research paper, SLANG.D: Fast, Modular and Differentiable Shader Programming. The paper shows how a single language can serve as a unified platform for real-time, inverse, and differentiable rendering. The work is a collaboration between MIT, UCSD, UW, and NVIDIA researchers.\nThis is part of a series on Differentiable Slang. For more information about practical examples of Slang with various machine learning (ML) rendering applications, see Differentiable Slang: Example Applications.\nSlang is an open-source language for real-time graphics programming that brings new capabilities for writing and maintaining large-scale, high-performance, cross-platform graphics codebases. Slang adapts modern language constructs to the high-performance demands of real-time graphics, and generates code for Direct 3D 12, Vulkan, OptiX, CUDA, and the CPU.\nWhile Slang began as a research project, it has grown into a practical solution used in the NVIDIA Omniverse and NVIDIA RTX Remix renderers, and the NVIDIA Game Works Falcor research infrastructure.\nThe new research pioneers a co-design approach. This approach shows that the complexities of automatic differentiation can be handled elegantly if differentiation is incorporated as a first-class citizen in the entire system:\nLanguage\nType system\nIntermediate representation (IR)\nOptimization passes\nAuto-completion engine\nSlang\u2019s automatic differentiation integrates seamlessly with Slang\u2019s modular programming model, GPU graphics pipelines, Python, and PyTorch. Slang supports differentiating arbitrary control flow, user-defined types, dynamic dispatch, generics, and global memory accesses. With Slang, existing real-time renderers can be made differentiable and learnable without major source code changes.", "Bridging computer graphics and machine learning\nData-driven rendering algorithms are changing computer graphics, enabling powerful new representations for shape, textures, volumetrics, materials, and post-processing algorithms that increase performance and image quality. In parallel, computer vision and ML researchers are increasingly leveraging computer graphics, for example, to improve 3D reconstruction through inverse rendering.\nBridging real-time graphics, ML, and computer vision development environments is challenging because of different tools, libraries, programming languages, and programming models. With the latest research, Slang enables you to easily take on the following tasks:\nBring learning to rendering. Slang enables graphics developers to use gradient-based optimization and solve traditional graphics problems in a data-driven manner, for example, learning mipmap hierarchies using appearance-based optimization.\nBuild differentiable renderers from existing graphics code. With Slang, we transformed a preexisting, real-time path tracer into a differentiable path tracer, reusing 90% of the Slang code.\nBring graphics to ML training frameworks. Slang generates custom PyTorch plugins from graphics shader code. In this post, we demonstrate using Slang in Nvdiffrec to generate auto-differentiated CUDA kernels.\nBring ML training inside the renderer. Slang facilitates training small neural networks inside a real-time renderer, such as the model used in neural radiance caching.", "Differentiable programming and machine learning need gradients\nPicture of a bunny figurine in a colored box.\n(l) Rendered scene\nPicture of the bunny with a green background and color translation.\n(m) Reference derivative for y-axis\nA similar picture of the bunny with the green background and color translation.\n(r) Same derivative computed by Slang\nFigure 1. Comparison of a reference derivative vs. the Slang autodiff-computed derivative Figure 1 shows the Stanford Bunny placed inside the Cornell box scene. The left column shows the rendered scene. The middle column shows a reference derivative with respect to the bunny\u2019s translation in the y-axis. The right column shows the same derivative computed by the Slang\u2019s autodiff feature, which appears identical to the reference image.\nA key pillar of ML methods is gradient-based optimization. Specifically, most ML algorithms are powered by reverse-mode automatic differentiation, an efficient way to propagate derivatives through a series of computations. This applies not only to large neural networks but also to many simpler data-driven algorithms that require the use of gradients and gradient descent.\nFrameworks like PyTorch expose high-level operations on tensors (multi-dimensional matrices) that come with hand-coded reverse-mode kernels. As you compose tensor operations to create your neural network, PyTorch composes derivative computations automatically by chaining those kernels. The result is an easy-to-use system where you do not have to write gradient flow manually, which is one of the reasons behind ML research\u2019s accelerated pace.\nUnfortunately, some computations aren\u2019t easily captured by those high-level operations on arrays, creating difficulties in expressing them efficiently. This is the case with graphics components such as a rasterizer or ray tracer, where diverging control flow and complex access patterns require a lot of inefficient active-mask tracking and other workarounds. Those workarounds are not only difficult to write and read but also have a significant performance and memory usage overhead.\nAs a result, most high-performance differentiable graphics pipelines, such as nvdiffrec, InstantNGP, and Gaussian splatting, are not written in pure Python. Instead, researchers write high-performance kernels in languages operating closer to the underlying hardware, such as CUDA, HLSL, or GLSL.\nBecause these languages do not provide automatic differentiation, these applications use hand-derived gradients. Hand-differentiation is tedious and error-prone, making it difficult for others to use or modify those algorithms. This is where Slang comes in, as it can automatically generate differentiated shader code for multiple backends.", "Designing Slang\u2019s automatic differentiation\nA split-screen image showing the original rendering of the Zero Day scene and the derivative of output color with respect to camera position.\nFigure 2. Propagated derivatives on the Zero Day scene\nFigure 2 shows propagated derivatives on the Zero Day scene computed by a differentiable path tracer written in the Falcor framework. The differentiable path tracer was built by reusing over 5K lines of preexisting shader code.\nSlang\u2019s roots can be traced to the Spark programming language presented at SIGGRAPH 2011 and, in its current form, to SIGGRAPH 2018. Adding automatic differentiation to Slang required years of research and many iterations of language design. Every part of the language and the compiler\u2014including the parser, type system, standard library, IR, optimization passes, and the Intellisense engine\u2014needed to be revised to support auto-diff as a first-class member of the language.\nSlang\u2019s type system has been extended to treat differentiability as a first-class property of functions and types. The type system enables compile-time checks to guard against common mistakes when working in differentiable programming frameworks, such as dropping derivatives unintentionally through calls to non-differentiable functions. We describe those and many more challenges and solutions in our technical paper, SLANG.D: Fast, Modular and Differentiable Shader Programming.\nIn Slang, automatic differentiation is represented as a composable operator on functions. Applying automatic differentiation on a function yields another function that can be used just as any other function. This functional design enables higher-order differentiation, which is absent in many other frameworks. The ability to differentiate a function multiple times in any combination of forward and reverse modes significantly eases the implementation of advanced rendering algorithms, such as warped-area sampling and Hessian-Hamiltonian MLT.\nSlang\u2019s standard library has also been extended to support differentiable computations, and most existing HLSL intrinsic functions are treated as differentiable functions, allowing existing code that uses these intrinsics to be automatically differentiated without modifications.\nScreenshot of Slang\u2019s Visual Studio Code extension showing the function signature of a backward-differentiated function at its call-site.\nFigure 3. Slang\u2019s Visual Studio Code extension providing interactive hinting on the signature of an automatically differentiated function.\nSlang offers a complete developer toolset, including a Visual Studio Code extension with comprehensive hinting and auto-completion support for differentiable entities, which improves productivity in our internal projects.", "Real-time graphics in the differentiable programming ecosystem\nThe Slang compiler can emit derivative function code in the following languages:\nHLSL: For use with Direct3D pipelines.\nGLSL or SPIR-V: For use with OpenGL and Vulkan.\nCUDA or OptiX: For use in standalone applications, in Python, or with tensor frameworks such as PyTorch.\nScalar C++: For debugging.\nYou can emit the same code to multiple targets. For instance, you could train efficient models with PyTorch optimizers and then deploy them in a video game or other interactive experience running on Vulkan or Direct3D without writing new or different code. A single representation written in one language is highly beneficial for long-term code maintenance and avoiding bugs arising if two versions are subtly different.\nSimilarly to the NVIDIA WARP framework for differentiable simulation, Slang contributes to the growing ecosystem of differentiable programming. Slang allows the generation of derivatives automatically and the use of them together with both lower and higher-level programming environments. It is possible to use Slang together with handwritten, heavily optimized CUDA kernels and libraries.\nIf you prefer a higher-level approach and use Python interactive notebooks for research and experimentation, you can use Slang through the slangpy package ( ```pip install slangpy``` ) from environments like Jupyter notebooks. Slang can be a part of a rich notebook, Python, PyTorch, and NumPy ecosystem to interface with data stored in various formats, interact with it using widgets, and visualize with plotting and data analysis libraries while offering an additional programming model, more suited for certain applications.", "Tensors vs. shading languages\nPyTorch and other tensor-based libraries, such as NumPy, TensorFlow, and Jax, offer fundamentally different programming models from Slang and, in general, shading languages. PyTorch is designed primarily for feed-forward neural networks where operations on each element are relatively uniform without diverging control flow. The NumPy and PyTorch n-dimensional array (ndarray) model operates on whole tensors, making it trivial to specify horizontal reductions like summing over axes and large matrix multiplications.\nBy contrast, shading languages occupy the other end of the spectrum and expose the single-instruction-multiple-threads (SIMT) model to enable you to specify programs operating on a single element or a small block of elements. This makes it easy to express intricate control flow where each set of elements executes a vastly different series of operations, such as when the rays of a path tracer strike different surfaces and execute different logic for their next bounce.\nBoth models co-exist and should be treated as complementary, as they fulfill different goals: A reduce-sum operation on a tensor would take one line of ndarray code, but hundreds of lines of code and multiple kernel launches to express efficiently in the SIMT style.\nConversely, a variable-step ray marcher can be written elegantly in the SIMT style using dynamic loops and stopping conditions, but the same ray marcher would devolve into complex and unmaintainable active-mask-tracking ndarray code. Such code is not only difficult to write and read but can perform worse as every branch gets executed for each element instead of only one or the other, depending on the active state.\nOne diagram shows that the ndarray framework wavefront model with operations on full batches and intermediate results stored in global memory. The other diagram shows that the SIMT framework fused model compiles multiple passes into one optimized kernel, using local intermediate results to save memory and bandwidth.\nFigure 4. Ndarray framework wavefront model (l) compared to SIMT framework fused model (r)", "Performance benefits\nPyTorch and other ML frameworks are built for the training and inference of large neural networks. They use heavily optimized platform libraries to perform large matrix-multiply and convolution operations.\nWhile each individual operation is extremely efficient, the intermediate data between them is serialized to the main memory and checkpointed. During training, the forward and backpropagation passes are computed serially and separately. This makes the overhead of PyTorch significant for tiny neural networks and other differentiable programming uses in real-time graphics.\nSlang\u2019s automatic differentiation feature gives you control over how gradient values are stored, accumulated, and computed, which enables significant performance and memory optimizations. By avoiding multiple kernel launches, excessive global memory accesses, and unnecessary synchronizations, it enables fusing forward and backward passes and up to 10x training speedups compared to the same small-network and graphics workloads written with standard PyTorch operations.\nThis speedup not only accelerates the training of ML models but also enables many novel applications that use smaller inline neural networks inside graphics workloads. Inline neural networks open up a whole new area of computer graphics research, such as neural radiance caching, neural texture compression, and neural appearance models.", "Show me the code!\nFor more information about Slang\u2019s open-source repository and the slangpy Python package, see the /shader-slang GitHub repo and Using Slang to Write PyTorch Kernels. The automatic differentiation language feature is documented in the Slang User Guide. We also include several differentiable Slang tutorials that walk through the code for common graphics components in Slang while introducing Slang\u2019s object-oriented differentiable programming model.\nFor more Slang and PyTorch tutorials using slangpy, see the following resources:\n1-Triangle Rasterizer (Non-Differentiable)\n1-Triangle Differentiable \u2018Soft\u2019 Rasterizer\n1-Triangle Differentiable Rasterizer using Monte Carlo Edge Sampling\nImage Fitting using Tiny Inline MLPs (using CUDA\u2019s WMMA API)\nFor more examples, see Differentiable Slang: Example Applications.", "Conclusion\nDifferentiable rendering is a powerful tool for computer graphics, computer vision, and image synthesis. While researchers have advanced its capabilities, built systems, and explored applications for years, the resulting systems were difficult to combine with existing large codebases. Now, with Slang, existing real-time renderers can be made differentiable.\nSlang greatly simplifies adding shader code to ML pipelines, and the reverse, adding learned components to rendering pipelines.\nReal-time rendering experts can now explore building ML rendering components without rewriting the rendering code in ML frameworks. Slang facilitates data-driven asset optimization and improvement and aids the research of novel neural components in traditional rendering.\nOn the other end of the spectrum, ML researchers can now leverage existing renderers and assets with complex shaders and incorporate expressive state-of-the-art shading models in new architectures.\nWe are looking forward to seeing how bridging real-time graphics and machine learning contributes to new photorealistic neural and data-driven techniques. For more information about Slang\u2019s automatic differentiation feature, see the SLANG.D: Fast, Modular and Differentiable Shader Programming paper."], "document_title": "Differentiable Slang: A Shading Language for Renderers That Learn", "document_url": "https://developer.nvidia.com/blog/differentiable-slang-a-shading-language-for-renderers-that-learn/", "document_date": "2023-10-23T04:02:00", "document_date_modified": "2023-11-02T20:23:44", "document_full_text": "Differentiable Slang: A Shading Language for Renderers That Learn\nNVIDIA just released a SIGGRAPH Asia 2023 research paper, SLANG.D: Fast, Modular and Differentiable Shader Programming. The paper shows how a single language can serve as a unified platform for real-time, inverse, and differentiable rendering. The work is a collaboration between MIT, UCSD, UW, and NVIDIA researchers.\nThis is part of a series on Differentiable Slang. For more information about practical examples of Slang with various machine learning (ML) rendering applications, see Differentiable Slang: Example Applications.\nSlang is an open-source language for real-time graphics programming that brings new capabilities for writing and maintaining large-scale, high-performance, cross-platform graphics codebases. Slang adapts modern language constructs to the high-performance demands of real-time graphics, and generates code for Direct 3D 12, Vulkan, OptiX, CUDA, and the CPU.\nWhile Slang began as a research project, it has grown into a practical solution used in the NVIDIA Omniverse and NVIDIA RTX Remix renderers, and the NVIDIA Game Works Falcor research infrastructure.\nThe new research pioneers a co-design approach. This approach shows that the complexities of automatic differentiation can be handled elegantly if differentiation is incorporated as a first-class citizen in the entire system:\nLanguage\nType system\nIntermediate representation (IR)\nOptimization passes\nAuto-completion engine\nSlang\u2019s automatic differentiation integrates seamlessly with Slang\u2019s modular programming model, GPU graphics pipelines, Python, and PyTorch. Slang supports differentiating arbitrary control flow, user-defined types, dynamic dispatch, generics, and global memory accesses. With Slang, existing real-time renderers can be made differentiable and learnable without major source code changes.\nBridging computer graphics and machine learning\nData-driven rendering algorithms are changing computer graphics, enabling powerful new representations for shape, textures, volumetrics, materials, and post-processing algorithms that increase performance and image quality. In parallel, computer vision and ML researchers are increasingly leveraging computer graphics, for example, to improve 3D reconstruction through inverse rendering.\nBridging real-time graphics, ML, and computer vision development environments is challenging because of different tools, libraries, programming languages, and programming models. With the latest research, Slang enables you to easily take on the following tasks:\nBring learning to rendering. Slang enables graphics developers to use gradient-based optimization and solve traditional graphics problems in a data-driven manner, for example, learning mipmap hierarchies using appearance-based optimization.\nBuild differentiable renderers from existing graphics code. With Slang, we transformed a preexisting, real-time path tracer into a differentiable path tracer, reusing 90% of the Slang code.\nBring graphics to ML training frameworks. Slang generates custom PyTorch plugins from graphics shader code. In this post, we demonstrate using Slang in Nvdiffrec to generate auto-differentiated CUDA kernels.\nBring ML training inside the renderer. Slang facilitates training small neural networks inside a real-time renderer, such as the model used in neural radiance caching.\nDifferentiable programming and machine learning need gradients\nPicture of a bunny figurine in a colored box.\n(l) Rendered scene\nPicture of the bunny with a green background and color translation.\n(m) Reference derivative for y-axis\nA similar picture of the bunny with the green background and color translation.\n(r) Same derivative computed by Slang\nFigure 1. Comparison of a reference derivative vs. the Slang autodiff-computed derivative Figure 1 shows the Stanford Bunny placed inside the Cornell box scene. The left column shows the rendered scene. The middle column shows a reference derivative with respect to the bunny\u2019s translation in the y-axis. The right column shows the same derivative computed by the Slang\u2019s autodiff feature, which appears identical to the reference image.\nA key pillar of ML methods is gradient-based optimization. Specifically, most ML algorithms are powered by reverse-mode automatic differentiation, an efficient way to propagate derivatives through a series of computations. This applies not only to large neural networks but also to many simpler data-driven algorithms that require the use of gradients and gradient descent.\nFrameworks like PyTorch expose high-level operations on tensors (multi-dimensional matrices) that come with hand-coded reverse-mode kernels. As you compose tensor operations to create your neural network, PyTorch composes derivative computations automatically by chaining those kernels. The result is an easy-to-use system where you do not have to write gradient flow manually, which is one of the reasons behind ML research\u2019s accelerated pace.\nUnfortunately, some computations aren\u2019t easily captured by those high-level operations on arrays, creating difficulties in expressing them efficiently. This is the case with graphics components such as a rasterizer or ray tracer, where diverging control flow and complex access patterns require a lot of inefficient active-mask tracking and other workarounds. Those workarounds are not only difficult to write and read but also have a significant performance and memory usage overhead.\nAs a result, most high-performance differentiable graphics pipelines, such as nvdiffrec, InstantNGP, and Gaussian splatting, are not written in pure Python. Instead, researchers write high-performance kernels in languages operating closer to the underlying hardware, such as CUDA, HLSL, or GLSL.\nBecause these languages do not provide automatic differentiation, these applications use hand-derived gradients. Hand-differentiation is tedious and error-prone, making it difficult for others to use or modify those algorithms. This is where Slang comes in, as it can automatically generate differentiated shader code for multiple backends.\nDesigning Slang\u2019s automatic differentiation\nA split-screen image showing the original rendering of the Zero Day scene and the derivative of output color with respect to camera position.\nFigure 2. Propagated derivatives on the Zero Day scene\nFigure 2 shows propagated derivatives on the Zero Day scene computed by a differentiable path tracer written in the Falcor framework. The differentiable path tracer was built by reusing over 5K lines of preexisting shader code.\nSlang\u2019s roots can be traced to the Spark programming language presented at SIGGRAPH 2011 and, in its current form, to SIGGRAPH 2018. Adding automatic differentiation to Slang required years of research and many iterations of language design. Every part of the language and the compiler\u2014including the parser, type system, standard library, IR, optimization passes, and the Intellisense engine\u2014needed to be revised to support auto-diff as a first-class member of the language.\nSlang\u2019s type system has been extended to treat differentiability as a first-class property of functions and types. The type system enables compile-time checks to guard against common mistakes when working in differentiable programming frameworks, such as dropping derivatives unintentionally through calls to non-differentiable functions. We describe those and many more challenges and solutions in our technical paper, SLANG.D: Fast, Modular and Differentiable Shader Programming.\nIn Slang, automatic differentiation is represented as a composable operator on functions. Applying automatic differentiation on a function yields another function that can be used just as any other function. This functional design enables higher-order differentiation, which is absent in many other frameworks. The ability to differentiate a function multiple times in any combination of forward and reverse modes significantly eases the implementation of advanced rendering algorithms, such as warped-area sampling and Hessian-Hamiltonian MLT.\nSlang\u2019s standard library has also been extended to support differentiable computations, and most existing HLSL intrinsic functions are treated as differentiable functions, allowing existing code that uses these intrinsics to be automatically differentiated without modifications.\nScreenshot of Slang\u2019s Visual Studio Code extension showing the function signature of a backward-differentiated function at its call-site.\nFigure 3. Slang\u2019s Visual Studio Code extension providing interactive hinting on the signature of an automatically differentiated function.\nSlang offers a complete developer toolset, including a Visual Studio Code extension with comprehensive hinting and auto-completion support for differentiable entities, which improves productivity in our internal projects.\nReal-time graphics in the differentiable programming ecosystem\nThe Slang compiler can emit derivative function code in the following languages:\nHLSL: For use with Direct3D pipelines.\nGLSL or SPIR-V: For use with OpenGL and Vulkan.\nCUDA or OptiX: For use in standalone applications, in Python, or with tensor frameworks such as PyTorch.\nScalar C++: For debugging.\nYou can emit the same code to multiple targets. For instance, you could train efficient models with PyTorch optimizers and then deploy them in a video game or other interactive experience running on Vulkan or Direct3D without writing new or different code. A single representation written in one language is highly beneficial for long-term code maintenance and avoiding bugs arising if two versions are subtly different.\nSimilarly to the NVIDIA WARP framework for differentiable simulation, Slang contributes to the growing ecosystem of differentiable programming. Slang allows the generation of derivatives automatically and the use of them together with both lower and higher-level programming environments. It is possible to use Slang together with handwritten, heavily optimized CUDA kernels and libraries.\nIf you prefer a higher-level approach and use Python interactive notebooks for research and experimentation, you can use Slang through the slangpy package ( ```pip install slangpy``` ) from environments like Jupyter notebooks. Slang can be a part of a rich notebook, Python, PyTorch, and NumPy ecosystem to interface with data stored in various formats, interact with it using widgets, and visualize with plotting and data analysis libraries while offering an additional programming model, more suited for certain applications.\nTensors vs. shading languages\nPyTorch and other tensor-based libraries, such as NumPy, TensorFlow, and Jax, offer fundamentally different programming models from Slang and, in general, shading languages. PyTorch is designed primarily for feed-forward neural networks where operations on each element are relatively uniform without diverging control flow. The NumPy and PyTorch n-dimensional array (ndarray) model operates on whole tensors, making it trivial to specify horizontal reductions like summing over axes and large matrix multiplications.\nBy contrast, shading languages occupy the other end of the spectrum and expose the single-instruction-multiple-threads (SIMT) model to enable you to specify programs operating on a single element or a small block of elements. This makes it easy to express intricate control flow where each set of elements executes a vastly different series of operations, such as when the rays of a path tracer strike different surfaces and execute different logic for their next bounce.\nBoth models co-exist and should be treated as complementary, as they fulfill different goals: A reduce-sum operation on a tensor would take one line of ndarray code, but hundreds of lines of code and multiple kernel launches to express efficiently in the SIMT style.\nConversely, a variable-step ray marcher can be written elegantly in the SIMT style using dynamic loops and stopping conditions, but the same ray marcher would devolve into complex and unmaintainable active-mask-tracking ndarray code. Such code is not only difficult to write and read but can perform worse as every branch gets executed for each element instead of only one or the other, depending on the active state.\nOne diagram shows that the ndarray framework wavefront model with operations on full batches and intermediate results stored in global memory. The other diagram shows that the SIMT framework fused model compiles multiple passes into one optimized kernel, using local intermediate results to save memory and bandwidth.\nFigure 4. Ndarray framework wavefront model (l) compared to SIMT framework fused model (r)\nPerformance benefits\nPyTorch and other ML frameworks are built for the training and inference of large neural networks. They use heavily optimized platform libraries to perform large matrix-multiply and convolution operations.\nWhile each individual operation is extremely efficient, the intermediate data between them is serialized to the main memory and checkpointed. During training, the forward and backpropagation passes are computed serially and separately. This makes the overhead of PyTorch significant for tiny neural networks and other differentiable programming uses in real-time graphics.\nSlang\u2019s automatic differentiation feature gives you control over how gradient values are stored, accumulated, and computed, which enables significant performance and memory optimizations. By avoiding multiple kernel launches, excessive global memory accesses, and unnecessary synchronizations, it enables fusing forward and backward passes and up to 10x training speedups compared to the same small-network and graphics workloads written with standard PyTorch operations.\nThis speedup not only accelerates the training of ML models but also enables many novel applications that use smaller inline neural networks inside graphics workloads. Inline neural networks open up a whole new area of computer graphics research, such as neural radiance caching, neural texture compression, and neural appearance models.\nShow me the code!\nFor more information about Slang\u2019s open-source repository and the slangpy Python package, see the /shader-slang GitHub repo and Using Slang to Write PyTorch Kernels. The automatic differentiation language feature is documented in the Slang User Guide. We also include several differentiable Slang tutorials that walk through the code for common graphics components in Slang while introducing Slang\u2019s object-oriented differentiable programming model.\nFor more Slang and PyTorch tutorials using slangpy, see the following resources:\n1-Triangle Rasterizer (Non-Differentiable)\n1-Triangle Differentiable \u2018Soft\u2019 Rasterizer\n1-Triangle Differentiable Rasterizer using Monte Carlo Edge Sampling\nImage Fitting using Tiny Inline MLPs (using CUDA\u2019s WMMA API)\nFor more examples, see Differentiable Slang: Example Applications.\nConclusion\nDifferentiable rendering is a powerful tool for computer graphics, computer vision, and image synthesis. While researchers have advanced its capabilities, built systems, and explored applications for years, the resulting systems were difficult to combine with existing large codebases. Now, with Slang, existing real-time renderers can be made differentiable.\nSlang greatly simplifies adding shader code to ML pipelines, and the reverse, adding learned components to rendering pipelines.\nReal-time rendering experts can now explore building ML rendering components without rewriting the rendering code in ML frameworks. Slang facilitates data-driven asset optimization and improvement and aids the research of novel neural components in traditional rendering.\nOn the other end of the spectrum, ML researchers can now leverage existing renderers and assets with complex shaders and incorporate expressive state-of-the-art shading models in new architectures.\nWe are looking forward to seeing how bridging real-time graphics and machine learning contributes to new photorealistic neural and data-driven techniques. For more information about Slang\u2019s automatic differentiation feature, see the SLANG.D: Fast, Modular and Differentiable Shader Programming paper."}], "https://developer.nvidia.com/blog/ai-red-team-machine-learning-security-training/": [{"text": "At Black Hat USA 2023, NVIDIA hosted a training session on machine learning (ML) security, aiming to equip professionals with the skills to evaluate and secure AI-enabled products. The two-day training covered various modules, including evasion, extraction, assessments, and poisoning, with a focus on large language models. Attendees were given Jupyter notebooks and slides to continue learning at their own pace. The training emphasized the importance of understanding threat models, techniques, and attack vectors in securing ML systems. The course aimed to provide a solid foundation in ML security for attendees from diverse backgrounds. Key lessons highlighted the need for tailored security controls and the industry's progression in addressing ML security challenges. The next iteration of the course will be at Black Hat EU, with opportunities for alternative delivery modalities. Overall, the training provided a base camp for individuals interested in ML security, offering a platform to share knowledge and collaborate in this evolving field.", "text_components": ["AI Red Team: Machine Learning Security Training\nAt Black Hat USA 2023, NVIDIA hosted a two-day training session that provided security professionals with a realistic environment and methodology to explore the unique risks presented by machine learning (ML) in today\u2019s environments.\nIn this post, the NVIDIA AI Red Team shares what was covered during the training and other opportunities to continue learning about ML security.", "Black Hat USA training\nThis has been a banner year for AI. Many security teams are being asked to evaluate and secure AI-enabled products without the skills and knowledge to appropriately evaluate their potential vulnerabilities.\nBy providing this training at one of the world\u2019s leading security conferences, we were able to share the NVIDIA AI Red Team\u2019s experience and knowledge across many different industry verticals. We helped ensure that these organizations can begin securely using and developing AI solutions. We also come from that community, so it was a comfortable space to teach in.\nThe two-day training consisted of over 20 Jupyter notebooks and 200 slides organized into the following modules:\nIntroduction\nEvasion\nExtraction\nAssessments\nInversion\nMembership Inference\nPoisoning\nAnd everything applied to large language models\nIt was a lot. Attendees took slides and notebooks home to continue iterating on the coursework at their own pace.\nThe course attempted to take students from all backgrounds and give them a solid foundation in the intersection of machine learning and security. It took students all the way from the basics of NumPy mechanics to algorithmic attacks against large language models. Each module gave some theory and then explored applied scenarios.\nStudents were given a basic methodology based on our own framework. ( NVIDIA AI Red Team\u2019s assessment framework ). They were given an environment and code that they could take back to their organizations and iterate on. There\u2019s a lot of cool work still to be done.", "Attendee questions and concerns\nOutside of reshaping questions, there were a lot of questions about the effect and likelihood of attacks against machine learning systems. Machine learning has been in defensive products for a number of years at this point. \u201cML bypasses\u201d happen every day.\nOur goal was to help attendees understand the threat models, techniques, and attack vectors so that they could design and calibrate their security controls appropriately. Security isn\u2019t one-size-fits-all, but having a working knowledge of the systems inside an organization is a prerequisite for building meaningful defenses.\nMachine learning security has an established history in academia. This course gave students experience applying those techniques to familiar security scenarios.", "Key lessons from the training\nPeople are super smart and creative. It\u2019s easy to look at security and point out flaws (and we do, many of them in ML), but security has matured significantly over the last decade. We think the security industry will rise to the challenges presented by ML as well.\nThe training was also a good exercise in industry baselining. We had a mix of professionals from all industries. Understanding where those industries are in adopting machine learning and securing systems was really interesting. It would be fair to say the majority of people are just beginning their journey.\nIn general, we came away happy that people who are interested in this field have a base camp from which to operate. It\u2019s a fun space to be in at the moment and we enjoyed sharing our material with peers.", "Opportunities to learn more about ML security\nThe next iteration of the NVIDIA Machine Learning Security course will be at Black Hat EU on December 4 and 5.\nWe are also researching alternative delivery modalities and mechanisms. If you have a request, contact the AI Red Team. Students who took the course will receive updates as we make them available!"], "document_title": "AI Red Team: Machine Learning Security Training", "document_url": "https://developer.nvidia.com/blog/ai-red-team-machine-learning-security-training/", "document_date": "2023-10-19T20:26:15", "document_date_modified": "2023-11-02T18:48:43", "document_full_text": "AI Red Team: Machine Learning Security Training\nAt Black Hat USA 2023, NVIDIA hosted a two-day training session that provided security professionals with a realistic environment and methodology to explore the unique risks presented by machine learning (ML) in today\u2019s environments.\nIn this post, the NVIDIA AI Red Team shares what was covered during the training and other opportunities to continue learning about ML security.\nBlack Hat USA training\nThis has been a banner year for AI. Many security teams are being asked to evaluate and secure AI-enabled products without the skills and knowledge to appropriately evaluate their potential vulnerabilities.\nBy providing this training at one of the world\u2019s leading security conferences, we were able to share the NVIDIA AI Red Team\u2019s experience and knowledge across many different industry verticals. We helped ensure that these organizations can begin securely using and developing AI solutions. We also come from that community, so it was a comfortable space to teach in.\nThe two-day training consisted of over 20 Jupyter notebooks and 200 slides organized into the following modules:\nIntroduction\nEvasion\nExtraction\nAssessments\nInversion\nMembership Inference\nPoisoning\nAnd everything applied to large language models\nIt was a lot. Attendees took slides and notebooks home to continue iterating on the coursework at their own pace.\nThe course attempted to take students from all backgrounds and give them a solid foundation in the intersection of machine learning and security. It took students all the way from the basics of NumPy mechanics to algorithmic attacks against large language models. Each module gave some theory and then explored applied scenarios.\nStudents were given a basic methodology based on our own framework. ( NVIDIA AI Red Team\u2019s assessment framework ). They were given an environment and code that they could take back to their organizations and iterate on. There\u2019s a lot of cool work still to be done.\nAttendee questions and concerns\nOutside of reshaping questions, there were a lot of questions about the effect and likelihood of attacks against machine learning systems. Machine learning has been in defensive products for a number of years at this point. \u201cML bypasses\u201d happen every day.\nOur goal was to help attendees understand the threat models, techniques, and attack vectors so that they could design and calibrate their security controls appropriately. Security isn\u2019t one-size-fits-all, but having a working knowledge of the systems inside an organization is a prerequisite for building meaningful defenses.\nMachine learning security has an established history in academia. This course gave students experience applying those techniques to familiar security scenarios.\nKey lessons from the training\nPeople are super smart and creative. It\u2019s easy to look at security and point out flaws (and we do, many of them in ML), but security has matured significantly over the last decade. We think the security industry will rise to the challenges presented by ML as well.\nThe training was also a good exercise in industry baselining. We had a mix of professionals from all industries. Understanding where those industries are in adopting machine learning and securing systems was really interesting. It would be fair to say the majority of people are just beginning their journey.\nIn general, we came away happy that people who are interested in this field have a base camp from which to operate. It\u2019s a fun space to be in at the moment and we enjoyed sharing our material with peers.\nOpportunities to learn more about ML security\nThe next iteration of the NVIDIA Machine Learning Security course will be at Black Hat EU on December 4 and 5.\nWe are also researching alternative delivery modalities and mechanisms. If you have a request, contact the AI Red Team. Students who took the course will receive updates as we make them available!"}], "https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/": [{"text": "NVIDIA has introduced the Jetson Generative AI Lab, allowing developers to explore generative AI possibilities with Jetson edge devices. Jetson can run large language models, vision transformers, and diffusion models locally, enabling real-time testing of the latest AI models. The lab offers tutorials and resources for running generative AI applications on Jetson devices, including stable diffusion, text generation, and object detection with OWL-ViT. Projects like llamaspeak enable interactive voice conversations with AI assistants, while NanoSAM optimizes image segmentation for real-time performance on Jetson devices. The Track Anything Model allows tracking and segmenting objects in videos, and NanoDB enables indexing and searching data at the edge. The webinar on November 7, 2023, will delve deeper into deploying LLMs, optimizing vision transformers, and using multimodal agents with Jetson. Overall, the Jetson Generative AI Lab provides a platform for developers to explore the potential of generative AI applications in real-world settings.", "text_components": ["Bringing Generative AI to Life with NVIDIA Jetson\nRecently, NVIDIA unveiled Jetson Generative AI Lab, which empowers developers to explore the limitless possibilities of generative AI in a real-world setting with NVIDIA Jetson edge devices. Unlike other embedded platforms, Jetson is capable of running large language models (LLMs), vision transformers, and stable diffusion locally. That includes the largest Llama-2-70B model on Jetson AGX Orin at interactive rates.\nFour vertical bar graphs for large language models, vision language models, vision transformers, and stable diffusion.\nFigure 1. Inferencing performance of leading Generative AI models on Jetson AGX Orin\nTo swiftly test the latest models and applications on Jetson, use the tutorials and resources provided on the Jetson Generative AI lab. Now you can focus on uncovering the untapped potential of generative AIs in the physical world.\nIn this post, we explore the exciting generative AI applications that you can run and experience on Jetson devices, all of which are comprehensively covered in the lab tutorials.", "Generative AI at the edge\nIn the rapidly evolving landscape of AI, the spotlight shines brightly on generative models and the following in particular:\nLLMs that are capable of engaging in human-like conversations.\nVision language models (VLMs) that provide LLMs with the ability to perceive and understand the real world through a camera.\nDiffusion models that can transform simple text prompts into stunning visual creations.\nThese remarkable AI advancements have captured the imagination of many. However, if you delve into the infrastructure supporting this cutting-edge model inference, you would often find them tethered to the cloud, reliant on data centers for their processing power. This cloud-centric approach leaves certain edge applications, requiring high-bandwidth low-latency data processing, largely unexplored.\nVideo 1. NVIDIA Jetson Orin Brings Powerful Generative AI Models to the Edge The emerging trend of running LLMs and other generative models in local environments is gaining momentum within developer communities. Thriving online communities, like r/LocalLlama on Reddit, provide a platform for enthusiasts to discuss the latest developments in generative AI technologies and their real-world applications. Numerous technical articles published on platforms like Medium delve into the intricacies of running open-source LLMs in local setups, with some taking advantage of NVIDIA Jetson.\nThe Jetson Generative AI Lab serves as a hub for discovering the latest generative AI models and applications and learning how to run them on Jetson devices. As the field evolves at a rapid pace, with new LLMs emerging almost daily and advancements in quantization libraries reshaping benchmarks overnight, NVIDIA recognizes the importance of offering the most up-to-date information and effective tools. We offer easy-to-follow tutorials and prebuilt containers.\nThe enabling force is jetson-containers, an open-source project thoughtfully designed and meticulously maintained to build containers for Jetson devices. Using GitHub Actions, it is building 100 containers in CI/CD fashion. These empower you to quickly test the latest AI models, libraries, and applications on Jetson without the hassle of configuring underlying tools and libraries.\nThe Jetson Generative AI lab and jetson-containers enable you to focus on exploring the limitless possibilities of generative AI in real-world settings with Jetson.", "Walkthrough\nHere are some of the exciting generative AI applications that run on the NVIDIA Jetson device available in the Jetson Generative AI lab.", "stable-diffusion-webui\nGIF of Stable Diffusion interface working in a web browser to generate images from user prompts on Jetson\nFigure 2. Stable Diffusion interface\nA1111 \u2019s stable-diffusion-webui provides a user-friendly interface to Stable Diffusion released by Stability AI. It enables you to perform many tasks, including the following:\nTxt2img: Generates an image based on a text prompt.\nimg2img: Generates an image from an input image and a corresponding text prompt.\ninpainting: Fills in the missing or masked parts of the input image.\noutpainting: Expands the input image beyond its original borders.\nThe web app downloads the Stable Diffusion v1.5 model automatically during the first start, so you can start generating your image right away. If you have a Jetson Orin device, it is as easy as executing the following commands, as explained in the tutorial.\n```\ngit clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\n./run.sh $(./autotag stable-diffusion-webui)\n```\nFor more information about running stable-diffusion-webui, see the Jetson Generative AI lab tutorial. Jetson AGX Orin is also capable of running the newer Stable Diffusion XL (SDXL) models, which generated the featured image at the top of this post.", "text-generation-webui\nGIF of text-generation-webui working in a web browser, showing a conversation with robot assistant about the comparison between UC Berkeley and Stanford University.\nFigure 3. Interactive chat with Llama-2-13B on Jetson AGX Orin\nOobabooga \u2019s text-generation-webui is another popular Gradio-based web interface for running LLMs in a local environment. The official repository provides one-click installers for platforms, but jetson-containers offer an even easier method.\nUsing the interface, you can easily download a model from the Hugging Face model repository. With 4-bit quantization, the rule of thumb is that Jetson Orin Nano can generally accommodate a 7B parameter model, Jetson Orin NX 16GB can run a 13B parameter model, and Jetson AGX Orin 64GB can run whopping 70B parameter models.\nMany people are now working on Llama-2, Meta\u2019s open-source large language model, available for free for research and commercial use. There are Llama-2\u2013based models also trained using techniques like supervised fine-turning (SFT) and reinforcement learning from human feedback (RLHF). Some even claim that it is surpassing GPT-4 on some benchmarks.\nText-generation-webui provides extensions and enables you to develop your own extensions. This can be used to integrate your application as you later see in the llamaspeak example. It also has support for multimodal VLMs like Llava and chatting about images.\nGIF shows quantized Llama model responding to a query about the NASA logo.\nFigure 4. Quantized Llava-13B VLM responding to image queries\nFor more information about running text-generation-webui, see the Jetson Generative AI lab tutorial.", "llamaspeak\nScreenshot of llamaspeak interface running in a web browser, showing voice conversation between the author and Llama, the AI assistant.\nFigure 5. llamaspeak voice conversation with an LLM using Riva ASR/TTS\nLlamaspeak is an interactive chat application that employs live NVIDIA Riva ASR/TTS to enable you to carry out verbal conversations with a LLM running locally. It is currently offered as a part of jetson-containers.\nTo carry out a smooth and seamless voice conversation, minimizing the time to the first output token of an LLM is critical. On top of that, llamaspeak is designed to handle conversational interruption so that you can start talking while llamaspeak is still TTS-ing the generated response. Container microservices are used for Riva, the LLM, and the chat server.\nBlock diagram shows the conversation flow with live speech recognition, large language model, and speech synthesis\nFigure 6. Live conversation control flow with streaming ASR/LLM/TTS pipeline to web clients\nllamaspeak has a responsive interface with low-latency audio streaming from browser microphones or a microphone connected to your Jetson device. For more information about running it yourself, see the jetson-containers documentation.", "NanoOWL\nA video of two people high-fiving while body parts are interactively detected and highlighted.\nFigure 7. NanoOWL can perform object detection in real time\nOpen World Localization with Vision Transformers (OWL-ViT) is an approach for open-vocabulary detection, developed by Google Research. This model enables you to detect objects by providing text prompts for those objects.\nFor example, to detect people and cars, prompt the system with text describing the classes:\n```prompt = \u201ca person, a car\u201d``` This is incredibly valuable for rapidly developing new applications, without needing to train a new model. To unlock applications at the edge, our team developed a project, NanoOWL, which optimizes this model with NVIDIA TensorRT to obtain real-time performance on NVIDIA Jetson Orin Platforms (~95FPS encoding speed on Jetson AGX Orin). This performance means that you can run OWL-ViT well above the common camera frame rates.\nThe project also contains a new tree detection pipeline that enables you to combine the accelerated OWL-ViT model with CLIP to enable zero-shot detection and classification at any level. For example, to detect faces and classify them as happy or sad, use the following prompt:\n```prompt = \u201c[a face (happy, sad)]\u201d``` To detect faces and then detect facial features in each region of interest, use the following prompt:\n```prompt = \u201c[a face [an eye, a nose, a mouth]]\u201d``` Combine them:\n```prompt = \u201c[a face (happy, sad)[an eye, a nose, a mouth]]\u201d``` The list goes on. While the accuracy of this model may be better for some objects or classes than others, the ease of development means you can quickly try different prompts and find out if it works for you. We look forward to seeing what amazing applications that you develop!", "Segment Anything Model\nScreenshot of a Jupyter notebook running a SAM example.\nFigure 8. Jupyter notebook of Segment Anything model (SAM)\nMeta released the Segment Anything model (SAM), an advanced image segmentation model designed to precisely identify and segment objects within images regardless of their complexity or context.\nTheir official repository also has Jupyter notebooks to easily check the impact of the model, and jetson-containers offer a convenient container that has Jupyter Lab built in.", "NanoSAM\nHandheld camera video footage showing objects on a desk with a computer mouse highlighted.\nFigure 9. NanoSAM working in real time to track and segment a computer mouse\nSegment Anything (SAM) is an incredible model that is capable of turning points into segmentation masks. Unfortunately, it does not run in real time, which limits its usefulness in edge applications.\nTo get past this limitation, we\u2019ve recently released a new project, NanoSAM, which distills the SAM image encoder into a lightweight model. It also optimizes the model with NVIDIA TensorRT to enable real-time performance on NVIDIA Jetson Orin platforms. Now, you can easily turn your existing bounding box or keypoint detector into an instance segmentation model, without any training required.", "Track Anything Model\nThe Track Anything Model (TAM) is, as the team\u2019s paper explains, \u201cSegment Anything meets videos.\u201d Their open-sourced, Gradio-based interface enables you to click on a frame of an input video to specify anything to track and segment. It even showcases an additional capability of removing the tracked object by inpainting.\nGIF of a web browser running the TAM interface to process a cat video.\nFigure 10. Track Anything interface", "NanoDB\nVideo 2. Hello AI World \u2013 Realtime Multi-Modal VectorDB on NVIDIA Jetson In addition to effectively indexing and searching your data at the edge, these vector databases are often used in tandem with LLMs for retrieval-augmented generation (RAG) for long-term memory beyond their built-in context length (4096 tokens for Llama-2 models). Vision-language models also use the same embeddings as inputs.\nMultimodal agent architecture using retrieval-augmented generation (RAG) and plugins to interact with live and archived data at the edge for cyber-physical integration.\nFigure 11. Architecture diagram with an LLM/VLM at the core\nWith all that incoming live data from the edge and the ability to understand it, they become agents capable of interacting with the real world. For more information about experimenting with using NanoDB on your own imagery and dataset, see the lab tutorial.", "Conclusion\nThere you have it! Numerous exciting generative AI applications are emerging, and you can easily run them on Jetson Orin following these tutorials. To witness the incredible capabilities of generative AIs running locally, explore the Jetson Generative AI lab.\nIf you build your own generative AI application on Jetson and are interested in sharing your ideas, be sure to showcase your creation on the Jetson Projects forum.\nBringing Generative AI to Life with NVIDIA Jetson webinar\nJoin us Tuesday, November 7, 2023 at 9 a.m. PT for a webinar diving even deeper into many of the topics discussed in this post, along with a live Q&A!\nAccelerated APIs and quantization methods for deploying LLMs and VLMs on NVIDIA Jetson\nOptimizing vision transformers with NVIDIA TensorRT\nMultimodal agents and vector databases\nLive conversations with NVIDIA Riva ASR/TTS\nRegister now!"], "document_title": "Bringing Generative AI to Life with NVIDIA Jetson", "document_url": "https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/", "document_date": "2023-10-19T22:17:37", "document_date_modified": "2023-11-12T05:30:27", "document_full_text": "Bringing Generative AI to Life with NVIDIA Jetson\nRecently, NVIDIA unveiled Jetson Generative AI Lab, which empowers developers to explore the limitless possibilities of generative AI in a real-world setting with NVIDIA Jetson edge devices. Unlike other embedded platforms, Jetson is capable of running large language models (LLMs), vision transformers, and stable diffusion locally. That includes the largest Llama-2-70B model on Jetson AGX Orin at interactive rates.\nFour vertical bar graphs for large language models, vision language models, vision transformers, and stable diffusion.\nFigure 1. Inferencing performance of leading Generative AI models on Jetson AGX Orin\nTo swiftly test the latest models and applications on Jetson, use the tutorials and resources provided on the Jetson Generative AI lab. Now you can focus on uncovering the untapped potential of generative AIs in the physical world.\nIn this post, we explore the exciting generative AI applications that you can run and experience on Jetson devices, all of which are comprehensively covered in the lab tutorials.\nGenerative AI at the edge\nIn the rapidly evolving landscape of AI, the spotlight shines brightly on generative models and the following in particular:\nLLMs that are capable of engaging in human-like conversations.\nVision language models (VLMs) that provide LLMs with the ability to perceive and understand the real world through a camera.\nDiffusion models that can transform simple text prompts into stunning visual creations.\nThese remarkable AI advancements have captured the imagination of many. However, if you delve into the infrastructure supporting this cutting-edge model inference, you would often find them tethered to the cloud, reliant on data centers for their processing power. This cloud-centric approach leaves certain edge applications, requiring high-bandwidth low-latency data processing, largely unexplored.\nVideo 1. NVIDIA Jetson Orin Brings Powerful Generative AI Models to the Edge The emerging trend of running LLMs and other generative models in local environments is gaining momentum within developer communities. Thriving online communities, like r/LocalLlama on Reddit, provide a platform for enthusiasts to discuss the latest developments in generative AI technologies and their real-world applications. Numerous technical articles published on platforms like Medium delve into the intricacies of running open-source LLMs in local setups, with some taking advantage of NVIDIA Jetson.\nThe Jetson Generative AI Lab serves as a hub for discovering the latest generative AI models and applications and learning how to run them on Jetson devices. As the field evolves at a rapid pace, with new LLMs emerging almost daily and advancements in quantization libraries reshaping benchmarks overnight, NVIDIA recognizes the importance of offering the most up-to-date information and effective tools. We offer easy-to-follow tutorials and prebuilt containers.\nThe enabling force is jetson-containers, an open-source project thoughtfully designed and meticulously maintained to build containers for Jetson devices. Using GitHub Actions, it is building 100 containers in CI/CD fashion. These empower you to quickly test the latest AI models, libraries, and applications on Jetson without the hassle of configuring underlying tools and libraries.\nThe Jetson Generative AI lab and jetson-containers enable you to focus on exploring the limitless possibilities of generative AI in real-world settings with Jetson.\nWalkthrough\nHere are some of the exciting generative AI applications that run on the NVIDIA Jetson device available in the Jetson Generative AI lab.\nstable-diffusion-webui\nGIF of Stable Diffusion interface working in a web browser to generate images from user prompts on Jetson\nFigure 2. Stable Diffusion interface\nA1111 \u2019s stable-diffusion-webui provides a user-friendly interface to Stable Diffusion released by Stability AI. It enables you to perform many tasks, including the following:\nTxt2img: Generates an image based on a text prompt.\nimg2img: Generates an image from an input image and a corresponding text prompt.\ninpainting: Fills in the missing or masked parts of the input image.\noutpainting: Expands the input image beyond its original borders.\nThe web app downloads the Stable Diffusion v1.5 model automatically during the first start, so you can start generating your image right away. If you have a Jetson Orin device, it is as easy as executing the following commands, as explained in the tutorial.\n```\ngit clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\n./run.sh $(./autotag stable-diffusion-webui)\n```\nFor more information about running stable-diffusion-webui, see the Jetson Generative AI lab tutorial. Jetson AGX Orin is also capable of running the newer Stable Diffusion XL (SDXL) models, which generated the featured image at the top of this post.\ntext-generation-webui\nGIF of text-generation-webui working in a web browser, showing a conversation with robot assistant about the comparison between UC Berkeley and Stanford University.\nFigure 3. Interactive chat with Llama-2-13B on Jetson AGX Orin\nOobabooga \u2019s text-generation-webui is another popular Gradio-based web interface for running LLMs in a local environment. The official repository provides one-click installers for platforms, but jetson-containers offer an even easier method.\nUsing the interface, you can easily download a model from the Hugging Face model repository. With 4-bit quantization, the rule of thumb is that Jetson Orin Nano can generally accommodate a 7B parameter model, Jetson Orin NX 16GB can run a 13B parameter model, and Jetson AGX Orin 64GB can run whopping 70B parameter models.\nMany people are now working on Llama-2, Meta\u2019s open-source large language model, available for free for research and commercial use. There are Llama-2\u2013based models also trained using techniques like supervised fine-turning (SFT) and reinforcement learning from human feedback (RLHF). Some even claim that it is surpassing GPT-4 on some benchmarks.\nText-generation-webui provides extensions and enables you to develop your own extensions. This can be used to integrate your application as you later see in the llamaspeak example. It also has support for multimodal VLMs like Llava and chatting about images.\nGIF shows quantized Llama model responding to a query about the NASA logo.\nFigure 4. Quantized Llava-13B VLM responding to image queries\nFor more information about running text-generation-webui, see the Jetson Generative AI lab tutorial.\nllamaspeak\nScreenshot of llamaspeak interface running in a web browser, showing voice conversation between the author and Llama, the AI assistant.\nFigure 5. llamaspeak voice conversation with an LLM using Riva ASR/TTS\nLlamaspeak is an interactive chat application that employs live NVIDIA Riva ASR/TTS to enable you to carry out verbal conversations with a LLM running locally. It is currently offered as a part of jetson-containers.\nTo carry out a smooth and seamless voice conversation, minimizing the time to the first output token of an LLM is critical. On top of that, llamaspeak is designed to handle conversational interruption so that you can start talking while llamaspeak is still TTS-ing the generated response. Container microservices are used for Riva, the LLM, and the chat server.\nBlock diagram shows the conversation flow with live speech recognition, large language model, and speech synthesis\nFigure 6. Live conversation control flow with streaming ASR/LLM/TTS pipeline to web clients\nllamaspeak has a responsive interface with low-latency audio streaming from browser microphones or a microphone connected to your Jetson device. For more information about running it yourself, see the jetson-containers documentation.\nNanoOWL\nA video of two people high-fiving while body parts are interactively detected and highlighted.\nFigure 7. NanoOWL can perform object detection in real time\nOpen World Localization with Vision Transformers (OWL-ViT) is an approach for open-vocabulary detection, developed by Google Research. This model enables you to detect objects by providing text prompts for those objects.\nFor example, to detect people and cars, prompt the system with text describing the classes:\n```prompt = \u201ca person, a car\u201d``` This is incredibly valuable for rapidly developing new applications, without needing to train a new model. To unlock applications at the edge, our team developed a project, NanoOWL, which optimizes this model with NVIDIA TensorRT to obtain real-time performance on NVIDIA Jetson Orin Platforms (~95FPS encoding speed on Jetson AGX Orin). This performance means that you can run OWL-ViT well above the common camera frame rates.\nThe project also contains a new tree detection pipeline that enables you to combine the accelerated OWL-ViT model with CLIP to enable zero-shot detection and classification at any level. For example, to detect faces and classify them as happy or sad, use the following prompt:\n```prompt = \u201c[a face (happy, sad)]\u201d``` To detect faces and then detect facial features in each region of interest, use the following prompt:\n```prompt = \u201c[a face [an eye, a nose, a mouth]]\u201d``` Combine them:\n```prompt = \u201c[a face (happy, sad)[an eye, a nose, a mouth]]\u201d``` The list goes on. While the accuracy of this model may be better for some objects or classes than others, the ease of development means you can quickly try different prompts and find out if it works for you. We look forward to seeing what amazing applications that you develop!\nSegment Anything Model\nScreenshot of a Jupyter notebook running a SAM example.\nFigure 8. Jupyter notebook of Segment Anything model (SAM)\nMeta released the Segment Anything model (SAM), an advanced image segmentation model designed to precisely identify and segment objects within images regardless of their complexity or context.\nTheir official repository also has Jupyter notebooks to easily check the impact of the model, and jetson-containers offer a convenient container that has Jupyter Lab built in.\nNanoSAM\nHandheld camera video footage showing objects on a desk with a computer mouse highlighted.\nFigure 9. NanoSAM working in real time to track and segment a computer mouse\nSegment Anything (SAM) is an incredible model that is capable of turning points into segmentation masks. Unfortunately, it does not run in real time, which limits its usefulness in edge applications.\nTo get past this limitation, we\u2019ve recently released a new project, NanoSAM, which distills the SAM image encoder into a lightweight model. It also optimizes the model with NVIDIA TensorRT to enable real-time performance on NVIDIA Jetson Orin platforms. Now, you can easily turn your existing bounding box or keypoint detector into an instance segmentation model, without any training required.\nTrack Anything Model\nThe Track Anything Model (TAM) is, as the team\u2019s paper explains, \u201cSegment Anything meets videos.\u201d Their open-sourced, Gradio-based interface enables you to click on a frame of an input video to specify anything to track and segment. It even showcases an additional capability of removing the tracked object by inpainting.\nGIF of a web browser running the TAM interface to process a cat video.\nFigure 10. Track Anything interface\nNanoDB\nVideo 2. Hello AI World \u2013 Realtime Multi-Modal VectorDB on NVIDIA Jetson In addition to effectively indexing and searching your data at the edge, these vector databases are often used in tandem with LLMs for retrieval-augmented generation (RAG) for long-term memory beyond their built-in context length (4096 tokens for Llama-2 models). Vision-language models also use the same embeddings as inputs.\nMultimodal agent architecture using retrieval-augmented generation (RAG) and plugins to interact with live and archived data at the edge for cyber-physical integration.\nFigure 11. Architecture diagram with an LLM/VLM at the core\nWith all that incoming live data from the edge and the ability to understand it, they become agents capable of interacting with the real world. For more information about experimenting with using NanoDB on your own imagery and dataset, see the lab tutorial.\nConclusion\nThere you have it! Numerous exciting generative AI applications are emerging, and you can easily run them on Jetson Orin following these tutorials. To witness the incredible capabilities of generative AIs running locally, explore the Jetson Generative AI lab.\nIf you build your own generative AI application on Jetson and are interested in sharing your ideas, be sure to showcase your creation on the Jetson Projects forum.\nBringing Generative AI to Life with NVIDIA Jetson webinar\nJoin us Tuesday, November 7, 2023 at 9 a.m. PT for a webinar diving even deeper into many of the topics discussed in this post, along with a live Q&A!\nAccelerated APIs and quantization methods for deploying LLMs and VLMs on NVIDIA Jetson\nOptimizing vision transformers with NVIDIA TensorRT\nMultimodal agents and vector databases\nLive conversations with NVIDIA Riva ASR/TTS\nRegister now!"}], "https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/": [{"text": "The article discusses the importance of data in AI systems and how synthetic data generation can be used to enhance training for computer vision models. The latest update to NVIDIA Omniverse Replicator introduces features such as low-code configuration, asynchronous rendering, and event-based triggers to improve the process of generating synthetic data. These updates make it easier for developers to create diverse and large sets of training data for various use cases in robotics, visual inspection, and autonomous driving. The Replicator extension is highly customizable and can be integrated into workflows for robotics and autonomous vehicles. Developers can now use YAML-based files to describe parameters for data generation, batch-generate data, and control triggers for specific events or conditions. The article encourages developers to explore the new features of Omniverse Replicator 1.10 and provides resources for getting started with synthetic data generation applications.", "text_components": ["Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10\nData is the lifeblood of AI systems, which rely on robust datasets to learn and make predictions or decisions. For perception AI models specifically, it is essential that data reflects real-world environments and incorporates the array of scenarios. This includes edge use cases for which data is often difficult to collect, such as street traffic and manufacturing assembly lines.\nTo bootstrap and accelerate training computer vision models, AI and machine learning (ML) engineers can leverage the power of synthetic data in conjunction with real-world data. Synthetic data generation (SDG) enables AI and ML engineers to generate large sets of diverse training data to address an infinite diversity of use cases that span visual inspection, robotics, and autonomous driving.\nWith the latest update of NVIDIA Omniverse Replicator, a core extension of the NVIDIA Omniverse platform built on Universal Scene Description (OpenUSD), developers can build more powerful synthetic data generation pipelines than ever before. New feature highlights include:\nUnlocking the power of synthetic data for AI developers with low-code, YAML-based configurator.\nScaling the overall rendering process through asynchronous rendering that disaggregates the sensor simulation from rendering tasks.\nAchieving greater flexibility during the data generation process with event-based conditional triggers.\nOmniverse Replicator enables developers to build a data factory for training computer vision models. Additionally, Replicator is highly customizable and extensible, making it amenable to fit into many computer vision workflows.\nReplicator is integrated into NVIDIA Isaac Sim for robotics and NVIDIA DRIVE Sim for autonomous vehicle workflows. At ROSCon 2023, NVIDIA announced major updates to the NVIDIA Isaac Robotics platform that simplify building and testing performant AI-based robotics applications.", "Simplified and tailored solutions\nPrevious limitations with the Replicator extension required developers to write extensive lines of code to generate data for model training. AI and ML engineers not familiar with 3D content generation lacked an efficient method for generating data.\nNow, rather than writing extensive lines of code for a pre-existing scene, developers can use the YAML-based descriptive file to simply describe the parameters to change using syntax (lights, environment, location, for example). This approach makes it easier to track SDG parameters as part of the model creation and performance lineage, empowering the true data-centric model development approach.\nIn addition, developers can use the YAML file to batch-generate data using Replicator through Omniverse Farm running on an NVIDIA OVX system with minimal user intervention. Users can easily share and distribute code recipes to create new versions of the same file and create an automated pipeline for data generation.\nVideo 1. Learn a simple YAML-based workflow for generating training data by randomizing the location of objects commonly found in a warehouse", "Scaling synthetic data generation with asynchronous rendering\nWorld simulation, sensor simulation, and rendering tasks for SDG are typically implemented as a tightly integrated synchronous application. This limits the flexibility to simulate sensors operating at different rates without compromising performance.\nAsynchronous rendering runs the simulation and rendering of sensors asynchronously from one another, empowering users with finer control over the entire process. This enables developers to render synthetic data \u200cat scale using multiple GPUs, thereby increasing \u200cthroughput.", "Superior flexibility for SDG with event-based triggers\nIn Replicator, triggers dictate when specific nodes, such as randomizers or writers, are activated. The system supports on-frame triggers, which activate nodes every frame, and on-time triggers, which activate nodes at set time intervals.\nThe latest Replicator release also introduces conditional triggers, which enable the activation of nodes based on specific events or conditions. Developers can now establish custom logic through their own functions, offering more refined control over randomizers and writers.\nRoboticists using the newest version of NVIDIA Isaac Sim can use this feature to initiate the movement of an autonomous mobile robot (AMR) in response to a particular event. This offers a robust method for controlling when and how SDG is produced, depending on simulation events.\nA gif showing a robot\u2019s movement in a scene guided by a conditional trigger towards fallen objects.\nFigure 1. A conditional trigger in NVIDIA Isaac Sim randomizes a scene for training an autonomous mobile robot", "Start developing with Omniverse Replicator\nThese are just a few of the new Omniverse Replicator 1.10 features for boosting SDG pipelines. To learn about additional features including material support, postrender augmentations, and 2D and 3D scatter node enhancements, see the Replicator documentation.\nTo start developing your own SDG applications with Omniverse Replicator, download Omniverse free and follow the instructions for getting started with Replicator in Omniverse Code.\nTo learn more about Replicator, check out the Replicator tutorials. Join the NVIDIA Omniverse Discord Server to chat with the community, and check out the synthetic data generation Discord channel.\nFollow Omniverse on Instagram, Twitter, YouTube, and Medium for additional resources and inspiration. You can also check out the NVIDIA Developer Forums for information from Omniverse experts."], "document_title": "Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10", "document_url": "https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/", "document_date": "2023-10-18T14:00:00", "document_date_modified": "2023-11-02T18:14:39", "document_full_text": "Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10\nData is the lifeblood of AI systems, which rely on robust datasets to learn and make predictions or decisions. For perception AI models specifically, it is essential that data reflects real-world environments and incorporates the array of scenarios. This includes edge use cases for which data is often difficult to collect, such as street traffic and manufacturing assembly lines.\nTo bootstrap and accelerate training computer vision models, AI and machine learning (ML) engineers can leverage the power of synthetic data in conjunction with real-world data. Synthetic data generation (SDG) enables AI and ML engineers to generate large sets of diverse training data to address an infinite diversity of use cases that span visual inspection, robotics, and autonomous driving.\nWith the latest update of NVIDIA Omniverse Replicator, a core extension of the NVIDIA Omniverse platform built on Universal Scene Description (OpenUSD), developers can build more powerful synthetic data generation pipelines than ever before. New feature highlights include:\nUnlocking the power of synthetic data for AI developers with low-code, YAML-based configurator.\nScaling the overall rendering process through asynchronous rendering that disaggregates the sensor simulation from rendering tasks.\nAchieving greater flexibility during the data generation process with event-based conditional triggers.\nOmniverse Replicator enables developers to build a data factory for training computer vision models. Additionally, Replicator is highly customizable and extensible, making it amenable to fit into many computer vision workflows.\nReplicator is integrated into NVIDIA Isaac Sim for robotics and NVIDIA DRIVE Sim for autonomous vehicle workflows. At ROSCon 2023, NVIDIA announced major updates to the NVIDIA Isaac Robotics platform that simplify building and testing performant AI-based robotics applications.\nSimplified and tailored solutions\nPrevious limitations with the Replicator extension required developers to write extensive lines of code to generate data for model training. AI and ML engineers not familiar with 3D content generation lacked an efficient method for generating data.\nNow, rather than writing extensive lines of code for a pre-existing scene, developers can use the YAML-based descriptive file to simply describe the parameters to change using syntax (lights, environment, location, for example). This approach makes it easier to track SDG parameters as part of the model creation and performance lineage, empowering the true data-centric model development approach.\nIn addition, developers can use the YAML file to batch-generate data using Replicator through Omniverse Farm running on an NVIDIA OVX system with minimal user intervention. Users can easily share and distribute code recipes to create new versions of the same file and create an automated pipeline for data generation.\nVideo 1. Learn a simple YAML-based workflow for generating training data by randomizing the location of objects commonly found in a warehouse\nScaling synthetic data generation with asynchronous rendering\nWorld simulation, sensor simulation, and rendering tasks for SDG are typically implemented as a tightly integrated synchronous application. This limits the flexibility to simulate sensors operating at different rates without compromising performance.\nAsynchronous rendering runs the simulation and rendering of sensors asynchronously from one another, empowering users with finer control over the entire process. This enables developers to render synthetic data \u200cat scale using multiple GPUs, thereby increasing \u200cthroughput.\nSuperior flexibility for SDG with event-based triggers\nIn Replicator, triggers dictate when specific nodes, such as randomizers or writers, are activated. The system supports on-frame triggers, which activate nodes every frame, and on-time triggers, which activate nodes at set time intervals.\nThe latest Replicator release also introduces conditional triggers, which enable the activation of nodes based on specific events or conditions. Developers can now establish custom logic through their own functions, offering more refined control over randomizers and writers.\nRoboticists using the newest version of NVIDIA Isaac Sim can use this feature to initiate the movement of an autonomous mobile robot (AMR) in response to a particular event. This offers a robust method for controlling when and how SDG is produced, depending on simulation events.\nA gif showing a robot\u2019s movement in a scene guided by a conditional trigger towards fallen objects.\nFigure 1. A conditional trigger in NVIDIA Isaac Sim randomizes a scene for training an autonomous mobile robot\nStart developing with Omniverse Replicator\nThese are just a few of the new Omniverse Replicator 1.10 features for boosting SDG pipelines. To learn about additional features including material support, postrender augmentations, and 2D and 3D scatter node enhancements, see the Replicator documentation.\nTo start developing your own SDG applications with Omniverse Replicator, download Omniverse free and follow the instructions for getting started with Replicator in Omniverse Code.\nTo learn more about Replicator, check out the Replicator tutorials. Join the NVIDIA Omniverse Discord Server to chat with the community, and check out the synthetic data generation Discord channel.\nFollow Omniverse on Instagram, Twitter, YouTube, and Medium for additional resources and inspiration. You can also check out the NVIDIA Developer Forums for information from Omniverse experts."}], "https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/": [{"text": "NVIDIA has announced updates to its Isaac Robotics platform, including Isaac ROS 2.0 and Isaac Sim 2023.1, to provide advanced simulation and perception tools for robotics developers. These updates aim to simplify building and testing AI-based robotic applications for developers using the Robot Operating System (ROS). The new Isaac ROS 2.0 offers accelerated computing capabilities, improved efficiency, and enhancements such as support for ROS 2 Humble, optimized ROS bridges, and integration with Stereolabs ZED cameras. On the other hand, Isaac Sim 2023.1, built on NVIDIA Omniverse, provides high-fidelity simulation for developing, training, testing, and deploying AI-enabled robots with features like synthetic data generation and support for various sensors. Companies like FarmX and .lumen are leveraging NVIDIA's platforms to develop innovative robotics solutions. The Isaac Robotics platform aims to advance AI in robotics by providing tools for developers to create and test AI robots in a virtual environment before deployment.", "text_components": ["Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform\nNVIDIA announced major updates to the NVIDIA Isaac Robotics platform today at ROSCon 2023. The platform delivers performant perception and high-fidelity simulation to robotics developers worldwide. These updates include the release of NVIDIA Isaac ROS 2.0 and NVIDIA Isaac Sim 2023.1 and perception and simulation upgrades that simplify building and testing performant AI-based robotic applications for ROS developers.\n\u201cROS continues to grow and evolve to provide open-source software for the whole robotics community,\u201d said Geoff Biggs, CTO of the Open Source Robotics Foundation. \u201cThe new NVIDIA prebuilt ROS 2 packages, launched with this release, will accelerate that growth by making ROS 2 readily available to the vast NVIDIA Jetson developer community.\u201d\nFour industrial images (Train, Simulate, Build, Deploy + Manage) and graphic representing software architecture.\nFigure 1. NVIDIA Isaac Robotics Platform is a collection of technologies for enabling AI in robotics", "Isaac ROS 2.0\nAs robotics evolves toward higher levels of autonomy and the proliferation of diverse sensor technologies continues, the constraints of CPU-bound processing become evident. Accelerated computing has emerged as a natural solution to overcome these bottlenecks.\nIsaac ROS brings much-needed accelerated computing capabilities to the ROS community, enabling the development of next-generation robotics solutions. Beyond offering a collection of accelerated ROS packages, NVIDIA also collaborates closely with Open Robotics to enhance the efficiency of the middleware itself. To learn more, see Improve Perception Performance for ROS 2 Applications with NVIDIA Isaac Transport for ROS.\nThe release of Isaac ROS 2.0 achieves production-quality readiness. Significant enhancements include:\nNative ROS 2 Humble support: NVIDIA will host installation-ready ROS 2 Humble packages for JetPack 5 on Ubuntu 20.04, eliminating the need for source code compilation.\nNITROS ROS bridge: This optimized ROS bridge delivers a 2x speedup compared to standard ROS bridges, extending Isaac ROS acceleration to ROS Noetic applications.\nCUDA NITROS: Empowers developers to use their own DNN models with custom encoders and decoders and harness the acceleration capabilities of NITROS.\nStereolabs ZED camera integration: Isaac ROS is now integrated with Stereolabs ZED cameras.\nNova Carter: Isaac ROS software including Nav 2 navigation stack now supported on Nova Carter, a reference AMR for robotics R&D, powered by NVIDIA Jetson AGX Orin.\nESS 3.0 performance: The new ESS 3.0 depth perception DNN model offers improved accuracy and performance.\nDevelopers worldwide leverage Isaac ROS for high-performance robotics solutions across diverse domains including agriculture, warehouse automation, last-mile delivery, and service robotics, among others.\nTwelve images in 4x3 matrix representing various software modules within NVIDIA Isaac ROS.\nFigure 2. Software modules within NVIDIA Isaac ROS, a key component of the NVIDIA Isaac Robotics platform \u201cIt is the breadth of NVIDIA\u2019s offering that makes it the perfect fit for FarmX,\u201d said Dan Hennage, VP of Robotics at FarmX. \u201cFrom the operating environment and libraries in Isaac ROS to the various modules that allow us to deploy on vehicles ranging from drones and large tractors, NVIDIA Jetson and Isaac ROS allow us to focus on developing our application and not worry about the platform.\u201d\u200b \u200b\n.lumen CEO and Founder Cornel Amariei said, \u201cWhat we\u2019ve built is the most advanced technology for blind assistance, scaling down autonomous driving to what people can comfortably wear on the head. It was only possible because of the NVIDIA Jetson platform, and the optimized packages, nodes, and features in Isaac ROS. From vSLAM to stereo perception, no other platform could have enabled us to do this.\u201d\u200b", "Isaac Sim 2023.1\nGrowing adoption of AI in robotics is set to accelerate automation across industries, from manufacturing to logistics to automotive. The challenge for those developing and testing AI models for robotics perception or control is often data scarcity. High-fidelity simulation is the key technology to address this scarcity.\nBuilt on NVIDIA Omniverse, Isaac Sim is a robotics simulator for developing, training, testing, and deploying AI-enabled robots. Powerful built-in capabilities include NVIDIA Omniverse Replicator for generating synthetic data, and Isaac Gym for GPU-accelerated reinforcement learning. With the latest release of Omniverse Replicator 1.10, developers can boost synthetic data generation with a low-code, YAML-based configurator and asynchronous rendering.\nIsaac Sim also includes accurate sensor simulations for most of the popular sensors available today. It supports ROS and ROS 2 and can additionally be controlled from a Python script.\nThe release of Isaac Sim 2023.1 provides many new features and improvements to advance AI-based robots, including:\nNew built-in robot models and sensors: Quickly get started using built-in robot models and sensors. New robot models include FANUC and Techman. New sensors include Orbbec, SENSING, Zvision, Ouster, and RealSense.\nImproved ROS and ROS 2 support: Create custom ROS and ROS 2 messages to support your simulation applications. URDF/MJCF importers are now open source to give you more power when importing your robots into Isaac Sim.\nEnhanced synthetic data generation (SDG) support: Randomization in simulation added for manipulator and mobile robot applications. Learn more about new SDG capabilities enabled by the latest release of Omniverse Replicator.\nWarehouse builder modular 3D assets: Use SimReady warehouse scenes and assets to quickly create compelling warehouse environments to test and exercise robot solutions.\nMany companies are developing robots and automation solutions following a simulation-first approach. This requires high-fidelity simulation to validate design and throughput before building the solution.\n\u201cAt Collaborative Robotics, we have a deep conviction that the future of robotics involves collaborative robots working alongside humans,\u201d said Jon Battles, VP of Technology Strategy. \u201cWe\u2019ve adopted a sim-first development approach, using Isaac Sim extensively to accelerate our development and deployment timelines.\u201d\nRIOS VP of Engineering Chris Paulson said, \u201cNVIDIA Isaac Sim is pivotal for how RIOS designs, tests, and implements advanced AI-powered robots-as-a-service (RaaS) work cells for the intelligent factory of the future. Isaac Sim is a critical platform for reducing project risk, derisking new work cell designs, and simplifying the development of complex robotic task execution. Our internal customer-facing tools also leverage Isaac Sim as a platform to quickly and efficiently deliver state-of-the-art robotics to our customers.\u201d\nImage of three robot arms in an industrial warehouse setting.\nFigure 3. Built-in robot models new to NVIDIA Isaac Sim 2023.1 include the FANUC Robot Arm (left), Fraunhofer EvoBOT (center), and Techman Cobot (right) The NVIDIA Isaac Robotics platform is designed from the ground up to advance AI in robotics. The latest Isaac ROS updates will make NVIDIA AI perception easier to leverage for ROS developers. And the latest release of Isaac Sim is packed with new features to easily develop, test, and train AI robots in the virtual world before deploying them to the real world. Join the thousands of developers working with NVIDIA Isaac ROS and Isaac Sim.\nTo learn more, register for the upcoming Isaac ROS webinar Need for Noetic Speed: Bringing NITROS to ROS. Join NVIDIA at ROSCon 2023 for the latest announcements, demos, contests, and partner news. And check out the NVIDIA speakers and panelists at RoboBusiness 2023."], "document_title": "Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform", "document_url": "https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/", "document_date": "2023-10-18T14:00:00", "document_date_modified": "2023-11-02T18:14:39", "document_full_text": "Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform\nNVIDIA announced major updates to the NVIDIA Isaac Robotics platform today at ROSCon 2023. The platform delivers performant perception and high-fidelity simulation to robotics developers worldwide. These updates include the release of NVIDIA Isaac ROS 2.0 and NVIDIA Isaac Sim 2023.1 and perception and simulation upgrades that simplify building and testing performant AI-based robotic applications for ROS developers.\n\u201cROS continues to grow and evolve to provide open-source software for the whole robotics community,\u201d said Geoff Biggs, CTO of the Open Source Robotics Foundation. \u201cThe new NVIDIA prebuilt ROS 2 packages, launched with this release, will accelerate that growth by making ROS 2 readily available to the vast NVIDIA Jetson developer community.\u201d\nFour industrial images (Train, Simulate, Build, Deploy + Manage) and graphic representing software architecture.\nFigure 1. NVIDIA Isaac Robotics Platform is a collection of technologies for enabling AI in robotics\nIsaac ROS 2.0\nAs robotics evolves toward higher levels of autonomy and the proliferation of diverse sensor technologies continues, the constraints of CPU-bound processing become evident. Accelerated computing has emerged as a natural solution to overcome these bottlenecks.\nIsaac ROS brings much-needed accelerated computing capabilities to the ROS community, enabling the development of next-generation robotics solutions. Beyond offering a collection of accelerated ROS packages, NVIDIA also collaborates closely with Open Robotics to enhance the efficiency of the middleware itself. To learn more, see Improve Perception Performance for ROS 2 Applications with NVIDIA Isaac Transport for ROS.\nThe release of Isaac ROS 2.0 achieves production-quality readiness. Significant enhancements include:\nNative ROS 2 Humble support: NVIDIA will host installation-ready ROS 2 Humble packages for JetPack 5 on Ubuntu 20.04, eliminating the need for source code compilation.\nNITROS ROS bridge: This optimized ROS bridge delivers a 2x speedup compared to standard ROS bridges, extending Isaac ROS acceleration to ROS Noetic applications.\nCUDA NITROS: Empowers developers to use their own DNN models with custom encoders and decoders and harness the acceleration capabilities of NITROS.\nStereolabs ZED camera integration: Isaac ROS is now integrated with Stereolabs ZED cameras.\nNova Carter: Isaac ROS software including Nav 2 navigation stack now supported on Nova Carter, a reference AMR for robotics R&D, powered by NVIDIA Jetson AGX Orin.\nESS 3.0 performance: The new ESS 3.0 depth perception DNN model offers improved accuracy and performance.\nDevelopers worldwide leverage Isaac ROS for high-performance robotics solutions across diverse domains including agriculture, warehouse automation, last-mile delivery, and service robotics, among others.\nTwelve images in 4x3 matrix representing various software modules within NVIDIA Isaac ROS.\nFigure 2. Software modules within NVIDIA Isaac ROS, a key component of the NVIDIA Isaac Robotics platform \u201cIt is the breadth of NVIDIA\u2019s offering that makes it the perfect fit for FarmX,\u201d said Dan Hennage, VP of Robotics at FarmX. \u201cFrom the operating environment and libraries in Isaac ROS to the various modules that allow us to deploy on vehicles ranging from drones and large tractors, NVIDIA Jetson and Isaac ROS allow us to focus on developing our application and not worry about the platform.\u201d\u200b \u200b\n.lumen CEO and Founder Cornel Amariei said, \u201cWhat we\u2019ve built is the most advanced technology for blind assistance, scaling down autonomous driving to what people can comfortably wear on the head. It was only possible because of the NVIDIA Jetson platform, and the optimized packages, nodes, and features in Isaac ROS. From vSLAM to stereo perception, no other platform could have enabled us to do this.\u201d\u200b\nIsaac Sim 2023.1\nGrowing adoption of AI in robotics is set to accelerate automation across industries, from manufacturing to logistics to automotive. The challenge for those developing and testing AI models for robotics perception or control is often data scarcity. High-fidelity simulation is the key technology to address this scarcity.\nBuilt on NVIDIA Omniverse, Isaac Sim is a robotics simulator for developing, training, testing, and deploying AI-enabled robots. Powerful built-in capabilities include NVIDIA Omniverse Replicator for generating synthetic data, and Isaac Gym for GPU-accelerated reinforcement learning. With the latest release of Omniverse Replicator 1.10, developers can boost synthetic data generation with a low-code, YAML-based configurator and asynchronous rendering.\nIsaac Sim also includes accurate sensor simulations for most of the popular sensors available today. It supports ROS and ROS 2 and can additionally be controlled from a Python script.\nThe release of Isaac Sim 2023.1 provides many new features and improvements to advance AI-based robots, including:\nNew built-in robot models and sensors: Quickly get started using built-in robot models and sensors. New robot models include FANUC and Techman. New sensors include Orbbec, SENSING, Zvision, Ouster, and RealSense.\nImproved ROS and ROS 2 support: Create custom ROS and ROS 2 messages to support your simulation applications. URDF/MJCF importers are now open source to give you more power when importing your robots into Isaac Sim.\nEnhanced synthetic data generation (SDG) support: Randomization in simulation added for manipulator and mobile robot applications. Learn more about new SDG capabilities enabled by the latest release of Omniverse Replicator.\nWarehouse builder modular 3D assets: Use SimReady warehouse scenes and assets to quickly create compelling warehouse environments to test and exercise robot solutions.\nMany companies are developing robots and automation solutions following a simulation-first approach. This requires high-fidelity simulation to validate design and throughput before building the solution.\n\u201cAt Collaborative Robotics, we have a deep conviction that the future of robotics involves collaborative robots working alongside humans,\u201d said Jon Battles, VP of Technology Strategy. \u201cWe\u2019ve adopted a sim-first development approach, using Isaac Sim extensively to accelerate our development and deployment timelines.\u201d\nRIOS VP of Engineering Chris Paulson said, \u201cNVIDIA Isaac Sim is pivotal for how RIOS designs, tests, and implements advanced AI-powered robots-as-a-service (RaaS) work cells for the intelligent factory of the future. Isaac Sim is a critical platform for reducing project risk, derisking new work cell designs, and simplifying the development of complex robotic task execution. Our internal customer-facing tools also leverage Isaac Sim as a platform to quickly and efficiently deliver state-of-the-art robotics to our customers.\u201d\nImage of three robot arms in an industrial warehouse setting.\nFigure 3. Built-in robot models new to NVIDIA Isaac Sim 2023.1 include the FANUC Robot Arm (left), Fraunhofer EvoBOT (center), and Techman Cobot (right) The NVIDIA Isaac Robotics platform is designed from the ground up to advance AI in robotics. The latest Isaac ROS updates will make NVIDIA AI perception easier to leverage for ROS developers. And the latest release of Isaac Sim is packed with new features to easily develop, test, and train AI robots in the virtual world before deploying them to the real world. Join the thousands of developers working with NVIDIA Isaac ROS and Isaac Sim.\nTo learn more, register for the upcoming Isaac ROS webinar Need for Noetic Speed: Bringing NITROS to ROS. Join NVIDIA at ROSCon 2023 for the latest announcements, demos, contests, and partner news. And check out the NVIDIA speakers and panelists at RoboBusiness 2023."}], "https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/": [{"text": "NVIDIA has released TensorRT-LLM, an open-source library to optimize and accelerate inference performance for large language models (LLMs) on NVIDIA GPUs. LLMs have transformed AI but can be expensive and slow to run due to their size. TensorRT-LLM offers optimizations like kernel fusion, quantization, and runtime optimizations to improve performance. The library includes pre- and post-processing steps, multi-GPU/multi-node communication, and supports models like Llama 1 and 2, ChatGLM, Falcon, and MPT. Users can compile models into TensorRT engines and use the C++ runtime for inference. Additionally, the library can be deployed with NVIDIA Triton Inference Server for production use. The release of TensorRT-LLM on GitHub provides organizations and developers with tools to harness the potential of LLMs. By exploring different models and optimizations, users can leverage the power of AI-driven language models. The library is available for native Windows as a beta release, enabling accelerated LLMs on NVIDIA RTX and GeForce RTX GPUs.", "text_components": ["Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available\nToday, NVIDIA announces the public release of TensorRT-LLM to accelerate and optimize inference performance for the latest LLMs on NVIDIA GPUs. This open-source library is now available for free on the /NVIDIA/TensorRT-LLM GitHub repo and as part of the NVIDIA NeMo framework.\nLarge language models (LLMs) have revolutionized the field of artificial intelligence and created entirely new ways of interacting with the digital world. But, as organizations and application developers around the world look to incorporate LLMs into their work, some of the challenges with running these models become apparent.\nPut simply, LLMs are large. That fact can make them expensive and slow to run without the right techniques.\nMany optimization techniques have risen to deal with this, from model optimizations like kernel fusion and quantization to runtime optimizations like C++ implementations, KV caching, continuous in-flight batching, and paged attention. It can be difficult to decide which of these are right for your use case, and to navigate the interactions between these techniques and their sometimes-incompatible implementations.\nThat\u2019s why NVIDIA introduced TensorRT-LLM, a comprehensive library for compiling and optimizing LLMs for inference. TensorRT-LLM incorporates all of those optimizations and more while providing an intuitive Python API for defining and building new models.\nThe TensorRT-LLM open-source library accelerates inference performance on the latest LLMs on NVIDIA GPUs. It is used as the optimization backbone for LLM inference in NVIDIA NeMo, an end-to-end framework to build, customize, and deploy generative AI applications into production. NeMo provides complete containers, including TensorRT-LLM and NVIDIA Triton, for generative AI deployments.\nTensorRT-LLM is also now available for native Windows as a beta release. Application developers and AI enthusiasts can now benefit from accelerated LLMs running locally on PCs and Workstations powered by NVIDIA RTX and NVIDIA GeForce RTX GPUs.\nTensorRT-LLM wraps TensorRT\u2019s deep learning compiler and includes the latest optimized kernels made for cutting-edge implementations of FlashAttention and masked multi-head attention (MHA) for LLM execution.\nTensorRT-LLM also consists of pre\u2013 and post-processing steps and multi-GPU/multi-node communication primitives in a simple, open-source Python API for groundbreaking LLM inference performance on GPUs.\nHighlights of TensorRT-LLM include the following:\nSupport for LLMs such as Llama 1 and 2, ChatGLM, Falcon, MPT, Baichuan, and Starcoder\nIn-flight batching and paged attention\nMulti-GPU multi-node (MGMN) inference\nNVIDIA Hopper tansformer engine with FP8\nSupport for NVIDIA Ampere architecture, NVIDIA Ada Lovelace architecture, and NVIDIA Hopper GPUs\nNative Windows support (beta)\nOver the past 2 years, NVIDIA has been working closely with leading LLM companies, including Anyscale, Baichuan, Cohere, Deci, Grammarly, Meta, Mistral AI, MosaicML, now part of Databricks, OctoML, Perplexity AI, Tabnine, Together.ai, Zhipu, and many others to accelerate and optimize LLM inference.\nTo help you get a feel for the library and how to use it, here\u2019s an example of how to use and deploy Llama 2, a popular publicly available LLM, with TensorRT-LLM and NVIDIA Triton on Linux. To get started with the beta release, see the TensorRT-LLM for native Windows GitHub repo.\nFor more information, including different models, different optimizations, and multi-GPU execution, see the full list of TensorRT-LLM examples.", "Getting started with installation\nStart by cloning and building the TensorRT-LLM library. The easiest way to build TensorRT-LLM and retrieve all its dependencies is to use the included Dockerfile:\n```\ngit lfs install\ngit clone -b release/0.5.0 https://github.com/NVIDIA/TensorRT-LLM.git\ncd TensorRT-LLM\ngit submodule update --init --recursive\nmake -C docker release_build\n```\nThese commands pull a base container and install all the dependencies needed for TensorRT-LLM inside the container. It then builds and installs TensorRT-LLM itself in the container.", "Retrieving the model weights\nTensorRT-LLM is a library for LLM inference, and so to use it, you need to supply a set of trained weights. You can either use your own model weights trained in a framework like NVIDIA NeMo, or pull a set of pretrained weights from repositories like the HuggingFace Hub.\nThe commands in this post automatically pull the weights and tokenizer files for the chat-tuned variant of the 7B parameter Llama 2 model from the HuggingFace Hub. You can also download the weights yourself to use offline with the following command. You just have to update the paths in later commands to point to this directory:\n```\ngit lfs install\ngit clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n```\nUsage of this model is subject to a particular license. To download the necessary files, agree to the terms and authenticate with Hugging Face.", "Compiling the model\nThe next step in the process is to compile the model into a TensorRT engine. For this, you need the model weights as well as a model definition written in the TensorRT-LLM Python API.\nThe TensorRT-LLM repository contains a wide variety of predefined model architectures. For this post, you use the included Llama model definition instead of writing your own. This is a minimal example of some of the optimizations available in TensorRT-LLM.\nFor more information about available plug-ins and quantizations, see the full Llama example and Numerical Precision.\n```\n# Launch the Tensorrt-LLM container\nmake -C docker release_run LOCAL_USER=1\n\n# Log in to huggingface-cli\n# You can get your token from huggingface.co/settings/token\nhuggingface-cli login --token *****\n\n# Compile model\npython3 examples/llama/build.py \\\n    --model_dir meta-llama/Llama-2-7b-chat-hf \\\n    --dtype float16 \\\n    --use_gpt_attention_plugin float16 \\\n    --use_gemm_plugin float16 \\\n    --remove_input_padding \\\n    --use_inflight_batching \\\n    --paged_kv_cache \\\n    --output_dir examples/llama/out\n```\nWhen you create the model definition with the TensorRT-LLM API, you build a graph of operations from NVIDIA TensorRT primitives that form the layers of your neural network. These operations map to specific kernels: prewritten programs for the GPU.\nThe TensorRT compiler can sweep through the graph to choose the best kernel for each operation and available GPU. Crucially, it can also identify patterns in the graph where multiple operations are good candidates for being fused into a single kernel. This reduces the required amount of memory movement and the overhead of launching multiple GPU kernels.\nTensorRT also compiles the graph of operations into a single CUDA Graph that can be launched all at one time, further reducing the kernel launch overhead.\nThe TensorRT compiler is extremely powerful for fusing layers and increasing execution speed, but there are some complex layer fusions\u2014like FlashAttention \u2014that involve interleaving many operations together and which can\u2019t be automatically discovered. For those, you can explicitly replace parts of the graph with plugins at compile time.\nIn this example, you include the ```gpt_attention``` plug-in, which implements a ```FlashAttention```-like fused attention kernel, and the ```gemm``` plug-in, which performs matrix multiplication with FP32 accumulation. You also call out your desired precision for the full model as FP16, matching the default precision of the weights that you downloaded from HuggingFace.\nHere\u2019s what this script produces when you finish running it. In the ```/examples/llama/out``` folder, there are now the following files:\n```Llama_float16_tp1_rank0.engine```: The main output of the build script, containing the executable graph of operations with the model weights embedded.\n```config.json```: Includes detailed information about the model, like its general structure and precision, as well as information about which plug-ins were incorporated into the engine.\n```model.cache```: Caches some of the timing and optimization information from model compilation, making successive builds quicker.", "Running the model\nSo, now that you\u2019ve got your model engine, what can you do with it?\nThe engine file contains the information that you need for executing the model, but LLM usage in practice requires much more than a single forward pass through the model. TensorRT-LLM includes a highly optimized C++ runtime for executing built LLM engines and managing processes like sampling tokens from the model output, managing the KV cache, and batching requests together.\nYou can use that runtime directly to execute the model locally, or you can use the TensorRT-LLM runtime backend for NVIDIA Triton Inference Server to serve the model for multiple users.\nTo run the model locally, execute the following command:\n```\npython3 examples/llama/run.py --engine_dir=examples/llama/out --max_output_len 100 --tokenizer_dir meta-llama/Llama-2-7b-chat-hf --input_text \"How do I count to nine in French?\"\n```", "Deploying with Triton Inference Server\nBeyond local execution, you can also use the NVIDIA Triton Inference Server to create a production-ready deployment of your LLM.\nNVIDIA is releasing a new Triton Inference Server backend for TensorRT-LLM that leverages the TensorRT-LLM C++ runtime for rapid inference execution and includes techniques like in-flight batching and paged KV-caching. Triton Inference Server with the TensorRT-LLM backend is available as a pre-built container through NGC.\nFirst, create a model repository so that Triton Inference Server can read the model and any associated metadata. The tensorrtllm_backend repository includes the skeleton of an appropriate model repository under ```all_models/inflight_batcher_llm/``` that you can use. In that directory are now four subfolders that hold artifacts for different parts of the model execution process:\n/ ```preprocessing``` and ```/postprocessing```: Contain scripts for the Triton Inference Server Python backend for tokenizing the text inputs and detokenizing the model outputs to convert between strings and the token IDs on which the model operates.\n```/tensorrt_llm```: Where you place the model engine that you previously compiled.\n```/ensemble```: Defines a model ensemble that links the previous three components together and tells Triton Inference Server how to flow data through them.\nPull down the example model repository and copy the model you compiled in the previous step over to it:\n```\n# After exiting the TensorRT-LLM docker container\ncd ..\ngit clone -b release/0.5.0 \\ \nhttps://github.com/triton-inference-server/tensorrtllm_backend.git\ncd tensorrtllm_backend\ncp ../TensorRT-LLM/examples/llama/out/*   all_models/inflight_batcher_llm/tensorrt_llm/1/\n```\nNext, modify some of the configuration files from the repository skeleton with information like the following:\nWhere the compiled model engine is\nWhat tokenizer to use\nHow to handle memory allocation for the KV cache when performing inference in batches\n```\npython3 tools/fill_template.py --in_place \\\n      all_models/inflight_batcher_llm/tensorrt_llm/config.pbtxt \\\n      decoupled_mode:true,engine_dir:/all_models/inflight_batcher_llm/tensorrt_llm/1,\\\nmax_tokens_in_paged_kv_cache:,batch_scheduler_policy:guaranteed_completion,kv_cache_free_gpu_mem_fraction:0.2,\\\nmax_num_sequences:4\n\npython tools/fill_template.py --in_place \\\n    all_models/inflight_batcher_llm/preprocessing/config.pbtxt \\\n    tokenizer_type:llama,tokenizer_dir:meta-llama/Llama-2-7b-chat-hf\n\npython tools/fill_template.py --in_place \\\n    all_models/inflight_batcher_llm/postprocessing/config.pbtxt \\\n    tokenizer_type:llama,tokenizer_dir:meta-llama/Llama-2-7b-chat-hf\n```\nNow, spin up the Docker container and launch the Triton server. Specify the \u201cworld size,\u201d which is the number of GPUs the model was built for, and point to the model_repo just set up.\n```\ndocker run -it --rm --gpus all --network host --shm-size=1g \\\n-v $(pwd)/all_models:/all_models \\\n-v $(pwd)/scripts:/opt/scripts \\\nnvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3\n\n# Log in to huggingface-cli to get tokenizer\nhuggingface-cli login --token *****\n\n# Install python dependencies\npip install sentencepiece protobuf\n\n# Launch Server\npython /opt/scripts/launch_triton_server.py --model_repo /all_models/inflight_batcher_llm --world_size 1\n```", "Sending requests\nTo send requests to and interact with the running server, you can use one of the Triton Inference Server client libraries or send HTTP requests to the generate endpoint. To get started, you can use the more fully featured client script or the following curl command:\n```\ncurl -X POST localhost:8000/v2/models/ensemble/generate -d \\\n'{\n\"text_input\": \"How do I count to nine in French?\",\n\"parameters\": {\n\"max_tokens\": 100,\n\"bad_words\":[\"\"],\n\"stop_words\":[\"\"]\n}\n}'\n```", "Conclusion\nTogether, TensorRT-LLM and Triton Inference Server provide an indispensable toolkit for optimizing, deploying, and running LLMs efficiently. With the release of TensorRT-LLM as an open-source library on GitHub, it\u2019s easier than ever for organizations and application developers to harness the potential of these models.\nIf you\u2019re eager to dive into the world of LLMs, now is the time to get started with TensorRT-LLM. Explore its capabilities, experiment with different models and optimizations, and embark on your journey to unlock the incredible power of AI-driven language models. I can\u2019t wait to see the incredible things you all will build.\nFor more information about getting started with TensorRT-LLM, see the following resources:\nAccess the open-source library on the /NVIDIA/TensorRT-LLM GitHub repo.\nLearn more about NVIDIA NeMo, which provides complete containers (including TensorRT-LLM and NVIDIA Triton) for generative AI deployments.\nExplore sample code, benchmarks, and TensorRT-LLM documentation on GitHub.\nPurchase NVIDIA AI Enterprise, an end-to-end AI software platform that includes TensorRT and will soon include TensorRT-LLM, for mission-critical AI inference with enterprise-grade security, stability, manageability, and support. To learn more, contact sales.\nExplore resources to get started with TensorRT and TensorRT-LLM."], "document_title": "Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available", "document_url": "https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/", "document_date": "2023-10-19T16:00:00", "document_date_modified": "2023-11-16T16:46:41", "document_full_text": "Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available\nToday, NVIDIA announces the public release of TensorRT-LLM to accelerate and optimize inference performance for the latest LLMs on NVIDIA GPUs. This open-source library is now available for free on the /NVIDIA/TensorRT-LLM GitHub repo and as part of the NVIDIA NeMo framework.\nLarge language models (LLMs) have revolutionized the field of artificial intelligence and created entirely new ways of interacting with the digital world. But, as organizations and application developers around the world look to incorporate LLMs into their work, some of the challenges with running these models become apparent.\nPut simply, LLMs are large. That fact can make them expensive and slow to run without the right techniques.\nMany optimization techniques have risen to deal with this, from model optimizations like kernel fusion and quantization to runtime optimizations like C++ implementations, KV caching, continuous in-flight batching, and paged attention. It can be difficult to decide which of these are right for your use case, and to navigate the interactions between these techniques and their sometimes-incompatible implementations.\nThat\u2019s why NVIDIA introduced TensorRT-LLM, a comprehensive library for compiling and optimizing LLMs for inference. TensorRT-LLM incorporates all of those optimizations and more while providing an intuitive Python API for defining and building new models.\nThe TensorRT-LLM open-source library accelerates inference performance on the latest LLMs on NVIDIA GPUs. It is used as the optimization backbone for LLM inference in NVIDIA NeMo, an end-to-end framework to build, customize, and deploy generative AI applications into production. NeMo provides complete containers, including TensorRT-LLM and NVIDIA Triton, for generative AI deployments.\nTensorRT-LLM is also now available for native Windows as a beta release. Application developers and AI enthusiasts can now benefit from accelerated LLMs running locally on PCs and Workstations powered by NVIDIA RTX and NVIDIA GeForce RTX GPUs.\nTensorRT-LLM wraps TensorRT\u2019s deep learning compiler and includes the latest optimized kernels made for cutting-edge implementations of FlashAttention and masked multi-head attention (MHA) for LLM execution.\nTensorRT-LLM also consists of pre\u2013 and post-processing steps and multi-GPU/multi-node communication primitives in a simple, open-source Python API for groundbreaking LLM inference performance on GPUs.\nHighlights of TensorRT-LLM include the following:\nSupport for LLMs such as Llama 1 and 2, ChatGLM, Falcon, MPT, Baichuan, and Starcoder\nIn-flight batching and paged attention\nMulti-GPU multi-node (MGMN) inference\nNVIDIA Hopper tansformer engine with FP8\nSupport for NVIDIA Ampere architecture, NVIDIA Ada Lovelace architecture, and NVIDIA Hopper GPUs\nNative Windows support (beta)\nOver the past 2 years, NVIDIA has been working closely with leading LLM companies, including Anyscale, Baichuan, Cohere, Deci, Grammarly, Meta, Mistral AI, MosaicML, now part of Databricks, OctoML, Perplexity AI, Tabnine, Together.ai, Zhipu, and many others to accelerate and optimize LLM inference.\nTo help you get a feel for the library and how to use it, here\u2019s an example of how to use and deploy Llama 2, a popular publicly available LLM, with TensorRT-LLM and NVIDIA Triton on Linux. To get started with the beta release, see the TensorRT-LLM for native Windows GitHub repo.\nFor more information, including different models, different optimizations, and multi-GPU execution, see the full list of TensorRT-LLM examples.\nGetting started with installation\nStart by cloning and building the TensorRT-LLM library. The easiest way to build TensorRT-LLM and retrieve all its dependencies is to use the included Dockerfile:\n```\ngit lfs install\ngit clone -b release/0.5.0 https://github.com/NVIDIA/TensorRT-LLM.git\ncd TensorRT-LLM\ngit submodule update --init --recursive\nmake -C docker release_build\n```\nThese commands pull a base container and install all the dependencies needed for TensorRT-LLM inside the container. It then builds and installs TensorRT-LLM itself in the container.\nRetrieving the model weights\nTensorRT-LLM is a library for LLM inference, and so to use it, you need to supply a set of trained weights. You can either use your own model weights trained in a framework like NVIDIA NeMo, or pull a set of pretrained weights from repositories like the HuggingFace Hub.\nThe commands in this post automatically pull the weights and tokenizer files for the chat-tuned variant of the 7B parameter Llama 2 model from the HuggingFace Hub. You can also download the weights yourself to use offline with the following command. You just have to update the paths in later commands to point to this directory:\n```\ngit lfs install\ngit clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n```\nUsage of this model is subject to a particular license. To download the necessary files, agree to the terms and authenticate with Hugging Face.\nCompiling the model\nThe next step in the process is to compile the model into a TensorRT engine. For this, you need the model weights as well as a model definition written in the TensorRT-LLM Python API.\nThe TensorRT-LLM repository contains a wide variety of predefined model architectures. For this post, you use the included Llama model definition instead of writing your own. This is a minimal example of some of the optimizations available in TensorRT-LLM.\nFor more information about available plug-ins and quantizations, see the full Llama example and Numerical Precision.\n```\n# Launch the Tensorrt-LLM container\nmake -C docker release_run LOCAL_USER=1\n\n# Log in to huggingface-cli\n# You can get your token from huggingface.co/settings/token\nhuggingface-cli login --token *****\n\n# Compile model\npython3 examples/llama/build.py \\\n    --model_dir meta-llama/Llama-2-7b-chat-hf \\\n    --dtype float16 \\\n    --use_gpt_attention_plugin float16 \\\n    --use_gemm_plugin float16 \\\n    --remove_input_padding \\\n    --use_inflight_batching \\\n    --paged_kv_cache \\\n    --output_dir examples/llama/out\n```\nWhen you create the model definition with the TensorRT-LLM API, you build a graph of operations from NVIDIA TensorRT primitives that form the layers of your neural network. These operations map to specific kernels: prewritten programs for the GPU.\nThe TensorRT compiler can sweep through the graph to choose the best kernel for each operation and available GPU. Crucially, it can also identify patterns in the graph where multiple operations are good candidates for being fused into a single kernel. This reduces the required amount of memory movement and the overhead of launching multiple GPU kernels.\nTensorRT also compiles the graph of operations into a single CUDA Graph that can be launched all at one time, further reducing the kernel launch overhead.\nThe TensorRT compiler is extremely powerful for fusing layers and increasing execution speed, but there are some complex layer fusions\u2014like FlashAttention \u2014that involve interleaving many operations together and which can\u2019t be automatically discovered. For those, you can explicitly replace parts of the graph with plugins at compile time.\nIn this example, you include the ```gpt_attention``` plug-in, which implements a ```FlashAttention```-like fused attention kernel, and the ```gemm``` plug-in, which performs matrix multiplication with FP32 accumulation. You also call out your desired precision for the full model as FP16, matching the default precision of the weights that you downloaded from HuggingFace.\nHere\u2019s what this script produces when you finish running it. In the ```/examples/llama/out``` folder, there are now the following files:\n```Llama_float16_tp1_rank0.engine```: The main output of the build script, containing the executable graph of operations with the model weights embedded.\n```config.json```: Includes detailed information about the model, like its general structure and precision, as well as information about which plug-ins were incorporated into the engine.\n```model.cache```: Caches some of the timing and optimization information from model compilation, making successive builds quicker.\nRunning the model\nSo, now that you\u2019ve got your model engine, what can you do with it?\nThe engine file contains the information that you need for executing the model, but LLM usage in practice requires much more than a single forward pass through the model. TensorRT-LLM includes a highly optimized C++ runtime for executing built LLM engines and managing processes like sampling tokens from the model output, managing the KV cache, and batching requests together.\nYou can use that runtime directly to execute the model locally, or you can use the TensorRT-LLM runtime backend for NVIDIA Triton Inference Server to serve the model for multiple users.\nTo run the model locally, execute the following command:\n```\npython3 examples/llama/run.py --engine_dir=examples/llama/out --max_output_len 100 --tokenizer_dir meta-llama/Llama-2-7b-chat-hf --input_text \"How do I count to nine in French?\"\n```\nDeploying with Triton Inference Server\nBeyond local execution, you can also use the NVIDIA Triton Inference Server to create a production-ready deployment of your LLM.\nNVIDIA is releasing a new Triton Inference Server backend for TensorRT-LLM that leverages the TensorRT-LLM C++ runtime for rapid inference execution and includes techniques like in-flight batching and paged KV-caching. Triton Inference Server with the TensorRT-LLM backend is available as a pre-built container through NGC.\nFirst, create a model repository so that Triton Inference Server can read the model and any associated metadata. The tensorrtllm_backend repository includes the skeleton of an appropriate model repository under ```all_models/inflight_batcher_llm/``` that you can use. In that directory are now four subfolders that hold artifacts for different parts of the model execution process:\n/ ```preprocessing``` and ```/postprocessing```: Contain scripts for the Triton Inference Server Python backend for tokenizing the text inputs and detokenizing the model outputs to convert between strings and the token IDs on which the model operates.\n```/tensorrt_llm```: Where you place the model engine that you previously compiled.\n```/ensemble```: Defines a model ensemble that links the previous three components together and tells Triton Inference Server how to flow data through them.\nPull down the example model repository and copy the model you compiled in the previous step over to it:\n```\n# After exiting the TensorRT-LLM docker container\ncd ..\ngit clone -b release/0.5.0 \\ \nhttps://github.com/triton-inference-server/tensorrtllm_backend.git\ncd tensorrtllm_backend\ncp ../TensorRT-LLM/examples/llama/out/*   all_models/inflight_batcher_llm/tensorrt_llm/1/\n```\nNext, modify some of the configuration files from the repository skeleton with information like the following:\nWhere the compiled model engine is\nWhat tokenizer to use\nHow to handle memory allocation for the KV cache when performing inference in batches\n```\npython3 tools/fill_template.py --in_place \\\n      all_models/inflight_batcher_llm/tensorrt_llm/config.pbtxt \\\n      decoupled_mode:true,engine_dir:/all_models/inflight_batcher_llm/tensorrt_llm/1,\\\nmax_tokens_in_paged_kv_cache:,batch_scheduler_policy:guaranteed_completion,kv_cache_free_gpu_mem_fraction:0.2,\\\nmax_num_sequences:4\n\npython tools/fill_template.py --in_place \\\n    all_models/inflight_batcher_llm/preprocessing/config.pbtxt \\\n    tokenizer_type:llama,tokenizer_dir:meta-llama/Llama-2-7b-chat-hf\n\npython tools/fill_template.py --in_place \\\n    all_models/inflight_batcher_llm/postprocessing/config.pbtxt \\\n    tokenizer_type:llama,tokenizer_dir:meta-llama/Llama-2-7b-chat-hf\n```\nNow, spin up the Docker container and launch the Triton server. Specify the \u201cworld size,\u201d which is the number of GPUs the model was built for, and point to the model_repo just set up.\n```\ndocker run -it --rm --gpus all --network host --shm-size=1g \\\n-v $(pwd)/all_models:/all_models \\\n-v $(pwd)/scripts:/opt/scripts \\\nnvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3\n\n# Log in to huggingface-cli to get tokenizer\nhuggingface-cli login --token *****\n\n# Install python dependencies\npip install sentencepiece protobuf\n\n# Launch Server\npython /opt/scripts/launch_triton_server.py --model_repo /all_models/inflight_batcher_llm --world_size 1\n```\nSending requests\nTo send requests to and interact with the running server, you can use one of the Triton Inference Server client libraries or send HTTP requests to the generate endpoint. To get started, you can use the more fully featured client script or the following curl command:\n```\ncurl -X POST localhost:8000/v2/models/ensemble/generate -d \\\n'{\n\"text_input\": \"How do I count to nine in French?\",\n\"parameters\": {\n\"max_tokens\": 100,\n\"bad_words\":[\"\"],\n\"stop_words\":[\"\"]\n}\n}'\n```\nConclusion\nTogether, TensorRT-LLM and Triton Inference Server provide an indispensable toolkit for optimizing, deploying, and running LLMs efficiently. With the release of TensorRT-LLM as an open-source library on GitHub, it\u2019s easier than ever for organizations and application developers to harness the potential of these models.\nIf you\u2019re eager to dive into the world of LLMs, now is the time to get started with TensorRT-LLM. Explore its capabilities, experiment with different models and optimizations, and embark on your journey to unlock the incredible power of AI-driven language models. I can\u2019t wait to see the incredible things you all will build.\nFor more information about getting started with TensorRT-LLM, see the following resources:\nAccess the open-source library on the /NVIDIA/TensorRT-LLM GitHub repo.\nLearn more about NVIDIA NeMo, which provides complete containers (including TensorRT-LLM and NVIDIA Triton) for generative AI deployments.\nExplore sample code, benchmarks, and TensorRT-LLM documentation on GitHub.\nPurchase NVIDIA AI Enterprise, an end-to-end AI software platform that includes TensorRT and will soon include TensorRT-LLM, for mission-critical AI inference with enterprise-grade security, stability, manageability, and support. To learn more, contact sales.\nExplore resources to get started with TensorRT and TensorRT-LLM."}], "https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/": [{"text": "The article discusses how leveraging NVIDIA TensorRT can significantly improve the performance of the Stable Diffusion generative AI model, particularly in the context of the Automatic 1111 Stable Diffusion Web UI. The Stable Diffusion model is computationally intensive and requires GPUs for efficient operation. NVIDIA TensorRT accelerates performance by optimizing deep learning inference, doubling the number of image generations per minute compared to previous methods. NVIDIA has provided a TensorRT demo of a Stable Diffusion pipeline for developers to reference and implement in their own projects, making generative AI more accessible. The integration of TensorRT into the Stable Diffusion Web UI simplifies installation and provides an intuitive interface for users to compile TensorRT engines and render different resolutions with minimal impact on performance. By downloading the Stable Diffusion Web UI TensorRT extension and exploring NVIDIA/TensorRT, developers can enhance their own Stable Diffusion pipelines and accelerate their AI applications.", "text_components": ["Unlock Faster Image Generation in Stable Diffusion Web UI with NVIDIA TensorRT\nStable Diffusion is an open-source generative AI image-based model that enables users to generate images with simple text descriptions. Gaining traction among developers, it has powered popular applications like Wombo and Lensa.\nEnd users typically access the model through distributions that package it together with a user interface and a set of tools. The most popular distribution is the Automatic 1111 Stable Diffusion Web UI. This post explains how leveraging NVIDIA TensorRT can double the performance of a model. It features an example using the Automatic 1111 Stable Diffusion Web UI.", "Efficient generative AI requires GPUs\nStable Diffusion is a deep learning model that uses diffusion processes to generate images based on input text and images. While it can be a useful tool to enhance creator workflows, the model is computationally intensive. Generating a single batch of four images takes minutes on nonspecialized hardware like CPUs, which breaks workflows and can be a barrier for many developers.\nWithout dedicated hardware, AI features are slow because CPUs are not inherently designed for the highly parallel operations demanded by neural networks, and are instead optimized for general-purpose tasks. Stable Diffusion exemplifies why GPUs are necessary to run AI efficiently.", "NVIDIA TensorRT accelerates performance\nGeForce RTX GPUs excel at parallelized work, required to run generative AI models. They are also equipped with dedicated hardware called Tensor Cores that accelerate matrix operations for AI use cases. The best way to enable these optimizations is with NVIDIA TensorRT SDK, a high-performance deep learning inference optimizer.\nTensorRT provides layer fusion, precision calibration, kernel auto-tuning, and other capabilities that significantly boost the efficiency and speed of deep learning models. This makes it indispensable for real-time applications and resource-intensive tasks like Stable Diffusion.\nTensorRT substantially accelerates performance. In the case of Stable Diffusion Web UI image generation, it doubled the number of image generations per minute, compared to the most accelerated method previously used (PyTorch xFormers).\nComparison of images generated per minute of Apple M2 Ultra and GeForce RTX 4090 (with both PyTorch xFormers and TensorRT acceleration).\nFigure 1. NVIDIA TensorRT acceleration doubles the number of image generations per minute\nImage generation: Stable Diffusion 1.5, 512 x 512, batch size 1, Stable Diffusion Web UI from Automatic 1111 (for NVIDIA) and Mochi (for Apple)\nHardware: GeForce RTX 4090 with Intel i9 12900K; Apple M2 Ultra with 76 cores", "Implementing TensorRT in a Stable Diffusion pipeline\nNVIDIA has published a TensorRT demo of a Stable Diffusion pipeline that provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point if you\u2019re interested in turbocharging your diffusion pipeline and bringing lightning-fast inference to your applications.\nBuilding on this foundation, the TensorRT pipeline was then applied to a project commonly used by Stable Diffusion developers. Implementing TensorRT into the Stable Diffusion Web UI further democratizes generative AI and provides broad, easy access.\nScreenshot of Stable Diffusion Web UI\nwith generated images.\nFigure 2. Images generated in the Stable Diffusion Web UI This journey began with the introduction of a TensorRT Python package for Windows, which significantly simplified the installation process. Even those with minimal technical knowledge can easily install and start using TensorRT.\nOnce installed, it provides an intuitive user interface that triggers the ahead-of-time compilation required for TensorRT engines. A caching mechanism drastically reduces compile times. These simplifications free users to focus on core tasks. The integration is flexible: dynamic shapes enable users to render different resolutions with minimal impact on performance. This implementation provides a useful tool for developers. Leverage this plug-in to enhance your own Stable Diffusion pipelines.", "Get started with TensorRT\nTo download the Stable Diffusion Web UI TensorRT extension, visit NVIDIA/Stable-Diffusion-WebUI-TensorRT on GitHub. And check out NVIDIA/TensorRT for a demo showcasing the acceleration of a Stable Diffusion pipeline. For more details about the Automatic 1111 TensorRT extension, see TensorRT Extension for Stable Diffusion Web UI.\nFor broader guidance on how to integrate TensorRT into your applications, see Getting Started with NVIDIA AI for Your Applications. Learn how to profile your pipeline to pinpoint where optimization is critical and where minor changes can have a big impact. Accelerate your AI pipeline by choosing a machine learning framework, and discover SDKs for video, graphic design, photography, and audio."], "document_title": "Unlock Faster Image Generation in Stable Diffusion Web UI with NVIDIA TensorRT", "document_url": "https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/", "document_date": "2023-10-17T13:00:00", "document_date_modified": "2023-11-02T18:14:40", "document_full_text": "Unlock Faster Image Generation in Stable Diffusion Web UI with NVIDIA TensorRT\nStable Diffusion is an open-source generative AI image-based model that enables users to generate images with simple text descriptions. Gaining traction among developers, it has powered popular applications like Wombo and Lensa.\nEnd users typically access the model through distributions that package it together with a user interface and a set of tools. The most popular distribution is the Automatic 1111 Stable Diffusion Web UI. This post explains how leveraging NVIDIA TensorRT can double the performance of a model. It features an example using the Automatic 1111 Stable Diffusion Web UI.\nEfficient generative AI requires GPUs\nStable Diffusion is a deep learning model that uses diffusion processes to generate images based on input text and images. While it can be a useful tool to enhance creator workflows, the model is computationally intensive. Generating a single batch of four images takes minutes on nonspecialized hardware like CPUs, which breaks workflows and can be a barrier for many developers.\nWithout dedicated hardware, AI features are slow because CPUs are not inherently designed for the highly parallel operations demanded by neural networks, and are instead optimized for general-purpose tasks. Stable Diffusion exemplifies why GPUs are necessary to run AI efficiently.\nNVIDIA TensorRT accelerates performance\nGeForce RTX GPUs excel at parallelized work, required to run generative AI models. They are also equipped with dedicated hardware called Tensor Cores that accelerate matrix operations for AI use cases. The best way to enable these optimizations is with NVIDIA TensorRT SDK, a high-performance deep learning inference optimizer.\nTensorRT provides layer fusion, precision calibration, kernel auto-tuning, and other capabilities that significantly boost the efficiency and speed of deep learning models. This makes it indispensable for real-time applications and resource-intensive tasks like Stable Diffusion.\nTensorRT substantially accelerates performance. In the case of Stable Diffusion Web UI image generation, it doubled the number of image generations per minute, compared to the most accelerated method previously used (PyTorch xFormers).\nComparison of images generated per minute of Apple M2 Ultra and GeForce RTX 4090 (with both PyTorch xFormers and TensorRT acceleration).\nFigure 1. NVIDIA TensorRT acceleration doubles the number of image generations per minute\nImage generation: Stable Diffusion 1.5, 512 x 512, batch size 1, Stable Diffusion Web UI from Automatic 1111 (for NVIDIA) and Mochi (for Apple)\nHardware: GeForce RTX 4090 with Intel i9 12900K; Apple M2 Ultra with 76 cores\nImplementing TensorRT in a Stable Diffusion pipeline\nNVIDIA has published a TensorRT demo of a Stable Diffusion pipeline that provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point if you\u2019re interested in turbocharging your diffusion pipeline and bringing lightning-fast inference to your applications.\nBuilding on this foundation, the TensorRT pipeline was then applied to a project commonly used by Stable Diffusion developers. Implementing TensorRT into the Stable Diffusion Web UI further democratizes generative AI and provides broad, easy access.\nScreenshot of Stable Diffusion Web UI\nwith generated images.\nFigure 2. Images generated in the Stable Diffusion Web UI This journey began with the introduction of a TensorRT Python package for Windows, which significantly simplified the installation process. Even those with minimal technical knowledge can easily install and start using TensorRT.\nOnce installed, it provides an intuitive user interface that triggers the ahead-of-time compilation required for TensorRT engines. A caching mechanism drastically reduces compile times. These simplifications free users to focus on core tasks. The integration is flexible: dynamic shapes enable users to render different resolutions with minimal impact on performance. This implementation provides a useful tool for developers. Leverage this plug-in to enhance your own Stable Diffusion pipelines.\nGet started with TensorRT\nTo download the Stable Diffusion Web UI TensorRT extension, visit NVIDIA/Stable-Diffusion-WebUI-TensorRT on GitHub. And check out NVIDIA/TensorRT for a demo showcasing the acceleration of a Stable Diffusion pipeline. For more details about the Automatic 1111 TensorRT extension, see TensorRT Extension for Stable Diffusion Web UI.\nFor broader guidance on how to integrate TensorRT into your applications, see Getting Started with NVIDIA AI for Your Applications. Learn how to profile your pipeline to pinpoint where optimization is critical and where minor changes can have a big impact. Accelerate your AI pipeline by choosing a machine learning framework, and discover SDKs for video, graphic design, photography, and audio."}], "https://developer.nvidia.com/blog/advanced-api-performance-debugging/": [{"text": "NVIDIA offers various tools for graphics debugging, such as Nsight System and Nsight Graphics. Nsight Aftermath is useful for analyzing crash dumps. It is recommended to check validation layers, use debug checkpoints, and build shaders with debug info. For advanced debugging, Nsight Aftermath crash dumps can help identify errors like device hung or page faults. Debugging advice for NVIDIA RTX-related problems includes checking valid input data and geometry, as well as ensuring valid memory and descriptors. Avoid excessive debug checkpoints and rely on Nsight Aftermath for crash debugging. Simplify debugging by removing parallelism, but only use debug flags sparingly. It is not recommended to rely solely on CPU call stacks for GPU issues, test on multiple machines, or allow users to run with outdated drivers. Special thanks to individuals who contributed to the article.", "text_components": ["Advanced API Performance: Debugging\nNVIDIA offers a large suite of tools for graphics debugging, including NVIDIA Nsight System for CPU debugging, and Nsight Graphics for GPU debugging. Nsight Aftermath is useful for analyzing crash dumps.", "Recommended\nAlways check the validation layers and make sure they don\u2019t output any errors.\nUse Nsight Aftermath for detailed DirectX 12 or Vulkan GPU exception debugging.\nUsing the Nsight Aftermath Monitor is an easy way to get started without the need for code integration.\nFor even more control over the GPU crash dump functionality, consider using the Aftermath SDK to integrate the crash dump capabilities into your own code.\nCrash dump generation can always be enabled since there is no associated runtime cost. As noted later, debug checkpoints can have measurable CPU overhead and should not be used in shipping applications.\nYou can use the callbacks provided in the API to save the crash dumps to the local disk or push them to the cloud.\nFor more information, see the Nsight Aftermath SDK product page.\nIsolate problems using debug checkpoints.\nThese APIs enable inserting checkpoints in the GPU command stream, making it possible to narrow down crashes to certain subsections of the command stream.\nUse the API supported by Nsight Aftermath. For more information and samples, see the /NVIDIA/nsight-aftermath-samples GitHub repo.\nAlternatively, use the DirectX 12 cross-vendor solution:\nUse ```ID3D12GraphicsCommandList2::WriteBufferImmediate``` or DRED.\nIt isn\u2019t supported in conjunction with Nsight Aftermath, so it is better to avoid mixing these.\nAdd as a runtime flag in the engine to be able to toggle this functionality.\nThese markers are far from free and have a runtime cost associated with them (serializing GPU, CPU call overhead, and the time to capture call-stacks).\nKeeping them enabled by default could have a severe performance cost.\nSee disadvantages in the Not recommended section.\nBuild shaders with debug info.\nCompile with /Zi to embed debug info into the shader binary.\nThis is helpful when using debugging tools like NVIDIA Nsight Graphics.\nAlso, Nsight Aftermath could give you source-level GPU crash info using this information.\nUse Nsight Aftermath crash dumps to identify the type of error that occurs during a crash.\nDevice hung:\nThese can occur due to a single command list taking longer than a few seconds to execute.\nMicrosoft Windows terminates the driver after a few seconds of no apparent feedback from the driver and GPU (TDR).\nThis can also happen in case of extreme workloads (massive pixel overdraw, or degraded ray tracing acceleration structures).\nPage faults\nThese are caused by invalid memory accesses: either an out-of-bounds read/write or a resource that is not valid anymore.\nFor more information, see How to Set Up and Inspect GPU Crash Dumps.\nGeneric debugging advice for graphics and compute-related problems:\nCheck whether all referenced memory is valid and correct at all times when the GPU accesses the data.\nCheck whether descriptors point to the right resources, which are fully allocated and initialized.\nCheck whether data reads and writes are not going out of bounds.\nUse debug checkpoints and GPU crash dump functionality from Nsight Aftermath to narrow down the location of the crash.\nDirectX 12: The debug layer could help here, but for these problems, GPU-based validation must be enabled, which generally makes the application run extremely slow with complex scenes. It could still be useful for unit or regression testing.\nGeneric debugging advice for NVIDIA RTX-related problems:\nCheck whether the input vertex or index data are all valid.\nInvalid indices could crash the GPU builder kernel.\nInvalid vertices could affect the acceleration structures and make performance extremely slow.\nDegenerate triangles or tricks to disconnect triangles that work in a rasterizer do not work as intended in an acceleration tree and can cause big problems. Check if such tricks are not being employed, for example, to disconnect or delete geometry. Exclusively use valid geometry instead.\nCheck whether memory is all still valid at the moment data is being referenced by builder or ray tracing kernels.\nCheck whether all textures and buffers used by ray tracing kernels are all valid.\nCheck whether descriptors are correct and the shader binding tables are valid.\nDebug checkpoints are not useful for debugging ray tracing workloads because of the indirections happening inside the RT kernels potentially touching thousands of shader permutations. At most, it can tell you if the crash happened in the builder or the ray-tracing kernel.\nInstead, use Nsight Aftermath crash debugging to get an approximate idea of the crash:\nPage fault: Memory-related issue, usually out-of-bounds read-write or trying to access a resource that was removed or not copied to the GPU yet.\nGPU hang (TDR): Infinite loops, too complex shading, or too many rays.\nHaving a way to simplify code and binding requirements could be useful, like disabling textures or reducing shader permutations.\nFor example, have a debug view showing barycentrics only (no shader binding table requirements).\nCheck for broken-looking geometry or spots where performance gets severely degraded due to corrupted triangle data.\nVisually verify output from dynamic sources, such as deformed geometry or skinned meshes in a ray tracing-only view.\nBeing able to fully disable dynamic geometry can help to isolate these kinds of issues as well.\nSimplify debugging by adding flags in the application to the following:\nSerialize the GPU/CPU at the queue level.\nSerialize the GPU/CPU at the command list level.\nDisable async compute.\nDisable async copies.\nAdd full barriers between compute, dispatch, and copy calls in the command lists (NULL UAV/memory barrier).\nDo anything else you can to remove parallelism. It\u2019s much harder to debug and pinpoint where a problem comes from when the GPU is running multiple workloads at the same time.\nDon\u2019t keep any of these suggestions enabled by default. They should be strictly debug-only flags. Reducing parallelism significantly degrades performance.", "Not recommended\nThe use of excessive debug checkpoints.\nThey have a non-negligible CPU and GPU performance cost.\nUse them sparingly. Aim for ~100 per frame, preferably less.\nBest not to use them at all for the end user (developer or QA only), or enable them when a GPU hang has been detected. You could also make them an option to toggle by the end user.\nAssuming that a CPU call stack will tell you anything about a GPU problem.\nCrashes with a call stack pointing to the driver usually manifest as a random graphics API call failing due to an internal device lost event.\nUse the Nsight Aftermath crash dump or debug checkpoints to pinpoint where the fault occurs.\nTesting on a single machine (excluding the effect of bad hardware).\nA corrupted memory (CPU or GPU), overclocking, and bad cooling can all contribute to random faults. Nsight Aftermath has no way of differentiating these from valid errors.\nA telltale sign could be that crashes happen randomly without any pattern across the GPU on a single machine but not another that has similar specifications.\nTry to validate results on more than one machine with similar hardware, software, and driver versions.\nPermitting users to run with extremely outdated NVIDIA drivers\nOutdated drivers can have unexpected behaviors and are harder to get reliable crash dumps from.\nFind a driver version that works reliably. Show a popup that says the driver is out of date when it is earlier than that version. Don\u2019t stop users from running the application or game, but discourage them from doing so as it can cause system instability.", "Acknowledgments\nThanks to Patrick Neill, Jeffrey Kiel, Justin Kim, Andrew Allan, and Louis Bavoil for their help with this post."], "document_title": "Advanced API Performance: Debugging", "document_url": "https://developer.nvidia.com/blog/advanced-api-performance-debugging/", "document_date": "2023-10-13T16:00:00", "document_date_modified": "2023-11-02T18:14:41", "document_full_text": "Advanced API Performance: Debugging\nNVIDIA offers a large suite of tools for graphics debugging, including NVIDIA Nsight System for CPU debugging, and Nsight Graphics for GPU debugging. Nsight Aftermath is useful for analyzing crash dumps.\nRecommended\nAlways check the validation layers and make sure they don\u2019t output any errors.\nUse Nsight Aftermath for detailed DirectX 12 or Vulkan GPU exception debugging.\nUsing the Nsight Aftermath Monitor is an easy way to get started without the need for code integration.\nFor even more control over the GPU crash dump functionality, consider using the Aftermath SDK to integrate the crash dump capabilities into your own code.\nCrash dump generation can always be enabled since there is no associated runtime cost. As noted later, debug checkpoints can have measurable CPU overhead and should not be used in shipping applications.\nYou can use the callbacks provided in the API to save the crash dumps to the local disk or push them to the cloud.\nFor more information, see the Nsight Aftermath SDK product page.\nIsolate problems using debug checkpoints.\nThese APIs enable inserting checkpoints in the GPU command stream, making it possible to narrow down crashes to certain subsections of the command stream.\nUse the API supported by Nsight Aftermath. For more information and samples, see the /NVIDIA/nsight-aftermath-samples GitHub repo.\nAlternatively, use the DirectX 12 cross-vendor solution:\nUse ```ID3D12GraphicsCommandList2::WriteBufferImmediate``` or DRED.\nIt isn\u2019t supported in conjunction with Nsight Aftermath, so it is better to avoid mixing these.\nAdd as a runtime flag in the engine to be able to toggle this functionality.\nThese markers are far from free and have a runtime cost associated with them (serializing GPU, CPU call overhead, and the time to capture call-stacks).\nKeeping them enabled by default could have a severe performance cost.\nSee disadvantages in the Not recommended section.\nBuild shaders with debug info.\nCompile with /Zi to embed debug info into the shader binary.\nThis is helpful when using debugging tools like NVIDIA Nsight Graphics.\nAlso, Nsight Aftermath could give you source-level GPU crash info using this information.\nUse Nsight Aftermath crash dumps to identify the type of error that occurs during a crash.\nDevice hung:\nThese can occur due to a single command list taking longer than a few seconds to execute.\nMicrosoft Windows terminates the driver after a few seconds of no apparent feedback from the driver and GPU (TDR).\nThis can also happen in case of extreme workloads (massive pixel overdraw, or degraded ray tracing acceleration structures).\nPage faults\nThese are caused by invalid memory accesses: either an out-of-bounds read/write or a resource that is not valid anymore.\nFor more information, see How to Set Up and Inspect GPU Crash Dumps.\nGeneric debugging advice for graphics and compute-related problems:\nCheck whether all referenced memory is valid and correct at all times when the GPU accesses the data.\nCheck whether descriptors point to the right resources, which are fully allocated and initialized.\nCheck whether data reads and writes are not going out of bounds.\nUse debug checkpoints and GPU crash dump functionality from Nsight Aftermath to narrow down the location of the crash.\nDirectX 12: The debug layer could help here, but for these problems, GPU-based validation must be enabled, which generally makes the application run extremely slow with complex scenes. It could still be useful for unit or regression testing.\nGeneric debugging advice for NVIDIA RTX-related problems:\nCheck whether the input vertex or index data are all valid.\nInvalid indices could crash the GPU builder kernel.\nInvalid vertices could affect the acceleration structures and make performance extremely slow.\nDegenerate triangles or tricks to disconnect triangles that work in a rasterizer do not work as intended in an acceleration tree and can cause big problems. Check if such tricks are not being employed, for example, to disconnect or delete geometry. Exclusively use valid geometry instead.\nCheck whether memory is all still valid at the moment data is being referenced by builder or ray tracing kernels.\nCheck whether all textures and buffers used by ray tracing kernels are all valid.\nCheck whether descriptors are correct and the shader binding tables are valid.\nDebug checkpoints are not useful for debugging ray tracing workloads because of the indirections happening inside the RT kernels potentially touching thousands of shader permutations. At most, it can tell you if the crash happened in the builder or the ray-tracing kernel.\nInstead, use Nsight Aftermath crash debugging to get an approximate idea of the crash:\nPage fault: Memory-related issue, usually out-of-bounds read-write or trying to access a resource that was removed or not copied to the GPU yet.\nGPU hang (TDR): Infinite loops, too complex shading, or too many rays.\nHaving a way to simplify code and binding requirements could be useful, like disabling textures or reducing shader permutations.\nFor example, have a debug view showing barycentrics only (no shader binding table requirements).\nCheck for broken-looking geometry or spots where performance gets severely degraded due to corrupted triangle data.\nVisually verify output from dynamic sources, such as deformed geometry or skinned meshes in a ray tracing-only view.\nBeing able to fully disable dynamic geometry can help to isolate these kinds of issues as well.\nSimplify debugging by adding flags in the application to the following:\nSerialize the GPU/CPU at the queue level.\nSerialize the GPU/CPU at the command list level.\nDisable async compute.\nDisable async copies.\nAdd full barriers between compute, dispatch, and copy calls in the command lists (NULL UAV/memory barrier).\nDo anything else you can to remove parallelism. It\u2019s much harder to debug and pinpoint where a problem comes from when the GPU is running multiple workloads at the same time.\nDon\u2019t keep any of these suggestions enabled by default. They should be strictly debug-only flags. Reducing parallelism significantly degrades performance.\nNot recommended\nThe use of excessive debug checkpoints.\nThey have a non-negligible CPU and GPU performance cost.\nUse them sparingly. Aim for ~100 per frame, preferably less.\nBest not to use them at all for the end user (developer or QA only), or enable them when a GPU hang has been detected. You could also make them an option to toggle by the end user.\nAssuming that a CPU call stack will tell you anything about a GPU problem.\nCrashes with a call stack pointing to the driver usually manifest as a random graphics API call failing due to an internal device lost event.\nUse the Nsight Aftermath crash dump or debug checkpoints to pinpoint where the fault occurs.\nTesting on a single machine (excluding the effect of bad hardware).\nA corrupted memory (CPU or GPU), overclocking, and bad cooling can all contribute to random faults. Nsight Aftermath has no way of differentiating these from valid errors.\nA telltale sign could be that crashes happen randomly without any pattern across the GPU on a single machine but not another that has similar specifications.\nTry to validate results on more than one machine with similar hardware, software, and driver versions.\nPermitting users to run with extremely outdated NVIDIA drivers\nOutdated drivers can have unexpected behaviors and are harder to get reliable crash dumps from.\nFind a driver version that works reliably. Show a popup that says the driver is out of date when it is earlier than that version. Don\u2019t stop users from running the application or game, but discourage them from doing so as it can cause system instability.\nAcknowledgments\nThanks to Patrick Neill, Jeffrey Kiel, Justin Kim, Andrew Allan, and Louis Bavoil for their help with this post."}], "https://developer.nvidia.com/blog/networking-for-data-centers-and-the-era-of-ai/": [{"text": "The article discusses the evolution of data centers to meet the demands of AI-driven computing, highlighting the importance of networking in shaping the future of data centers. Two emerging classes of data centers, AI factories, and AI clouds, are tailored to handle AI workloads that rely on accelerated computing. Distributed computing is crucial for AI success, requiring a high-speed, low-latency network to distribute workloads across multiple nodes efficiently. InfiniBand technology plays a key role in driving AI performance with features like in-network computing and adaptive routing. Deploying Ethernet networks for AI infrastructure requires addressing specific needs to optimize performance, with Spectrum-X offering enhancements like RDMA over Converged Ethernet for multi-tenant generative AI clouds. Addressing networking considerations is essential for unlocking the full potential of AI technologies and driving innovation in the data center industry. NVIDIA Quantum InfiniBand and Spectrum-X are recommended networking platforms to meet the demands of AI applications effectively.", "text_components": ["Networking for Data Centers and the Era of AI\nTraditional cloud data centers have served as the bedrock of computing infrastructure for over a decade, catering to a diverse range of users and applications. However, data centers have evolved in recent years to keep up with advancements in technology and the surging demand for AI-driven computing. This post explores the pivotal role that networking plays in shaping the future of data centers and facilitating the era of AI.", "Specialized data centers: AI factories and AI clouds\nTwo distinct classes of data centers are currently emerging: AI factories and AI clouds. Both of these are tailored to meet the unique demands of AI workloads, which are characterized by their reliance on accelerated computing.\nAI factories are designed to handle massive, large-scale workflows and the development of large language models (LLMs) and other foundational AI models. These models are the building blocks with which more advanced AI systems are constructed. To enable seamless scaling and efficient utilization of resources across thousands of GPUs, a robust and high-performance network is imperative.\nAI clouds extend the capabilities of traditional cloud infrastructure to support large-scale generative AI applications. Generative AI goes beyond conventional AI systems by creating new content, such as images, text, and audio, based on the data it\u2019s been trained on. Managing AI clouds with thousands of users requires advanced management tools and a networking infrastructure that can handle diverse workloads efficiently.", "AI and distributed computing\nAI workloads are computationally intensive, particularly those involving large and complex models like ChatGPT and BERT. To expedite model training and processing vast datasets, AI practitioners have turned to distributed computing. This approach involves distributing the workload across multiple interconnected servers or nodes connected through a high-speed, low-latency network.\nDistributed computing is pivotal for the success of AI, and the network\u2019s scalability and capacity to handle a growing number of nodes is crucial. A highly scalable network enables AI researchers to tap into more computational resources, leading to faster and improved performance.\nWhen crafting the network architecture for AI data centers, it\u2019s essential to create an integrated solution with distributed computing as a top priority. Data center architects must carefully consider network design and tailor solutions to the unique demands of the AI workloads they plan to deploy.\nNVIDIA Quantum-2 InfiniBand and NVIDIA Spectrum-X are two networking platforms specifically designed and optimized to meet the networking challenges of the AI data center, each with its own unique features and innovations.", "InfiniBand drives AI performance\nInfiniBand technology has been a driving force behind large-scale supercomputing deployments for complex distributed scientific computing. It has become the de facto network for AI factories. With ultra-low latencies, InfiniBand has become a linchpin for accelerating today\u2019s mainstream high-performance computing (HPC) and AI applications. Many crucial network capabilities required for efficient AI systems are native to the NVIDIA Quantum-2 InfiniBand platform.\nIn-network computing, driven by InfiniBand, integrates hardware-based computing engines into the network. This offloads complex operations at scale and utilizes the NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP), an in-network aggregation mechanism. SHARP supports multiple concurrent collective operations, doubling data bandwidth for data reductions and performance enhancements.\nThe InfiniBand adaptive routing optimally spreads traffic, mitigating congestion and enhancing resource utilization. Directed by a Subnet Manager, InfiniBand selects congestion-free routes based on network conditions, maximizing efficiency without compromising order of packet arrival.\nThe InfiniBand Congestion Control Architecture guarantees deterministic bandwidth and latency. It uses a three-stage process to manage congestion, preventing performance bottlenecks in AI workloads.\nThese inherent optimizations empower InfiniBand to meet the demands of AI applications, ultimately driving superior performance and efficiency.", "Navigating Ethernet for AI deployments\nDeploying Ethernet networks for an AI infrastructure requires addressing needs specific to the Ethernet protocol. Over time, Ethernet has incorporated an expansive, comprehensive, and (at times) complex feature set that caters to a huge range of network scenarios.\nAs such, out-of-the-box or traditional Ethernet isn\u2019t explicitly designed for high performance. AI clouds that use traditional Ethernet for their compute fabric can only achieve a fraction of the performance that they would achieve with an optimized network.\nIn multi-tenant environments where multiple AI jobs run simultaneously, performance isolation is critical to prevent further degradation of performance. If there is a link fault, the traditional Ethernet fabric can cause the cluster\u2019s AI performance to drop by half. This is because traditional Ethernet has primarily been optimized for everyday enterprise workflows and isn\u2019t designed to meet the demands of high-performance AI applications that rely on the NVIDIA Collective Communications Library (NCCL).\nThese performance issues are due to factors inherent to traditional Ethernet, including:\nHigher switch latencies, common across commodity ASICs\nSplit buffer switch architecture, which can lead to bandwidth unfairness\nLoad balancing that is suboptimized for the large flows generated by AI workloads\nPerformance isolation and noisy neighbor issues\nThe Spectrum-X networking platform solves these issues and more. Spectrum-X builds on the standard Ethernet protocol with RDMA over Converged Ethernet (RoCE) Extensions, enhancing performance for AI. These extensions leverage the best practices native to InfiniBand and bring innovations such as adaptive routing and congestion control to Ethernet.\nSpectrum-X is the only Ethernet platform that delivers the high effective bandwidth and performance isolation needed for multi-tenant generative AI clouds, enabled due to Spectrum-4 working in close coordination with NVIDIA BlueField-3 DPUs.", "Summary\nThe era of AI is here, and the network is the cornerstone of its success. To fully embrace the potential of AI, data center architects must carefully consider network design and tailor these designs to the unique demands of AI workloads. Addressing \u200cnetworking considerations is key to unlocking the full potential of AI technologies and driving innovation in the data center industry.\nNVIDIA Quantum InfiniBand is an ideal choice for AI factories, thanks to its ultra-low latencies, scalable performance, and advanced feature sets. NVIDIA Spectrum-X, with its purpose-built technology innovations for AI, offers a groundbreaking solution for organizations building Ethernet-based AI clouds.\nTo learn more about AI performance demands and network requirements, see the Networking for the Era of AI whitepaper. Join the conversation in the NVIDIA Developer Infrastructure and Networking Forum."], "document_title": "Networking for Data Centers and the Era of AI", "document_url": "https://developer.nvidia.com/blog/networking-for-data-centers-and-the-era-of-ai/", "document_date": "2023-10-12T16:30:00", "document_date_modified": "2023-11-02T18:14:42", "document_full_text": "Networking for Data Centers and the Era of AI\nTraditional cloud data centers have served as the bedrock of computing infrastructure for over a decade, catering to a diverse range of users and applications. However, data centers have evolved in recent years to keep up with advancements in technology and the surging demand for AI-driven computing. This post explores the pivotal role that networking plays in shaping the future of data centers and facilitating the era of AI.\nSpecialized data centers: AI factories and AI clouds\nTwo distinct classes of data centers are currently emerging: AI factories and AI clouds. Both of these are tailored to meet the unique demands of AI workloads, which are characterized by their reliance on accelerated computing.\nAI factories are designed to handle massive, large-scale workflows and the development of large language models (LLMs) and other foundational AI models. These models are the building blocks with which more advanced AI systems are constructed. To enable seamless scaling and efficient utilization of resources across thousands of GPUs, a robust and high-performance network is imperative.\nAI clouds extend the capabilities of traditional cloud infrastructure to support large-scale generative AI applications. Generative AI goes beyond conventional AI systems by creating new content, such as images, text, and audio, based on the data it\u2019s been trained on. Managing AI clouds with thousands of users requires advanced management tools and a networking infrastructure that can handle diverse workloads efficiently.\nAI and distributed computing\nAI workloads are computationally intensive, particularly those involving large and complex models like ChatGPT and BERT. To expedite model training and processing vast datasets, AI practitioners have turned to distributed computing. This approach involves distributing the workload across multiple interconnected servers or nodes connected through a high-speed, low-latency network.\nDistributed computing is pivotal for the success of AI, and the network\u2019s scalability and capacity to handle a growing number of nodes is crucial. A highly scalable network enables AI researchers to tap into more computational resources, leading to faster and improved performance.\nWhen crafting the network architecture for AI data centers, it\u2019s essential to create an integrated solution with distributed computing as a top priority. Data center architects must carefully consider network design and tailor solutions to the unique demands of the AI workloads they plan to deploy.\nNVIDIA Quantum-2 InfiniBand and NVIDIA Spectrum-X are two networking platforms specifically designed and optimized to meet the networking challenges of the AI data center, each with its own unique features and innovations.\nInfiniBand drives AI performance\nInfiniBand technology has been a driving force behind large-scale supercomputing deployments for complex distributed scientific computing. It has become the de facto network for AI factories. With ultra-low latencies, InfiniBand has become a linchpin for accelerating today\u2019s mainstream high-performance computing (HPC) and AI applications. Many crucial network capabilities required for efficient AI systems are native to the NVIDIA Quantum-2 InfiniBand platform.\nIn-network computing, driven by InfiniBand, integrates hardware-based computing engines into the network. This offloads complex operations at scale and utilizes the NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP), an in-network aggregation mechanism. SHARP supports multiple concurrent collective operations, doubling data bandwidth for data reductions and performance enhancements.\nThe InfiniBand adaptive routing optimally spreads traffic, mitigating congestion and enhancing resource utilization. Directed by a Subnet Manager, InfiniBand selects congestion-free routes based on network conditions, maximizing efficiency without compromising order of packet arrival.\nThe InfiniBand Congestion Control Architecture guarantees deterministic bandwidth and latency. It uses a three-stage process to manage congestion, preventing performance bottlenecks in AI workloads.\nThese inherent optimizations empower InfiniBand to meet the demands of AI applications, ultimately driving superior performance and efficiency.\nNavigating Ethernet for AI deployments\nDeploying Ethernet networks for an AI infrastructure requires addressing needs specific to the Ethernet protocol. Over time, Ethernet has incorporated an expansive, comprehensive, and (at times) complex feature set that caters to a huge range of network scenarios.\nAs such, out-of-the-box or traditional Ethernet isn\u2019t explicitly designed for high performance. AI clouds that use traditional Ethernet for their compute fabric can only achieve a fraction of the performance that they would achieve with an optimized network.\nIn multi-tenant environments where multiple AI jobs run simultaneously, performance isolation is critical to prevent further degradation of performance. If there is a link fault, the traditional Ethernet fabric can cause the cluster\u2019s AI performance to drop by half. This is because traditional Ethernet has primarily been optimized for everyday enterprise workflows and isn\u2019t designed to meet the demands of high-performance AI applications that rely on the NVIDIA Collective Communications Library (NCCL).\nThese performance issues are due to factors inherent to traditional Ethernet, including:\nHigher switch latencies, common across commodity ASICs\nSplit buffer switch architecture, which can lead to bandwidth unfairness\nLoad balancing that is suboptimized for the large flows generated by AI workloads\nPerformance isolation and noisy neighbor issues\nThe Spectrum-X networking platform solves these issues and more. Spectrum-X builds on the standard Ethernet protocol with RDMA over Converged Ethernet (RoCE) Extensions, enhancing performance for AI. These extensions leverage the best practices native to InfiniBand and bring innovations such as adaptive routing and congestion control to Ethernet.\nSpectrum-X is the only Ethernet platform that delivers the high effective bandwidth and performance isolation needed for multi-tenant generative AI clouds, enabled due to Spectrum-4 working in close coordination with NVIDIA BlueField-3 DPUs.\nSummary\nThe era of AI is here, and the network is the cornerstone of its success. To fully embrace the potential of AI, data center architects must carefully consider network design and tailor these designs to the unique demands of AI workloads. Addressing \u200cnetworking considerations is key to unlocking the full potential of AI technologies and driving innovation in the data center industry.\nNVIDIA Quantum InfiniBand is an ideal choice for AI factories, thanks to its ultra-low latencies, scalable performance, and advanced feature sets. NVIDIA Spectrum-X, with its purpose-built technology innovations for AI, offers a groundbreaking solution for organizations building Ethernet-based AI clouds.\nTo learn more about AI performance demands and network requirements, see the Networking for the Era of AI whitepaper. Join the conversation in the NVIDIA Developer Infrastructure and Networking Forum."}], "https://developer.nvidia.com/blog/supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance/": [{"text": "The article discusses the importance of graph analytics in modern data and analytics capabilities and the potential for GPU-CPU fusion to significantly improve performance. By harnessing the parallel processing power of GPUs, graph training times can be accelerated by over 100x compared to CPU-based computations. The architecture outlined in the article includes components such as cuGraph for GPU acceleration, TigerGraph for graph database capabilities, and custom user-defined functions to bridge the gap between the two. Practical examples of traditional CPU-based and accelerated GPU-CPU fusion PageRank calculations are provided. The article also highlights best practices for algorithm selection, efficient data preprocessing, and GPU memory management to optimize graph analytics. Future possibilities include reducing GPU memory footprint, expanding the graph algorithm library, integrating support for GNN and RAPIDS libraries, and continuous performance optimization. Overall, the seamless blending of GPUs and CPUs in graph analytics shows promising potential for reshaping data exploration and unlocking new possibilities in various domains.", "text_components": ["Supercharge Graph Analytics at Scale with GPU-CPU Fusion for 100x Performance\nGraphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. According to one study, by 2025 graph technologies will be used in 80% of data and analytics innovations, which will help facilitate rapid decision making across organizations.\nWhen working with graphs containing millions of nodes, the execution duration of algorithms such as Louvain on a CPU can stretch up to several hours. This prolonged processing time not only impacts developer productivity, but also leads to suboptimal overall performance outcomes.\nHarnessing the parallel processing power of GPUs can significantly accelerate graph training times. The benchmark results demonstrate the remarkable potential of GPU acceleration in outpacing CPU-based computations by more than 100x.\nThis significant increase in speed showcases the clear advantages of incorporating GPUs into graph analytics. This post explains the architecture behind achieving these 100x performance gains.", "Understanding the components\nAt the core of this game-changing architecture are the following three key components, each playing a crucial role.", "GPU acceleration with cuGraph\ncuGraph, the NVIDIA GPU-accelerated graph analytics library, takes the lead in turbocharging your graph computations. Traditional CPU-based graph processing can often be a bottleneck, especially when dealing with large-scale graphs. cuGraph unleashes the raw processing power of NVIDIA A100 GPUs, specifically designed for high-performance computing (HPC), to handle complex graph algorithms with unrivaled speed.\nGraph algorithms such as PageRank, Louvain, and Betweenness Centrality are inherently parallelizable, making them ideal candidates for GPU acceleration. The thousands of cores in the NVIDIA A100 GPUs enable simultaneous processing of data, drastically reducing computation times compared to CPU-based approaches.", "TigerGraph graph database capabilities\nWhile cuGraph optimizes graph analytics with GPUs, the TigerGraph graph database complements the GPU acceleration with its efficiency in storing and querying interconnected data. TigerGraph\u2019s distributed architecture, along with its Turing-complete GSQL language and native support for graph data, enable it to handle complex relationships and real-time queries and updates with remarkable flexibility.\nTigerGraph\u2019s data structure, known as the Graph Model, provides a highly scalable representation of graph data. It optimizes data locality and traversal, reducing I/O bottlenecks during graph processing. The seamless integration of TigerGraph with cuGraph ensures that data flows effortlessly between the graph database and the GPU-accelerated analytics, maximizing \u200cperformance gains.", "Enhancing GSQL with advanced functionality\nTo complete this fusion, ZettaBolt has designed custom user-defined functions (UDFs) that act as the bridge between TigerGraph and cuGraph. UDFs enable you to write and seamlessly integrate your own C++ code into the TigerGraph ecosystem.\nThey enable communication between GSQL and cuGraph\u2019s Python service over the Thrift RPC layer. This enables the smooth flow of data and computations between the graph database and the GPU-accelerated analytics, unlocking new possibilities for graph algorithm optimization.", "Traditional and accelerated PageRank calculation\nThis section explores how to put this powerful GPU-CPU fusion to work with practical examples. It introduces two distinct methods for running PageRank calculations: the traditional CPU-based ```tg_pagerank``` and the accelerated ```accel_pagerank``` that leverages the GPU-CPU fusion architecture.", "Traditional approach: tg_pagerank\nThis traditional approach employs the ```tg_pagerank``` query to calculate PageRank scores. It relies on CPU-based processing and is suitable for scenarios where GPU acceleration is not available or required. For more details, see tigergraph/gsql-graph-algorithms on GitHub.\nQuery tg_pagerank(\nv_type, # Vertex type representing persons in the graph\ne_type\", # Edge type representing friendships between persons\nmax_change=0.001, # Maximum change in PageRank scores for convergence\nmaximum_iteration=25,# Maximum number of iterations for convergence\ndamping=0.85, # Damping factor for the PageRank calculation\ntop_k=100, # Number of top results to display\nprint_results=True,# Whether to print the PageRank results\nresult_attribute=\"\", # Optional attribute to store the PageRank results\nfile_path=\"\", # Optional file path to save the results\ndisplay_edges=False# Whether to display the edges during computation\n)", "Accelerated approach: accel_pagerank\nAccelerate your PageRank computations with ```accel_pagerank```. This query harnesses the GPU-CPU fusion architecture to significantly boost performance, making it ideal for large-scale graph processing ( GSQL ).\nQUERY accel_pagerank(\nINT numServers, #Number of servers to distribute computation\nINT seg_size, #Segment size for processing\nSTRING v_type, #Vertex type\nSTRING e_type, #Edge type\nFLOAT max_change=0.00001,#Maximum change threshold for convergence\nINT maximum_iteration=50,# Maximum number of iterations\nFLOAT damping=0.85, #Damping factor for PageRank calculation\nINT top_k=100, #Top-k results to retrieve\nBOOL print_accum=TRUE, #Print accumulated results (default: TRUE)\nSTRING result_attr=\"\", #Result attribute name\nSTRING file_path, #File path for storing results\nBOOL display_edges=FALSE,#Display edges in results (default: FALSE)\nSTRING profile_path, #Path for profiling data\nSTRING graph_name, #Name of the graph\nSTRING server_name, #Server address\nUINT port, #Port for communication\nUINT total_segments, #Total segments for computation\nSTRING tmp_dir, #Temporary directory for processing\nINT streaming_limit=300000, #Streaming data limit\nBOOL cache_graph=FALSE #Cache the graph (default: FALSE)\n)", "Seamless transition\nSwitching between the traditional ```tg_pagerank``` and accelerated ```accel_pagerank``` queries is as simple as changing your API call. You don\u2019t need to worry about the underlying technical details. The transition is designed to be effortless, so you can easily adapt the graph processing to your needs. Whether you require the raw power of GPU acceleration or prefer the familiarity of CPU-based processing, your graph analytics will seamlessly align with your requirements.", "Creating the architecture\nThis section walks you through the step-by-step process of the architecture, where the magic of GPU-CPU fusion comes to life.\nDiagram showing the three-step process for high-performance graph analytics with TigerGraph, cuGraph, and GSQL. Step 1: Streamlining data flow from TigerGraph nodes to NVIDIA GPUs with Zetta bridge and GSQL. Step 2: Invoking GPU-accelerated algorithms on streamed data with GSQL. Step 3: Parallel data retrieval from GPU and storage on TigerGraph as vertex attributes.\nFigure 1. Process for high-performance graph analytics with TigerGraph, cuGraph, and GSQL", "Streaming edges from TigerGraph to cuGraph\nThe journey begins with the efficient streaming of edges from TigerGraph to cuGraph for GPU-accelerated processing. As the edges are read in parallel from TigerGraph, they are collected in batches until the edge count reaches a predetermined threshold (1 million, for example). Once the batch is complete, it is flushed to cuGraph through the Thrift RPC layer.\nStreaming edges in batches optimizes data transfer, reducing overhead and ensuring that the GPU-accelerated processing receives a continuous flow of data. The streaming process efficiently prepares the graph data for \u200csubsequent GPU-based computations.", "GPU-accelerated computation with PageRank\nWith the entire graph data now residing in cuGraph memory on the NVIDIA A100 GPUs, the architecture is poised to unleash its true potential.\nConsider the classic PageRank algorithm as an example. PageRank calculates the importance of nodes in a graph based on the number and quality of incoming links. The algorithm is an excellent candidate for GPU acceleration because it is iterative and parallelizable. cuGraph\u2019s GPU-based PageRank algorithm processes the entire graph in parallel, efficiently traversing the network and updating the PageRank scores iteratively.", "Obtaining results using Thrift RPC layer\nOnce the GPU-accelerated computation is complete, it\u2019s time to get the results. The PageRank scores, now residing in cuGraph memory, are obtained using the Thrift RPC layer and brought back to the UDFs.", "Achieving the 100x speedup breakthrough\nThe performance comparisons speak for themselves\u2014real-world benchmarks demonstrate the awe-inspiring results achieved with this hybrid architecture. The 100x speedup is a significant leap in graph algorithm performance.\nWith this architecture, developers gain a competitive edge in diverse domains including social networks, recommendation systems, graph-based machine learning (ML), and more.", "Graph algorithm performance comparison\nThis benchmark demonstrates the performance of two prominent graph algorithms, Louvain and PageRank, using TigerGraph and TigerGraph alongside cuGraph on a high-performance GPU infrastructure powered by the NVIDIA A100 80GB GPUs and the AMD EPYC 7713 64-Core Processor, with a single-node configuration featuring 512GB of RAM.", "Benchmark dataset\nGraphalytics, developed by the Linked Data Benchmark Council (LDBC), is a comprehensive benchmark suite designed for evaluating the performance of graph database management systems (GDBMSs) and graph processing frameworks. It offers real-world datasets, diverse workloads, and a range of graph algorithms to help researchers and organizations assess system efficiency and scalability. For more information, see LDBC Graphalytics Benchmark.\nGraph\nLouvain\nPageRank\nVertices\nEdges\nCPU\n(sec)\nCPU + GPU\n(sec)\nSpeedup\n(x)\nCPU\n(sec)\nCPU + GPU\n(sec)\nSpeedup\n(x)\n2,396,657\n64,155,735\n1,265\n7\n172\n1,030\n7\n147\n4,610,222\n129,333,677\n2,288\n12\n188\n2,142\n19\n113\n8,870,942\n260,379,520\n4,723\n27\n174\n4,542\n38\n120\n17,062,472\n523,602,831\n9,977\n77\n130\n8,643\n46\n188\nTable 1. The TigerGraph CPU-based solution compared to its cuGraph-accelerated counterpart", "Best practices and considerations\nAs you embark on your own GPU-CPU fusion journey, consider the following best practices and considerations to optimize your graph analytics.", "Algorithm selection for GPU acceleration\nThe synergy between CPU and GPU is crucial in selecting the right algorithms for GPU acceleration. While some graph algorithms are highly parallelizable and benefit significantly from the thousands of cores in NVIDIA A100 GPUs, others may require more sequential processing, making CPU a better choice. By strategically offloading parallelizable algorithms to the GPU, you can achieve remarkable speedup and efficiency in graph computations, while leveraging the strengths of both CPU and GPU for optimal performance.", "Efficient data preprocessing\nThe efficient data preprocessing phase is where CPU and GPU synergy plays a vital role. While TigerGraph\u2019s powerful graph database capabilities handle the initial data retrieval and processing on the CPU, the GPU-accelerated cuGraph library efficiently streams and batches the graph data for further computation. The parallel processing power of the GPU ensures a continuous flow of data, minimizing overhead and bottlenecks during the data transfer phase, leading to a seamless flow of data for accelerated analytics.", "GPU memory management\nEffective GPU memory management is a critical consideration to avoid performance bottlenecks during graph computations. Both CPU and GPU play significant roles in this process. The CPU ensures efficient data handling and allocation before transferring the relevant data to the GPU for processing. On the GPU side, the parallel processing capabilities efficiently utilize the available memory to perform computations on large-scale graphs. The tight collaboration between CPU and GPU in memory management contributes to smooth, optimized GPU-accelerated graph processing.", "Future work and possibilities\nThere are exciting possibilities and enhancements to explore in the area of GPU-CPU fusion for graph analytics. Some key areas for future work and potential developments are detailed below.", "Reducing GPU memory footprint\nOptimizing GPU memory use is essential for handling massive graphs and increasing algorithm scalability. Future areas of focus include memory-efficient data structures, graph partitioning techniques, and smart caching mechanisms to reduce GPU memory footprint. Efficiently managing memory enables processing even larger graphs with improved performance.", "Expanding the graph algorithm library\nThe current architecture supports graph algorithms like PageRank, Louvain, and Betweenness Centrality. However, there is a vast landscape of graph algorithms waiting to be explored. Expanding the graph algorithm library to include more diverse and complex algorithms will enable developers to address a broader range of graph analytics challenges.", "Integrating support for GNN and other RAPIDS libraries\nGeneralized graph neural networks (GNNs) have become a popular choice for various graph-related tasks. Integrating support for GNN and other RAPIDS libraries, such as cuML and cuGraphML, will enrich the architecture with cutting-edge deep learning capabilities for graph-based ML tasks. This integration will enable seamless exploration of both traditional graph algorithms and emerging ML models, fostering innovation and versatility in graph analytics.", "Performance optimization and tuning\nContinuous performance optimization and tuning are critical to unlocking the full potential of GPU-CPU fusion. Conducting in-depth profiling and benchmarking, leveraging GPU-specific optimizations, and fine-tuning algorithms for specific graph characteristics will lead to more speedups and efficiency gains.", "Summary\nThis post has explored how the seamless blending of GPUs and CPUs can supercharge graph algorithm performance. The power of TigerGraph\u2019s database prowess combined with cuGraph GPU acceleration creates an unbeatable partnership. The smooth data flow achieves an astonishing 100x speed boost, propelling graph analytics into new frontiers.\nGPU-CPU fusion is set to reshape how teams explore data and navigate intricate networks and relationships. From social networks to machine learning, GPU-CPU fusion unlocks endless possibilities. Future areas of work include optimizing memory efficiency, broadening algorithm coverage, and fine-tuning performance, ensuring that this architecture remains at the forefront of graph analytics. Embrace the fusion and redefine the boundaries of data exploration.\nReady to get started with accelerated graph processing? If you have TigerGraph (3.9.X) and NVIDIA GPUs (with RAPIDS support), reach out to TigerGraph or Zettabolt to express your interest in exploring accelerated graph processing. They will guide you through the initial steps, provide you with the necessary information, and assist in setting up the infrastructure for accelerated graph processing.\nOnce the infrastructure is in place, you\u2019ll have access to accelerated queries designed to optimize your graph processing tasks. These queries harness the power of NVIDIA GPUs and the TigerGraph platform for enhanced performance. Explore and benchmark the accelerated graph processing capabilities to experience significantly improved performance in graph analytics.\nTigerGraph and Zettabolt will continue to provide assistance and answer questions as you explore accelerated graph processing and new possibilities for handling large-scale graph data efficiently."], "document_title": "Supercharge Graph Analytics at Scale with GPU-CPU Fusion for 100x Performance", "document_url": "https://developer.nvidia.com/blog/supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance/", "document_date": "2023-10-13T21:18:33", "document_date_modified": "2023-11-02T18:14:40", "document_full_text": "Supercharge Graph Analytics at Scale with GPU-CPU Fusion for 100x Performance\nGraphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. According to one study, by 2025 graph technologies will be used in 80% of data and analytics innovations, which will help facilitate rapid decision making across organizations.\nWhen working with graphs containing millions of nodes, the execution duration of algorithms such as Louvain on a CPU can stretch up to several hours. This prolonged processing time not only impacts developer productivity, but also leads to suboptimal overall performance outcomes.\nHarnessing the parallel processing power of GPUs can significantly accelerate graph training times. The benchmark results demonstrate the remarkable potential of GPU acceleration in outpacing CPU-based computations by more than 100x.\nThis significant increase in speed showcases the clear advantages of incorporating GPUs into graph analytics. This post explains the architecture behind achieving these 100x performance gains.\nUnderstanding the components\nAt the core of this game-changing architecture are the following three key components, each playing a crucial role.\nGPU acceleration with cuGraph\ncuGraph, the NVIDIA GPU-accelerated graph analytics library, takes the lead in turbocharging your graph computations. Traditional CPU-based graph processing can often be a bottleneck, especially when dealing with large-scale graphs. cuGraph unleashes the raw processing power of NVIDIA A100 GPUs, specifically designed for high-performance computing (HPC), to handle complex graph algorithms with unrivaled speed.\nGraph algorithms such as PageRank, Louvain, and Betweenness Centrality are inherently parallelizable, making them ideal candidates for GPU acceleration. The thousands of cores in the NVIDIA A100 GPUs enable simultaneous processing of data, drastically reducing computation times compared to CPU-based approaches.\nTigerGraph graph database capabilities\nWhile cuGraph optimizes graph analytics with GPUs, the TigerGraph graph database complements the GPU acceleration with its efficiency in storing and querying interconnected data. TigerGraph\u2019s distributed architecture, along with its Turing-complete GSQL language and native support for graph data, enable it to handle complex relationships and real-time queries and updates with remarkable flexibility.\nTigerGraph\u2019s data structure, known as the Graph Model, provides a highly scalable representation of graph data. It optimizes data locality and traversal, reducing I/O bottlenecks during graph processing. The seamless integration of TigerGraph with cuGraph ensures that data flows effortlessly between the graph database and the GPU-accelerated analytics, maximizing \u200cperformance gains.\nEnhancing GSQL with advanced functionality\nTo complete this fusion, ZettaBolt has designed custom user-defined functions (UDFs) that act as the bridge between TigerGraph and cuGraph. UDFs enable you to write and seamlessly integrate your own C++ code into the TigerGraph ecosystem.\nThey enable communication between GSQL and cuGraph\u2019s Python service over the Thrift RPC layer. This enables the smooth flow of data and computations between the graph database and the GPU-accelerated analytics, unlocking new possibilities for graph algorithm optimization.\nTraditional and accelerated PageRank calculation\nThis section explores how to put this powerful GPU-CPU fusion to work with practical examples. It introduces two distinct methods for running PageRank calculations: the traditional CPU-based ```tg_pagerank``` and the accelerated ```accel_pagerank``` that leverages the GPU-CPU fusion architecture.\nTraditional approach: tg_pagerank\nThis traditional approach employs the ```tg_pagerank``` query to calculate PageRank scores. It relies on CPU-based processing and is suitable for scenarios where GPU acceleration is not available or required. For more details, see tigergraph/gsql-graph-algorithms on GitHub.\nQuery tg_pagerank(\nv_type, # Vertex type representing persons in the graph\ne_type\", # Edge type representing friendships between persons\nmax_change=0.001, # Maximum change in PageRank scores for convergence\nmaximum_iteration=25,# Maximum number of iterations for convergence\ndamping=0.85, # Damping factor for the PageRank calculation\ntop_k=100, # Number of top results to display\nprint_results=True,# Whether to print the PageRank results\nresult_attribute=\"\", # Optional attribute to store the PageRank results\nfile_path=\"\", # Optional file path to save the results\ndisplay_edges=False# Whether to display the edges during computation\n)\nAccelerated approach: accel_pagerank\nAccelerate your PageRank computations with ```accel_pagerank```. This query harnesses the GPU-CPU fusion architecture to significantly boost performance, making it ideal for large-scale graph processing ( GSQL ).\nQUERY accel_pagerank(\nINT numServers, #Number of servers to distribute computation\nINT seg_size, #Segment size for processing\nSTRING v_type, #Vertex type\nSTRING e_type, #Edge type\nFLOAT max_change=0.00001,#Maximum change threshold for convergence\nINT maximum_iteration=50,# Maximum number of iterations\nFLOAT damping=0.85, #Damping factor for PageRank calculation\nINT top_k=100, #Top-k results to retrieve\nBOOL print_accum=TRUE, #Print accumulated results (default: TRUE)\nSTRING result_attr=\"\", #Result attribute name\nSTRING file_path, #File path for storing results\nBOOL display_edges=FALSE,#Display edges in results (default: FALSE)\nSTRING profile_path, #Path for profiling data\nSTRING graph_name, #Name of the graph\nSTRING server_name, #Server address\nUINT port, #Port for communication\nUINT total_segments, #Total segments for computation\nSTRING tmp_dir, #Temporary directory for processing\nINT streaming_limit=300000, #Streaming data limit\nBOOL cache_graph=FALSE #Cache the graph (default: FALSE)\n)\nSeamless transition\nSwitching between the traditional ```tg_pagerank``` and accelerated ```accel_pagerank``` queries is as simple as changing your API call. You don\u2019t need to worry about the underlying technical details. The transition is designed to be effortless, so you can easily adapt the graph processing to your needs. Whether you require the raw power of GPU acceleration or prefer the familiarity of CPU-based processing, your graph analytics will seamlessly align with your requirements.\nCreating the architecture\nThis section walks you through the step-by-step process of the architecture, where the magic of GPU-CPU fusion comes to life.\nDiagram showing the three-step process for high-performance graph analytics with TigerGraph, cuGraph, and GSQL. Step 1: Streamlining data flow from TigerGraph nodes to NVIDIA GPUs with Zetta bridge and GSQL. Step 2: Invoking GPU-accelerated algorithms on streamed data with GSQL. Step 3: Parallel data retrieval from GPU and storage on TigerGraph as vertex attributes.\nFigure 1. Process for high-performance graph analytics with TigerGraph, cuGraph, and GSQL\nStreaming edges from TigerGraph to cuGraph\nThe journey begins with the efficient streaming of edges from TigerGraph to cuGraph for GPU-accelerated processing. As the edges are read in parallel from TigerGraph, they are collected in batches until the edge count reaches a predetermined threshold (1 million, for example). Once the batch is complete, it is flushed to cuGraph through the Thrift RPC layer.\nStreaming edges in batches optimizes data transfer, reducing overhead and ensuring that the GPU-accelerated processing receives a continuous flow of data. The streaming process efficiently prepares the graph data for \u200csubsequent GPU-based computations.\nGPU-accelerated computation with PageRank\nWith the entire graph data now residing in cuGraph memory on the NVIDIA A100 GPUs, the architecture is poised to unleash its true potential.\nConsider the classic PageRank algorithm as an example. PageRank calculates the importance of nodes in a graph based on the number and quality of incoming links. The algorithm is an excellent candidate for GPU acceleration because it is iterative and parallelizable. cuGraph\u2019s GPU-based PageRank algorithm processes the entire graph in parallel, efficiently traversing the network and updating the PageRank scores iteratively.\nObtaining results using Thrift RPC layer\nOnce the GPU-accelerated computation is complete, it\u2019s time to get the results. The PageRank scores, now residing in cuGraph memory, are obtained using the Thrift RPC layer and brought back to the UDFs.\nAchieving the 100x speedup breakthrough\nThe performance comparisons speak for themselves\u2014real-world benchmarks demonstrate the awe-inspiring results achieved with this hybrid architecture. The 100x speedup is a significant leap in graph algorithm performance.\nWith this architecture, developers gain a competitive edge in diverse domains including social networks, recommendation systems, graph-based machine learning (ML), and more.\nGraph algorithm performance comparison\nThis benchmark demonstrates the performance of two prominent graph algorithms, Louvain and PageRank, using TigerGraph and TigerGraph alongside cuGraph on a high-performance GPU infrastructure powered by the NVIDIA A100 80GB GPUs and the AMD EPYC 7713 64-Core Processor, with a single-node configuration featuring 512GB of RAM.\nBenchmark dataset\nGraphalytics, developed by the Linked Data Benchmark Council (LDBC), is a comprehensive benchmark suite designed for evaluating the performance of graph database management systems (GDBMSs) and graph processing frameworks. It offers real-world datasets, diverse workloads, and a range of graph algorithms to help researchers and organizations assess system efficiency and scalability. For more information, see LDBC Graphalytics Benchmark.\nGraph\nLouvain\nPageRank\nVertices\nEdges\nCPU\n(sec)\nCPU + GPU\n(sec)\nSpeedup\n(x)\nCPU\n(sec)\nCPU + GPU\n(sec)\nSpeedup\n(x)\n2,396,657\n64,155,735\n1,265\n7\n172\n1,030\n7\n147\n4,610,222\n129,333,677\n2,288\n12\n188\n2,142\n19\n113\n8,870,942\n260,379,520\n4,723\n27\n174\n4,542\n38\n120\n17,062,472\n523,602,831\n9,977\n77\n130\n8,643\n46\n188\nTable 1. The TigerGraph CPU-based solution compared to its cuGraph-accelerated counterpart\nBest practices and considerations\nAs you embark on your own GPU-CPU fusion journey, consider the following best practices and considerations to optimize your graph analytics.\nAlgorithm selection for GPU acceleration\nThe synergy between CPU and GPU is crucial in selecting the right algorithms for GPU acceleration. While some graph algorithms are highly parallelizable and benefit significantly from the thousands of cores in NVIDIA A100 GPUs, others may require more sequential processing, making CPU a better choice. By strategically offloading parallelizable algorithms to the GPU, you can achieve remarkable speedup and efficiency in graph computations, while leveraging the strengths of both CPU and GPU for optimal performance.\nEfficient data preprocessing\nThe efficient data preprocessing phase is where CPU and GPU synergy plays a vital role. While TigerGraph\u2019s powerful graph database capabilities handle the initial data retrieval and processing on the CPU, the GPU-accelerated cuGraph library efficiently streams and batches the graph data for further computation. The parallel processing power of the GPU ensures a continuous flow of data, minimizing overhead and bottlenecks during the data transfer phase, leading to a seamless flow of data for accelerated analytics.\nGPU memory management\nEffective GPU memory management is a critical consideration to avoid performance bottlenecks during graph computations. Both CPU and GPU play significant roles in this process. The CPU ensures efficient data handling and allocation before transferring the relevant data to the GPU for processing. On the GPU side, the parallel processing capabilities efficiently utilize the available memory to perform computations on large-scale graphs. The tight collaboration between CPU and GPU in memory management contributes to smooth, optimized GPU-accelerated graph processing.\nFuture work and possibilities\nThere are exciting possibilities and enhancements to explore in the area of GPU-CPU fusion for graph analytics. Some key areas for future work and potential developments are detailed below.\nReducing GPU memory footprint\nOptimizing GPU memory use is essential for handling massive graphs and increasing algorithm scalability. Future areas of focus include memory-efficient data structures, graph partitioning techniques, and smart caching mechanisms to reduce GPU memory footprint. Efficiently managing memory enables processing even larger graphs with improved performance.\nExpanding the graph algorithm library\nThe current architecture supports graph algorithms like PageRank, Louvain, and Betweenness Centrality. However, there is a vast landscape of graph algorithms waiting to be explored. Expanding the graph algorithm library to include more diverse and complex algorithms will enable developers to address a broader range of graph analytics challenges.\nIntegrating support for GNN and other RAPIDS libraries\nGeneralized graph neural networks (GNNs) have become a popular choice for various graph-related tasks. Integrating support for GNN and other RAPIDS libraries, such as cuML and cuGraphML, will enrich the architecture with cutting-edge deep learning capabilities for graph-based ML tasks. This integration will enable seamless exploration of both traditional graph algorithms and emerging ML models, fostering innovation and versatility in graph analytics.\nPerformance optimization and tuning\nContinuous performance optimization and tuning are critical to unlocking the full potential of GPU-CPU fusion. Conducting in-depth profiling and benchmarking, leveraging GPU-specific optimizations, and fine-tuning algorithms for specific graph characteristics will lead to more speedups and efficiency gains.\nSummary\nThis post has explored how the seamless blending of GPUs and CPUs can supercharge graph algorithm performance. The power of TigerGraph\u2019s database prowess combined with cuGraph GPU acceleration creates an unbeatable partnership. The smooth data flow achieves an astonishing 100x speed boost, propelling graph analytics into new frontiers.\nGPU-CPU fusion is set to reshape how teams explore data and navigate intricate networks and relationships. From social networks to machine learning, GPU-CPU fusion unlocks endless possibilities. Future areas of work include optimizing memory efficiency, broadening algorithm coverage, and fine-tuning performance, ensuring that this architecture remains at the forefront of graph analytics. Embrace the fusion and redefine the boundaries of data exploration.\nReady to get started with accelerated graph processing? If you have TigerGraph (3.9.X) and NVIDIA GPUs (with RAPIDS support), reach out to TigerGraph or Zettabolt to express your interest in exploring accelerated graph processing. They will guide you through the initial steps, provide you with the necessary information, and assist in setting up the infrastructure for accelerated graph processing.\nOnce the infrastructure is in place, you\u2019ll have access to accelerated queries designed to optimize your graph processing tasks. These queries harness the power of NVIDIA GPUs and the TigerGraph platform for enhanced performance. Explore and benchmark the accelerated graph processing capabilities to experience significantly improved performance in graph analytics.\nTigerGraph and Zettabolt will continue to provide assistance and answer questions as you explore accelerated graph processing and new possibilities for handling large-scale graph data efficiently."}], "https://developer.nvidia.com/blog/power-optimization-with-nvidia-jetson/": [{"text": "The article discusses power optimization with NVIDIA Jetson modules, emphasizing the importance of monitoring power and compute resources to avoid performance issues. Jetson modules feature a high-efficiency power management integrated circuit and various tools provided by NVIDIA to optimize power efficiency. These tools include power modes, thermal monitoring, and the Jetson Power GUI for real-time monitoring and adjustment of power modes. The article also mentions Tegrastats, a command-line utility for reporting memory and processor usage, and JTOP, a user-friendly tool for monitoring and controlling resources on the Jetson. JTOP provides real-time updates on system performance, enabling users to analyze CPU and GPU usage, temperature, and memory usage. The article concludes by encouraging users to leverage NVIDIA tools to optimize power efficiency and performance for their applications.", "text_components": ["Power Optimization with NVIDIA Jetson\nWhen working with embedded systems such as the Jetson modules, you must optimize your application based on your power budget and compute resources. To avoid performance or even thermal throttling issues, monitoring these resources becomes really important.\nJetson modules are designed with a GPU, CPU, and various AI accelerators. They also feature a high-efficiency power management integrated circuit (PMIC), voltage regulators, and a power tree to optimize power efficiency. NVIDIA provides several tools and resources that can help you take advantage of the power architecture and optimize your resource usage:\nVarious power modes\nPower, thermal, and electrical management features\nA Jetson Power GUI for monitoring power and thermal status\nTegrastats for providing command-line statistics regarding the module\nJTOP", "Jetson power modes\nEach Jetson module supports multiple preconfigured power modes that are optimized for certain power budgets: 10 Watts, 15 Watts, 30 Watts, and so on. For each power budget, there are various configurations possible in terms of resource utilization.\nThese power modes are set using nvpmodel, and you can choose to use one of the preconfigured modes or create a custom power mode that is tuned to your requirements. The nvpmodel configuration enables a certain number of online GPU TPC, CPU, DLA, and PVA cores, along with certain frequencies, to keep the module within a certain power budget.\nMAXN mode is also available as an unconstrained power mode. It enables the maximum number of cores and clock frequencies for the various processors and engines that can then be tuned to create custom power modes that balance performance and power consumption.", "Power, thermal, and electrical management features\nJetson provides various power, thermal, and electrical management features:\nClock gating\nPower gating\nDynamic voltage frequency scaling\nDeep sleep (SC7) modes\nIdle power modes\nFor more information, see Supported Modes and Power Efficiency in the Jetson Linux Developer Guide.", "Jetson Power GUI\nThere are many tools that NVIDIA provides with JetPack that can assist you in thermal and power management. One such tool is the Jetson Power GUI, which gets installed as part of the JetPack image.\nThe Jetson Power GUI lets you monitor the power and thermal status of the Jetson board. With the Main tab, you can track CPU and GPU usage, as well as the device temperature. With real-time monitoring, you can quickly identify any performance bottlenecks or excessive power consumption that may lead to performance throttling.\nWith the Jetson Power GUI, you can adjust power modes, which optimize the trade-off between performance and power consumption. You can choose from one of the predefined power modes depending on the Jetson board that you are using.\nScreenshot shows information about GPU/CPU utilization as well as power metrics.\nFigure 1. Jetson Power launch page\nThe Power GUI tool also enables you to record pertinent power-related information to a log file for a specific duration. This is useful for capturing and analyzing behavior during specific tasks or specific time duration. For example, you could capture information about the performance of Jetson within the first 3 minutes of startup.\nScreenshot of example log file shows different CPU load over a period of time.\nFigure 2. Example log file from Power GUI\nThe plot graph\u2026 button provides data visualization functionality, so you can plot real-time, power-related information. The captured log file can also be used to plot a graph to help you visualize with even more simplicity how your system is performing.\nScreenshot of power channels for plotting graphs shows how to select different information for capturing in the graph.\nFigure 3. Graph plot channel selection\nScreenshot shows graphs for GPU, CPU, SOC, CV, VDDRQ, and SYS5V.\nFigure 4. Example graph plotted from Jetson Power GUI", "Tegrastats\nTegrastats is a command line utility provided by NVIDIA that reports memory and processor usage on the Jetson platform. This utility is provided with JetPack and can be found at ```<top>/core/utils/tegrastats```.\nTegrastats provides insights into several usage metrics, such as CPU, GPU, and memory. It also gives you the ability to monitor power consumption and provides real-time updates on power usage. These metrics are crucial in understanding how the system is performing.\nTegrastats also offers information about thermal behavior, such as the operating temperature for the CPU and GPU. This can help you prevent thermal throttling.", "Usage\nTo use the Tegrastats utility on Jetson, use the following commands.\nIn the foreground, run the following command:\n```\n$ tegrastats \u2014interval <int>\n```\nIn this command, ```<int>``` is the interval between log prints in milliseconds. By default, Tegrastats updates the statistics every second.\nIn the background, run the following command:\n```\n$ tegrastats \u2014interval <int> -logfile <out_file> &\n```\nIn this command, ```<out_file>``` is the pathname of the output file to which Tegrastats writes the log prints.\nThe following is a sample print of Tegrastats:\n```\nRAM 1545/31919MB (lfb 7400x4MB) SWAP 0/15959MB (cached 0MB) CPU\n[0%@1190,0%@1190,0%@1190,0%@1190,0%@1190,0%@1190,0%@1190,0%@1190]\nEMC_FREQ 1%@408 GR3D_FREQ 0%@318 VIC_FREQ 0%@115 APE 150 MTS fg 0% bg 0%\nAO@38C GPU@39.5C Tdiode@43.25C PMIC@100C AUX@38.5C CPU@39.5C\nthermal@38.8C Tboard@39C GPU 0/0 CPU 468/468 SOC 937/937 CV 0/0 VDDRQ\n312/234 SYS5V 1458/1458\n```\nTegrastats can also be integrated into scripts or applications to capture system statistics, which enables more automation scenarios.", "JTOP (Jetson-stats)\nJTOP is a user-friendly way to monitor and control the resources on the Jetson. It can help you visualize and understand various bottlenecks in your applications. For example, it shows whether an application is heavy on memory operations or an application is not using the hardware-accelerated engines in the Jetson module. In this way, JTOP leads to a more efficient and streamlined application optimized for the Jetson module.\nScreenshot displays the JTOP screen where you can visualize the current status of your device in terms of power and resource consumption.\nFigure 5. JTOP launch screen\nJTOP is specifically designed for monitoring and managing NVIDIA Jetson modules:\nNVIDIA Orin\nNVIDIA Xavier\nNVIDIA Nano\nNVIDIA TX\nIt provides real-time updates about system performance. This enables you to analyze the CPU and GPU usage, operating temperature, memory usage, and other relevant information.\nWith JTOP, this information can be accessed in a GUI for better visualization of the information. This way, it provides a convenient way to keep track of the system\u2019s metrics and performance figures, especially when running heavy AI workloads.\nJTOP also brings out the capability to tweak the system\u2019s performance. You can select the power mode in which they want their Jetson devices to be running, as well as control over the fan speed. This helps in optimizing system performance and thermals.\nScreenshot shows memory monitor tab from the JTOP GUI that gives details about memory usage from the Jetson system.\nFigure 6. JTOP memory monitor\nJTOP is especially useful for building a system on a power budget but still squeezing out the maximum performance possible. To install it, use ```pip```:\n```\n$ sudo apt update \n$ sudo apt-get install python3-pip\n$ sudo pip install -u jetson-stats\n```\nUse the ```jtop``` command:\n```\n$ jtop\n```\nThe JTOP tool is also available as a Python library with APIs that enable you to include JTOP functions in your scripts with ease.\n```\nfrom jtop import jtop\n\nwith jtop() as jetson:\n\twhile jetson.ok():\n\t\t#read jetson stats\n\t\tprint(jetson.stats)\n```\nFor more information, see the following video from JetsonHacks.\nVideo 1. JTOP: The Tool Jetson Pros Use. Do You?", "Get started with optimizing your power today\nThe NVIDIA Jetson platform is continuously advancing the edge inference performance for robotics and edge AI. It is an important factor in most edge AI applications to follow a power budget. With the NVIDIA tools, you can monitor system performance and model a custom power profile.\nFor more information, see the following resources:\nSupported Modes and Power Efficiency\nJTOP (jetson-stats) User Guide\nTroubleshooting (Jetson_stats)\n/rbonghi/ros_jetson_stats GitHub repo\n/NVIDIA-AI-IOT/ros2_jetson_stats GitHub repo\nWith all the tools mentioned in this post, you can easily create a power-efficient and performance-optimized system for your application."], "document_title": "Power Optimization with NVIDIA Jetson", "document_url": "https://developer.nvidia.com/blog/power-optimization-with-nvidia-jetson/", "document_date": "2023-10-05T18:56:17", "document_date_modified": "2023-11-02T18:14:44", "document_full_text": "Power Optimization with NVIDIA Jetson\nWhen working with embedded systems such as the Jetson modules, you must optimize your application based on your power budget and compute resources. To avoid performance or even thermal throttling issues, monitoring these resources becomes really important.\nJetson modules are designed with a GPU, CPU, and various AI accelerators. They also feature a high-efficiency power management integrated circuit (PMIC), voltage regulators, and a power tree to optimize power efficiency. NVIDIA provides several tools and resources that can help you take advantage of the power architecture and optimize your resource usage:\nVarious power modes\nPower, thermal, and electrical management features\nA Jetson Power GUI for monitoring power and thermal status\nTegrastats for providing command-line statistics regarding the module\nJTOP\nJetson power modes\nEach Jetson module supports multiple preconfigured power modes that are optimized for certain power budgets: 10 Watts, 15 Watts, 30 Watts, and so on. For each power budget, there are various configurations possible in terms of resource utilization.\nThese power modes are set using nvpmodel, and you can choose to use one of the preconfigured modes or create a custom power mode that is tuned to your requirements. The nvpmodel configuration enables a certain number of online GPU TPC, CPU, DLA, and PVA cores, along with certain frequencies, to keep the module within a certain power budget.\nMAXN mode is also available as an unconstrained power mode. It enables the maximum number of cores and clock frequencies for the various processors and engines that can then be tuned to create custom power modes that balance performance and power consumption.\nPower, thermal, and electrical management features\nJetson provides various power, thermal, and electrical management features:\nClock gating\nPower gating\nDynamic voltage frequency scaling\nDeep sleep (SC7) modes\nIdle power modes\nFor more information, see Supported Modes and Power Efficiency in the Jetson Linux Developer Guide.\nJetson Power GUI\nThere are many tools that NVIDIA provides with JetPack that can assist you in thermal and power management. One such tool is the Jetson Power GUI, which gets installed as part of the JetPack image.\nThe Jetson Power GUI lets you monitor the power and thermal status of the Jetson board. With the Main tab, you can track CPU and GPU usage, as well as the device temperature. With real-time monitoring, you can quickly identify any performance bottlenecks or excessive power consumption that may lead to performance throttling.\nWith the Jetson Power GUI, you can adjust power modes, which optimize the trade-off between performance and power consumption. You can choose from one of the predefined power modes depending on the Jetson board that you are using.\nScreenshot shows information about GPU/CPU utilization as well as power metrics.\nFigure 1. Jetson Power launch page\nThe Power GUI tool also enables you to record pertinent power-related information to a log file for a specific duration. This is useful for capturing and analyzing behavior during specific tasks or specific time duration. For example, you could capture information about the performance of Jetson within the first 3 minutes of startup.\nScreenshot of example log file shows different CPU load over a period of time.\nFigure 2. Example log file from Power GUI\nThe plot graph\u2026 button provides data visualization functionality, so you can plot real-time, power-related information. The captured log file can also be used to plot a graph to help you visualize with even more simplicity how your system is performing.\nScreenshot of power channels for plotting graphs shows how to select different information for capturing in the graph.\nFigure 3. Graph plot channel selection\nScreenshot shows graphs for GPU, CPU, SOC, CV, VDDRQ, and SYS5V.\nFigure 4. Example graph plotted from Jetson Power GUI\nTegrastats\nTegrastats is a command line utility provided by NVIDIA that reports memory and processor usage on the Jetson platform. This utility is provided with JetPack and can be found at ```<top>/core/utils/tegrastats```.\nTegrastats provides insights into several usage metrics, such as CPU, GPU, and memory. It also gives you the ability to monitor power consumption and provides real-time updates on power usage. These metrics are crucial in understanding how the system is performing.\nTegrastats also offers information about thermal behavior, such as the operating temperature for the CPU and GPU. This can help you prevent thermal throttling.\nUsage\nTo use the Tegrastats utility on Jetson, use the following commands.\nIn the foreground, run the following command:\n```\n$ tegrastats \u2014interval <int>\n```\nIn this command, ```<int>``` is the interval between log prints in milliseconds. By default, Tegrastats updates the statistics every second.\nIn the background, run the following command:\n```\n$ tegrastats \u2014interval <int> -logfile <out_file> &\n```\nIn this command, ```<out_file>``` is the pathname of the output file to which Tegrastats writes the log prints.\nThe following is a sample print of Tegrastats:\n```\nRAM 1545/31919MB (lfb 7400x4MB) SWAP 0/15959MB (cached 0MB) CPU\n[0%@1190,0%@1190,0%@1190,0%@1190,0%@1190,0%@1190,0%@1190,0%@1190]\nEMC_FREQ 1%@408 GR3D_FREQ 0%@318 VIC_FREQ 0%@115 APE 150 MTS fg 0% bg 0%\nAO@38C GPU@39.5C Tdiode@43.25C PMIC@100C AUX@38.5C CPU@39.5C\nthermal@38.8C Tboard@39C GPU 0/0 CPU 468/468 SOC 937/937 CV 0/0 VDDRQ\n312/234 SYS5V 1458/1458\n```\nTegrastats can also be integrated into scripts or applications to capture system statistics, which enables more automation scenarios.\nJTOP (Jetson-stats)\nJTOP is a user-friendly way to monitor and control the resources on the Jetson. It can help you visualize and understand various bottlenecks in your applications. For example, it shows whether an application is heavy on memory operations or an application is not using the hardware-accelerated engines in the Jetson module. In this way, JTOP leads to a more efficient and streamlined application optimized for the Jetson module.\nScreenshot displays the JTOP screen where you can visualize the current status of your device in terms of power and resource consumption.\nFigure 5. JTOP launch screen\nJTOP is specifically designed for monitoring and managing NVIDIA Jetson modules:\nNVIDIA Orin\nNVIDIA Xavier\nNVIDIA Nano\nNVIDIA TX\nIt provides real-time updates about system performance. This enables you to analyze the CPU and GPU usage, operating temperature, memory usage, and other relevant information.\nWith JTOP, this information can be accessed in a GUI for better visualization of the information. This way, it provides a convenient way to keep track of the system\u2019s metrics and performance figures, especially when running heavy AI workloads.\nJTOP also brings out the capability to tweak the system\u2019s performance. You can select the power mode in which they want their Jetson devices to be running, as well as control over the fan speed. This helps in optimizing system performance and thermals.\nScreenshot shows memory monitor tab from the JTOP GUI that gives details about memory usage from the Jetson system.\nFigure 6. JTOP memory monitor\nJTOP is especially useful for building a system on a power budget but still squeezing out the maximum performance possible. To install it, use ```pip```:\n```\n$ sudo apt update \n$ sudo apt-get install python3-pip\n$ sudo pip install -u jetson-stats\n```\nUse the ```jtop``` command:\n```\n$ jtop\n```\nThe JTOP tool is also available as a Python library with APIs that enable you to include JTOP functions in your scripts with ease.\n```\nfrom jtop import jtop\n\nwith jtop() as jetson:\n\twhile jetson.ok():\n\t\t#read jetson stats\n\t\tprint(jetson.stats)\n```\nFor more information, see the following video from JetsonHacks.\nVideo 1. JTOP: The Tool Jetson Pros Use. Do You?\nGet started with optimizing your power today\nThe NVIDIA Jetson platform is continuously advancing the edge inference performance for robotics and edge AI. It is an important factor in most edge AI applications to follow a power budget. With the NVIDIA tools, you can monitor system performance and model a custom power profile.\nFor more information, see the following resources:\nSupported Modes and Power Efficiency\nJTOP (jetson-stats) User Guide\nTroubleshooting (Jetson_stats)\n/rbonghi/ros_jetson_stats GitHub repo\n/NVIDIA-AI-IOT/ros2_jetson_stats GitHub repo\nWith all the tools mentioned in this post, you can easily create a power-efficient and performance-optimized system for your application."}], "https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/": [{"text": "NVIDIA has introduced SteerLM, a new technique that allows for customization of large language models (LLMs) during inference, aiming to address the limitations of existing approaches such as supervised fine-tuning and reinforcement learning from human feedback. SteerLM simplifies LLM customization by enabling dynamic steering of model outputs based on user-specified attributes. The four-step technique involves training an attribute prediction model, annotating datasets, performing attribute-conditioned supervised fine-tuning, and bootstrapping training through model sampling. By allowing users to specify desired attributes at inference time, SteerLM opens up opportunities for personalized applications in gaming, education, enterprise, and accessibility. The training process for a SteerLM model involves data cleaning and preprocessing, training the attribute prediction model and the SteerLM model, and finally running inference with different attribute values. SteerLM aims to democratize state-of-the-art customization by simplifying the training process and making it more accessible to developers. The future of AI with SteerLM looks promising, offering transformative possibilities for user-steerable AI systems aligned with human preferences.", "text_components": ["Announcing NVIDIA SteerLM: A Simple and Practical Technique to Customize LLMs During Inference\nWith the advent of large language models (LLMs) such as GPT-3, Megatron-Turing, Chinchilla, PaLM-2, Falcon, and Llama 2, remarkable progress in natural language generation has been made in recent years. However, despite their ability to produce human-like text, \u200cfoundation LLMs can fail to provide helpful and nuanced responses aligned with user preferences.\nThe current approach to improving LLMs involves supervised fine-tuning (SFT ) on human demonstrations followed by reinforcement learning from human feedback (RLHF). While RLHF can enhance performance, it has some limitations, including training complexity and a lack of user control.\nTo overcome these challenges, NVIDIA Research developed and released NVIDIA SteerLM, a new four-step technique that simplifies LLM customization while enabling dynamic steering of model outputs based on attributes you specify, as part of NVIDIA NeMo. This post provides an in-depth look at how SteerLM works, why it marks a significant advance, and how to train a SteerLM model.", "Language models bring promise and potential pitfalls\nBy pretraining on massive text corpora, LLMs acquire broad linguistic capabilities and world knowledge. Researchers have successfully applied LLMs to diverse natural language processing (NLP) tasks like translation, question answering, and text generation. However, these models often fail to follow user-provided instructions, and instead produce generic, repetitive, or nonsensical text. Access to human feedback is essential for customizing LLMs.", "Opportunities with existing approaches\nSFT augments model capabilities, but causes responses to become short and mechanical. RLHF further optimizes models by favoring human-preferred responses over alternatives. However, RLHF requires an extremely complex training infrastructure, hindering broad adoption.", "Introducing SteerLM\nSteerLM leverages a supervised fine-tuning method that empowers you to control responses during inference. It overcomes the limitations of prior alignment techniques, and consists of four key steps:\nTrain an attribute prediction model on human-annotated datasets to evaluate response quality on any number of attributes like helpfulness, humor, and creativity.\nAnnotate diverse datasets by predicting their attribute scores, using the model from Step 1 to enrich the diversity of data available to the model.\nPerform attribute-conditioned SFT by training the LLM to generate responses conditioned on specified combinations of attributes, like user-perceived quality and helpfulness.\nBootstrap training through model sampling by generating diverse responses conditioned on maximum quality (Figure 1, 4a), then fine-tuning on them to further improve alignment (Figure 1, 4b).\nDiagram of the four steps of Steer LM. Step 1. The base language model is trained to assess the quality of responses by predicting attribute values. Step 2. The attribute prediction model is used to annotate response quality across diverse datasets. Step 3. Given a prompt and desired attribute values, a new base model is fine-tuned to generate responses that align with the specified attributes. Step 4. Multiple responses are sampled from the fine-tuned model in Step 3, specifying maximum quality. The sampled responses are evaluated by the trained attribute prediction model, leading to another round of fine-tuning.\nFigure 1. The four steps of SteerLM By relying solely on the standard language modeling objective, SteerLM simplifies alignment compared to RLHF. It supports user-steerable AI by enabling you to adjust attributes at inference time. This enables the developer to define preferences relevant to the application, unlike other techniques that require using predetermined preferences.", "Unlocking customizable AI with user steering\nA key innovation of SteerLM is enabling the user to specify desired attributes (humor level and toxicity tolerance, for example) at inference time when querying models. You can move from running one customization with SteerLM, to serving many use cases at inference time.\nSteerLM empowers a range of applications, including:\nGaming: Vary non-player character dialogue for game scenarios. To learn more, see NVIDIA ACE Adds Emotion to AI-Powered NPCs with NeMo SteerLM.\nEducation: Maintain a formal and helpful persona for student queries.\nEnterprise: Serve multiple teams in an organization with personalized capabilities.\nAccessibility: Curb undesired model biases by controlling sensitive attributes.\nSuch flexibility promises to unlock a new generation of bespoke AI systems tailored to individual needs.", "Democratizing state-of-the-art customization through simplified training\nCompared to the specialized infrastructure required for other advanced customization techniques, the straightforward training scheme of SteerLM makes state-of-the-art customization capabilities more accessible to developers. Its performance clearly demonstrates that techniques like reinforcement learning are not required for robust instruction tuning.\nLeveraging standard techniques such as SFT simplifies the complexity, requiring minimal changes to infrastructure and code. Reasonable results can be achieved with limited hyperparameter optimization.\nOverall, this leads to a simple and practical approach to obtaining highly accurate customized LLMs. In our experiments, SteerLM 43B achieved state-of-the-art performance on the Vicuna benchmark, outperforming existing RLHF models like LLaMA 30B RLHF. Specifically, SteerLM 43B obtained an average score of 655.75 on the Vicuna automatic evaluation, compared to scores of 646.25 for Guanaco 65B and 612.75 for LLaMA 30B RLHF.\nThese results highlight that the straightforward training process of SteerLM can lead to customized LLMs with accuracy on par with more complex RLHF techniques. Through simplified training, SteerLM makes achieving such high levels of accuracy more accessible, enabling easier democratization of customization among developers.\nFor more details, see our paper, SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF. You can also get information about how to experiment with a Llama 2 13B model customized using the SteerLM method.", "How to train a SteerLM model\nThis section is a step-by-step tutorial that walks you through how to run a full SteerLM pipeline on OASST data with a 2B NeMo LLM model. It includes the following:\nData cleaning and preprocessing\nTraining the attribute prediction (value model)\nTraining the attribute-conditioned SFT (SteerLM model)\nInference on the SteerLM model with different attribute values", "Step 1: Install requirements\nStart by installing the necessary Python libraries:\npip install fire langchain==0.0.133 Get access to NeMo.", "Step 2: Download and subset data\nThis tutorial uses a small subset of the OASST dataset. OASST contains open-domain conversations with human annotations for 13 different quality attributes.\nFirst download and subset it:\nmkdir -p data\ncd data\nwget https://huggingface.co/datasets/OpenAssistant/oasst1/resolve/main/2023-04-12_oasst_all.trees.jsonl.gz\ngunzip -f 2023-04-12_oasst_all.trees.jsonl.gz\nmv 2023-04-12_oasst_all.trees.jsonl data.jsonl\nhead -5000 data.jsonl > subset_data.jsonl\ncd -", "Step 3: Download Llama 2 LLM model and tokenizer and convert\nDownload the Llama 2 7B LLM model and tokenizer into the models folder.\nThen convert the Llama 2 LLM into .nemo format:\npython NeMo/scripts/nlp_language_modeling/convert_hf_llama_to_nemo.py --in-file /path/to/llama --out-file /output_path/llama7b.nemo Untar the .nemo file to obtain the tokenizer in NeMo format:\ntar <path-to-model>/llama7b.nemo\nmv ba4632640484461f8ae9d61f6dfe0d0b_tokenizer.model tokenizer.model\nThe prefix for the tokenizer would be different when extracted. Ensure that the correct tokenizer file is used when running the preceding command.", "Step 4: Preprocess OASST data\nPreprocess the data using the NeMo preprocessing scripts. Then create separate text-to-value and value-to-text versions:\npython scripts/nlp_language_modeling/sft/preprocessing.py \\\n--input_file=data/subset_data.jsonl \\\n--output_file_prefix=data/subset_data_output \\\n--mask_role=User \\\n--type=TEXT_TO_VALUE \\\n--split_ratio=0.95 \\\n--seed=10\npython scripts/nlp_language_modeling/sft/preprocessing.py \\\n--input_file=data/subset_data.jsonl \\\n--output_file_prefix=data/subset_data_output_v2t \\\n--mask_role=User \\\n--type=VALUE_TO_TEXT \\\n--split_ratio=0.95 \\\n--seed=10", "Step 5: Clean text-to-value data\nRunning the following script will remove the records if all the tokens are masked due to truncation by sequence length.\npython scripts/nlp_language_modeling/sft/data_clean.py \\\n--dataset_file=data/subset_data_output_train.jsonl \\\n--output_file=data/subset_data_output_train_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096\npython scripts/nlp_language_modeling/sft/data_clean.py \\\n--dataset_file=data/subset_data_output_val.jsonl \\\n--output_file=data/subset_data_output_val_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096", "Step 6: Train the value model on cleaned OASST data\nFor this tutorial, train the value model for 1K steps. Note that we recommend training much longer on more data to get a good value model.\npython examples/nlp/language_modeling/tuning/megatron_gpt_sft.py \\\n++trainer.limit_val_batches=10 \\\ntrainer.num_nodes=1 \\\ntrainer.devices=2 \\\ntrainer.max_epochs=null \\\ntrainer.max_steps=1000 \\\ntrainer.val_check_interval=100 \\\ntrainer.precision=bf16 \\\nmodel.megatron_amp_O2=False \\\nmodel.restore_from_path=/model/llama7b.nemo \\\nmodel.tensor_model_parallel_size=2 \\\nmodel.pipeline_model_parallel_size=1 \\\nmodel.optim.lr=5e-6 \\\nmodel.optim.name=distributed_fused_adam \\\nmodel.optim.weight_decay=0.01 \\\nmodel.answer_only_loss=True \\\nmodel.activations_checkpoint_granularity=selective \\\nmodel.activations_checkpoint_method=uniform \\\nmodel.data.chat=True \\\nmodel.data.train_ds.max_seq_length=4096 \\\nmodel.data.train_ds.micro_batch_size=1 \\\nmodel.data.train_ds.global_batch_size=1 \\\nmodel.data.train_ds.file_names=[data/subset_data_output_train_clean.jsonl] \\\nmodel.data.train_ds.concat_sampling_probabilities=[1.0] \\\nmodel.data.train_ds.num_workers=0 \\\n\u200b\u200b model.data.train_ds.hf_dataset=True \\\nmodel.data.train_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.train_ds.add_eos=False \\\nmodel.data.validation_ds.max_seq_length=4096 \\\nmodel.data.validation_ds.file_names=[data/subset_data_output_val_clean.jsonl] \\\nmodel.data.validation_ds.names=[\"oasst\"] \\\nmodel.data.validation_ds.micro_batch_size=1 \\\nmodel.data.validation_ds.global_batch_size=1 \\\nmodel.data.validation_ds.num_workers=0 \\\nmodel.data.validation_ds.metric.name=loss \\\nmodel.data.validation_ds.index_mapping_dir=/indexmap_dir \\\nmodel.data.validation_ds.hf_dataset=True \\\nmodel.data.validation_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.validation_ds.add_eos=False \\\nmodel.data.test_ds.max_seq_length=4096 \\\nmodel.data.test_ds.file_names=[data/subset_data_output_val_clean.jsonl] \\\nmodel.data.test_ds.names=[\"oasst\"] \\\nmodel.data.test_ds.micro_batch_size=1 \\\nmodel.data.test_ds.global_batch_size=1 \\\nmodel.data.test_ds.num_workers=0 \\\nmodel.data.test_ds.metric.name=loss \\\nmodel.data.test_ds.hf_dataset=True \\\nmodel.data.test_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.test_ds.add_eos=False \\\nexp_manager.explicit_log_dir=\"/home/value_model/\" \\\nexp_manager.create_checkpoint_callback=True \\\nexp_manager.checkpoint_callback_params.monitor=val_loss \\\nexp_manager.checkpoint_callback_params.mode=min", "Step 7: Generate annotations\nTo generate annotation, run the following command in the background to run an inference server:\npython examples/nlp/language_modeling/megatron_gpt_eval.py \\\ngpt_model_file=/models/<TRAINED_ATTR_PREDICTION_MODEL.nemo> \\\npipeline_model_parallel_split_rank=0 \\\nserver=True \\\ntensor_model_parallel_size=1 \\\npipeline_model_parallel_size=1 \\\ntrainer.precision=bf16 \\\ntrainer.devices=1 \\\ntrainer.num_nodes=1 \\\nweb_server=False \\\nport=1424\nNow execute:\npython scripts/nlp_language_modeling/sft/attribute_annotate.py --batch_size=1 --host=localhost --input_file_name=data/subset_data_output_v2t_train.jsonl --output_file_name=data/subset_data_v2t_train_value_output.jsonl --port_num=1424\npython scripts/nlp_language_modeling/sft/attribute_annotate.py --batch_size=1 --host=localhost --input_file_name=data/subset_data_output_v2t_val.jsonl --output_file_name=data/subset_data_v2t_val_value_output.jsonl --port_num=1424", "Step 8: Clean the value-to-text data\nRemove the record if all tokens are masked after truncation by sequence length:\npython scripts/data_clean.py \\\n--dataset_file=data/subset_data_v2t_train_value_output.jsonl \\\n--output_file=data/subset_data_v2t_train_value_output_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096\npython scripts/data_clean.py \\\n--dataset_file=data/subset_data_v2t_val_value_output.jsonl \\\n--output_file=data/subset_data_v2t_val_value_output_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096", "Step 9: Train the SteerLM model\nFor the purposes of this tutorial, the SteerLM model is trained for 1K steps. Note that we recommend training much longer and on more data to get a well-tuned model.\npython examples/nlp/language_modeling/tuning/megatron_gpt_sft.py \\\n++trainer.limit_val_batches=10 \\\ntrainer.num_nodes=1 \\\ntrainer.devices=2 \\\ntrainer.max_epochs=null \\\ntrainer.max_steps=1000 \\\ntrainer.val_check_interval=100 \\\ntrainer.precision=bf16 \\\nmodel.megatron_amp_O2=False \\\nmodel.restore_from_path=/model/llama7b.nemo \\\nmodel.tensor_model_parallel_size=2 \\\nmodel.pipeline_model_parallel_size=1 \\\nmodel.optim.lr=5e-6 \\\nmodel.optim.name=distributed_fused_adam \\\nmodel.optim.weight_decay=0.01 \\\nmodel.answer_only_loss=True \\\nmodel.activations_checkpoint_granularity=selective \\\nmodel.activations_checkpoint_method=uniform \\\nmodel.data.chat=True \\\nmodel.data.train_ds.max_seq_length=4096 \\\nmodel.data.train_ds.micro_batch_size=1 \\\nmodel.data.train_ds.global_batch_size=1 \\\nmodel.data.train_ds.file_names=[data/subset_data_v2t_train_value_output_clean.jsonl] \\\nmodel.data.train_ds.concat_sampling_probabilities=[1.0] \\\nmodel.data.train_ds.num_workers=0 \\\nmodel.data.train_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.train_ds.add_eos=False \\\nmodel.data.validation_ds.max_seq_length=4096 \\\nmodel.data.validation_ds.file_names=[data/subset_data_v2t_val_value_output_clean.jsonl] \\\nmodel.data.validation_ds.names=[\"oasst\"] \\\nmodel.data.validation_ds.micro_batch_size=1 \\\nmodel.data.validation_ds.global_batch_size=1 \\\nmodel.data.validation_ds.num_workers=0 \\\nmodel.data.validation_ds.metric.name=loss \\\nmodel.data.validation_ds.index_mapping_dir=/indexmap_dir \\\nmodel.data.validation_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.validation_ds.add_eos=False \\\nmodel.data.test_ds.max_seq_length=4096 \\\nmodel.data.test_ds.file_names=[data/subset_data_v2t_val_value_output_clean.jsonl] \\\nmodel.data.test_ds.names=[\"oasst\"] \\\nmodel.data.test_ds.micro_batch_size=1 \\\nmodel.data.test_ds.global_batch_size=1 \\\nmodel.data.test_ds.num_workers=0 \\\nmodel.data.test_ds.metric.name=loss \\\nmodel.data.test_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.test_ds.add_eos=False \\\nexp_manager.explicit_log_dir=\"/home/steerlm_model/\" \\\nexp_manager.create_checkpoint_callback=True \\\nexp_manager.checkpoint_callback_params.monitor=val_loss \\\nexp_manager.checkpoint_callback_params.mode=min", "Step 10: Inference\nTo start inference, run an inference server in the background using the following command:\npython examples/nlp/language_modeling/megatron_gpt_eval.py \\\ngpt_model_file=/models/<TRAINED_STEERLM_MODEL.nemo> \\\npipeline_model_parallel_split_rank=0 \\\nserver=True \\\ntensor_model_parallel_size=1 \\\npipeline_model_parallel_size=1 \\\ntrainer.precision=bf16 \\\ntrainer.devices=1 \\\ntrainer.num_nodes=1 \\\nweb_server=False \\\nport=1427 Next, create Python helper functions:\n```\ndef get_answer(question, max_tokens, values, eval_port='1427'):\n\n    prompt = f\"\"\"<extra_id_0>System\nA chat between a curious user and an artificial intelligence assistant. \nThe assistant gives helpful, detailed, and polite answers to the user's questions.\n\n<extra_id_1>User\n\n{question}\n\n<extra_id_1>Assistant\n\n<extra_id_2>{values}\n\n\"\"\"\n\n    prompts = [prompt]\n    data = {\n        \"sentences\": prompts,\n        \"tokens_to_generate\": max_tokens,\n        \"top_k\": 1,\n        'greedy': True,\n        'end_strings': [\"<extra_id_1>\", \"quality:\", \"quality:4\", \"quality:0\"]\n    }\n\n    url = f\"http://localhost:{eval_port}/generate\"\n    response = requests.put(url, json=data)\n    json_response = response.json()\n\n    response_sentence = json_response['sentences'][0][len(prompt):]\n\n    return response_sentence\n\n\ndef encode_labels(labels):\n    items = []\n    for key in labels:\n        value = labels[key]\n        items.append(f'{key}:{value}')\n    return ','.join(items)\n```\nNext, change the values below to steer the language model:\n```\nvalues = OrderedDict([\n    ('quality', 4),\n    ('toxicity', 0),\n    ('humor', 0),\n    ('creativity', 0),\n    ('violence', 0),\n    ('helpfulness', 4),\n    ('not_appropriate', 0),\n    ('hate_speech', 0),\n    ('sexual_content', 0),\n    ('fails_task', 0),\n    ('political_content', 0),\n    ('moral_judgement', 0),\n])\nvalues = encode_labels(values)\n```\nFinally, ask questions and generate responses:\n```\nquestion = \"\"\"Where and when did techno music originate?\"\"\"\nprint (get_answer(question, 4096, values))\n```\nSteerLM users can perform additional bootstrapping steps using the scripts and utilities mentioned in this tutorial. This step can help further improve model accuracy on different benchmarks.", "The future of AI with SteerLM\nSteerLM provides a novel technique for realizing a new generation of AI systems aligned with human preferences in a controllable manner. Its conceptual simplicity, performance gains, and customizability highlight the transformative possibilities of user-steerable AI. SteerLM is now available as open-source software, accessible through the NVIDIA/NeMo GitHub repo. You can also get information about how to experiment with a Llama 2 13B model customized using the SteerLM method.\nFor full enterprise security and support, SteerLM will be integrated into NVIDIA NeMo, a rich framework for building, customizing, and deploying large generative AI models. The SteerLM method works on all models supported on NeMo, including popular community-built pretrained LLMs such as Llama 2, Falcon LLM, and MPT. We hope our work catalyzes further research into developing models that empower users rather than constrain them. The future of AI is steerable with SteerLM.", "Acknowledgments\nWe would like to thank Xianchao Wu and Oleksii Kuchaiev for contributing to this post and to the inception of SteerLM."], "document_title": "Announcing NVIDIA SteerLM: A Simple and Practical Technique to Customize LLMs During Inference", "document_url": "https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/", "document_date": "2023-10-11T14:30:00", "document_date_modified": "2023-11-30T01:12:19", "document_full_text": "Announcing NVIDIA SteerLM: A Simple and Practical Technique to Customize LLMs During Inference\nWith the advent of large language models (LLMs) such as GPT-3, Megatron-Turing, Chinchilla, PaLM-2, Falcon, and Llama 2, remarkable progress in natural language generation has been made in recent years. However, despite their ability to produce human-like text, \u200cfoundation LLMs can fail to provide helpful and nuanced responses aligned with user preferences.\nThe current approach to improving LLMs involves supervised fine-tuning (SFT ) on human demonstrations followed by reinforcement learning from human feedback (RLHF). While RLHF can enhance performance, it has some limitations, including training complexity and a lack of user control.\nTo overcome these challenges, NVIDIA Research developed and released NVIDIA SteerLM, a new four-step technique that simplifies LLM customization while enabling dynamic steering of model outputs based on attributes you specify, as part of NVIDIA NeMo. This post provides an in-depth look at how SteerLM works, why it marks a significant advance, and how to train a SteerLM model.\nLanguage models bring promise and potential pitfalls\nBy pretraining on massive text corpora, LLMs acquire broad linguistic capabilities and world knowledge. Researchers have successfully applied LLMs to diverse natural language processing (NLP) tasks like translation, question answering, and text generation. However, these models often fail to follow user-provided instructions, and instead produce generic, repetitive, or nonsensical text. Access to human feedback is essential for customizing LLMs.\nOpportunities with existing approaches\nSFT augments model capabilities, but causes responses to become short and mechanical. RLHF further optimizes models by favoring human-preferred responses over alternatives. However, RLHF requires an extremely complex training infrastructure, hindering broad adoption.\nIntroducing SteerLM\nSteerLM leverages a supervised fine-tuning method that empowers you to control responses during inference. It overcomes the limitations of prior alignment techniques, and consists of four key steps:\nTrain an attribute prediction model on human-annotated datasets to evaluate response quality on any number of attributes like helpfulness, humor, and creativity.\nAnnotate diverse datasets by predicting their attribute scores, using the model from Step 1 to enrich the diversity of data available to the model.\nPerform attribute-conditioned SFT by training the LLM to generate responses conditioned on specified combinations of attributes, like user-perceived quality and helpfulness.\nBootstrap training through model sampling by generating diverse responses conditioned on maximum quality (Figure 1, 4a), then fine-tuning on them to further improve alignment (Figure 1, 4b).\nDiagram of the four steps of Steer LM. Step 1. The base language model is trained to assess the quality of responses by predicting attribute values. Step 2. The attribute prediction model is used to annotate response quality across diverse datasets. Step 3. Given a prompt and desired attribute values, a new base model is fine-tuned to generate responses that align with the specified attributes. Step 4. Multiple responses are sampled from the fine-tuned model in Step 3, specifying maximum quality. The sampled responses are evaluated by the trained attribute prediction model, leading to another round of fine-tuning.\nFigure 1. The four steps of SteerLM By relying solely on the standard language modeling objective, SteerLM simplifies alignment compared to RLHF. It supports user-steerable AI by enabling you to adjust attributes at inference time. This enables the developer to define preferences relevant to the application, unlike other techniques that require using predetermined preferences.\nUnlocking customizable AI with user steering\nA key innovation of SteerLM is enabling the user to specify desired attributes (humor level and toxicity tolerance, for example) at inference time when querying models. You can move from running one customization with SteerLM, to serving many use cases at inference time.\nSteerLM empowers a range of applications, including:\nGaming: Vary non-player character dialogue for game scenarios. To learn more, see NVIDIA ACE Adds Emotion to AI-Powered NPCs with NeMo SteerLM.\nEducation: Maintain a formal and helpful persona for student queries.\nEnterprise: Serve multiple teams in an organization with personalized capabilities.\nAccessibility: Curb undesired model biases by controlling sensitive attributes.\nSuch flexibility promises to unlock a new generation of bespoke AI systems tailored to individual needs.\nDemocratizing state-of-the-art customization through simplified training\nCompared to the specialized infrastructure required for other advanced customization techniques, the straightforward training scheme of SteerLM makes state-of-the-art customization capabilities more accessible to developers. Its performance clearly demonstrates that techniques like reinforcement learning are not required for robust instruction tuning.\nLeveraging standard techniques such as SFT simplifies the complexity, requiring minimal changes to infrastructure and code. Reasonable results can be achieved with limited hyperparameter optimization.\nOverall, this leads to a simple and practical approach to obtaining highly accurate customized LLMs. In our experiments, SteerLM 43B achieved state-of-the-art performance on the Vicuna benchmark, outperforming existing RLHF models like LLaMA 30B RLHF. Specifically, SteerLM 43B obtained an average score of 655.75 on the Vicuna automatic evaluation, compared to scores of 646.25 for Guanaco 65B and 612.75 for LLaMA 30B RLHF.\nThese results highlight that the straightforward training process of SteerLM can lead to customized LLMs with accuracy on par with more complex RLHF techniques. Through simplified training, SteerLM makes achieving such high levels of accuracy more accessible, enabling easier democratization of customization among developers.\nFor more details, see our paper, SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF. You can also get information about how to experiment with a Llama 2 13B model customized using the SteerLM method.\nHow to train a SteerLM model\nThis section is a step-by-step tutorial that walks you through how to run a full SteerLM pipeline on OASST data with a 2B NeMo LLM model. It includes the following:\nData cleaning and preprocessing\nTraining the attribute prediction (value model)\nTraining the attribute-conditioned SFT (SteerLM model)\nInference on the SteerLM model with different attribute values\nStep 1: Install requirements\nStart by installing the necessary Python libraries:\npip install fire langchain==0.0.133 Get access to NeMo.\nStep 2: Download and subset data\nThis tutorial uses a small subset of the OASST dataset. OASST contains open-domain conversations with human annotations for 13 different quality attributes.\nFirst download and subset it:\nmkdir -p data\ncd data\nwget https://huggingface.co/datasets/OpenAssistant/oasst1/resolve/main/2023-04-12_oasst_all.trees.jsonl.gz\ngunzip -f 2023-04-12_oasst_all.trees.jsonl.gz\nmv 2023-04-12_oasst_all.trees.jsonl data.jsonl\nhead -5000 data.jsonl > subset_data.jsonl\ncd -\nStep 3: Download Llama 2 LLM model and tokenizer and convert\nDownload the Llama 2 7B LLM model and tokenizer into the models folder.\nThen convert the Llama 2 LLM into .nemo format:\npython NeMo/scripts/nlp_language_modeling/convert_hf_llama_to_nemo.py --in-file /path/to/llama --out-file /output_path/llama7b.nemo Untar the .nemo file to obtain the tokenizer in NeMo format:\ntar <path-to-model>/llama7b.nemo\nmv ba4632640484461f8ae9d61f6dfe0d0b_tokenizer.model tokenizer.model\nThe prefix for the tokenizer would be different when extracted. Ensure that the correct tokenizer file is used when running the preceding command.\nStep 4: Preprocess OASST data\nPreprocess the data using the NeMo preprocessing scripts. Then create separate text-to-value and value-to-text versions:\npython scripts/nlp_language_modeling/sft/preprocessing.py \\\n--input_file=data/subset_data.jsonl \\\n--output_file_prefix=data/subset_data_output \\\n--mask_role=User \\\n--type=TEXT_TO_VALUE \\\n--split_ratio=0.95 \\\n--seed=10\npython scripts/nlp_language_modeling/sft/preprocessing.py \\\n--input_file=data/subset_data.jsonl \\\n--output_file_prefix=data/subset_data_output_v2t \\\n--mask_role=User \\\n--type=VALUE_TO_TEXT \\\n--split_ratio=0.95 \\\n--seed=10\nStep 5: Clean text-to-value data\nRunning the following script will remove the records if all the tokens are masked due to truncation by sequence length.\npython scripts/nlp_language_modeling/sft/data_clean.py \\\n--dataset_file=data/subset_data_output_train.jsonl \\\n--output_file=data/subset_data_output_train_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096\npython scripts/nlp_language_modeling/sft/data_clean.py \\\n--dataset_file=data/subset_data_output_val.jsonl \\\n--output_file=data/subset_data_output_val_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096\nStep 6: Train the value model on cleaned OASST data\nFor this tutorial, train the value model for 1K steps. Note that we recommend training much longer on more data to get a good value model.\npython examples/nlp/language_modeling/tuning/megatron_gpt_sft.py \\\n++trainer.limit_val_batches=10 \\\ntrainer.num_nodes=1 \\\ntrainer.devices=2 \\\ntrainer.max_epochs=null \\\ntrainer.max_steps=1000 \\\ntrainer.val_check_interval=100 \\\ntrainer.precision=bf16 \\\nmodel.megatron_amp_O2=False \\\nmodel.restore_from_path=/model/llama7b.nemo \\\nmodel.tensor_model_parallel_size=2 \\\nmodel.pipeline_model_parallel_size=1 \\\nmodel.optim.lr=5e-6 \\\nmodel.optim.name=distributed_fused_adam \\\nmodel.optim.weight_decay=0.01 \\\nmodel.answer_only_loss=True \\\nmodel.activations_checkpoint_granularity=selective \\\nmodel.activations_checkpoint_method=uniform \\\nmodel.data.chat=True \\\nmodel.data.train_ds.max_seq_length=4096 \\\nmodel.data.train_ds.micro_batch_size=1 \\\nmodel.data.train_ds.global_batch_size=1 \\\nmodel.data.train_ds.file_names=[data/subset_data_output_train_clean.jsonl] \\\nmodel.data.train_ds.concat_sampling_probabilities=[1.0] \\\nmodel.data.train_ds.num_workers=0 \\\n\u200b\u200b model.data.train_ds.hf_dataset=True \\\nmodel.data.train_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.train_ds.add_eos=False \\\nmodel.data.validation_ds.max_seq_length=4096 \\\nmodel.data.validation_ds.file_names=[data/subset_data_output_val_clean.jsonl] \\\nmodel.data.validation_ds.names=[\"oasst\"] \\\nmodel.data.validation_ds.micro_batch_size=1 \\\nmodel.data.validation_ds.global_batch_size=1 \\\nmodel.data.validation_ds.num_workers=0 \\\nmodel.data.validation_ds.metric.name=loss \\\nmodel.data.validation_ds.index_mapping_dir=/indexmap_dir \\\nmodel.data.validation_ds.hf_dataset=True \\\nmodel.data.validation_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.validation_ds.add_eos=False \\\nmodel.data.test_ds.max_seq_length=4096 \\\nmodel.data.test_ds.file_names=[data/subset_data_output_val_clean.jsonl] \\\nmodel.data.test_ds.names=[\"oasst\"] \\\nmodel.data.test_ds.micro_batch_size=1 \\\nmodel.data.test_ds.global_batch_size=1 \\\nmodel.data.test_ds.num_workers=0 \\\nmodel.data.test_ds.metric.name=loss \\\nmodel.data.test_ds.hf_dataset=True \\\nmodel.data.test_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.test_ds.add_eos=False \\\nexp_manager.explicit_log_dir=\"/home/value_model/\" \\\nexp_manager.create_checkpoint_callback=True \\\nexp_manager.checkpoint_callback_params.monitor=val_loss \\\nexp_manager.checkpoint_callback_params.mode=min\nStep 7: Generate annotations\nTo generate annotation, run the following command in the background to run an inference server:\npython examples/nlp/language_modeling/megatron_gpt_eval.py \\\ngpt_model_file=/models/<TRAINED_ATTR_PREDICTION_MODEL.nemo> \\\npipeline_model_parallel_split_rank=0 \\\nserver=True \\\ntensor_model_parallel_size=1 \\\npipeline_model_parallel_size=1 \\\ntrainer.precision=bf16 \\\ntrainer.devices=1 \\\ntrainer.num_nodes=1 \\\nweb_server=False \\\nport=1424\nNow execute:\npython scripts/nlp_language_modeling/sft/attribute_annotate.py --batch_size=1 --host=localhost --input_file_name=data/subset_data_output_v2t_train.jsonl --output_file_name=data/subset_data_v2t_train_value_output.jsonl --port_num=1424\npython scripts/nlp_language_modeling/sft/attribute_annotate.py --batch_size=1 --host=localhost --input_file_name=data/subset_data_output_v2t_val.jsonl --output_file_name=data/subset_data_v2t_val_value_output.jsonl --port_num=1424\nStep 8: Clean the value-to-text data\nRemove the record if all tokens are masked after truncation by sequence length:\npython scripts/data_clean.py \\\n--dataset_file=data/subset_data_v2t_train_value_output.jsonl \\\n--output_file=data/subset_data_v2t_train_value_output_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096\npython scripts/data_clean.py \\\n--dataset_file=data/subset_data_v2t_val_value_output.jsonl \\\n--output_file=data/subset_data_v2t_val_value_output_clean.jsonl \\\n--library sentencepiece \\\n--model_file tokenizer.model \\\n--seq_len 4096\nStep 9: Train the SteerLM model\nFor the purposes of this tutorial, the SteerLM model is trained for 1K steps. Note that we recommend training much longer and on more data to get a well-tuned model.\npython examples/nlp/language_modeling/tuning/megatron_gpt_sft.py \\\n++trainer.limit_val_batches=10 \\\ntrainer.num_nodes=1 \\\ntrainer.devices=2 \\\ntrainer.max_epochs=null \\\ntrainer.max_steps=1000 \\\ntrainer.val_check_interval=100 \\\ntrainer.precision=bf16 \\\nmodel.megatron_amp_O2=False \\\nmodel.restore_from_path=/model/llama7b.nemo \\\nmodel.tensor_model_parallel_size=2 \\\nmodel.pipeline_model_parallel_size=1 \\\nmodel.optim.lr=5e-6 \\\nmodel.optim.name=distributed_fused_adam \\\nmodel.optim.weight_decay=0.01 \\\nmodel.answer_only_loss=True \\\nmodel.activations_checkpoint_granularity=selective \\\nmodel.activations_checkpoint_method=uniform \\\nmodel.data.chat=True \\\nmodel.data.train_ds.max_seq_length=4096 \\\nmodel.data.train_ds.micro_batch_size=1 \\\nmodel.data.train_ds.global_batch_size=1 \\\nmodel.data.train_ds.file_names=[data/subset_data_v2t_train_value_output_clean.jsonl] \\\nmodel.data.train_ds.concat_sampling_probabilities=[1.0] \\\nmodel.data.train_ds.num_workers=0 \\\nmodel.data.train_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.train_ds.add_eos=False \\\nmodel.data.validation_ds.max_seq_length=4096 \\\nmodel.data.validation_ds.file_names=[data/subset_data_v2t_val_value_output_clean.jsonl] \\\nmodel.data.validation_ds.names=[\"oasst\"] \\\nmodel.data.validation_ds.micro_batch_size=1 \\\nmodel.data.validation_ds.global_batch_size=1 \\\nmodel.data.validation_ds.num_workers=0 \\\nmodel.data.validation_ds.metric.name=loss \\\nmodel.data.validation_ds.index_mapping_dir=/indexmap_dir \\\nmodel.data.validation_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.validation_ds.add_eos=False \\\nmodel.data.test_ds.max_seq_length=4096 \\\nmodel.data.test_ds.file_names=[data/subset_data_v2t_val_value_output_clean.jsonl] \\\nmodel.data.test_ds.names=[\"oasst\"] \\\nmodel.data.test_ds.micro_batch_size=1 \\\nmodel.data.test_ds.global_batch_size=1 \\\nmodel.data.test_ds.num_workers=0 \\\nmodel.data.test_ds.metric.name=loss \\\nmodel.data.test_ds.prompt_template='\\{input\\}\\{output\\}' \\\nmodel.data.test_ds.add_eos=False \\\nexp_manager.explicit_log_dir=\"/home/steerlm_model/\" \\\nexp_manager.create_checkpoint_callback=True \\\nexp_manager.checkpoint_callback_params.monitor=val_loss \\\nexp_manager.checkpoint_callback_params.mode=min\nStep 10: Inference\nTo start inference, run an inference server in the background using the following command:\npython examples/nlp/language_modeling/megatron_gpt_eval.py \\\ngpt_model_file=/models/<TRAINED_STEERLM_MODEL.nemo> \\\npipeline_model_parallel_split_rank=0 \\\nserver=True \\\ntensor_model_parallel_size=1 \\\npipeline_model_parallel_size=1 \\\ntrainer.precision=bf16 \\\ntrainer.devices=1 \\\ntrainer.num_nodes=1 \\\nweb_server=False \\\nport=1427 Next, create Python helper functions:\n```\ndef get_answer(question, max_tokens, values, eval_port='1427'):\n\n    prompt = f\"\"\"<extra_id_0>System\nA chat between a curious user and an artificial intelligence assistant. \nThe assistant gives helpful, detailed, and polite answers to the user's questions.\n\n<extra_id_1>User\n\n{question}\n\n<extra_id_1>Assistant\n\n<extra_id_2>{values}\n\n\"\"\"\n\n    prompts = [prompt]\n    data = {\n        \"sentences\": prompts,\n        \"tokens_to_generate\": max_tokens,\n        \"top_k\": 1,\n        'greedy': True,\n        'end_strings': [\"<extra_id_1>\", \"quality:\", \"quality:4\", \"quality:0\"]\n    }\n\n    url = f\"http://localhost:{eval_port}/generate\"\n    response = requests.put(url, json=data)\n    json_response = response.json()\n\n    response_sentence = json_response['sentences'][0][len(prompt):]\n\n    return response_sentence\n\n\ndef encode_labels(labels):\n    items = []\n    for key in labels:\n        value = labels[key]\n        items.append(f'{key}:{value}')\n    return ','.join(items)\n```\nNext, change the values below to steer the language model:\n```\nvalues = OrderedDict([\n    ('quality', 4),\n    ('toxicity', 0),\n    ('humor', 0),\n    ('creativity', 0),\n    ('violence', 0),\n    ('helpfulness', 4),\n    ('not_appropriate', 0),\n    ('hate_speech', 0),\n    ('sexual_content', 0),\n    ('fails_task', 0),\n    ('political_content', 0),\n    ('moral_judgement', 0),\n])\nvalues = encode_labels(values)\n```\nFinally, ask questions and generate responses:\n```\nquestion = \"\"\"Where and when did techno music originate?\"\"\"\nprint (get_answer(question, 4096, values))\n```\nSteerLM users can perform additional bootstrapping steps using the scripts and utilities mentioned in this tutorial. This step can help further improve model accuracy on different benchmarks.\nThe future of AI with SteerLM\nSteerLM provides a novel technique for realizing a new generation of AI systems aligned with human preferences in a controllable manner. Its conceptual simplicity, performance gains, and customizability highlight the transformative possibilities of user-steerable AI. SteerLM is now available as open-source software, accessible through the NVIDIA/NeMo GitHub repo. You can also get information about how to experiment with a Llama 2 13B model customized using the SteerLM method.\nFor full enterprise security and support, SteerLM will be integrated into NVIDIA NeMo, a rich framework for building, customizing, and deploying large generative AI models. The SteerLM method works on all models supported on NeMo, including popular community-built pretrained LLMs such as Llama 2, Falcon LLM, and MPT. We hope our work catalyzes further research into developing models that empower users rather than constrain them. The future of AI is steerable with SteerLM.\nAcknowledgments\nWe would like to thank Xianchao Wu and Oleksii Kuchaiev for contributing to this post and to the inception of SteerLM."}], "https://developer.nvidia.com/blog/analyzing-the-security-of-machine-learning-research-code/": [{"text": "The NVIDIA AI Red Team analyzed the security of machine learning research code using the Meta Kaggle for Code dataset, covering 3 years and almost 140GB of source code. They found that despite available security documentation and tools, machine learning researchers still use insecure coding practices, such as plaintext credentials and insecure deserialization. Additionally, there was a lack of adoption of adversarial training or assessment in ML research pipelines. The analysis also identified typos in code and the continued use of the insecure pickle serialization format. Recommendations included developing alternatives to entering long-lived credentials in source code, using automation to catch mistakes, establishing guidelines to limit deserialization exploitation risks, and mitigating potential adversarial ML attacks. The study highlighted the importance of maintaining good security hygiene in research code to mitigate risks and ensure the integrity of experiments. The team utilized open-source security tools such as TruffleHog and Semgrep for the analysis and provided recommendations for improving secure coding practices in machine learning research.", "text_components": ["Analyzing the Security of Machine Learning Research Code\nThe NVIDIA AI Red Team is focused on scaling secure development practices across the data, science, and AI ecosystems. We participate in open-source security initiatives, release tools, present at industry conferences, host educational competitions, and provide innovative training.\nCovering 3 years and totaling almost 140GB of source code, the recently released Meta Kaggle for Code dataset is a great opportunity to analyze the security of machine learning (ML) research and experimentation competition code at scale. Our goal was to use this data to answer the following questions:\nWhat is the state of security hygiene in ML research code?\nHow can security organizations improve the secure coding practices of ML researchers?\nOur analysis shows that ML researchers continue to use insecure coding practices despite public documentation about security risks and relatively frictionless and advanced security tooling. We theorize that researchers prioritize rapid experimentation and do not think of themselves or their projects as targets because they are usually not running production services.\nAdditionally, the Kaggle environment may exacerbate security negligence based on \u200cisolation from researchers\u2019 \u201creal infrastructure.\u201d However, researchers must acknowledge their position in the software supply chain and should be aware of risks to their research and systems from insecure coding practices.\nAlthough originally proposed in the 2015 research paper Explaining and Harnessing Adversarial Examples, we also found little evidence of adoption of adversarial training or assessment in ML research pipelines. This may be partly due to the structure of Kaggle competitions and scoring metrics, but it is consistent with our other research and observations. However, with recent advances in multimodal models and demonstrated image-based prompt injection attacks, researchers should prioritize testing their models under adversarial conditions and perturbations.", "Observations\nThe most significant observations were the use of plaintext credentials, insecure deserialization (primarily pickle), a lack of adversarial robustness and evaluation techniques, and typos. Focus defensive controls and education around these topics.", "Plaintext credentials\nResearchers still use long-lived, plaintext credentials and commit them to source control. We found over 140 unique active, plaintext credentials to third-party services like OpenAI, AWS, GitHub, and others. For some credential types, the credentials can be associated with other user data such as emails, exposing the user to phishing and other attacks.\nIn keeping with standard industry practices around Coordinated Vulnerability Disclosure, we reported this credential exposure to Kaggle on August 24, 2023. They have taken steps to mitigate this risk.", "Insecure deserialization\nThe most prevalent risk is insecure deserialization. Furthermore, many notebooks included path traversal iteration that increased the likelihood of a malicious payload being executed.\nFor example, a user iterated through ```feature_dir``` and ```fileName``` to repeatedly execute the following command: ```feature = np.load(feature_dir + fileName + '.npy', allow_pickle=True)```.\nThe majority of the XML Injection findings were generated from pandas ```read html``` calls that can be vulnerable to XML external entity attacks and most of the mishandled sensitive information findings were related to ```http``` usage instead of ```https```.\nA horizontal bar graph with a logarithmic scale showing findings by category.\nFigure 1. Aggregated findings by category", "Pickle is still the standard\nThe de facto serialization format for researchers is still the pickle module. It was among the top 50 most imported modules with almost 5,000 imports. ONNX, a more secure serialization format for ML models, was directly imported just nine times. The pickle module may also have been used indirectly through built-in serialization formats in other libraries.\nFor instance, many NumPy or PyTorch serialization calls rely on built-in save methods that are still pickle-based. For example, we identified over 45,000 examples where pickle files were loaded as part of pandas, joblib, and NumPy deserialization.", "Lack of adversarial retraining or testing\nThere is no evidence of adversarial retraining or testing. Common adversarial retraining and testing libraries like Adversarial RobustnessToolbox (ART), Counterfit, TextAttack, and CleverHans do not appear in any of the imports. For common explainability tools, there were no imports for Alibi, but Fairlearn was imported 34 times.\nOther training techniques to protect privacy, such as federated learning and differential privacy, were almost entirely absent too (with the exception of PyDP, which was imported once).", "Typos\nWe observed typos such as imports for ```panda``` and ```mathplotlib``` (which should have been ```pandas``` and ```matplotlib```, respectively), among others. Typosquatting on PyPI is a notorious mechanism for spreading malware. For more details, see A PyPI Typosquatting Campaign Post-Mortem.", "A few positives\nWe did not identify any instances where researchers loaded serialized objects or models from sources that would be easy to hijack or modify in transit. Additionally, no URLs found in the dataset were associated with malware or social engineering according to the Google Safe Browsing Lookup API.", "Recommendations\nParticularly with research, security controls must be layered and calibrated to minimally impact velocity. Understand what controls are necessary to protect the researcher, research, and network, and what additional controls may be necessary to transition successful research to production. Our recommendations, listed below, are based on the preceding observations.", "Develop alternatives to entering long-lived credentials in source code\nAlternatives include using a secrets manager, environment variables, input prompts, and credential vending services that provide short-lived tokens. Multifactor authentication (MFA) also reduces the impact of credentials leaked in source code.\nExperience shows repeatedly that if developers start using credentials in source code, the chance of a leak significantly increases. Leaks can happen in datasets like this, accidental commits into version control, or exposure through history and logging as illustrated by our demonstration at JupyterCon 2023.", "Use automation to catch mistakes before they are committed to remote resources\nVersion control systems like GitHub and continuous deployment systems like Jenkins have often been \u201ccrown jewels\u201d for attackers. Use precommit hooks to run security automation and prevent local mistakes from being broadcast to these targets.", "Establish guidelines, standards, and toolings to limit deserialization exploitation risks\nThe security risks of pickle are well documented, but still do require a malicious actor to have sufficient access and privileges to execute an attack. Our team recommends that organizations move towards more robust serialization formats like ONNX and protocol buffers. If your tool or organization must support pickle, implement integrity verification steps that can be validated without deserialization.", "Identify and mitigate potential adversarial ML attacks\nVulnerabilities and exploits during research and development can impact the security and effectiveness of the resulting service. Understand the various attacks against ML systems to build appropriate threat models and defensive controls. To learn more, see NVIDIA AI Red Team: An Introduction.\nFor instance, including adversarial retraining during training time can ensure your classifier is more robust to adversarial evasion attacks. Consider adding an adversarial robustness metric to your evaluation framework when comparing model performance. If you are sponsoring a Kaggle competition, consider adding adversarial examples to the evaluation dataset so you are rewarding the most robust solution.", "Consider the lifespan and isolation of your development environments\nAll analyzed user code ran on the Kaggle platform\u2019s ephemeral environments, isolated from developer host machines. However, your organization may not have the same level of tenant isolation. It is important to not unnecessarily hinder the velocity of researchers, but consider the potential impact and blast-radius of a simple mistake (such as a misspelled import statement) and work to ensure domain resource isolation and network segmentation where possible.", "Use allow/block lists and internal artifact repositories for artifacts like imports and datasets\nAcknowledging the potential impact to research velocity, consider maintaining internal repositories of \u201cknown good\u201d libraries and datasets or implement an import hooking scheme to reduce the risk of malicious package installs and imports. Similar hygiene for datasets improves security, reproducibility, and auditability.", "Methodology\nThe dataset contains about 140GB of R, Python, and Jupyter Notebook source code publicly hosted on Kaggle. Kaggle allows users to save versions, so many of these artifacts are simply updates and changes to other files. Our analysis is limited to the Python files and Jupyter Notebooks\u2014around 3.5 million files that were executed on Kaggle from April 2020 to August 2023.\nSome analysis was manual, but we also relied heavily on two existing open-source security tools, TruffleHog to identify credentials and Semgrep to perform static analysis. Use these tools to repeat our analysis and consider them for inclusion in your kit of security tools.\nTo identify and validate credentials, TruffleHog can be run in a Docker container against source code repositories or local files. For this analysis, we ran TruffleHog against a local download with ```docker run --rm -it -v \"kaggle:/pwd\" trufflesecurity/trufflehog:latest filesystem /pwd --json --only-verified > trufflehog_findings.json```.\nTruffleHog also supports precommit hooks to help ensure credentials are not committed to remote repositories and CI/CD integration to continually monitor for leakage. TruffleHog was able to run against the Kaggle dataset without modification and we deduplicated findings based on unique secret values.\nSemgrep is a static code analyzer that uses rules to identify potential weaknesses in the target source code. Since Semgrep does not natively support Jupyter notebooks, we used ```nbconvert``` to convert them to Python files before Semgrep processing. We used 162 rules from the default Python rules and rules maintained by Trail of Bits that are more focused on ML applications.\nAfter installing Semgrep, run these two rulesets against your local Kaggle download with ```semgrep --config \"p/trailofbits\" --config \"p/python\" --json kaggle/ -o semgrep_findings.json```. During analysis, we filtered out the Trail of Bits automatic memory pinning rule because we could not find a direct path or evidence of previous exploitation.\nThe NVIDIA AI Red Team wrapped these tools in a meta-tool called lintML. To reproduce our results, try it with ```lintML \u2013semgrep-options \u201c--config \u2018p/python\u2019 \u2013config \u2018p/trailofbits\u2019\u201d <directory>```.", "Limitations\nWhile we are proud of the volume of this analysis, it is still \u201csingle source\u201d in that all samples were collected through Kaggle. It is likely that, while many findings may be the same, the underlying distribution of security observations from other data sources would be different.\nFor example, a similar analysis performed on GitHub artifacts may skew towards \u201cmore secure\u201d as those repositories are more likely to contain productionized code.\nFurthermore, Kaggle competitions reward rapid iteration and accuracy, which potentially lead to different library imports, techniques, and security considerations that productionized research. For instance, Kaggle competitions usually provide the necessary data. In reality, sourcing, cleaning, and labeling data are often significant design decisions and sources of potential vulnerabilities.\nThis analysis was simultaneously enabled and limited by the tools we used. If a credential or validator did not exist in TruffleHog, there is no associated finding here. Likewise, Semgrep analysis was limited by the rulesets we chose. Only a subset of these findings are likely exploitable, but the quantity may be correlated with overall project risk.\nFurther, the finding quantity analysis may have been biased by the distribution of rules (more deserialization rules yielding more findings). Security researchers should continue contributing to established tools for findings related to machine learning security (as the NVIDIA AI Red Team has done for both TruffleHog and Semgrep rules). The NVIDIA AI Red Team is particularly interested in applications of data flow and taint analysis to machine learning applications.", "Conclusion\nKaggle is a place for experimentation, research, and competition. It rewards rapid experiment iteration and performance, so these code artifacts are not representative of production services.\nOr are they? Code is often reused, habits are formed during research, and defaults are sticky. In a similar analysis of over 300 highly ranked machine learning repositories on GitHub, we still found hardcoded credentials to third-party services and the full range of findings presented here. Increasing security awareness and informative and preventative controls during research helps ensure secure products and increases the professionalism and security posture of your enterprise.\nSecurity professionals should use this analysis as a foundation for analyzing research and development practices in their organizations. Most of these findings represent baseline security controls. If you start to find them in your organization\u2019s research code, they are signals to engage more thoroughly with research and development teams.\nUse similar techniques to evaluate artifacts across your machine learning development cycle to ensure relaxed research practices are not propagating risk into production products. Identify opportunities to provide low-friction tooling early, not just in production delivery pipelines. Leverage proactive adversarial assessments and exercises to increase education, awareness, and reasonable security controls.\nResearchers should focus on establishing and maintaining good security hygiene as part of scientific integrity. Security risks should be viewed as unwanted variables that should be mitigated to ensure the veracity of the experiments. Think about the provenance of data and code that you are pulling into your organization. Engage with your security teams for guidance on best-practices and environment hardening.\nJust as you would when rigorously testing a hypothesis, be critical when testing your project and identify opportunities where accuracy may not be the sole metric you want to optimize and consider including robustness, explainability, and fairness tests. Even if you aren\u2019t writing a production service, you may still be\u200c exposing yourself, your research, and your organization to potential risk.\nUse our Security Practices notebook to begin analyzing this data yourself, or download a local copy of the Meta Kaggle Code to evaluate with TruffleHog and Semgrep. Experiment with lintML to identify risks in your ML training code.\nTo learn more about ML security, check out Black Hat Machine Learning at Black Hat Europe 2023.", "Acknowledgments\nWe would like to thank Kaggle for making this dataset available. This kind of data can help elevate security awareness and baseline the industry. The NVIDIA AI Red Team is constantly trying to meet ML practitioners where they are, and Kaggle has been a great partner and enabler for that mission. For more details, see Improving Machine Learning Security Skills at a DEF CON Competition. We would also like to thank all of the Kaggle competitors for contributing code to the dataset.\nAdditionally, we would like to thank TruffleHog, Semgrep, and Trail of Bits for open sourcing security tools that enabled this research and Jupyter, pandas, NumPy, and Matplotlib for high-quality data analysis and visualization tools."], "document_title": "Analyzing the Security of Machine Learning Research Code", "document_url": "https://developer.nvidia.com/blog/analyzing-the-security-of-machine-learning-research-code/", "document_date": "2023-10-04T18:00:00", "document_date_modified": "2023-11-02T18:14:45", "document_full_text": "Analyzing the Security of Machine Learning Research Code\nThe NVIDIA AI Red Team is focused on scaling secure development practices across the data, science, and AI ecosystems. We participate in open-source security initiatives, release tools, present at industry conferences, host educational competitions, and provide innovative training.\nCovering 3 years and totaling almost 140GB of source code, the recently released Meta Kaggle for Code dataset is a great opportunity to analyze the security of machine learning (ML) research and experimentation competition code at scale. Our goal was to use this data to answer the following questions:\nWhat is the state of security hygiene in ML research code?\nHow can security organizations improve the secure coding practices of ML researchers?\nOur analysis shows that ML researchers continue to use insecure coding practices despite public documentation about security risks and relatively frictionless and advanced security tooling. We theorize that researchers prioritize rapid experimentation and do not think of themselves or their projects as targets because they are usually not running production services.\nAdditionally, the Kaggle environment may exacerbate security negligence based on \u200cisolation from researchers\u2019 \u201creal infrastructure.\u201d However, researchers must acknowledge their position in the software supply chain and should be aware of risks to their research and systems from insecure coding practices.\nAlthough originally proposed in the 2015 research paper Explaining and Harnessing Adversarial Examples, we also found little evidence of adoption of adversarial training or assessment in ML research pipelines. This may be partly due to the structure of Kaggle competitions and scoring metrics, but it is consistent with our other research and observations. However, with recent advances in multimodal models and demonstrated image-based prompt injection attacks, researchers should prioritize testing their models under adversarial conditions and perturbations.\nObservations\nThe most significant observations were the use of plaintext credentials, insecure deserialization (primarily pickle), a lack of adversarial robustness and evaluation techniques, and typos. Focus defensive controls and education around these topics.\nPlaintext credentials\nResearchers still use long-lived, plaintext credentials and commit them to source control. We found over 140 unique active, plaintext credentials to third-party services like OpenAI, AWS, GitHub, and others. For some credential types, the credentials can be associated with other user data such as emails, exposing the user to phishing and other attacks.\nIn keeping with standard industry practices around Coordinated Vulnerability Disclosure, we reported this credential exposure to Kaggle on August 24, 2023. They have taken steps to mitigate this risk.\nInsecure deserialization\nThe most prevalent risk is insecure deserialization. Furthermore, many notebooks included path traversal iteration that increased the likelihood of a malicious payload being executed.\nFor example, a user iterated through ```feature_dir``` and ```fileName``` to repeatedly execute the following command: ```feature = np.load(feature_dir + fileName + '.npy', allow_pickle=True)```.\nThe majority of the XML Injection findings were generated from pandas ```read html``` calls that can be vulnerable to XML external entity attacks and most of the mishandled sensitive information findings were related to ```http``` usage instead of ```https```.\nA horizontal bar graph with a logarithmic scale showing findings by category.\nFigure 1. Aggregated findings by category\nPickle is still the standard\nThe de facto serialization format for researchers is still the pickle module. It was among the top 50 most imported modules with almost 5,000 imports. ONNX, a more secure serialization format for ML models, was directly imported just nine times. The pickle module may also have been used indirectly through built-in serialization formats in other libraries.\nFor instance, many NumPy or PyTorch serialization calls rely on built-in save methods that are still pickle-based. For example, we identified over 45,000 examples where pickle files were loaded as part of pandas, joblib, and NumPy deserialization.\nLack of adversarial retraining or testing\nThere is no evidence of adversarial retraining or testing. Common adversarial retraining and testing libraries like Adversarial RobustnessToolbox (ART), Counterfit, TextAttack, and CleverHans do not appear in any of the imports. For common explainability tools, there were no imports for Alibi, but Fairlearn was imported 34 times.\nOther training techniques to protect privacy, such as federated learning and differential privacy, were almost entirely absent too (with the exception of PyDP, which was imported once).\nTypos\nWe observed typos such as imports for ```panda``` and ```mathplotlib``` (which should have been ```pandas``` and ```matplotlib```, respectively), among others. Typosquatting on PyPI is a notorious mechanism for spreading malware. For more details, see A PyPI Typosquatting Campaign Post-Mortem.\nA few positives\nWe did not identify any instances where researchers loaded serialized objects or models from sources that would be easy to hijack or modify in transit. Additionally, no URLs found in the dataset were associated with malware or social engineering according to the Google Safe Browsing Lookup API.\nRecommendations\nParticularly with research, security controls must be layered and calibrated to minimally impact velocity. Understand what controls are necessary to protect the researcher, research, and network, and what additional controls may be necessary to transition successful research to production. Our recommendations, listed below, are based on the preceding observations.\nDevelop alternatives to entering long-lived credentials in source code\nAlternatives include using a secrets manager, environment variables, input prompts, and credential vending services that provide short-lived tokens. Multifactor authentication (MFA) also reduces the impact of credentials leaked in source code.\nExperience shows repeatedly that if developers start using credentials in source code, the chance of a leak significantly increases. Leaks can happen in datasets like this, accidental commits into version control, or exposure through history and logging as illustrated by our demonstration at JupyterCon 2023.\nUse automation to catch mistakes before they are committed to remote resources\nVersion control systems like GitHub and continuous deployment systems like Jenkins have often been \u201ccrown jewels\u201d for attackers. Use precommit hooks to run security automation and prevent local mistakes from being broadcast to these targets.\nEstablish guidelines, standards, and toolings to limit deserialization exploitation risks\nThe security risks of pickle are well documented, but still do require a malicious actor to have sufficient access and privileges to execute an attack. Our team recommends that organizations move towards more robust serialization formats like ONNX and protocol buffers. If your tool or organization must support pickle, implement integrity verification steps that can be validated without deserialization.\nIdentify and mitigate potential adversarial ML attacks\nVulnerabilities and exploits during research and development can impact the security and effectiveness of the resulting service. Understand the various attacks against ML systems to build appropriate threat models and defensive controls. To learn more, see NVIDIA AI Red Team: An Introduction.\nFor instance, including adversarial retraining during training time can ensure your classifier is more robust to adversarial evasion attacks. Consider adding an adversarial robustness metric to your evaluation framework when comparing model performance. If you are sponsoring a Kaggle competition, consider adding adversarial examples to the evaluation dataset so you are rewarding the most robust solution.\nConsider the lifespan and isolation of your development environments\nAll analyzed user code ran on the Kaggle platform\u2019s ephemeral environments, isolated from developer host machines. However, your organization may not have the same level of tenant isolation. It is important to not unnecessarily hinder the velocity of researchers, but consider the potential impact and blast-radius of a simple mistake (such as a misspelled import statement) and work to ensure domain resource isolation and network segmentation where possible.\nUse allow/block lists and internal artifact repositories for artifacts like imports and datasets\nAcknowledging the potential impact to research velocity, consider maintaining internal repositories of \u201cknown good\u201d libraries and datasets or implement an import hooking scheme to reduce the risk of malicious package installs and imports. Similar hygiene for datasets improves security, reproducibility, and auditability.\nMethodology\nThe dataset contains about 140GB of R, Python, and Jupyter Notebook source code publicly hosted on Kaggle. Kaggle allows users to save versions, so many of these artifacts are simply updates and changes to other files. Our analysis is limited to the Python files and Jupyter Notebooks\u2014around 3.5 million files that were executed on Kaggle from April 2020 to August 2023.\nSome analysis was manual, but we also relied heavily on two existing open-source security tools, TruffleHog to identify credentials and Semgrep to perform static analysis. Use these tools to repeat our analysis and consider them for inclusion in your kit of security tools.\nTo identify and validate credentials, TruffleHog can be run in a Docker container against source code repositories or local files. For this analysis, we ran TruffleHog against a local download with ```docker run --rm -it -v \"kaggle:/pwd\" trufflesecurity/trufflehog:latest filesystem /pwd --json --only-verified > trufflehog_findings.json```.\nTruffleHog also supports precommit hooks to help ensure credentials are not committed to remote repositories and CI/CD integration to continually monitor for leakage. TruffleHog was able to run against the Kaggle dataset without modification and we deduplicated findings based on unique secret values.\nSemgrep is a static code analyzer that uses rules to identify potential weaknesses in the target source code. Since Semgrep does not natively support Jupyter notebooks, we used ```nbconvert``` to convert them to Python files before Semgrep processing. We used 162 rules from the default Python rules and rules maintained by Trail of Bits that are more focused on ML applications.\nAfter installing Semgrep, run these two rulesets against your local Kaggle download with ```semgrep --config \"p/trailofbits\" --config \"p/python\" --json kaggle/ -o semgrep_findings.json```. During analysis, we filtered out the Trail of Bits automatic memory pinning rule because we could not find a direct path or evidence of previous exploitation.\nThe NVIDIA AI Red Team wrapped these tools in a meta-tool called lintML. To reproduce our results, try it with ```lintML \u2013semgrep-options \u201c--config \u2018p/python\u2019 \u2013config \u2018p/trailofbits\u2019\u201d <directory>```.\nLimitations\nWhile we are proud of the volume of this analysis, it is still \u201csingle source\u201d in that all samples were collected through Kaggle. It is likely that, while many findings may be the same, the underlying distribution of security observations from other data sources would be different.\nFor example, a similar analysis performed on GitHub artifacts may skew towards \u201cmore secure\u201d as those repositories are more likely to contain productionized code.\nFurthermore, Kaggle competitions reward rapid iteration and accuracy, which potentially lead to different library imports, techniques, and security considerations that productionized research. For instance, Kaggle competitions usually provide the necessary data. In reality, sourcing, cleaning, and labeling data are often significant design decisions and sources of potential vulnerabilities.\nThis analysis was simultaneously enabled and limited by the tools we used. If a credential or validator did not exist in TruffleHog, there is no associated finding here. Likewise, Semgrep analysis was limited by the rulesets we chose. Only a subset of these findings are likely exploitable, but the quantity may be correlated with overall project risk.\nFurther, the finding quantity analysis may have been biased by the distribution of rules (more deserialization rules yielding more findings). Security researchers should continue contributing to established tools for findings related to machine learning security (as the NVIDIA AI Red Team has done for both TruffleHog and Semgrep rules). The NVIDIA AI Red Team is particularly interested in applications of data flow and taint analysis to machine learning applications.\nConclusion\nKaggle is a place for experimentation, research, and competition. It rewards rapid experiment iteration and performance, so these code artifacts are not representative of production services.\nOr are they? Code is often reused, habits are formed during research, and defaults are sticky. In a similar analysis of over 300 highly ranked machine learning repositories on GitHub, we still found hardcoded credentials to third-party services and the full range of findings presented here. Increasing security awareness and informative and preventative controls during research helps ensure secure products and increases the professionalism and security posture of your enterprise.\nSecurity professionals should use this analysis as a foundation for analyzing research and development practices in their organizations. Most of these findings represent baseline security controls. If you start to find them in your organization\u2019s research code, they are signals to engage more thoroughly with research and development teams.\nUse similar techniques to evaluate artifacts across your machine learning development cycle to ensure relaxed research practices are not propagating risk into production products. Identify opportunities to provide low-friction tooling early, not just in production delivery pipelines. Leverage proactive adversarial assessments and exercises to increase education, awareness, and reasonable security controls.\nResearchers should focus on establishing and maintaining good security hygiene as part of scientific integrity. Security risks should be viewed as unwanted variables that should be mitigated to ensure the veracity of the experiments. Think about the provenance of data and code that you are pulling into your organization. Engage with your security teams for guidance on best-practices and environment hardening.\nJust as you would when rigorously testing a hypothesis, be critical when testing your project and identify opportunities where accuracy may not be the sole metric you want to optimize and consider including robustness, explainability, and fairness tests. Even if you aren\u2019t writing a production service, you may still be\u200c exposing yourself, your research, and your organization to potential risk.\nUse our Security Practices notebook to begin analyzing this data yourself, or download a local copy of the Meta Kaggle Code to evaluate with TruffleHog and Semgrep. Experiment with lintML to identify risks in your ML training code.\nTo learn more about ML security, check out Black Hat Machine Learning at Black Hat Europe 2023.\nAcknowledgments\nWe would like to thank Kaggle for making this dataset available. This kind of data can help elevate security awareness and baseline the industry. The NVIDIA AI Red Team is constantly trying to meet ML practitioners where they are, and Kaggle has been a great partner and enabler for that mission. For more details, see Improving Machine Learning Security Skills at a DEF CON Competition. We would also like to thank all of the Kaggle competitors for contributing code to the dataset.\nAdditionally, we would like to thank TruffleHog, Semgrep, and Trail of Bits for open sourcing security tools that enabled this research and Jupyter, pandas, NumPy, and Matplotlib for high-quality data analysis and visualization tools."}], "https://developer.nvidia.com/blog/ai-powered-simulation-tools-for-surrogate-modeling-engineering-workflows-with-siml-ai-and-nvidia-modulus/": [{"text": "AI-powered simulation tools like Siml.ai and NVIDIA Modulus are revolutionizing engineering workflows by allowing for the creation of high-fidelity, parameterized surrogate models with near-real-time latency. These tools blend physics with deep learning training data to streamline the process of constructing and training AI-driven simulators, saving engineers and scientists time and cost. Siml.ai's Model Engineer application provides powerful abstractions for domain experts, enabling them to express their knowledge of complex systems without needing to become proficient in AI. The visual editor in Model Engineer simplifies the process of setting up simulations, allowing for rapid prototyping and training on the Modulus platform. Simulation Studio in Siml.ai provides real-time visualization of simulation results, allowing for interactive virtual physics experimentation. An example case study demonstrated that using physics-ML approaches with Siml.ai and NVIDIA Modulus resulted in significant cost and time savings compared to traditional simulation methods. These tools are empowering engineers and scientists to explore physics-ML models for a wide range of applications, from manufacturing to healthcare.", "text_components": ["AI-Powered Simulation Tools for Surrogate Modeling Engineering Workflows with Siml.ai and NVIDIA Modulus\nSimulations are quintessential for complex engineering challenges, like designing nuclear fusion reactors, optimizing wind farms, developing carbon capture and storage techniques, or building hydrogen batteries. Designing such systems often requires many iterations of scientific simulations that are computationally expensive to run. Solvers and parameters must often be tuned individually to each system studied. Thanks to AI and physics-informed machine learning (physics-ML) frameworks such as NVIDIA Modulus, it is now possible to overcome these challenges and turbocharge these simulations.\nNVIDIA Modulus is a state-of-the-art open-source physics-ML platform that blends physics with deep learning training data to build high-fidelity, parameterized surrogate models with near-real-time latency. Engineers and scientists can explore and build physics-based AI surrogate models using NVIDIA Modulus. These principles are being applied across a wide range of solutions, from manufacturing to healthcare, including high-performance computing (HPC) scale applications like weather forecasting and industrial digital twins.\nDimensionLab, an NVIDIA Inception partner, is a software studio building next-generation tools for engineers, scientists, makers, and creators. With an increased focus on user experience, simplicity, and modern design, the DimensionLab team is leveraging recent advancements in scientific machine learning to revolutionize how numerical simulation can be used for technology and product development.\nRecognizing the significant potential of NVIDIA Modulus, DimensionLab used it as a backbone of their flagship product, a web platform for AI-driven engineering called Siml.ai.", "Simplifying AI surrogate modeling for domain experts\nSiml.ai provides powerful no-code abstractions within its Model Engineer application, which enables engineers to express their domain expertise more natively rather than having to become proficient in AI. The application builds on the Modulus interface. This enables domain experts to express their knowledge of the system using governing partial differential equations in symbolic form, for example, rather than in terms of how the loss function of a deep learning model captures such information.\nThe goal of Model Engineer is to simplify and streamline the entire process, including:\nConstructing large datasets from traditional simulation exports or physical sensors that collect precise measurements from real-world experiments.\nConstructing the correct model architecture for the simulators\u2019 desired capabilities and constraints.\nTraining and aggressively optimizing the learnable simulators in high-performance, GPU-powered cloud or HPC centers without the need to deal with the complexities of managing the cloud infrastructure.\nA flowchart-style visual editor in Model Engineer sets up the problem to launch training on the Modulus platform. Under the hood, the visual representation of the flowchart diagram is compiled into NVIDIA Modulus API calls. It represents the entire simulator state, with parameterized variables, neural network architectures, geometries, or constraints.\nYou can drag-and-drop CAD geometries into the geometry node where they are automatically converted into STLs for the NVIDIA Modulus tessellation module. They are simultaneously converted into a fast web format for a 3D geometry viewer.\nScreenshot of Siml.ai Model Engineer showing how to use the flowchart-style visual editor to specify the problem with no need for explicit coding.\nFigure 1. Siml.ai Model Engineer provides a flowchart-style visual editor for rapid simulator prototyping These Siml.ai interactive and visual tools free engineers and scientists to focus on exploring physics-ML models for simulating physics, without dealing with its complexities. They can start training their simulation models in just a few clicks by creating and deploying an Environment through the platform. The Environment uses a Simulator Inference and Training Environment (SITE): a containerized solution that does all the heavy lifting to compile a visual representation of the simulator into code for a trainable Modulus simulator. SITE is optimized for NVIDIA GPUs and offers everything needed for working with AI-driven simulators.\nThe second part of the Siml.ai platform is a tool for real-time visualization of the simulation results, called Simulation Studio. It is a hybrid application (web-based and native), based on Unreal Engine and its pixel streaming capabilities. It runs the rendering in the cloud and sends rendered frames as a video stream to the Siml.ai frontend.\nUsers can load parameterized simulators created in Model Engineer into the Simulation Studio to create interactive digital twins for fast virtual physics experimentation workflows.\nScreenshot of the interface of the Simulation Studio application that enables engineers to interact with the Modulus trained model and visualize using Unreal Engine.\nFigure 2. Simulation Studio in Siml.ai encapsulates the parameterized surrogate AI model to provide interactive simulation exploration using Unreal Engine", "Physics-ML surrogate models save cost and time\nTo demonstrate the impact of these new tools and workflows in leveraging physics-ML, developers at DimensionLab used Siml.ai for a specific customer case study. The objective was to quantify the value of frameworks like NVIDIA Modulus and Siml.ai in building physics-ML models for a small consulting company with limited budget. The goal was to deliver an AI solution on par with the output of an industry standard simulation software, but with a fraction of the cost and time.\nThe customer needed numerical simulations of a river flow into a hydropower plant to analyze the flood damage from past events. It took them 4 years to finalize and fine-tune the mathematical model in a popular numerical solver, and they spent months on complex geometry meshing.\nIn contrast to classical modeling and simulation approaches, developers at DimensionLab helped the customer to apply physics-ML approaches to this engineering challenge. They created multiple comparable AI-driven simulations in Siml.ai that leverage Navier-Stokes equations, physics-informed neural networks, and Fourier neural operators from the NVIDIA Modulus framework.\nThese models took just 3 weeks to develop and train multiple times with various configurations and parameterizations. They resulted in 96% cost and time savings compared to the previous approaches tried by the company.", "Simulation for engineering resources\nTo learn more, see the Siml.ai documentation and Siml.ai training content. You can also read more about the tools in the Siml.ai blog.\nTo learn more about physics-ML and NVIDIA Modulus, check out the NVIDIA Deep Learning Institute course, Introduction to Physics-Informed Machine Learning with Modulus and visit the NVIDIA/modulus repo on GitHub.\nLearn more about NVIDIA Inception, designed to help startups evolve faster through cutting-edge technology, gain opportunities to connect with venture capitalists, and access to the latest technical resources from NVIDIA."], "document_title": "AI-Powered Simulation Tools for Surrogate Modeling Engineering Workflows with Siml.ai and NVIDIA Modulus", "document_url": "https://developer.nvidia.com/blog/ai-powered-simulation-tools-for-surrogate-modeling-engineering-workflows-with-siml-ai-and-nvidia-modulus/", "document_date": "2023-10-02T19:01:48", "document_date_modified": "2023-10-19T19:05:56", "document_full_text": "AI-Powered Simulation Tools for Surrogate Modeling Engineering Workflows with Siml.ai and NVIDIA Modulus\nSimulations are quintessential for complex engineering challenges, like designing nuclear fusion reactors, optimizing wind farms, developing carbon capture and storage techniques, or building hydrogen batteries. Designing such systems often requires many iterations of scientific simulations that are computationally expensive to run. Solvers and parameters must often be tuned individually to each system studied. Thanks to AI and physics-informed machine learning (physics-ML) frameworks such as NVIDIA Modulus, it is now possible to overcome these challenges and turbocharge these simulations.\nNVIDIA Modulus is a state-of-the-art open-source physics-ML platform that blends physics with deep learning training data to build high-fidelity, parameterized surrogate models with near-real-time latency. Engineers and scientists can explore and build physics-based AI surrogate models using NVIDIA Modulus. These principles are being applied across a wide range of solutions, from manufacturing to healthcare, including high-performance computing (HPC) scale applications like weather forecasting and industrial digital twins.\nDimensionLab, an NVIDIA Inception partner, is a software studio building next-generation tools for engineers, scientists, makers, and creators. With an increased focus on user experience, simplicity, and modern design, the DimensionLab team is leveraging recent advancements in scientific machine learning to revolutionize how numerical simulation can be used for technology and product development.\nRecognizing the significant potential of NVIDIA Modulus, DimensionLab used it as a backbone of their flagship product, a web platform for AI-driven engineering called Siml.ai.\nSimplifying AI surrogate modeling for domain experts\nSiml.ai provides powerful no-code abstractions within its Model Engineer application, which enables engineers to express their domain expertise more natively rather than having to become proficient in AI. The application builds on the Modulus interface. This enables domain experts to express their knowledge of the system using governing partial differential equations in symbolic form, for example, rather than in terms of how the loss function of a deep learning model captures such information.\nThe goal of Model Engineer is to simplify and streamline the entire process, including:\nConstructing large datasets from traditional simulation exports or physical sensors that collect precise measurements from real-world experiments.\nConstructing the correct model architecture for the simulators\u2019 desired capabilities and constraints.\nTraining and aggressively optimizing the learnable simulators in high-performance, GPU-powered cloud or HPC centers without the need to deal with the complexities of managing the cloud infrastructure.\nA flowchart-style visual editor in Model Engineer sets up the problem to launch training on the Modulus platform. Under the hood, the visual representation of the flowchart diagram is compiled into NVIDIA Modulus API calls. It represents the entire simulator state, with parameterized variables, neural network architectures, geometries, or constraints.\nYou can drag-and-drop CAD geometries into the geometry node where they are automatically converted into STLs for the NVIDIA Modulus tessellation module. They are simultaneously converted into a fast web format for a 3D geometry viewer.\nScreenshot of Siml.ai Model Engineer showing how to use the flowchart-style visual editor to specify the problem with no need for explicit coding.\nFigure 1. Siml.ai Model Engineer provides a flowchart-style visual editor for rapid simulator prototyping These Siml.ai interactive and visual tools free engineers and scientists to focus on exploring physics-ML models for simulating physics, without dealing with its complexities. They can start training their simulation models in just a few clicks by creating and deploying an Environment through the platform. The Environment uses a Simulator Inference and Training Environment (SITE): a containerized solution that does all the heavy lifting to compile a visual representation of the simulator into code for a trainable Modulus simulator. SITE is optimized for NVIDIA GPUs and offers everything needed for working with AI-driven simulators.\nThe second part of the Siml.ai platform is a tool for real-time visualization of the simulation results, called Simulation Studio. It is a hybrid application (web-based and native), based on Unreal Engine and its pixel streaming capabilities. It runs the rendering in the cloud and sends rendered frames as a video stream to the Siml.ai frontend.\nUsers can load parameterized simulators created in Model Engineer into the Simulation Studio to create interactive digital twins for fast virtual physics experimentation workflows.\nScreenshot of the interface of the Simulation Studio application that enables engineers to interact with the Modulus trained model and visualize using Unreal Engine.\nFigure 2. Simulation Studio in Siml.ai encapsulates the parameterized surrogate AI model to provide interactive simulation exploration using Unreal Engine\nPhysics-ML surrogate models save cost and time\nTo demonstrate the impact of these new tools and workflows in leveraging physics-ML, developers at DimensionLab used Siml.ai for a specific customer case study. The objective was to quantify the value of frameworks like NVIDIA Modulus and Siml.ai in building physics-ML models for a small consulting company with limited budget. The goal was to deliver an AI solution on par with the output of an industry standard simulation software, but with a fraction of the cost and time.\nThe customer needed numerical simulations of a river flow into a hydropower plant to analyze the flood damage from past events. It took them 4 years to finalize and fine-tune the mathematical model in a popular numerical solver, and they spent months on complex geometry meshing.\nIn contrast to classical modeling and simulation approaches, developers at DimensionLab helped the customer to apply physics-ML approaches to this engineering challenge. They created multiple comparable AI-driven simulations in Siml.ai that leverage Navier-Stokes equations, physics-informed neural networks, and Fourier neural operators from the NVIDIA Modulus framework.\nThese models took just 3 weeks to develop and train multiple times with various configurations and parameterizations. They resulted in 96% cost and time savings compared to the previous approaches tried by the company.\nSimulation for engineering resources\nTo learn more, see the Siml.ai documentation and Siml.ai training content. You can also read more about the tools in the Siml.ai blog.\nTo learn more about physics-ML and NVIDIA Modulus, check out the NVIDIA Deep Learning Institute course, Introduction to Physics-Informed Machine Learning with Modulus and visit the NVIDIA/modulus repo on GitHub.\nLearn more about NVIDIA Inception, designed to help startups evolve faster through cutting-edge technology, gain opportunities to connect with venture capitalists, and access to the latest technical resources from NVIDIA."}], "https://developer.nvidia.com/blog/building-software-defined-high-performance-and-efficient-vran-requires-programmable-inline-acceleration/": [{"text": "The article discusses the challenges of implementing layer 1 (L1) processing in 5G cellular networks, which is compute-intensive and requires high performance to maintain low latency. Traditionally, this has been done with purpose-built hardware, but the industry is moving towards virtualized RAN (vRAN) using commercial-off-the-shelf servers. However, achieving the desired performance on these servers is difficult due to the complex signal processing needs of L1. Some companies are using fixed-function accelerators to supplement CPU performance, but these have limitations in terms of scalability, agility, and cost. NVIDIA has developed the Aerial platform, which uses programmable inline acceleration with GPUs to address these challenges. This approach offloads the entire L1 processing pipeline to the GPU without involving the CPU, leading to improved performance, energy efficiency, and scalability. The article compares the fixed-function lookaside acceleration model with the GPU-based inline acceleration, highlighting the benefits of the latter in terms of performance, energy efficiency, cloud economics, application portability, scalability, and agility. NVIDIA Aerial aims to deliver a high-performance, software-defined, cloud-native, and AI-capable vRAN solution that meets the key requirements of modern RAN infrastructure.", "text_components": ["Building Software-Defined, High-Performance, and Efficient vRAN Requires Programmable Inline Acceleration\nIn 3GPP fifth generation (5G) cellular standard, layer 1 (L1) or the physical layer (PHY) is the most compute-intensive part of the radio access network (RAN) workload. It involves some of the most complex mathematical operations with sophisticated algorithms like channel estimation and equalization, modulation/demodulation, and forward error correction (FEC). These functions require high compute densities to maintain 5G\u2019s low latency requirements and signal integrity in different radio conditions.\nTraditionally, this layer has been implemented using purpose-built hardware, for example, specialized application-specific integrated circuits (ASICs) with digital signal processing (DSP) cores. This approach, has several drawbacks, however, namely an inability to scale performance, tight hardware-software coupling, and closed single vendor solution. This all results in the high cost of deploying and operating the RAN.\nTo address these challenges, the industry has been evolving towards virtualized RAN (vRAN) and open RAN ( O-RAN ) architectures using x86 CPU-based commercial-off-the-shelf (COTS) servers. There\u2019s an expectation that this will lower the cost, and that the resultant hardware-software disaggregation will drive faster innovation cycles, leading the path toward a cloud-native architecture.\nHowever, the complex signal processing needs of L1 make it difficult to achieve the desired vRAN performance on x86 CPU-based COTS servers. To address this L1 performance gap, some industry players are building fixed-function accelerators. Examples include discrete ASICs, field programmable gate arrays (FPGAs), or integrated system-on-chips (SoCs).\nFixed-function accelerators supplement the CPU performance and accelerate the processing of a selected set of functions offloaded from a vRAN L1 pipeline, while retaining the majority of the L1 processing within the CPU. This is an acceleration approach dubbed in the industry as lookaside acceleration.\nIn many ways, fixed-function lookaside accelerator-based vRAN platforms are akin to going back in time to the appliance-like, macro base station architecture models, which lack scalability and agility. What our industry needs is a fully software-defined vRAN that can deliver programmability, performance, and software scalability while supporting interoperability and multi-vendor solutions, a key tenet of O-RAN.\nWith the emergence of artificial intelligence and machine learning (AI/ML) as one of the key driving forces shaping the landscape beyond 5G, it is equally important for the industry to embrace a vRAN platform that is future-proof. It should be ready to enable new capabilities like AI/ML as augmented features on top of the existing RAN infrastructure.", "NVIDIA Aerial platform\nThe NVIDIA Aerial platform brings together the NVIDIA Aerial vRAN stack for 5G, AI frameworks, and the accelerated compute infrastructure. It delivers the key virtues by using the high degree of programmability and parallel processing capabilities of GPUs. The platform differs from the traditional fixed-function lookaside acceleration approach in two ways:\nIt does not use any fixed-function accelerator\nInstead of selectively offloading a subset of L1 functions to the accelerator, NVIDIA Aerial implements the entire L1 processing pipeline within GPU, an approach called inline acceleration.\nThe NVIDIA Aerial vRAN stack is a fully programmable, software-defined, AI-capable, and cloud-native 5G vRAN. For more information about how NVIDIA Aerial got started, see the NVIDIA cuBB GPU Accelerated 5G vRAN session from the 2019 Mobile World Congress.\nOur goal with this post is to show the merits of NVIDIA Aerial, a GPU-based inline architecture. We explain why programmable, inline acceleration is a critical foundation to deliver high-performance, energy-efficient, scalable, and cloud-native vRAN.", "Understanding the lookaside and inline acceleration models\nDiagram models architecture and data flows to highlight the challenges and benefits of the two acceleration models.\nFigure 1. vRAN acceleration with the lookaside and inline models\nTo begin with, look into the generic working principles of the lookaside and Inline acceleration models.\nFigure 1 shows the data flow in downlink and uplink directions for two different acceleration models: Lookaside and Inline. For more information, see Hardware Acceleration for Open Radio Access Networks: A Contemporary Overview.\nIn a lookaside acceleration model, the host CPU invokes data processing offload to the accelerator and receives the results back when the processing is done. The lookaside approach requires a back-and-forth data transfer between the CPU and the accelerator. If there are multiple, non-contiguous functional blocks offload (for example, FEC decode and channel estimation), the overhead of the host-to-device data transfer and resulting memory bandwidth consumption becomes significantly high.\nIn an inline acceleration model, the accelerator directly exchanges data with the network interface card (NIC) without involving the CPU in the critical path. For full L1 acceleration in an inline model, the entire L1 processing is offloaded to the accelerator.\nInline acceleration does not require a back-and-forth redundant data transfer between the host and the device, unlike lookaside acceleration. The net effect of this is more efficient usage of memory and PCIe bandwidth.", "Programmable, inline acceleration is better for vRAN\nTake a closer look at vRAN solutions based on two acceleration approaches:\nLookaside with a fixed-function accelerator\nInline with a programmable accelerator\nIn this section, we highlight the merits and limitations of each and explain why the inline approach with programmable accelerators is more suitable for vRAN compared to lookaside with fixed-function accelerators.\nLookaside offload costs affect latency and performance\nLookaside quality of service guarantees increase complexity\nLookaside accelerator integrated as a PCIe device is not equivalent to an inline accelerator\nFixed-function acceleration is inherently not cloud-native\nFixed-function accelerators lack scalability\nFixed-function accelerators are not agile", "Lookaside offload costs affect latency and performance\nLookaside acceleration results in a cost of offload due to the request/response transaction across the PCIe interface between the CPU and the accelerator. In the case of multiple, back-and-forth transactions (due to offloading a set of non-contiguous functions), lookaside acceleration increases both the CPU cycle consumption and latency, impacting perf/Watt and perf/$$.\nTo reduce the cost of offload, the accelerator driver may combine or batch several requests together. However, this leads to undesired buffering and queuing, resulting in significantly higher latency for various user data flows.\nDiagram shows that lookaside acceleration consumes significantly higher PCIe and DDR bandwidth compared to inline acceleration.\nFigure 2. Comparison of PCIe/DDR bandwidth\nFigure 2 shows the expected host PCIe and the double data rate bandwidth (DDR BW) consumption (Gbps) as the number of supported 4-transmit-4-receive (4T4R) 100MHz cells increases. The graph shows that the aggregated transaction bandwidth needed to support four downlink (DL) layers and two uplink (UL) layers (for each 100MHz cell) when deploying with a lookaside accelerator is much worse. Approximately 40x more bandwidth is consumed when compared to the inline accelerator.\nIt is also worth mentioning that as the number of cells increases, PCIe Gen4 technology cannot sustain the required bandwidth and PCIe Gen5 technology is required to support the lookaside accelerator.", "Lookaside quality of service guarantees increase complexity\nFine-grained QoS support for various user data flows is another challenge with lookaside accelerators. The complex queuing architecture required across the PCIe interface to meet the QoS needs can result in performance degradation and impact tail latency of queued requests to the accelerator.\nAs an example, consider a DU system that supports mixed user data flows for voice over Internet protocol (VoIP), Internet of things (IoT), enhanced mobile broadband (eMBB), and ultra-reliable low-latency communications (URLLC) applications. In the lookaside model, if a VoIP or URLLC packet gets stuck behind large blocks of eMBB data queued to the accelerator, it incurs significant latency and jitter and degrades the QoS. The fact that this can accumulate over time as every transaction needs to go through a Lookaside accelerator results in significant performance degradation.\nThere are ways to address these issues through QoS guarantees and hierarchical scheduling across the lookaside PCIe interface. However, this increases both the hardware and the software complexity, resulting in increased cost and energy consumption, as well as reduced cell capacity.\nTo further demonstrate the cell capacity and power efficiency benefits of deploying an inline accelerator compared to lookaside, we evaluated the performance of both acceleration modes with respect to two metrics for the following system configuration: 100 MHz, 4T4R, 4 DL/2 UL layers:\nNumber of 100MHz cells supported\nMHz*Layers/Watt\nBar charts show normalized cells and power efficiency for CPU only, CPU plus fixed-function lookaside acceleration, and NVIDIA Aerial inline acceleration.\nFigure 3 shows the performance comparison, with the number of supported cells metric (normalized) displayed on the left and the MHz*Layers/Watt metric displayed on the right. For each metric, the cell capacity and power efficiency benefits are clear when deploying an inline accelerator, compared to a fixed-function lookaside accelerator or no accelerator (that is, CPU only).", "Lookaside accelerator integrated as a PCIe device is not equivalent to an inline accelerator\nSome argue that the integration of lookaside accelerators in the CPU makes it an inline architecture. Nothing could be farthest from the truth.\nWhile integration may result in limited power optimization and reduce the component price, any fixed-function lookaside accelerator, such as FEC, integrated in the CPU still manifests as a PCIe device and is accessed through DPDK BBDEV. The net effect is that the same inefficiency exists with fixed-function lookaside accelerators, no matter whether they are discrete components, or integrated in the CPU.\nIn fact, integrating lookaside accelerators introduce a new set of problems: managing specific CPU stock-keeping units (SKUs), juggling feature prioritization, increasing CPU costs, and so on.", "Fixed-function acceleration is inherently not cloud-native\nThe key tenet of cloud computing is that the infrastructure resources can be shared across applications, increasing the utilization, and delivering better economies of scale.\nFixed-function accelerators (such as FPGA-based, low-density parity check (LDPC), SoC-based L1 high-PHY and so on) are single-purpose. When a fixed-function accelerator is not being used by 5G vRAN, it is a wasted resource that is not used by any other application.\nTypical 5G networks run at less than 50% average utilization. This means a fixed-function lookaside accelerator can just sit there in the cloud, not being used for more than 50% of the time. On the other hand, a general-purpose and programmable accelerator such as a GPU can be reused for other applications, such as large language model (LLM) training and inferencing, computer vision, and analytics.\nData Plane Development Kit Baseband Device ( DPDK BBDEV ) is the commonly used application programming interface (API) for lookaside acceleration. It is not well-suited for cloud-native deployments. DPDK has many constructs that were designed for high-performance, in-network appliances, including the following:\nHuge page tables\nPre-allocated buffers\nPinned memory\nSingle-root input/output virtualization (SR-IOV)\nQueue-centric enqueue-dequeue operations\nThese features, however, create a strong affinity towards the underlying hardware, not enabling seamless portability and workload movement in a true cloud-native fashion.", "Fixed-function accelerators lack scalability\nA major drawback of fixed-function accelerators like FEC LDPC, discrete Fourier transform (DFT), inverse DFT (iDFT), and other selected baseband Layer 1 functions is that, while it may be right-sized for one configuration or use case, it is suboptimal for another configuration.\nTake FEC LDPC as an example. In a typical 5G frequency range 1 (FR1) sub-6 GHz system with 4T4R antenna and DDDSUUDDDD channel configuration (D: downlink, U: uplink, S: special) and 4 DL/2 UL layers, the LDPC decoder may constitute about 25% of the physical uplink shared channel (PUSCH) workload in an UL slot.\nKeeping other configurations unchanged, if the system dimension scales from a 4T4R to 64T64R antenna configuration (massive MIMO), the LDPC decoder compute load on the PUSCH pipeline, as it turns out, does not increase commensurately. In fact, in this higher dimensional system, LDPC constitutes approximately just 10% of the overall uplink workload.\nWhy does this happen? It\u2019s because the complexity of the LDPC decoder scales only linearly with the number of layers, where other algorithms, such as channel estimation or detection, scale superlinearly. This can easily lead to suboptimal designs from the point of view of resource utilization and power consumption, if these functions are implemented in fixed-function acceleration logic.", "Fixed-function accelerators are not agile\nFixed-function accelerators are difficult to evolve with 3GPP releases (for example, with new features) as they are designed for a particular release of the specification. Upgrades of these complex algorithms running on fixed-function accelerators are difficult (especially when implemented in hardware), thereby throttling improvements over time. Also, hardware bug fixes are problematic to resolve, often resulting in costly replacement as being the only viable resolution.\nTo summarize, fixed-function lookaside acceleration has several drawbacks: an impact on performance and latency, lower energy efficiency, and lack of programmability and scalability. These issues directly result in higher CapEx and OpEx for telco operators.\nNext, we discuss an alternate approach taken by NVIDIA, that addresses many of the issues highlighted earlier by harnessing the principles of programmability and inline acceleration. This solution paves the way for industry-leading vRAN.", "NVIDIA Aerial: Programmable, GPU-based inline acceleration for vRAN\nNVIDIA has taken a thoughtful architectural approach to use the inline architecture for a full L1 offload to a programmable GPU. The architecture uses Bluefield DPUs to bring all fronthaul-enhanced common public radio interface (eCPRI) data traffic into the GPU without the CPU in the data path.\nA natural question to ask is why GPUs? The signal processing requirements of the 5G PHY are computationally challenging, compounded by intensive matrix operations. The massive parallelism of the GPU architecture brings the right hardware resources to support this class of workloads.\nFrom a developer perspective, GPUs are programmed using CUDA, the world\u2019s most commercially successful parallel programming framework. This makes your job simpler as you can use mature tools and expansive libraries for software lifecycle management including planning, design, development, optimization, testing, and maintenance. This has been proven by the widespread adoption of GPUs in the computationally complex field of AI and machine learning.\nThe second question is why inline? Inline architecture provides a full offload of vRAN L1 processing to the GPU without any CPU interactions. The interface for the offload is the functional application platform interface ( FAPI ), which is an industry standard developed within the Small Cell Forum ( SCF ). The full offload also avoids the complex and inefficient ping-pong effects of the lookaside model between the CPU and the accelerator across the host PCIe interface, resulting in the improved performance and lower latency explained earlier.\nNVIDIA Aerial, enabling fully programmable, cloud-native, AI-capable, and high-performance end-to-end L1 high-PHY (7.2-x split) inline acceleration is built upon two fundamental principles:\nAccelerated compute\nFast I/O\nAccelerated compute is manifested through the component CUDA baseband (cuBB), the software stack providing a GPU-accelerated 5G L1 signal processing pipeline. cuBB delivers unprecedented throughput and efficiency by keeping all PHY layer processing within the high-performance GPU memory. cuBB includes 5G L1 high-PHY acceleration library cuPHY, which is highly optimized for NVIDIA GPUs and offers unparalleled scalability by using a GPU\u2019s massive computing power and high degree of parallelism.\nFast I/O is manifested through the NVIDIA DOCA GPUNetIO module, providing optimized I/O and packet processing by exchanging packets directly between GPU memory and a GPUDirect-capable NVIDIA ConnectX SmartNIC. Enabling fast I/O processing and direct memory access (DMA) technology is essential in unleashing the full potential of inline acceleration.\nTowards that goal, the NVIDIA Aerial platform has adopted a GPU-centric approach, implemented with NVIDIA DOCA GPUNetIO Library. In this approach, an NVIDIA GPU directly interacts with an NVIDIA SmartNIC using GPUDirect Async Kernel-initiated Network (GDAKIN) communications to configure and update NIC registers for orchestrating network send and receive operations without the intervention of the CPU. For more information, see Inline GPU Packet Processing with NVIDIA DOCA GPUNetIO.\nDiagram shows that the programmable inline accelerator (L1 high-PHY offload) delivers 10-40x efficiency in PCIe bandwidth usage compared to the fixed-function lookaside accelerator (FEC offload using DPDK BBDEV).\nFigure 4. Comparison between fixed-function hardware-based lookaside acceleration and GPU-based programmable inline acceleration implementations for L1 high-PHY\nFigure 4 shows the architectural comparison for the PHY layer between the GPU-based inline acceleration implementation using NVIDIA Aerial and the typical fixed-function hardware accelerator (HWA)-based lookaside acceleration. On the right side, the NVIDIA Aerial platform offers a fast, efficient, and streamlined data flow all the way from L2 to L1 and to fronthaul, without requiring CPU staging copies or throttling of the host PCIe bandwidth.\nHigher level acceleration abstraction layer (AAL) between L2 and L1 (that is, FAPI)\nConverged architecture with GPU and DPU\nInterconnect powered by the NVIDIA DOCA GPUNetIO and GPUDirect technologies\nAs the entire L1 processing pipeline and corresponding data are contained within the GPU kernels and dynamic random-access memory (DRAM) on the same converged card, NVIDIA Aerial does not consume critical shared resources with L2+ (for example, host DRAM or host PCIe), unlike the traditional lookaside architecture (left).\nWith less CPU core consumption and a high degree of GPU parallelism in processing the entire L1 workload, the NVIDIA Aerial platform delivers a lower CapEx and OpEx solution with unmatched performance, scalability, agility, programmability, and energy efficiency.", "NVIDIA Aerial addresses the key requirements\nTable 1 presents a snapshot of the key requirements for 5G vRAN, the limitations of the lookaside architecture with a fixed-function accelerator in meeting these requirements, and the benefits of the inline architecture with a GPU-programmable accelerator in addressing those shortcomings.\nRequirements\nFixed-function lookaside architecture\nGPU-programmable inline architecture\nHigh performance and low latency\nMultiple requests and responses across PCIe lead to increased CPU consumption and worse perf/Watt and perf/$$. Higher L1 processing latency due to batching and queuing of lookaside requests.\nL2 \u2194 L1 \u2194 FH streamlined processing pipeline, no back-and-forth transactions over PCIe, leading to better perf/Watt and perf/$$. No buffering/queuing during L1 runtime, resulting in optimal L1 processing latency.\nCloud economics\nNo reuse: only does \u2018fixed\u2019 function and not sharable with other applications in cloud infrastructure.\nFully programmable and general purpose resulting in high resource utilization.\nApplication portability\nDPDK BBDEV: Not easily portable because of the strong affinity to hardware.\nFAPI: Better portability with higher level abstraction between L2 and L1.\nScalability\nDesigned and optimized for a specific system configuration.\nFully programmable and scalable for a range of system configs.\nAgility\nNot programmable, long design cycles, and difficult to update with evolving standards and algorithms.\nFully programmable and software-defined, easy to update for evolving standards and new algorithms.\nTable 1. Five key tenets of OpenRAN and the comparison between the fixed-function lookaside and the GPU programmable inline architectures", "Conclusion\nIn this post, we highlighted the inefficiencies of fixed-function accelerators and the lookaside processing model. We showed you how the lookaside model impacts performance and energy efficiency along with many scalability challenges.\nThe inline processing model with a programmable accelerator addresses the technical bottlenecks of the fixed-function lookaside acceleration model and delivers high performance, energy efficiency, and scalability across various RAN configurations.\nNVIDIA Aerial is the only commercial platform that delivers the key tenets of emerging vRAN: high-performance, software-defined, COTS-based, cloud-native, and AI-ready. It implements the GPU-programmable inline processing model and full L1 offload to deliver efficient performance for a wide range of RAN configurations and use cases with a software architecture that is fully compliant with O-RAN standards.\nWe invite you to collaborate with us in our quest to modernize the RAN infrastructure and enable an efficient, high-performance, scalable, agile, cloud-native, fully software-defined, and AI-ready vRAN."], "document_title": "Building Software-Defined, High-Performance, and Efficient vRAN Requires Programmable Inline Acceleration", "document_url": "https://developer.nvidia.com/blog/building-software-defined-high-performance-and-efficient-vran-requires-programmable-inline-acceleration/", "document_date": "2023-10-02T18:54:26", "document_date_modified": "2023-10-23T17:13:13", "document_full_text": "Building Software-Defined, High-Performance, and Efficient vRAN Requires Programmable Inline Acceleration\nIn 3GPP fifth generation (5G) cellular standard, layer 1 (L1) or the physical layer (PHY) is the most compute-intensive part of the radio access network (RAN) workload. It involves some of the most complex mathematical operations with sophisticated algorithms like channel estimation and equalization, modulation/demodulation, and forward error correction (FEC). These functions require high compute densities to maintain 5G\u2019s low latency requirements and signal integrity in different radio conditions.\nTraditionally, this layer has been implemented using purpose-built hardware, for example, specialized application-specific integrated circuits (ASICs) with digital signal processing (DSP) cores. This approach, has several drawbacks, however, namely an inability to scale performance, tight hardware-software coupling, and closed single vendor solution. This all results in the high cost of deploying and operating the RAN.\nTo address these challenges, the industry has been evolving towards virtualized RAN (vRAN) and open RAN ( O-RAN ) architectures using x86 CPU-based commercial-off-the-shelf (COTS) servers. There\u2019s an expectation that this will lower the cost, and that the resultant hardware-software disaggregation will drive faster innovation cycles, leading the path toward a cloud-native architecture.\nHowever, the complex signal processing needs of L1 make it difficult to achieve the desired vRAN performance on x86 CPU-based COTS servers. To address this L1 performance gap, some industry players are building fixed-function accelerators. Examples include discrete ASICs, field programmable gate arrays (FPGAs), or integrated system-on-chips (SoCs).\nFixed-function accelerators supplement the CPU performance and accelerate the processing of a selected set of functions offloaded from a vRAN L1 pipeline, while retaining the majority of the L1 processing within the CPU. This is an acceleration approach dubbed in the industry as lookaside acceleration.\nIn many ways, fixed-function lookaside accelerator-based vRAN platforms are akin to going back in time to the appliance-like, macro base station architecture models, which lack scalability and agility. What our industry needs is a fully software-defined vRAN that can deliver programmability, performance, and software scalability while supporting interoperability and multi-vendor solutions, a key tenet of O-RAN.\nWith the emergence of artificial intelligence and machine learning (AI/ML) as one of the key driving forces shaping the landscape beyond 5G, it is equally important for the industry to embrace a vRAN platform that is future-proof. It should be ready to enable new capabilities like AI/ML as augmented features on top of the existing RAN infrastructure.\nNVIDIA Aerial platform\nThe NVIDIA Aerial platform brings together the NVIDIA Aerial vRAN stack for 5G, AI frameworks, and the accelerated compute infrastructure. It delivers the key virtues by using the high degree of programmability and parallel processing capabilities of GPUs. The platform differs from the traditional fixed-function lookaside acceleration approach in two ways:\nIt does not use any fixed-function accelerator\nInstead of selectively offloading a subset of L1 functions to the accelerator, NVIDIA Aerial implements the entire L1 processing pipeline within GPU, an approach called inline acceleration.\nThe NVIDIA Aerial vRAN stack is a fully programmable, software-defined, AI-capable, and cloud-native 5G vRAN. For more information about how NVIDIA Aerial got started, see the NVIDIA cuBB GPU Accelerated 5G vRAN session from the 2019 Mobile World Congress.\nOur goal with this post is to show the merits of NVIDIA Aerial, a GPU-based inline architecture. We explain why programmable, inline acceleration is a critical foundation to deliver high-performance, energy-efficient, scalable, and cloud-native vRAN.\nUnderstanding the lookaside and inline acceleration models\nDiagram models architecture and data flows to highlight the challenges and benefits of the two acceleration models.\nFigure 1. vRAN acceleration with the lookaside and inline models\nTo begin with, look into the generic working principles of the lookaside and Inline acceleration models.\nFigure 1 shows the data flow in downlink and uplink directions for two different acceleration models: Lookaside and Inline. For more information, see Hardware Acceleration for Open Radio Access Networks: A Contemporary Overview.\nIn a lookaside acceleration model, the host CPU invokes data processing offload to the accelerator and receives the results back when the processing is done. The lookaside approach requires a back-and-forth data transfer between the CPU and the accelerator. If there are multiple, non-contiguous functional blocks offload (for example, FEC decode and channel estimation), the overhead of the host-to-device data transfer and resulting memory bandwidth consumption becomes significantly high.\nIn an inline acceleration model, the accelerator directly exchanges data with the network interface card (NIC) without involving the CPU in the critical path. For full L1 acceleration in an inline model, the entire L1 processing is offloaded to the accelerator.\nInline acceleration does not require a back-and-forth redundant data transfer between the host and the device, unlike lookaside acceleration. The net effect of this is more efficient usage of memory and PCIe bandwidth.\nProgrammable, inline acceleration is better for vRAN\nTake a closer look at vRAN solutions based on two acceleration approaches:\nLookaside with a fixed-function accelerator\nInline with a programmable accelerator\nIn this section, we highlight the merits and limitations of each and explain why the inline approach with programmable accelerators is more suitable for vRAN compared to lookaside with fixed-function accelerators.\nLookaside offload costs affect latency and performance\nLookaside quality of service guarantees increase complexity\nLookaside accelerator integrated as a PCIe device is not equivalent to an inline accelerator\nFixed-function acceleration is inherently not cloud-native\nFixed-function accelerators lack scalability\nFixed-function accelerators are not agile\nLookaside offload costs affect latency and performance\nLookaside acceleration results in a cost of offload due to the request/response transaction across the PCIe interface between the CPU and the accelerator. In the case of multiple, back-and-forth transactions (due to offloading a set of non-contiguous functions), lookaside acceleration increases both the CPU cycle consumption and latency, impacting perf/Watt and perf/$$.\nTo reduce the cost of offload, the accelerator driver may combine or batch several requests together. However, this leads to undesired buffering and queuing, resulting in significantly higher latency for various user data flows.\nDiagram shows that lookaside acceleration consumes significantly higher PCIe and DDR bandwidth compared to inline acceleration.\nFigure 2. Comparison of PCIe/DDR bandwidth\nFigure 2 shows the expected host PCIe and the double data rate bandwidth (DDR BW) consumption (Gbps) as the number of supported 4-transmit-4-receive (4T4R) 100MHz cells increases. The graph shows that the aggregated transaction bandwidth needed to support four downlink (DL) layers and two uplink (UL) layers (for each 100MHz cell) when deploying with a lookaside accelerator is much worse. Approximately 40x more bandwidth is consumed when compared to the inline accelerator.\nIt is also worth mentioning that as the number of cells increases, PCIe Gen4 technology cannot sustain the required bandwidth and PCIe Gen5 technology is required to support the lookaside accelerator.\nLookaside quality of service guarantees increase complexity\nFine-grained QoS support for various user data flows is another challenge with lookaside accelerators. The complex queuing architecture required across the PCIe interface to meet the QoS needs can result in performance degradation and impact tail latency of queued requests to the accelerator.\nAs an example, consider a DU system that supports mixed user data flows for voice over Internet protocol (VoIP), Internet of things (IoT), enhanced mobile broadband (eMBB), and ultra-reliable low-latency communications (URLLC) applications. In the lookaside model, if a VoIP or URLLC packet gets stuck behind large blocks of eMBB data queued to the accelerator, it incurs significant latency and jitter and degrades the QoS. The fact that this can accumulate over time as every transaction needs to go through a Lookaside accelerator results in significant performance degradation.\nThere are ways to address these issues through QoS guarantees and hierarchical scheduling across the lookaside PCIe interface. However, this increases both the hardware and the software complexity, resulting in increased cost and energy consumption, as well as reduced cell capacity.\nTo further demonstrate the cell capacity and power efficiency benefits of deploying an inline accelerator compared to lookaside, we evaluated the performance of both acceleration modes with respect to two metrics for the following system configuration: 100 MHz, 4T4R, 4 DL/2 UL layers:\nNumber of 100MHz cells supported\nMHz*Layers/Watt\nBar charts show normalized cells and power efficiency for CPU only, CPU plus fixed-function lookaside acceleration, and NVIDIA Aerial inline acceleration.\nFigure 3 shows the performance comparison, with the number of supported cells metric (normalized) displayed on the left and the MHz*Layers/Watt metric displayed on the right. For each metric, the cell capacity and power efficiency benefits are clear when deploying an inline accelerator, compared to a fixed-function lookaside accelerator or no accelerator (that is, CPU only).\nLookaside accelerator integrated as a PCIe device is not equivalent to an inline accelerator\nSome argue that the integration of lookaside accelerators in the CPU makes it an inline architecture. Nothing could be farthest from the truth.\nWhile integration may result in limited power optimization and reduce the component price, any fixed-function lookaside accelerator, such as FEC, integrated in the CPU still manifests as a PCIe device and is accessed through DPDK BBDEV. The net effect is that the same inefficiency exists with fixed-function lookaside accelerators, no matter whether they are discrete components, or integrated in the CPU.\nIn fact, integrating lookaside accelerators introduce a new set of problems: managing specific CPU stock-keeping units (SKUs), juggling feature prioritization, increasing CPU costs, and so on.\nFixed-function acceleration is inherently not cloud-native\nThe key tenet of cloud computing is that the infrastructure resources can be shared across applications, increasing the utilization, and delivering better economies of scale.\nFixed-function accelerators (such as FPGA-based, low-density parity check (LDPC), SoC-based L1 high-PHY and so on) are single-purpose. When a fixed-function accelerator is not being used by 5G vRAN, it is a wasted resource that is not used by any other application.\nTypical 5G networks run at less than 50% average utilization. This means a fixed-function lookaside accelerator can just sit there in the cloud, not being used for more than 50% of the time. On the other hand, a general-purpose and programmable accelerator such as a GPU can be reused for other applications, such as large language model (LLM) training and inferencing, computer vision, and analytics.\nData Plane Development Kit Baseband Device ( DPDK BBDEV ) is the commonly used application programming interface (API) for lookaside acceleration. It is not well-suited for cloud-native deployments. DPDK has many constructs that were designed for high-performance, in-network appliances, including the following:\nHuge page tables\nPre-allocated buffers\nPinned memory\nSingle-root input/output virtualization (SR-IOV)\nQueue-centric enqueue-dequeue operations\nThese features, however, create a strong affinity towards the underlying hardware, not enabling seamless portability and workload movement in a true cloud-native fashion.\nFixed-function accelerators lack scalability\nA major drawback of fixed-function accelerators like FEC LDPC, discrete Fourier transform (DFT), inverse DFT (iDFT), and other selected baseband Layer 1 functions is that, while it may be right-sized for one configuration or use case, it is suboptimal for another configuration.\nTake FEC LDPC as an example. In a typical 5G frequency range 1 (FR1) sub-6 GHz system with 4T4R antenna and DDDSUUDDDD channel configuration (D: downlink, U: uplink, S: special) and 4 DL/2 UL layers, the LDPC decoder may constitute about 25% of the physical uplink shared channel (PUSCH) workload in an UL slot.\nKeeping other configurations unchanged, if the system dimension scales from a 4T4R to 64T64R antenna configuration (massive MIMO), the LDPC decoder compute load on the PUSCH pipeline, as it turns out, does not increase commensurately. In fact, in this higher dimensional system, LDPC constitutes approximately just 10% of the overall uplink workload.\nWhy does this happen? It\u2019s because the complexity of the LDPC decoder scales only linearly with the number of layers, where other algorithms, such as channel estimation or detection, scale superlinearly. This can easily lead to suboptimal designs from the point of view of resource utilization and power consumption, if these functions are implemented in fixed-function acceleration logic.\nFixed-function accelerators are not agile\nFixed-function accelerators are difficult to evolve with 3GPP releases (for example, with new features) as they are designed for a particular release of the specification. Upgrades of these complex algorithms running on fixed-function accelerators are difficult (especially when implemented in hardware), thereby throttling improvements over time. Also, hardware bug fixes are problematic to resolve, often resulting in costly replacement as being the only viable resolution.\nTo summarize, fixed-function lookaside acceleration has several drawbacks: an impact on performance and latency, lower energy efficiency, and lack of programmability and scalability. These issues directly result in higher CapEx and OpEx for telco operators.\nNext, we discuss an alternate approach taken by NVIDIA, that addresses many of the issues highlighted earlier by harnessing the principles of programmability and inline acceleration. This solution paves the way for industry-leading vRAN.\nNVIDIA Aerial: Programmable, GPU-based inline acceleration for vRAN\nNVIDIA has taken a thoughtful architectural approach to use the inline architecture for a full L1 offload to a programmable GPU. The architecture uses Bluefield DPUs to bring all fronthaul-enhanced common public radio interface (eCPRI) data traffic into the GPU without the CPU in the data path.\nA natural question to ask is why GPUs? The signal processing requirements of the 5G PHY are computationally challenging, compounded by intensive matrix operations. The massive parallelism of the GPU architecture brings the right hardware resources to support this class of workloads.\nFrom a developer perspective, GPUs are programmed using CUDA, the world\u2019s most commercially successful parallel programming framework. This makes your job simpler as you can use mature tools and expansive libraries for software lifecycle management including planning, design, development, optimization, testing, and maintenance. This has been proven by the widespread adoption of GPUs in the computationally complex field of AI and machine learning.\nThe second question is why inline? Inline architecture provides a full offload of vRAN L1 processing to the GPU without any CPU interactions. The interface for the offload is the functional application platform interface ( FAPI ), which is an industry standard developed within the Small Cell Forum ( SCF ). The full offload also avoids the complex and inefficient ping-pong effects of the lookaside model between the CPU and the accelerator across the host PCIe interface, resulting in the improved performance and lower latency explained earlier.\nNVIDIA Aerial, enabling fully programmable, cloud-native, AI-capable, and high-performance end-to-end L1 high-PHY (7.2-x split) inline acceleration is built upon two fundamental principles:\nAccelerated compute\nFast I/O\nAccelerated compute is manifested through the component CUDA baseband (cuBB), the software stack providing a GPU-accelerated 5G L1 signal processing pipeline. cuBB delivers unprecedented throughput and efficiency by keeping all PHY layer processing within the high-performance GPU memory. cuBB includes 5G L1 high-PHY acceleration library cuPHY, which is highly optimized for NVIDIA GPUs and offers unparalleled scalability by using a GPU\u2019s massive computing power and high degree of parallelism.\nFast I/O is manifested through the NVIDIA DOCA GPUNetIO module, providing optimized I/O and packet processing by exchanging packets directly between GPU memory and a GPUDirect-capable NVIDIA ConnectX SmartNIC. Enabling fast I/O processing and direct memory access (DMA) technology is essential in unleashing the full potential of inline acceleration.\nTowards that goal, the NVIDIA Aerial platform has adopted a GPU-centric approach, implemented with NVIDIA DOCA GPUNetIO Library. In this approach, an NVIDIA GPU directly interacts with an NVIDIA SmartNIC using GPUDirect Async Kernel-initiated Network (GDAKIN) communications to configure and update NIC registers for orchestrating network send and receive operations without the intervention of the CPU. For more information, see Inline GPU Packet Processing with NVIDIA DOCA GPUNetIO.\nDiagram shows that the programmable inline accelerator (L1 high-PHY offload) delivers 10-40x efficiency in PCIe bandwidth usage compared to the fixed-function lookaside accelerator (FEC offload using DPDK BBDEV).\nFigure 4. Comparison between fixed-function hardware-based lookaside acceleration and GPU-based programmable inline acceleration implementations for L1 high-PHY\nFigure 4 shows the architectural comparison for the PHY layer between the GPU-based inline acceleration implementation using NVIDIA Aerial and the typical fixed-function hardware accelerator (HWA)-based lookaside acceleration. On the right side, the NVIDIA Aerial platform offers a fast, efficient, and streamlined data flow all the way from L2 to L1 and to fronthaul, without requiring CPU staging copies or throttling of the host PCIe bandwidth.\nHigher level acceleration abstraction layer (AAL) between L2 and L1 (that is, FAPI)\nConverged architecture with GPU and DPU\nInterconnect powered by the NVIDIA DOCA GPUNetIO and GPUDirect technologies\nAs the entire L1 processing pipeline and corresponding data are contained within the GPU kernels and dynamic random-access memory (DRAM) on the same converged card, NVIDIA Aerial does not consume critical shared resources with L2+ (for example, host DRAM or host PCIe), unlike the traditional lookaside architecture (left).\nWith less CPU core consumption and a high degree of GPU parallelism in processing the entire L1 workload, the NVIDIA Aerial platform delivers a lower CapEx and OpEx solution with unmatched performance, scalability, agility, programmability, and energy efficiency.\nNVIDIA Aerial addresses the key requirements\nTable 1 presents a snapshot of the key requirements for 5G vRAN, the limitations of the lookaside architecture with a fixed-function accelerator in meeting these requirements, and the benefits of the inline architecture with a GPU-programmable accelerator in addressing those shortcomings.\nRequirements\nFixed-function lookaside architecture\nGPU-programmable inline architecture\nHigh performance and low latency\nMultiple requests and responses across PCIe lead to increased CPU consumption and worse perf/Watt and perf/$$. Higher L1 processing latency due to batching and queuing of lookaside requests.\nL2 \u2194 L1 \u2194 FH streamlined processing pipeline, no back-and-forth transactions over PCIe, leading to better perf/Watt and perf/$$. No buffering/queuing during L1 runtime, resulting in optimal L1 processing latency.\nCloud economics\nNo reuse: only does \u2018fixed\u2019 function and not sharable with other applications in cloud infrastructure.\nFully programmable and general purpose resulting in high resource utilization.\nApplication portability\nDPDK BBDEV: Not easily portable because of the strong affinity to hardware.\nFAPI: Better portability with higher level abstraction between L2 and L1.\nScalability\nDesigned and optimized for a specific system configuration.\nFully programmable and scalable for a range of system configs.\nAgility\nNot programmable, long design cycles, and difficult to update with evolving standards and algorithms.\nFully programmable and software-defined, easy to update for evolving standards and new algorithms.\nTable 1. Five key tenets of OpenRAN and the comparison between the fixed-function lookaside and the GPU programmable inline architectures\nConclusion\nIn this post, we highlighted the inefficiencies of fixed-function accelerators and the lookaside processing model. We showed you how the lookaside model impacts performance and energy efficiency along with many scalability challenges.\nThe inline processing model with a programmable accelerator addresses the technical bottlenecks of the fixed-function lookaside acceleration model and delivers high performance, energy efficiency, and scalability across various RAN configurations.\nNVIDIA Aerial is the only commercial platform that delivers the key tenets of emerging vRAN: high-performance, software-defined, COTS-based, cloud-native, and AI-ready. It implements the GPU-programmable inline processing model and full L1 offload to deliver efficient performance for a wide range of RAN configurations and use cases with a software architecture that is fully compliant with O-RAN standards.\nWe invite you to collaborate with us in our quest to modernize the RAN infrastructure and enable an efficient, high-performance, scalable, agile, cloud-native, fully software-defined, and AI-ready vRAN."}], "https://developer.nvidia.com/blog/comparing-solutions-for-boosting-data-center-redundancy/": [{"text": "The article discusses the importance of achieving system redundancy in data centers and compares the proprietary multi-chassis link aggregation group (MLAG) with the standards-based EVPN multihoming (EVPN-MH) solution. MLAG, while effective in providing redundancy, is vendor-dependent and complex, with issues such as state and MAC synchronization. On the other hand, EVPN-MH offers a more flexible and technically superior solution, using BGP as the control plane and providing fast convergence, control-plane based MAC and state synchronization, and fabric-wide route distribution. EVPN-MH also supports multihoming to end hosts with more than two gateways and active-active load balancing. The article recommends using EVPN-MH for modern data center networks, as it is a future-proof technology that fits well into the Clos architecture, reducing cost and complexity. Existing networks using MLAG can continue as is, but new deployments should consider adopting EVPN-MH for improved efficiency and scalability.", "text_components": ["Comparing Solutions for Boosting Data Center Redundancy\nIn today\u2019s data center, there are many ways to achieve system redundancy from a server connected to a fabric. Customers usually seek redundancy to increase service availability (such as achieving end-to-end AI workloads) and find system efficiency using different multihoming techniques.\nIn this post, we discuss the pros and cons of the well-known proprietary multi-chassis link aggregation group (MLAG) compared to standards-based EVPN multihoming (EVPN-MH).", "Introduction to MLAG\nMultihoming is necessary for all modern data centers, which enables a single host to connect to two or more nodes and serve in an all-active or single-active manner. All-active focuses on increasing capacity first and redundancy second. Single-active focuses primarily on redundancy.\nIn the Internet Service Provider world, multihoming is a familiar concept, primarily for Point of Presence locations, where customer equipment interconnects with Provider Edge equipment locations.\nThis connection is almost always a layer 3 routed connection and doesn\u2019t introduce the challenges of the layer 2 world because it is intended to solve redundant site access or Internet access. However, in data centers, when we connect servers or end nodes into the network in a redundant way, we must get down to layer 2.\nMLAG came along in the early 2010s and many vendors implemented similar features that performed similar functionalities. One important thing to keep in mind is that MLAG is vendor-dependent proprietary technology. According to Wikipedia, MLAG\u2019s \u201cimplementation varies by vendor; notably, the protocol existing between the chassis is proprietary.\u201d This is a fundamental problem for MLAG that triggers many other issues.\nImage shows a typical MLAG wiring.\nFigure 1. Typical MLAG wiring\nWith MLAG (Figure 1), a client device can be a server or hypervisor, and a switch or router forms a classical link aggregation group (LAG) that typically bonds two physical links into a single logical link. On the other side of these links, you typically have two switches, which these links connect to. From an LACP point of view, these two switches act like a single switch with the same LACP system ID. This makes MLAG work from a server perspective.\nHowever, for the two MLAG participating switches, things are a bit more complex. Because they require state and MAC synchronization between them, a heartbeat is also needed to prevent split-brain situations and traffic flow over peer links in case one of the participating switches loses its uplink. This peer link makes the entire design non-standard, complex, and error-prone (not fitting in a CLOS leaf and spine architecture).\nThere are efforts to make state and MAC synchronization standard. RFC7275 focuses on solving this problem and addressing it with a new protocol called Inter-Chassis Control Protocol (ICCP). However, different vendors still implement various flavors of RFC7275 and end up with the same issues. This MLAG solution solved the multihoming problem in a limited scope.\nWhile the future of MLAG is bleak, there\u2019s a more flexible and technically superior multihoming solution: EVPN multihoming (also called EVPN-LAG or ESI-LAG).", "Benefits of EVPN multihoming\nMultihoming is no stranger to the ISP world and initially came along as a WAN technology. However, it became clear that modern data centers require their own way of implementing multihoming.\nCoincidentally, EVPN itself was first introduced as a WAN technology, then evolved into a data center technology. EVPN adopted multihoming functionality rather quickly. With RFC7432, EVPN-MH uses a new addressing field called the Ethernet Segment Identifier (ESI). This fundamental building block that makes EVPN-MH work is used everywhere across the fabric, as far as type-1 and type-4 routes are propagated. ESI is a 10-byte field that specifies a specific multihomed segment.\nLet\u2019s talk about what is under the hood of EVPN-MH, route types, and what makes it attractive compared to legacy and proprietary MLAG.\nEVPN-MH uses Border Gateway Protocol (BGP) as the control plane in contrast to ICCP, which MLAG uses. And, EVPN-MH uses several different types of EVPN route types as per RFC7432.", "EVPN Route Type-1\nEVPN Type-1 Route functions can be listed as mass withdrawal, aliasing, and load sharing (Figure 2).", "Mass withdrawal\nMass withdrawal makes sure that if a particular link goes down on an ES, you can withdraw all dependent MAC addresses connected to that particular link. This way, you achieve fast convergence by sending a mass withdrawal instead of one by one for each MAC. This assumes that a hypervisor is connected to that ES with many VMs, over the same VLAN, or over hundreds of VLANs.", "Aliasing and load balancing\nAliasing and load balancing makes sure that downstream traffic towards an ES is load-balanced across ES member switches, also known as EVI. This way, the ES member switches can receive traffic from other switches in the fabric in a load-shared manner, regardless of whether they are advertising that particular MAC behind their ES.\nImage shows EVPN Type-1 Route frame format\nFigure 2. EVPN Ethernet Auto-Discovery route Type-1 frame format", "EVPN Route Type-2\nType-2 (MAC/IP) routes are advertised by the same ES member leafs and they include the ESI value for each MAC attached to this Ethernet Segment (Figure 3).\nImage shows EVPN Type-2 Route frame format\nFigure 3. EVPN MAC/IP Advertisement Route type-2 frame format\nType-2 routes are not part of the EVPN-MH setup, however, they make use of ESI information when it\u2019s present for a specific destination MAC.", "EVPN Route Type-4\nEVPN Type-4 routes are used for election of designated forwarder (DF) and autodiscovery of multihomed ES (Figure 4).\nImage shows EVPN Type-4 Route frame format\nFigure 4. EVPN Ethernet Segment Route type-4 frame format\nEVPN type-1 and type-4 routes make EVPN-MH work and provide standards-based interoperability. Type-4 routes are imported by only routers or leafs that participate in that particular ES. Other routers or leafs in the fabric that don\u2019t participate in that ES don\u2019t import type-4 routes. Type-4 routes are used for DF elections to select where to send local BUM traffic.As BUM traffic has to be flooded across the network, in multihomed scenarios, only the DF is responsible for sending BUM traffic to its clients (such as multihomed servers).\nTypical EVPN-MH topology can be seen in Figure 5.\nImage shows typical EVPN-MH wiring.\nFigure 5. Typical EVPN-MH wiring", "Advantages of EVPN-MH:\nControl-plane based MAC and state synchronization\nStandards-based, BGP EVPN route types, and interoperability\nFabric-wide-route distribution of multihomed connections\nFast convergence, withdrawal\nCapable of 2+ multihoming\nNo need for physical peer link connectivity\nFuture proof\nScalable with BGP", "Conclusion\nEVPN-MH is a future-proof technology that uses BGP as its control plane. Its standards-based architecture, ability to provide multihoming to end hosts with more than two gateways, and active-active load balancing make it an attractive de facto solution in modern data center networks. Also, removing the need for a peer link between leafs fits EVPN-MH into the Clos architecture perfectly, reducing cost and complexity.\nI recommend using EVPN-MH for data centers with EVPN as the control plane, which will soon substitute all MLAG deployments in the field. Existing networks can stay with MLAG as they are already operational. However, new deployments and designs should certainly be based on EVPN-MH.\nFor more resources, check out the NVIDIA Cumulus Linux Multi-Chassis Link Aggregation\u2013MLAG configuration guide."], "document_title": "Comparing Solutions for Boosting Data Center Redundancy", "document_url": "https://developer.nvidia.com/blog/comparing-solutions-for-boosting-data-center-redundancy/", "document_date": "2023-09-29T19:46:58", "document_date_modified": "2023-10-19T19:05:58", "document_full_text": "Comparing Solutions for Boosting Data Center Redundancy\nIn today\u2019s data center, there are many ways to achieve system redundancy from a server connected to a fabric. Customers usually seek redundancy to increase service availability (such as achieving end-to-end AI workloads) and find system efficiency using different multihoming techniques.\nIn this post, we discuss the pros and cons of the well-known proprietary multi-chassis link aggregation group (MLAG) compared to standards-based EVPN multihoming (EVPN-MH).\nIntroduction to MLAG\nMultihoming is necessary for all modern data centers, which enables a single host to connect to two or more nodes and serve in an all-active or single-active manner. All-active focuses on increasing capacity first and redundancy second. Single-active focuses primarily on redundancy.\nIn the Internet Service Provider world, multihoming is a familiar concept, primarily for Point of Presence locations, where customer equipment interconnects with Provider Edge equipment locations.\nThis connection is almost always a layer 3 routed connection and doesn\u2019t introduce the challenges of the layer 2 world because it is intended to solve redundant site access or Internet access. However, in data centers, when we connect servers or end nodes into the network in a redundant way, we must get down to layer 2.\nMLAG came along in the early 2010s and many vendors implemented similar features that performed similar functionalities. One important thing to keep in mind is that MLAG is vendor-dependent proprietary technology. According to Wikipedia, MLAG\u2019s \u201cimplementation varies by vendor; notably, the protocol existing between the chassis is proprietary.\u201d This is a fundamental problem for MLAG that triggers many other issues.\nImage shows a typical MLAG wiring.\nFigure 1. Typical MLAG wiring\nWith MLAG (Figure 1), a client device can be a server or hypervisor, and a switch or router forms a classical link aggregation group (LAG) that typically bonds two physical links into a single logical link. On the other side of these links, you typically have two switches, which these links connect to. From an LACP point of view, these two switches act like a single switch with the same LACP system ID. This makes MLAG work from a server perspective.\nHowever, for the two MLAG participating switches, things are a bit more complex. Because they require state and MAC synchronization between them, a heartbeat is also needed to prevent split-brain situations and traffic flow over peer links in case one of the participating switches loses its uplink. This peer link makes the entire design non-standard, complex, and error-prone (not fitting in a CLOS leaf and spine architecture).\nThere are efforts to make state and MAC synchronization standard. RFC7275 focuses on solving this problem and addressing it with a new protocol called Inter-Chassis Control Protocol (ICCP). However, different vendors still implement various flavors of RFC7275 and end up with the same issues. This MLAG solution solved the multihoming problem in a limited scope.\nWhile the future of MLAG is bleak, there\u2019s a more flexible and technically superior multihoming solution: EVPN multihoming (also called EVPN-LAG or ESI-LAG).\nBenefits of EVPN multihoming\nMultihoming is no stranger to the ISP world and initially came along as a WAN technology. However, it became clear that modern data centers require their own way of implementing multihoming.\nCoincidentally, EVPN itself was first introduced as a WAN technology, then evolved into a data center technology. EVPN adopted multihoming functionality rather quickly. With RFC7432, EVPN-MH uses a new addressing field called the Ethernet Segment Identifier (ESI). This fundamental building block that makes EVPN-MH work is used everywhere across the fabric, as far as type-1 and type-4 routes are propagated. ESI is a 10-byte field that specifies a specific multihomed segment.\nLet\u2019s talk about what is under the hood of EVPN-MH, route types, and what makes it attractive compared to legacy and proprietary MLAG.\nEVPN-MH uses Border Gateway Protocol (BGP) as the control plane in contrast to ICCP, which MLAG uses. And, EVPN-MH uses several different types of EVPN route types as per RFC7432.\nEVPN Route Type-1\nEVPN Type-1 Route functions can be listed as mass withdrawal, aliasing, and load sharing (Figure 2).\nMass withdrawal\nMass withdrawal makes sure that if a particular link goes down on an ES, you can withdraw all dependent MAC addresses connected to that particular link. This way, you achieve fast convergence by sending a mass withdrawal instead of one by one for each MAC. This assumes that a hypervisor is connected to that ES with many VMs, over the same VLAN, or over hundreds of VLANs.\nAliasing and load balancing\nAliasing and load balancing makes sure that downstream traffic towards an ES is load-balanced across ES member switches, also known as EVI. This way, the ES member switches can receive traffic from other switches in the fabric in a load-shared manner, regardless of whether they are advertising that particular MAC behind their ES.\nImage shows EVPN Type-1 Route frame format\nFigure 2. EVPN Ethernet Auto-Discovery route Type-1 frame format\nEVPN Route Type-2\nType-2 (MAC/IP) routes are advertised by the same ES member leafs and they include the ESI value for each MAC attached to this Ethernet Segment (Figure 3).\nImage shows EVPN Type-2 Route frame format\nFigure 3. EVPN MAC/IP Advertisement Route type-2 frame format\nType-2 routes are not part of the EVPN-MH setup, however, they make use of ESI information when it\u2019s present for a specific destination MAC.\nEVPN Route Type-4\nEVPN Type-4 routes are used for election of designated forwarder (DF) and autodiscovery of multihomed ES (Figure 4).\nImage shows EVPN Type-4 Route frame format\nFigure 4. EVPN Ethernet Segment Route type-4 frame format\nEVPN type-1 and type-4 routes make EVPN-MH work and provide standards-based interoperability. Type-4 routes are imported by only routers or leafs that participate in that particular ES. Other routers or leafs in the fabric that don\u2019t participate in that ES don\u2019t import type-4 routes. Type-4 routes are used for DF elections to select where to send local BUM traffic.As BUM traffic has to be flooded across the network, in multihomed scenarios, only the DF is responsible for sending BUM traffic to its clients (such as multihomed servers).\nTypical EVPN-MH topology can be seen in Figure 5.\nImage shows typical EVPN-MH wiring.\nFigure 5. Typical EVPN-MH wiring\nAdvantages of EVPN-MH:\nControl-plane based MAC and state synchronization\nStandards-based, BGP EVPN route types, and interoperability\nFabric-wide-route distribution of multihomed connections\nFast convergence, withdrawal\nCapable of 2+ multihoming\nNo need for physical peer link connectivity\nFuture proof\nScalable with BGP\nConclusion\nEVPN-MH is a future-proof technology that uses BGP as its control plane. Its standards-based architecture, ability to provide multihoming to end hosts with more than two gateways, and active-active load balancing make it an attractive de facto solution in modern data center networks. Also, removing the need for a peer link between leafs fits EVPN-MH into the Clos architecture perfectly, reducing cost and complexity.\nI recommend using EVPN-MH for data centers with EVPN as the control plane, which will soon substitute all MLAG deployments in the field. Existing networks can stay with MLAG as they are already operational. However, new deployments and designs should certainly be based on EVPN-MH.\nFor more resources, check out the NVIDIA Cumulus Linux Multi-Chassis Link Aggregation\u2013MLAG configuration guide."}], "https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/": [{"text": "The article discusses the challenges of performing exhaustive exact k-nearest neighbor (kNN) search in large datasets, as it can be expensive and inefficient. To address this issue, the RAPIDS RAFT library offers the IVF-Flat algorithm, which uses an inverted file index to accelerate vector search by grouping dataset vectors into clusters. By searching only a few clusters instead of the entire dataset, IVF-Flat significantly improves search time while still maintaining accuracy. The article provides insights into how to build an index, tune parameters for index building and search, and optimize performance using GPUs. Benchmarks show that the RAFT IVF-Flat algorithm on GPU outperforms CPU implementations by a significant margin, making it a valuable tool for accelerating vector search tasks. The article also mentions ongoing collaborations to integrate RAFT into the FAISS library for even broader accessibility. Overall, the RAFT library offers efficient and scalable solutions for vector search, and users are encouraged to explore its capabilities and provide feedback for further improvement.", "text_components": ["Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat\nPerforming an exhaustive exact k-nearest neighbor (kNN) search, also known as brute-force search, is expensive, and it doesn\u2019t scale particularly well to larger datasets. During vector search, brute-force search requires the distance to be calculated between every query vector and database vector. For the frequently used Euclidean and cosine distances, the computation task becomes equivalent to a large matrix multiplication.\nAlthough GPUs are efficient at performing matrix multiplications, the computational cost becomes prohibitive with increasing data volumes. Yet many applications don\u2019t require exact results and can instead trade off some accuracy for faster searches. When exact results are not needed, approximate nearest neighbor (ANN) methods can often reduce the number of distance computations that must be performed during search.\nThis post focuses on IVF-Flat, an ANN algorithm found in RAPIDS RAFT. The IVF-Flat method uses an inverted file index (IVF) with unmodified (that is, flat) vectors. This algorithm provides simple knobs to reduce the overall search space and to trade-off accuracy for speed.\nTo help you understand how to use IVF-Flat, we discuss how the algorithm works, and demonstrate the usage of both the Python and C++ APIs in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. These steps can also be followed in the example Python notebook and C++ project. Finally, we demonstrate that GPU-accelerated vector search can be an order of magnitude faster than CPU search.", "IVF-Flat algorithm\nIVF methods accelerate vector search by grouping the dataset vectors into clusters and limiting the search to some number of nearest clusters for each query (Figure 1).\nSearching only a few clusters (instead of the whole dataset) is the actual approximation in the IVF-Flat algorithm. Using this approximation, you might miss some neighbors that are assigned to clusters you aren\u2019t searching, but it greatly improves search time.\nTwo diagrams show a) dataset points grouped into clusters and b) a subset of the clusters highlighted.\nFigure 1. A dataset divided into clusters (left), and search is restricted to clusters in the vicinity of the queries (right)\nBefore you can search the dataset, you must build an index, which is a structure that stores the information that you need for efficient search. For IVF-Flat, the index stores the description of the clusters: the coordinates of their center, and the list of vectors that belong to the cluster. This list is the inverted list, also known as an inverted file, and that is where the IVF acronym comes from.\nIn the following sections, after discussing inverted files, we demonstrate how to construct an index and explain how the search is performed.", "IVF meaning\nFor completeness, here\u2019s some historical context. The term inverted file (or inverted index) comes from the information retrieval field.\nConsider a simple example of a few text documents. To search documents that contain a given word, a forward index stores a list of words for each document. You must read each document explicitly to find the relevant ones.\nIn contrast, an inverted index would contain a dictionary of all the words that you can search, and for each word, you have a list of document indices where the word occurs. This is the inverted list (inverted file), and it enables you to restrict the search to the selected lists.\nToday, text data is often represented as vector embeddings. The IVF-Flat method defines cluster centers and these centers are analogous to the dictionary of words in the preceding example. For each cluster center, you have a list of vector indices that belong to the cluster, and search is accelerated because you only have to inspect the selected clusters.", "Index building\nThe index building is mainly a clustering operation on the dataset. An ```ivf_flat``` index can be created in Python using the following code example:\n```\nfrom pylibraft.neighbors import ivf_flat\n\nbuild_params = ivf_flat.IndexParams(\n        n_lists=1024,\n        metric=\"sqeuclidean\"\n    )\n\nindex = ivf_flat.build(build_params, dataset)\n```\nIn C++, you have the following syntax:\n```\n#include <raft/neighbors/ivf_flat.cuh> \nusing namespace raft::neighbors;\nraft::device_resources dev_resources;\n\nivf_flat::index_params index_params;\nindex_params.n_lists = 1024;\nindex_params.metric = raft::distance::DistanceType::L2Expanded;\n\nauto index = ivf_flat::build(dev_resources, index_params,\nraft::make_const_mdspan(dataset.view()));\n```\nThe most important hyperparameter for creating the index is ```n_lists```, which tells how many clusters to use. You also specify the metric for distance calculation.", "Search\nAfter the index is built, search is simple. In Python, the following call returns two arrays: the indices of the neighbors and their distances from the query vectors:\n```\ndistances, indices = ivf_flat.search(ivf_flat.SearchParams(n_probes=50), index, queries, k=10)\n```\nThe equivalent call in C++ requires preallocating the output arrays:\n```\nint topk = 10;\nauto neighbors = raft::make_device_matrix<int64_t, int64_t>(dev_resources, n_queries, topk);\nauto distances = raft::make_device_matrix<float, int64_t>(dev_resources, n_queries, topk);\n\nivf_flat::search_params search_params;\nsearch_params.n_probes = 50;\n\nivf_flat::search(dev_resources,\n                search_params,\n                index,\n                raft::make_const_mdspan(queries.view()),\n                neighbors.view(),\n                distances.view());\n```\nHere you search ```k=10``` neighbors for each query. The parameter ```n_probes``` tells you how many clusters to search (or probe) for each query, and it determines the accuracy of the search.\nBy testing only ```n_probes``` clusters for each query, you might omit some neighbors that were assigned to clusters whose centers are farther from the query point. The quality of the search is usually measured as the recall rate, which is the fraction of the actual nearest k-neighbors out of all the returned neighbors.\nInternally, the search is performed in two steps (Figure 2):\nThe coarse search selects ```n_probes``` nearby clusters for each query.\nA fine search compares the query vectors to all the dataset vectors in the selected clusters.\nDiagram of clusters represented by their centers with the clusters highlighted that are closest to the queries. Selected clusters shown with the individual points within these clusters.\nFigure 2. Two-step search: select nearby clusters by comparing the queries to cluster centers (left) and compare all the vectors in the selected clusters to the corresponding queries (right)", "Coarse search\nThe coarse search is done using an exact kNN search between the cluster centers and the query vectors. Select the nearest cluster centers, ```n_probes clusters``` for each query. Coarse search is relatively cheap because the number of clusters is much smaller than the dataset size (for example, 10K clusters for 100M vectors).", "Fine search\nFor IVF-Flat, the fine search is again an exact search. But each query has its own set of clusters to search (to probe), and the distance between the query vector and all the vectors in the probed clusters are calculated.\nFor small batch sizes, the regions that you search around a query point do not overlap. Therefore, the problem structure becomes a batched matrix-vector multiplication (GEMV) operation. This operation is memory bandwidth bound, and the large bandwidth of GPU memory greatly accelerates this step.\nThe top-k neighbors from each probed cluster are selected, which results in ```n_probes * k neighbor candidates``` for each query. This is reduced to the k-nearest neighbors.", "Tuning parameters for index building\nIn the previous sections, you got an overview of the index building and search. Here\u2019s a detailed look at how to set the parameters for index building.\nConstruction of the index consists of two phases:\nTraining or computing the clusters (build): A balanced hierarchical k-means algorithm clusters the training data.\nAdding the dataset vectors to the index (extend): Dataset vectors are assigned to their cluster and added to the appropriate list of vectors in the clusters.", "Number of clusters\nThe ```n_lists``` parameter has a profound impact on overall performance during both training and search: it defines the number of clusters into which the index data is partitioned. Setting ```n_lists = sqrt(n_samples)``` is a good starting point (where ```n_samples``` is the number of vectors in the dataset).\nTo make sure that the GPU resources are used efficiently, the average cluster size (that is, ```n_samples/n_lists``` ) should be in the range of at least 1K vectors to keep individual streaming multiprocessors (SMs) busy.", "Index building with automatic data subsampling\nK-means clustering is compute-intensive. To accelerate index building, sub-sample the dataset. Using parameter ```kmeans_trainset_fraction=0.1``` means that you use one-tenth of the dataset for training the cluster centers.\n```\nbuild_params = ivf_flat.IndexParams(\n        n_lists=1024,\n        metric=\"sqeuclidean\",\n        kmeans_trainset_fraction=0.1,\n        kmeans_n_iters=20\n    )\n```\nThe ```kmeans_n_iters``` parameter is passed directly to the k-means algorithm during training. It\u2019s set to a reasonable default of 20, which works for most datasets. However, this parameter is just a recommendation for the clustering algorithm. Under the hood, it usually performs more iterations in a \u201cbalancing\u201d phase to make sure the clusters have similar sizes.", "Index building with specific training data for clustering\nIn the previous examples, a single call to ```ivf_flat.build``` performed the clustering and added the whole dataset into the index. Alternatively, you could call ```ivf_flat.build``` to train the vectors without adding them to the index (by setting ```add_data_on_build=False``` ). This allows exact control of what vectors are used for training the index. Subsequently, ```ivf_flat.extend``` can be used to add vectors to the index.\nThis is shown in the following Python code example:\n```\nn_train = 10000\ntrain_set = dataset[cp.random.choice(dataset.shape[0], n_train, replace=False),:]\n\nbuild_params = ivf_flat.IndexParams(\n        n_lists=1024,\n        metric=\"sqeuclidean\",\n        kmeans_trainset_fraction=1,\n        kmeans_n_iters=20,\n        add_data_on_build=False\n    )\n\nindex = ivf_flat.build(build_params, train_set)\nivf_flat.extend(index, dataset, cp.arange(dataset.shape[0], dtype=cp.int64))\n```\nThe dataset vectors can be added to the index by a single call to ```ivf_flat.extend```. Internally, the data is processed batch-wise if needed to reduce memory consumption. The corresponding C++ code is as follows:\n```\nindex_params.add_data_on_build = false;\n// Sub sample the dataset to create trainset.\n// ...\n// Run k-means clustering using the training set\nauto index = ivf_flat::build(dev_resources, index_params,\n    raft::make_const_mdspan(trainset.view()));\n\n// Fill the index with the dataset vectors\nindex = ivf_flat::extend(dev_resources,\n    raft::make_const_mdspan(dataset.view()),\n    std::optional<raft::device_vector_view<const int64_t, int64_t>>(),\n    index);\n```", "Adding new vectors to the index\nNew vectors can be added at any time to the dataset by calling ```ivf_flat.extend```. By default, the cost of growing the list of vectors is amortized away by allocating extra space when the list size is increased. C++ API users can change this behavior by setting the following parameter:\n```\nindex_params.conservative_memory_allocation = true;\n```\nThis can be beneficial if the number of clusters is large, and it is not expected to add vectors often.\nBy default, the cluster centers do not change when you add vectors to the dataset. The ```adaptive_centers``` flag can be enabled during index construction if you want the cluster centers to drift with the new data.", "Tuning parameters for search\nHere\u2019s how to set the parameters for search: use GPU resources efficiently and increase the value of ```n_probes```.", "GPU resources\nDuring search, you create internal workspace memory. We recommend using a pooling allocator to reduce the overhead of memory allocation.\nConstructing the RAFT ```resources``` object is time-consuming. The ```resources``` object should be reused by passing a resource handle to the search function. In Python, you can configure the device resources and the memory pool in the following way:\n```\nfrom pylibraft.common import DeviceResources\nimport rmm\nmr = rmm.mr.PoolMemoryResource(\n     rmm.mr.CudaMemoryResource(),\n     initial_pool_size=2**30\n)\nrmm.mr.set_current_device_resource(mr)\n\nhandle = DeviceResources()\n\nsearch_params = ivf_flat.SearchParams(n_probes=50)\ndistances, indices = ivf_flat.search(search_params, index, queries, k=10, handle=handle)\nhandle.sync()\n```\nUsers of the C++ API always have to pass an explicit device_resources handle, and this should be reused among separate calls to search. The pool allocator can be set up in the following way:\n```\nraft::device_resources dev_resources;\nraft::resource::set_workspace_to_pool_resource(\n    dev_resources, 2 * 1024 * 1024 * 1024ull);\nivf_flat::search(dev_resources, ...)\n```\nC++ users can specify a separate allocator for temporary workspace arrays, and this is used in the preceding example. The global allocator (used for creating input/output arrays) can be set using rmm::mr::set_current_device_resource.", "Number of probes\nThe ratio ```n_probes/n_lists``` tells what fraction of the dataset is compared to each query. The number of distance computations is reduced to the ```n_probes/n_clusters``` fraction of what brute force search would compute. The quality of the search, as well as the compute time, increases as you increase ```n_probes```, and the right value depends on the dataset.\nIn Figure 3 and Figure 4, respectively, you can observe how throughput (queries per second) and search accuracy (recall) depends on the number of probes. Here, you are searching through 100M vectors from the DEEP1B dataset, and an H100 GPU is used for the search.\nThe throughput is inversely proportional to the number of probes. The dataset was divided into 100 thousand clusters. Searching just the 100 closest clusters for each query leads to a recall of 96% and searching 1000 clusters (1% of the dataset) leads to an accuracy of 99.8%.\nThe throughput graph follows 1/x trend.\nFigure 3. Search throughput (queries per second) as a function of the n_probes search parameter\nSearch accuracy graph shows that recall improves quickly as you increase n_probes from 20 to 200 and flattens out above that (region with 99% recall).\nFigure 4. Accuracy (recall) as a function of the n_probes search parameter\nWe often combine these plots in a single QPS vs. recall plot (Figure 5). This is useful when you want to have a compact picture of the trade-off between accuracy and search throughput. It is also beneficial while comparing different ANN methods.\nGraph shows that the QPS drops when you require high recall.\nFigure 5. Combined QPS-recall plot\nIf ```n_lists == n_probes```, that is like an exact (brute force) search: you compare all dataset vectors to all query vectors. You\u2019d expect the recall to be equal to 1 in such a case (apart from small round-off errors).\nAs ```n_probes``` approach ```n_lists```, IVF-Flat becomes slower than brute force because of the extra work the algorithm does (coarse plus fine search). In practice, searching around 0.1-1% of lists is enough for many datasets. But this depends on how well the input can be clustered.\nDue to the surprising behavior of distance metrics in high dimensions space, clustering becomes difficult if the dataset has no structure (for example, uniform random numbers). In those cases, IVF methods don\u2019t work well.", "Performance\nThe RAFT library provides a fast implementation of the IVF-Flat algorithm. Indexing 100M vectors can be done in under a minute (Figure 6). This is 14x faster than a CPU.\nBar chart showing high index building time on the CPU and significantly faster times with GPU implementations.\nFigure 6. Index build times for different dataset and cluster sizes\nMeasurements were performed on an NVIDIA H100 SXM GPU using RAFT 23.10 for GPU tests and on Intel Xeon Platinum 8480CL CPU with FAISS 1.7.4.\nThere are two main factors that enable this speedup:\nHigh compute throughput of the GPU: RAFT uses Tensor Cores to accelerate the k-means clustering during index building.\nThe improved algorithm: RAFT uses a balanced hierarchical k-means clustering, which clusters the dataset efficiently even as the number of vectors reaches hundreds of millions.\nYou can also observe that the time to construct the index increases linearly with the number of vectors, and linearly with the number of clusters.\nSearching through the index is facilitated by the high memory throughput of the GPU. RAFT\u2019s IVF-Flat index uses an optimized memory layout. The vectors are interleaved for vectorized memory access to ensure large bandwidth utilization while looping through the dataset vectors in each probed cluster.\nAnother important step during the fine search is to filter out the top-k candidates. We have highly optimized methods to select the top-k candidates. We use optimized block-select-k kernel fused into the distance computation kernels. This enables a more than 20x speedup (at recall=0.95), when we compare the performance of RAFT IVF-Flat to a CPU implementation, as the plot in Figure 7 shows.\nGraph compares IVF-Flat search throughput on the GPU and on the CPU.\nFigure 7. Search throughput for different levels of recall (accuracy)\nFor the purpose of this benchmark, the CPU implementation of FAISS IVF-Flat was used. FAISS also provides a GPU implementation of this algorithm. If you use FAISS, you can already benefit from GPU acceleration with a minor change in your code. We are collaborating with Meta to bring the performance improvements from RAFT to FAISS, so you will soon be able to use RAFT through FAISS as well.", "Summary\nWhen performing vector search in large databases, it\u2019s important to be aware of the high cost of an exact search, as it can result in low latency not suitable for online services.\nThe RAPIDS RAFT library provides efficient algorithms that improve vector search latency and throughput by focusing the search to the most relevant part of the dataset. This post discussed how the RAFT IVF-Flat algorithm works and how to set the parameters for index building and searching. Finally, we presented benchmarks to highlight the superior performance of GPUs for IVF-Flat search. You can test it out yourself using our benchmark tools.\nRAFT is an open-source library for vector search and more. It provides an easy-to-use C++ and Python API so you can integrate GPU-accelerated vector search into your applications. We love to hear your feedback! Send us questions and report issues on the /rapidsai/raft GitHub repo. You can also find us at @rapidsai."], "document_title": "Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat", "document_url": "https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/", "document_date": "2023-10-02T18:16:58", "document_date_modified": "2023-10-19T19:05:57", "document_full_text": "Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat\nPerforming an exhaustive exact k-nearest neighbor (kNN) search, also known as brute-force search, is expensive, and it doesn\u2019t scale particularly well to larger datasets. During vector search, brute-force search requires the distance to be calculated between every query vector and database vector. For the frequently used Euclidean and cosine distances, the computation task becomes equivalent to a large matrix multiplication.\nAlthough GPUs are efficient at performing matrix multiplications, the computational cost becomes prohibitive with increasing data volumes. Yet many applications don\u2019t require exact results and can instead trade off some accuracy for faster searches. When exact results are not needed, approximate nearest neighbor (ANN) methods can often reduce the number of distance computations that must be performed during search.\nThis post focuses on IVF-Flat, an ANN algorithm found in RAPIDS RAFT. The IVF-Flat method uses an inverted file index (IVF) with unmodified (that is, flat) vectors. This algorithm provides simple knobs to reduce the overall search space and to trade-off accuracy for speed.\nTo help you understand how to use IVF-Flat, we discuss how the algorithm works, and demonstrate the usage of both the Python and C++ APIs in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. These steps can also be followed in the example Python notebook and C++ project. Finally, we demonstrate that GPU-accelerated vector search can be an order of magnitude faster than CPU search.\nIVF-Flat algorithm\nIVF methods accelerate vector search by grouping the dataset vectors into clusters and limiting the search to some number of nearest clusters for each query (Figure 1).\nSearching only a few clusters (instead of the whole dataset) is the actual approximation in the IVF-Flat algorithm. Using this approximation, you might miss some neighbors that are assigned to clusters you aren\u2019t searching, but it greatly improves search time.\nTwo diagrams show a) dataset points grouped into clusters and b) a subset of the clusters highlighted.\nFigure 1. A dataset divided into clusters (left), and search is restricted to clusters in the vicinity of the queries (right)\nBefore you can search the dataset, you must build an index, which is a structure that stores the information that you need for efficient search. For IVF-Flat, the index stores the description of the clusters: the coordinates of their center, and the list of vectors that belong to the cluster. This list is the inverted list, also known as an inverted file, and that is where the IVF acronym comes from.\nIn the following sections, after discussing inverted files, we demonstrate how to construct an index and explain how the search is performed.\nIVF meaning\nFor completeness, here\u2019s some historical context. The term inverted file (or inverted index) comes from the information retrieval field.\nConsider a simple example of a few text documents. To search documents that contain a given word, a forward index stores a list of words for each document. You must read each document explicitly to find the relevant ones.\nIn contrast, an inverted index would contain a dictionary of all the words that you can search, and for each word, you have a list of document indices where the word occurs. This is the inverted list (inverted file), and it enables you to restrict the search to the selected lists.\nToday, text data is often represented as vector embeddings. The IVF-Flat method defines cluster centers and these centers are analogous to the dictionary of words in the preceding example. For each cluster center, you have a list of vector indices that belong to the cluster, and search is accelerated because you only have to inspect the selected clusters.\nIndex building\nThe index building is mainly a clustering operation on the dataset. An ```ivf_flat``` index can be created in Python using the following code example:\n```\nfrom pylibraft.neighbors import ivf_flat\n\nbuild_params = ivf_flat.IndexParams(\n        n_lists=1024,\n        metric=\"sqeuclidean\"\n    )\n\nindex = ivf_flat.build(build_params, dataset)\n```\nIn C++, you have the following syntax:\n```\n#include <raft/neighbors/ivf_flat.cuh> \nusing namespace raft::neighbors;\nraft::device_resources dev_resources;\n\nivf_flat::index_params index_params;\nindex_params.n_lists = 1024;\nindex_params.metric = raft::distance::DistanceType::L2Expanded;\n\nauto index = ivf_flat::build(dev_resources, index_params,\nraft::make_const_mdspan(dataset.view()));\n```\nThe most important hyperparameter for creating the index is ```n_lists```, which tells how many clusters to use. You also specify the metric for distance calculation.\nSearch\nAfter the index is built, search is simple. In Python, the following call returns two arrays: the indices of the neighbors and their distances from the query vectors:\n```\ndistances, indices = ivf_flat.search(ivf_flat.SearchParams(n_probes=50), index, queries, k=10)\n```\nThe equivalent call in C++ requires preallocating the output arrays:\n```\nint topk = 10;\nauto neighbors = raft::make_device_matrix<int64_t, int64_t>(dev_resources, n_queries, topk);\nauto distances = raft::make_device_matrix<float, int64_t>(dev_resources, n_queries, topk);\n\nivf_flat::search_params search_params;\nsearch_params.n_probes = 50;\n\nivf_flat::search(dev_resources,\n                search_params,\n                index,\n                raft::make_const_mdspan(queries.view()),\n                neighbors.view(),\n                distances.view());\n```\nHere you search ```k=10``` neighbors for each query. The parameter ```n_probes``` tells you how many clusters to search (or probe) for each query, and it determines the accuracy of the search.\nBy testing only ```n_probes``` clusters for each query, you might omit some neighbors that were assigned to clusters whose centers are farther from the query point. The quality of the search is usually measured as the recall rate, which is the fraction of the actual nearest k-neighbors out of all the returned neighbors.\nInternally, the search is performed in two steps (Figure 2):\nThe coarse search selects ```n_probes``` nearby clusters for each query.\nA fine search compares the query vectors to all the dataset vectors in the selected clusters.\nDiagram of clusters represented by their centers with the clusters highlighted that are closest to the queries. Selected clusters shown with the individual points within these clusters.\nFigure 2. Two-step search: select nearby clusters by comparing the queries to cluster centers (left) and compare all the vectors in the selected clusters to the corresponding queries (right)\nCoarse search\nThe coarse search is done using an exact kNN search between the cluster centers and the query vectors. Select the nearest cluster centers, ```n_probes clusters``` for each query. Coarse search is relatively cheap because the number of clusters is much smaller than the dataset size (for example, 10K clusters for 100M vectors).\nFine search\nFor IVF-Flat, the fine search is again an exact search. But each query has its own set of clusters to search (to probe), and the distance between the query vector and all the vectors in the probed clusters are calculated.\nFor small batch sizes, the regions that you search around a query point do not overlap. Therefore, the problem structure becomes a batched matrix-vector multiplication (GEMV) operation. This operation is memory bandwidth bound, and the large bandwidth of GPU memory greatly accelerates this step.\nThe top-k neighbors from each probed cluster are selected, which results in ```n_probes * k neighbor candidates``` for each query. This is reduced to the k-nearest neighbors.\nTuning parameters for index building\nIn the previous sections, you got an overview of the index building and search. Here\u2019s a detailed look at how to set the parameters for index building.\nConstruction of the index consists of two phases:\nTraining or computing the clusters (build): A balanced hierarchical k-means algorithm clusters the training data.\nAdding the dataset vectors to the index (extend): Dataset vectors are assigned to their cluster and added to the appropriate list of vectors in the clusters.\nNumber of clusters\nThe ```n_lists``` parameter has a profound impact on overall performance during both training and search: it defines the number of clusters into which the index data is partitioned. Setting ```n_lists = sqrt(n_samples)``` is a good starting point (where ```n_samples``` is the number of vectors in the dataset).\nTo make sure that the GPU resources are used efficiently, the average cluster size (that is, ```n_samples/n_lists``` ) should be in the range of at least 1K vectors to keep individual streaming multiprocessors (SMs) busy.\nIndex building with automatic data subsampling\nK-means clustering is compute-intensive. To accelerate index building, sub-sample the dataset. Using parameter ```kmeans_trainset_fraction=0.1``` means that you use one-tenth of the dataset for training the cluster centers.\n```\nbuild_params = ivf_flat.IndexParams(\n        n_lists=1024,\n        metric=\"sqeuclidean\",\n        kmeans_trainset_fraction=0.1,\n        kmeans_n_iters=20\n    )\n```\nThe ```kmeans_n_iters``` parameter is passed directly to the k-means algorithm during training. It\u2019s set to a reasonable default of 20, which works for most datasets. However, this parameter is just a recommendation for the clustering algorithm. Under the hood, it usually performs more iterations in a \u201cbalancing\u201d phase to make sure the clusters have similar sizes.\nIndex building with specific training data for clustering\nIn the previous examples, a single call to ```ivf_flat.build``` performed the clustering and added the whole dataset into the index. Alternatively, you could call ```ivf_flat.build``` to train the vectors without adding them to the index (by setting ```add_data_on_build=False``` ). This allows exact control of what vectors are used for training the index. Subsequently, ```ivf_flat.extend``` can be used to add vectors to the index.\nThis is shown in the following Python code example:\n```\nn_train = 10000\ntrain_set = dataset[cp.random.choice(dataset.shape[0], n_train, replace=False),:]\n\nbuild_params = ivf_flat.IndexParams(\n        n_lists=1024,\n        metric=\"sqeuclidean\",\n        kmeans_trainset_fraction=1,\n        kmeans_n_iters=20,\n        add_data_on_build=False\n    )\n\nindex = ivf_flat.build(build_params, train_set)\nivf_flat.extend(index, dataset, cp.arange(dataset.shape[0], dtype=cp.int64))\n```\nThe dataset vectors can be added to the index by a single call to ```ivf_flat.extend```. Internally, the data is processed batch-wise if needed to reduce memory consumption. The corresponding C++ code is as follows:\n```\nindex_params.add_data_on_build = false;\n// Sub sample the dataset to create trainset.\n// ...\n// Run k-means clustering using the training set\nauto index = ivf_flat::build(dev_resources, index_params,\n    raft::make_const_mdspan(trainset.view()));\n\n// Fill the index with the dataset vectors\nindex = ivf_flat::extend(dev_resources,\n    raft::make_const_mdspan(dataset.view()),\n    std::optional<raft::device_vector_view<const int64_t, int64_t>>(),\n    index);\n```\nAdding new vectors to the index\nNew vectors can be added at any time to the dataset by calling ```ivf_flat.extend```. By default, the cost of growing the list of vectors is amortized away by allocating extra space when the list size is increased. C++ API users can change this behavior by setting the following parameter:\n```\nindex_params.conservative_memory_allocation = true;\n```\nThis can be beneficial if the number of clusters is large, and it is not expected to add vectors often.\nBy default, the cluster centers do not change when you add vectors to the dataset. The ```adaptive_centers``` flag can be enabled during index construction if you want the cluster centers to drift with the new data.\nTuning parameters for search\nHere\u2019s how to set the parameters for search: use GPU resources efficiently and increase the value of ```n_probes```.\nGPU resources\nDuring search, you create internal workspace memory. We recommend using a pooling allocator to reduce the overhead of memory allocation.\nConstructing the RAFT ```resources``` object is time-consuming. The ```resources``` object should be reused by passing a resource handle to the search function. In Python, you can configure the device resources and the memory pool in the following way:\n```\nfrom pylibraft.common import DeviceResources\nimport rmm\nmr = rmm.mr.PoolMemoryResource(\n     rmm.mr.CudaMemoryResource(),\n     initial_pool_size=2**30\n)\nrmm.mr.set_current_device_resource(mr)\n\nhandle = DeviceResources()\n\nsearch_params = ivf_flat.SearchParams(n_probes=50)\ndistances, indices = ivf_flat.search(search_params, index, queries, k=10, handle=handle)\nhandle.sync()\n```\nUsers of the C++ API always have to pass an explicit device_resources handle, and this should be reused among separate calls to search. The pool allocator can be set up in the following way:\n```\nraft::device_resources dev_resources;\nraft::resource::set_workspace_to_pool_resource(\n    dev_resources, 2 * 1024 * 1024 * 1024ull);\nivf_flat::search(dev_resources, ...)\n```\nC++ users can specify a separate allocator for temporary workspace arrays, and this is used in the preceding example. The global allocator (used for creating input/output arrays) can be set using rmm::mr::set_current_device_resource.\nNumber of probes\nThe ratio ```n_probes/n_lists``` tells what fraction of the dataset is compared to each query. The number of distance computations is reduced to the ```n_probes/n_clusters``` fraction of what brute force search would compute. The quality of the search, as well as the compute time, increases as you increase ```n_probes```, and the right value depends on the dataset.\nIn Figure 3 and Figure 4, respectively, you can observe how throughput (queries per second) and search accuracy (recall) depends on the number of probes. Here, you are searching through 100M vectors from the DEEP1B dataset, and an H100 GPU is used for the search.\nThe throughput is inversely proportional to the number of probes. The dataset was divided into 100 thousand clusters. Searching just the 100 closest clusters for each query leads to a recall of 96% and searching 1000 clusters (1% of the dataset) leads to an accuracy of 99.8%.\nThe throughput graph follows 1/x trend.\nFigure 3. Search throughput (queries per second) as a function of the n_probes search parameter\nSearch accuracy graph shows that recall improves quickly as you increase n_probes from 20 to 200 and flattens out above that (region with 99% recall).\nFigure 4. Accuracy (recall) as a function of the n_probes search parameter\nWe often combine these plots in a single QPS vs. recall plot (Figure 5). This is useful when you want to have a compact picture of the trade-off between accuracy and search throughput. It is also beneficial while comparing different ANN methods.\nGraph shows that the QPS drops when you require high recall.\nFigure 5. Combined QPS-recall plot\nIf ```n_lists == n_probes```, that is like an exact (brute force) search: you compare all dataset vectors to all query vectors. You\u2019d expect the recall to be equal to 1 in such a case (apart from small round-off errors).\nAs ```n_probes``` approach ```n_lists```, IVF-Flat becomes slower than brute force because of the extra work the algorithm does (coarse plus fine search). In practice, searching around 0.1-1% of lists is enough for many datasets. But this depends on how well the input can be clustered.\nDue to the surprising behavior of distance metrics in high dimensions space, clustering becomes difficult if the dataset has no structure (for example, uniform random numbers). In those cases, IVF methods don\u2019t work well.\nPerformance\nThe RAFT library provides a fast implementation of the IVF-Flat algorithm. Indexing 100M vectors can be done in under a minute (Figure 6). This is 14x faster than a CPU.\nBar chart showing high index building time on the CPU and significantly faster times with GPU implementations.\nFigure 6. Index build times for different dataset and cluster sizes\nMeasurements were performed on an NVIDIA H100 SXM GPU using RAFT 23.10 for GPU tests and on Intel Xeon Platinum 8480CL CPU with FAISS 1.7.4.\nThere are two main factors that enable this speedup:\nHigh compute throughput of the GPU: RAFT uses Tensor Cores to accelerate the k-means clustering during index building.\nThe improved algorithm: RAFT uses a balanced hierarchical k-means clustering, which clusters the dataset efficiently even as the number of vectors reaches hundreds of millions.\nYou can also observe that the time to construct the index increases linearly with the number of vectors, and linearly with the number of clusters.\nSearching through the index is facilitated by the high memory throughput of the GPU. RAFT\u2019s IVF-Flat index uses an optimized memory layout. The vectors are interleaved for vectorized memory access to ensure large bandwidth utilization while looping through the dataset vectors in each probed cluster.\nAnother important step during the fine search is to filter out the top-k candidates. We have highly optimized methods to select the top-k candidates. We use optimized block-select-k kernel fused into the distance computation kernels. This enables a more than 20x speedup (at recall=0.95), when we compare the performance of RAFT IVF-Flat to a CPU implementation, as the plot in Figure 7 shows.\nGraph compares IVF-Flat search throughput on the GPU and on the CPU.\nFigure 7. Search throughput for different levels of recall (accuracy)\nFor the purpose of this benchmark, the CPU implementation of FAISS IVF-Flat was used. FAISS also provides a GPU implementation of this algorithm. If you use FAISS, you can already benefit from GPU acceleration with a minor change in your code. We are collaborating with Meta to bring the performance improvements from RAFT to FAISS, so you will soon be able to use RAFT through FAISS as well.\nSummary\nWhen performing vector search in large databases, it\u2019s important to be aware of the high cost of an exact search, as it can result in low latency not suitable for online services.\nThe RAPIDS RAFT library provides efficient algorithms that improve vector search latency and throughput by focusing the search to the most relevant part of the dataset. This post discussed how the RAFT IVF-Flat algorithm works and how to set the parameters for index building and searching. Finally, we presented benchmarks to highlight the superior performance of GPUs for IVF-Flat search. You can test it out yourself using our benchmark tools.\nRAFT is an open-source library for vector search and more. It provides an easy-to-use C++ and Python API so you can integrate GPU-accelerated vector search into your applications. We love to hear your feedback! Send us questions and report issues on the /rapidsai/raft GitHub repo. You can also find us at @rapidsai."}], "https://developer.nvidia.com/blog/preventing-health-data-leaks-with-federated-learning-using-nvidia-flare/": [{"text": "The article discusses the importance of preventing health data leaks through federated learning using NVIDIA FLARE. It highlights the risks of data breaches and the need for stronger data protection measures in collaborative healthcare settings. The article explains how NVIDIA FLARE allows data owners to review and approve code before it is executed on their data, reducing the risk of data leakage. The new features in NVIDIA FLARE 2.3.2, such as custom event handlers, enable data owners to control and monitor the code executed on their data. By implementing code acceptance strategies and event handlers, data owners can prevent malicious code from being executed on their data. While this solution does not eliminate all data protection issues, it provides an additional layer of security in federated learning networks. The article emphasizes the importance of empowering data owners and building secure federated learning networks to protect sensitive health data and prevent data leaks.", "text_components": ["Preventing Health Data Leaks with Federated Learning Using NVIDIA FLARE\nMore than 40 million people had their health data leaked in 2021, and the trend is not optimistic.\nThe key goal of federated learning and analytics is to perform data analytics and machine learning without accessing the raw data of the remote sites. That\u2019s the data you don\u2019t own and are not supposed to access directly. But how can you make this happen with a higher degree of confidence?\nImagine a siloed federated learning scenario with a biotech company collaborating with a network of hospitals. They\u2019re collaborating on an improved lung cancer detection model based on CT scans of images stored locally in local infrastructure.\nBoth the aggregator and the biotech data scientists are not permitted to access images directly or download them. They are only permitted to perform federated learning, training, and validation of remote models and to build better-aggregated models, which are then shared with all hospitals for improved generalization and better detection accuracy.\nThe goal of data protection seems to be obvious at first. It\u2019s a question of permissions, roles, and maybe some encryption here and there. Unfortunately, it\u2019s not as easy as it might seem.\nThe diagram shows how the NVFlare server connects to clients at multiple hospitals to access CT data.\nFigure 1. Configuration of the NVIDIA FLARE server", "Federated privacy defaults\nFederated learning solutions tend to focus on the following:\nTransmission channel security\nPoint-to-point trust (using certificates)\nEfficiency of the workflows\nSupport for many existing algorithms\nAll these reduce the risk of inference of the raw data from the model itself.\nHowever, most of the products and publications omit the important threat of a too-curious data scientist. Many products, by design or default, prioritize the near-absolute freedom of data processing by enabling external data scientists to send any queries or operations against remote data.\nThis is a perfectly acceptable setup for networks with virtually unlimited trust between the participants or whenever no sensitive data is being used. For instance, the network might be used for demonstration purposes or everything could happen within the boundaries of the data-owner organization.\nThis is how NVIDIA FLARE works by default when you want to use custom training and validation code.\nNo matter what encryption you use, how strong the transmission channel security guarantees are, or how up-to-date all the systems are on component vulnerabilities, when you permit data scientists to send any queries or code against remote data, you cannot guarantee data leakage protection.\nOn the contrary, you are fully dependent on trust levels for individuals or contracts. Or, you could worry later by analyzing logs of the operations and queries executed against the data that you don\u2019t own in a federated network.\nOne or more data scientists may one day abandon honesty. Or, the damage could be accidental instead of malicious. Market data shows that the majority of data attacks are coming from the inside. Unfortunately, inside threats are also on the rise.\nProper employee education, contract clauses, trust, privacy awareness, and work ethics are important but you should provide much stronger guarantees using technical measures.", "What to do about it\nData owners should be in full control: who does what when against what data.\nLogging all data-related operations is often promised as a solution by multiple product vendors. Well, after the potential damage of the data leak is done, it\u2019s too late. You must be proactive and prevent such things during the design phase.\nWhat about existing permission systems? In the case of NVIDIA FLARE, it is either on or off, enabling remote data scientists to execute any jobs on remote sites (local policies). Also, the biotech administration can\u2019t manage those permissions centrally, as they would be able to override local policies remotely.\nOther solutions opt for binary Docker images pushed from a central repository to remote sites (hospitals), based on the trust that whatever is there can be trusted. This practically eliminates the data owners from the acceptance process, as they can only trust the closed box of the image. Technically, they could download the image, mount it, and review files but it\u2019s impractical at scale.\nThere\u2019s a more practical approach available.", "Step up your data protection game using NVIDIA FLARE 2.3.2\nHere\u2019s how to lower the risk of data leakage significantly with all its consequences, both financial and reputational. You can deliver on the promise of data protection, which is a key element of federated learning and analytics using the latest features introduced in NVIDIA FLARE 2.3.2, as a result of our fruitful collaboration.", "Job acceptance and rejection requirements\nYou need a solution that enables data owners to review the code to be executed against their data before it happens. Practically, in NVIDIA FLARE, it means Python code implementing the trainer, validation, and federated statistics plus configuration settings.\nData owners should be able to review and accept or reject the code themselves or use trusted third-party reviewers. Nothing should happen against data without the explicit acceptance of the data owner.\nNo job code should be changed from the previously accepted code to malicious code overnight. It should be rejected because its contents have changed and it must be re-reviewed and re-accepted.", "Solution\nNVIDIA FLARE 2.3.2 delivers enablement of custom event handlers that perform actions to enable the components to be created at the site and controlled by the site.\nBut why object creation? Can\u2019t you just focus on the execution?\nBecause too-curious data scientists could easily inject the code into object initialization (constructor), object creation is essential. Act as early as possible to prevent the code that data owners don\u2019t want from running against their data.\nThe following simplified flow is the default:\nA central server controlled by external data scientists submits jobs to clients.\nClients schedule and execute those jobs.\nIf the code contains uploading data to the cloud, virtually anything that Python permits is executed.\nEven worse, the code may change with each job submission and it won\u2019t be detected or prevented from execution.\nJobs are sent from the orchestration node to local nodes for execution. They contain code and configuration.\nAfter the federated network is built using the default NVIDIA FLARE configuration, there is unconditional trust in external data scientists by data owners. They are then permitted to submit jobs with local policies.\nAfter the change, with a custom implementation:\nA central server controlled by external data scientists submits jobs to clients.\nA data owner reviews the job code and determines if it\u2019s acceptable from the data leakage risk perspective.\nIf the code is approved, the hash is added to the list of accepted hashes.\nThe job code (hash, signature) is checked locally at the site against the list of accepted hashes of jobs.\nThe job is executed on the client and results are returned to the server.\nHere\u2019s more detail about how the job workflow changes.\nWorkflow diagram highlights the first job being sent from the BioTech orchestration node to the first hospital node.\nFigure 2. Federated learning network with full local trust\nFigure 2 shows a federated learning network consisting of a BioTech orchestration node and three hospital nodes. The same federated training job is sent from the orchestration node for the local nodes. The job is not even created to prevent any malicious code as part of the initialization or constructor of the job object.\nFigure 3 shows the flow of job acceptance and execution using a new type of event and event handler.\nWorkflow diagram for job 1 showing job acceptance and rejection to prevent data leaks.\nFigure 3. Job acceptance and execution flow with data protection", "Job code acceptance strategies\nThanks to the open event\u2013based model delivered in NVIDIA FLARE 2.3.2, it is possible to implement any suitable code verification strategy. Such strategies must always be designed, defined, and agreed upon in a federated network and then deployed on each node (a client, such as a hospital).\nFor demonstration purposes, you can compare the code content hash against the accepted code stored in a different directory.\nFor more real-world, enterprise-grade scenarios, you can also provide implementations based on digital signatures of the code and even co-signatures provided by the third party trusted by data owners. This is independent of biotech performing the federated training.", "Code example\nNVIDIA FLARE raises the BEFORE_BUILD_COMPONENT event before a new component (that is, a job) is instantiated. All you must do is write the event handler to analyze the code and determine if it was accepted. There\u2019s no turnkey solution for that, as different federated networks may require different strategies. The following code example demonstrates such a handler. For demonstration purposes, the example only focuses on a subset of jobs.\n```\nfocuses on a subset of jobs.\ndef handle_event(self, event_type: str, fl_ctx: FLContext):\n    if event_type == EventType.BEFORE_BUILD_COMPONENT:\n       \n        # scanning only too curious data scientist jobs\n        if self.playground_mode:\n            meta = fl_ctx.get_prop(FLContextKey.JOB_META)\n            log.info(f\"meta: {meta}\")\n            if not \"too-curious-data-scientist\" in meta[\"name\"]:\n                return\n           \n        workspace: Workspace = fl_ctx.get_prop(key=ReservedKey.WORKSPACE_OBJECT) \n        job_id = fl_ctx.get_job_id()\n       \n        log.debug(fl_ctx.get_prop(FLContextKey.COMPONENT_CONFIG))\n        log.debug(f\"Run id in filter: \" + job_id)\n        log.debug(f\"rootdir: {workspace.get_root_dir()}, app_config_dir: {workspace.get_app_config_dir(job_id)}, app_custom_dir: {workspace.get_app_custom_dir(job_id)}\" )\n       \n        #making sure that approved_configs hash set is up to date (it's possible to update )\n        self.populate_approved_hash_set(os.path.join(workspace.get_root_dir(), self.approved_config_directory_name))\n        log.debug(f\"Approved hash list contains: {len(self.approved_hash_set)} items\")\n       \n        # check if client configuration json is approved (job configuration)\n        current_hash = self.hash_file(os.path.join(workspace.get_app_config_dir(job_id), JobConstants.CLIENT_JOB_CONFIG))\n        if current_hash in self.approved_hash_set:\n            log.info(f\"Client job configuration in approved list! with hash {current_hash}\")\n        else:\n            log.error(f\"Client job configuration not in approved list! with hash {current_hash}\")\n            log.error(\"Not approved job configuration! Throwing UnsafeComponentError!\")\n            raise UnsafeComponentError(\"Not approved job configuration! Killing workflow\")\n       \n        # check if all classes added to custom directory are approved\n        job_custom_classes = list(Path(os.path.join(workspace.get_app_custom_dir(job_id))).rglob(\"*.py\"))\n        for current_class_file in job_custom_classes:\n            current_class_file_hash = self.hash_file(current_class_file)\n            if current_class_file_hash in self.approved_hash_set:\n                log.info(f\"Custom class {current_class_file} in approved list!\")\n            else:\n                log.error(f\"Class {current_class_file} not in approved list!\")\n                log.error(\"Not approved job! Throwing UnsafeComponentError!\")\n                raise UnsafeComponentError(f\"Class {current_class_file} not in approved list! with hash {current_class_file_hash}. Not approved job! Killing workflow\")\n```\nAs we demonstrated earlier, all the required contextual data is provided by NVIDIA FLARE to be able to perform required actions such as finding the custom code files, calculating their hashes, and so on.", "There\u2019s more\nThis does not solve all the problems related to data protection. However, in all the scenarios with data owners having limited trust for the remote data scientists training models on their data without seeing it, addressing this problem is imperative.\nWhile this feature is not a definitive fail-safe measure against malicious users, it provides an additional layer of protection. It empowers nodes and fosters collaborative research through shared responsibility.\nNext, consider focusing on other important areas, such as the following:\nModel inference attacks\nDifferential privacy\nTransmission channel security\nOutput filters", "Summary\nThe principle of defense in depth in the case of federated learning and analytics makes it necessary to protect the data owner from possibly malicious remote code sent by external data scientists. In the case of truly federated scenarios, when there\u2019s no full trust relationship between data owners and remote scientists, this is not optional.\nThe promise of remote data inaccessibility doesn\u2019t deliver itself; you must empower data owners. It\u2019s not guaranteed by default.\nIn this post, we demonstrated how to address this important threat using NVIDIA FLARE 2.3.2 to enable better data protection and build more secure federated learning networks today and in the future."], "document_title": "Preventing Health Data Leaks with Federated Learning Using NVIDIA FLARE", "document_url": "https://developer.nvidia.com/blog/preventing-health-data-leaks-with-federated-learning-using-nvidia-flare/", "document_date": "2023-09-28T20:37:48", "document_date_modified": "2023-10-19T19:05:59", "document_full_text": "Preventing Health Data Leaks with Federated Learning Using NVIDIA FLARE\nMore than 40 million people had their health data leaked in 2021, and the trend is not optimistic.\nThe key goal of federated learning and analytics is to perform data analytics and machine learning without accessing the raw data of the remote sites. That\u2019s the data you don\u2019t own and are not supposed to access directly. But how can you make this happen with a higher degree of confidence?\nImagine a siloed federated learning scenario with a biotech company collaborating with a network of hospitals. They\u2019re collaborating on an improved lung cancer detection model based on CT scans of images stored locally in local infrastructure.\nBoth the aggregator and the biotech data scientists are not permitted to access images directly or download them. They are only permitted to perform federated learning, training, and validation of remote models and to build better-aggregated models, which are then shared with all hospitals for improved generalization and better detection accuracy.\nThe goal of data protection seems to be obvious at first. It\u2019s a question of permissions, roles, and maybe some encryption here and there. Unfortunately, it\u2019s not as easy as it might seem.\nThe diagram shows how the NVFlare server connects to clients at multiple hospitals to access CT data.\nFigure 1. Configuration of the NVIDIA FLARE server\nFederated privacy defaults\nFederated learning solutions tend to focus on the following:\nTransmission channel security\nPoint-to-point trust (using certificates)\nEfficiency of the workflows\nSupport for many existing algorithms\nAll these reduce the risk of inference of the raw data from the model itself.\nHowever, most of the products and publications omit the important threat of a too-curious data scientist. Many products, by design or default, prioritize the near-absolute freedom of data processing by enabling external data scientists to send any queries or operations against remote data.\nThis is a perfectly acceptable setup for networks with virtually unlimited trust between the participants or whenever no sensitive data is being used. For instance, the network might be used for demonstration purposes or everything could happen within the boundaries of the data-owner organization.\nThis is how NVIDIA FLARE works by default when you want to use custom training and validation code.\nNo matter what encryption you use, how strong the transmission channel security guarantees are, or how up-to-date all the systems are on component vulnerabilities, when you permit data scientists to send any queries or code against remote data, you cannot guarantee data leakage protection.\nOn the contrary, you are fully dependent on trust levels for individuals or contracts. Or, you could worry later by analyzing logs of the operations and queries executed against the data that you don\u2019t own in a federated network.\nOne or more data scientists may one day abandon honesty. Or, the damage could be accidental instead of malicious. Market data shows that the majority of data attacks are coming from the inside. Unfortunately, inside threats are also on the rise.\nProper employee education, contract clauses, trust, privacy awareness, and work ethics are important but you should provide much stronger guarantees using technical measures.\nWhat to do about it\nData owners should be in full control: who does what when against what data.\nLogging all data-related operations is often promised as a solution by multiple product vendors. Well, after the potential damage of the data leak is done, it\u2019s too late. You must be proactive and prevent such things during the design phase.\nWhat about existing permission systems? In the case of NVIDIA FLARE, it is either on or off, enabling remote data scientists to execute any jobs on remote sites (local policies). Also, the biotech administration can\u2019t manage those permissions centrally, as they would be able to override local policies remotely.\nOther solutions opt for binary Docker images pushed from a central repository to remote sites (hospitals), based on the trust that whatever is there can be trusted. This practically eliminates the data owners from the acceptance process, as they can only trust the closed box of the image. Technically, they could download the image, mount it, and review files but it\u2019s impractical at scale.\nThere\u2019s a more practical approach available.\nStep up your data protection game using NVIDIA FLARE 2.3.2\nHere\u2019s how to lower the risk of data leakage significantly with all its consequences, both financial and reputational. You can deliver on the promise of data protection, which is a key element of federated learning and analytics using the latest features introduced in NVIDIA FLARE 2.3.2, as a result of our fruitful collaboration.\nJob acceptance and rejection requirements\nYou need a solution that enables data owners to review the code to be executed against their data before it happens. Practically, in NVIDIA FLARE, it means Python code implementing the trainer, validation, and federated statistics plus configuration settings.\nData owners should be able to review and accept or reject the code themselves or use trusted third-party reviewers. Nothing should happen against data without the explicit acceptance of the data owner.\nNo job code should be changed from the previously accepted code to malicious code overnight. It should be rejected because its contents have changed and it must be re-reviewed and re-accepted.\nSolution\nNVIDIA FLARE 2.3.2 delivers enablement of custom event handlers that perform actions to enable the components to be created at the site and controlled by the site.\nBut why object creation? Can\u2019t you just focus on the execution?\nBecause too-curious data scientists could easily inject the code into object initialization (constructor), object creation is essential. Act as early as possible to prevent the code that data owners don\u2019t want from running against their data.\nThe following simplified flow is the default:\nA central server controlled by external data scientists submits jobs to clients.\nClients schedule and execute those jobs.\nIf the code contains uploading data to the cloud, virtually anything that Python permits is executed.\nEven worse, the code may change with each job submission and it won\u2019t be detected or prevented from execution.\nJobs are sent from the orchestration node to local nodes for execution. They contain code and configuration.\nAfter the federated network is built using the default NVIDIA FLARE configuration, there is unconditional trust in external data scientists by data owners. They are then permitted to submit jobs with local policies.\nAfter the change, with a custom implementation:\nA central server controlled by external data scientists submits jobs to clients.\nA data owner reviews the job code and determines if it\u2019s acceptable from the data leakage risk perspective.\nIf the code is approved, the hash is added to the list of accepted hashes.\nThe job code (hash, signature) is checked locally at the site against the list of accepted hashes of jobs.\nThe job is executed on the client and results are returned to the server.\nHere\u2019s more detail about how the job workflow changes.\nWorkflow diagram highlights the first job being sent from the BioTech orchestration node to the first hospital node.\nFigure 2. Federated learning network with full local trust\nFigure 2 shows a federated learning network consisting of a BioTech orchestration node and three hospital nodes. The same federated training job is sent from the orchestration node for the local nodes. The job is not even created to prevent any malicious code as part of the initialization or constructor of the job object.\nFigure 3 shows the flow of job acceptance and execution using a new type of event and event handler.\nWorkflow diagram for job 1 showing job acceptance and rejection to prevent data leaks.\nFigure 3. Job acceptance and execution flow with data protection\nJob code acceptance strategies\nThanks to the open event\u2013based model delivered in NVIDIA FLARE 2.3.2, it is possible to implement any suitable code verification strategy. Such strategies must always be designed, defined, and agreed upon in a federated network and then deployed on each node (a client, such as a hospital).\nFor demonstration purposes, you can compare the code content hash against the accepted code stored in a different directory.\nFor more real-world, enterprise-grade scenarios, you can also provide implementations based on digital signatures of the code and even co-signatures provided by the third party trusted by data owners. This is independent of biotech performing the federated training.\nCode example\nNVIDIA FLARE raises the BEFORE_BUILD_COMPONENT event before a new component (that is, a job) is instantiated. All you must do is write the event handler to analyze the code and determine if it was accepted. There\u2019s no turnkey solution for that, as different federated networks may require different strategies. The following code example demonstrates such a handler. For demonstration purposes, the example only focuses on a subset of jobs.\n```\nfocuses on a subset of jobs.\ndef handle_event(self, event_type: str, fl_ctx: FLContext):\n    if event_type == EventType.BEFORE_BUILD_COMPONENT:\n       \n        # scanning only too curious data scientist jobs\n        if self.playground_mode:\n            meta = fl_ctx.get_prop(FLContextKey.JOB_META)\n            log.info(f\"meta: {meta}\")\n            if not \"too-curious-data-scientist\" in meta[\"name\"]:\n                return\n           \n        workspace: Workspace = fl_ctx.get_prop(key=ReservedKey.WORKSPACE_OBJECT) \n        job_id = fl_ctx.get_job_id()\n       \n        log.debug(fl_ctx.get_prop(FLContextKey.COMPONENT_CONFIG))\n        log.debug(f\"Run id in filter: \" + job_id)\n        log.debug(f\"rootdir: {workspace.get_root_dir()}, app_config_dir: {workspace.get_app_config_dir(job_id)}, app_custom_dir: {workspace.get_app_custom_dir(job_id)}\" )\n       \n        #making sure that approved_configs hash set is up to date (it's possible to update )\n        self.populate_approved_hash_set(os.path.join(workspace.get_root_dir(), self.approved_config_directory_name))\n        log.debug(f\"Approved hash list contains: {len(self.approved_hash_set)} items\")\n       \n        # check if client configuration json is approved (job configuration)\n        current_hash = self.hash_file(os.path.join(workspace.get_app_config_dir(job_id), JobConstants.CLIENT_JOB_CONFIG))\n        if current_hash in self.approved_hash_set:\n            log.info(f\"Client job configuration in approved list! with hash {current_hash}\")\n        else:\n            log.error(f\"Client job configuration not in approved list! with hash {current_hash}\")\n            log.error(\"Not approved job configuration! Throwing UnsafeComponentError!\")\n            raise UnsafeComponentError(\"Not approved job configuration! Killing workflow\")\n       \n        # check if all classes added to custom directory are approved\n        job_custom_classes = list(Path(os.path.join(workspace.get_app_custom_dir(job_id))).rglob(\"*.py\"))\n        for current_class_file in job_custom_classes:\n            current_class_file_hash = self.hash_file(current_class_file)\n            if current_class_file_hash in self.approved_hash_set:\n                log.info(f\"Custom class {current_class_file} in approved list!\")\n            else:\n                log.error(f\"Class {current_class_file} not in approved list!\")\n                log.error(\"Not approved job! Throwing UnsafeComponentError!\")\n                raise UnsafeComponentError(f\"Class {current_class_file} not in approved list! with hash {current_class_file_hash}. Not approved job! Killing workflow\")\n```\nAs we demonstrated earlier, all the required contextual data is provided by NVIDIA FLARE to be able to perform required actions such as finding the custom code files, calculating their hashes, and so on.\nThere\u2019s more\nThis does not solve all the problems related to data protection. However, in all the scenarios with data owners having limited trust for the remote data scientists training models on their data without seeing it, addressing this problem is imperative.\nWhile this feature is not a definitive fail-safe measure against malicious users, it provides an additional layer of protection. It empowers nodes and fosters collaborative research through shared responsibility.\nNext, consider focusing on other important areas, such as the following:\nModel inference attacks\nDifferential privacy\nTransmission channel security\nOutput filters\nSummary\nThe principle of defense in depth in the case of federated learning and analytics makes it necessary to protect the data owner from possibly malicious remote code sent by external data scientists. In the case of truly federated scenarios, when there\u2019s no full trust relationship between data owners and remote scientists, this is not optional.\nThe promise of remote data inaccessibility doesn\u2019t deliver itself; you must empower data owners. It\u2019s not guaranteed by default.\nIn this post, we demonstrated how to address this important threat using NVIDIA FLARE 2.3.2 to enable better data protection and build more secure federated learning networks today and in the future."}], "https://developer.nvidia.com/blog/nvidia-h100-system-sets-records-for-hpc-and-generative-ai-financial-risk-calculations/": [{"text": "NVIDIA's H100 system, featuring Tensor Core GPUs, has set records for financial risk calculations in a recent STAC-A2 audit. This system, powered by HPE ProLiant XL675d servers, offers incredible speed and efficiency in quantitative financial applications. The STAC-A2 benchmark assesses technology stacks for compute-intensive analytic workloads in finance, and the NVIDIA H100-based solution outperformed previous generations and competitors in terms of speed and efficiency. The HPE ProLiant XL675d server supports a dense node strategy for financial HPC, enabling high performance with fewer nodes. NVIDIA's hardware plus software solutions provide optimal performance for real-time calculations and decision-making in trading and risk management. The NVIDIA H100 GPUs, with their cutting-edge technology, deliver superior performance for AI, HPC, and data analytics applications. Organizations are increasingly leveraging NVIDIA's solutions to accelerate workloads in HPC, quantitative finance, and generative AI, driving innovation and efficiency in the financial industry.", "text_components": ["NVIDIA H100 System for HPC and Generative AI Sets Record for Financial Risk Calculations\nGenerative AI is taking the world by storm, from large language models (LLMs) to generative pretrained transformer (GPT) models to diffusion models. NVIDIA is uniquely positioned to accelerate generative AI workloads, but also those for data processing, analytics, high-performance computing (HPC), quantitative financial applications, and more. NVIDIA offers a one-stop solution for diverse workload needs.\nIn quantitative applications for financial risk management, for example, NVIDIA GPUs offer incredible speed with great efficiency. NVIDIA H100 Tensor Core GPUs were featured in a stack that set several records in a recent STAC-A2 audit.", "New STAC-A2 derivative risk results performed on NVIDIA H100s with HPE ProLiant XL675d\nSTAC-A2 is a risk management benchmark created by the Strategic Technology Analysis Center (STAC) Benchmark Council for the assessment of technology stacks used for compute-intensive analytic workloads in finance. Designed by quantitative analysts and technologists from some of the world\u2019s largest banks, STAC-A2 reports the performance, scaling, quality, and resource efficiency of any technology stack that can handle the workload: Monte Carlo estimation of Heston-based Greeks for path-dependent, multi-asset options with early exercise.\nSTAC-A2 is the technology benchmark standard based on financial market risk analysis. The workload can be a proxy extended to price discovery, market risk calculations such as sensitivity Greeks, profit and loss, and value at risk (VaR) in market risk. The benchmark as a proxy can also be extended to counterparty credit risk (CCR) workloads such as credit valuation adjustment (CVA) and margin that financial institutions calculate for trading as well as risk management.\nCompared to all publicly reported solutions to date, this NVIDIA H100-based solution set numerous performance and efficiency records, including (but not limited to) the following:\nThe first sub-10ms warm time (8.9 ms) in the baseline Greeks benchmark.\nA cold time in the baseline Greeks benchmark of 38 ms, more than 3x faster than any previous benchmark.\nThe fastest warm (0.51 s) and cold (1.85 s) times in the large Greeks benchmarks.\nThe most correlated assets (400) and most paths (310,000,000) simulated in 10 minutes.\nThe best energy efficiency (311,045 options/kWh).\nCompared to a solution using 8x NVIDIA A100 SXM4 80GB GPUs, as well as previous generations of the NVIDIA STAC Pack and NVIDIA CUDA, this solution performed as follows:\n10x faster in the cold run of the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.COLD ).\n1.38x faster in the warm runs of the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.WARM ).\n1.38x faster in the cold run of the large Greeks benchmark ( STAC-A2.\u03b22.GREEKS.10-100k-1260TIME.COLD ).\n1.4x faster in the warm runs of the large Greeks benchmark ( STAC-A2.\u03b22.GREEKS.10-100k-1260TIME.WARM ).\nWith 10% more energy efficiency for benchmark 7.", "Integrated hardware plus software solutions\nOne of the industry\u2019s key concerns is risk calculation, which relies heavily on the latest technologies for real-time calculations and instant decision-making for trading and risk management.\nThe HPE ProLiant XL675d Gen10 Plus server supports a dense node strategy for financial HPC, enabling up to 10 NVIDIA GPUs to efficiently populate a single HPC server. With this type of dense node, a compute farm for portfolio risk calculations can be realized with far fewer nodes achieving higher performance with lower operational costs for real-world calculations such as price discovery, market risk (such as VaR), and counterparty risk (such as CVA).\nIn areas such as CVA, such setups have been shown to reduce the number of nodes from 100 to 4 in simulation\u2013 and compute-intensive calculations (separately from STAC benchmarking).\nThis dense node solution enables an exciting strategy of scaling up with NVIDIA GPUs for a reduced number of nodes. It enables the highest performance at the lowest operating cost and the smallest data center footprint. The solutions can be extended to other workloads, such as AI language inference and backtesting as needed for a scaling strategy with such dense servers.\nIn addition to the hardware, NVIDIA provides all the key software component layers. This offers multiple options to developers, including the language of choice such as CUDA C++.\nIn calculations that are typically developed for fast run times, this implementation was developed on CUDA 12.0. The implementation uses the highly optimized libraries delivered with CUDA:\ncuBLAS: The GPU-enabled implementation of the linear algebra package BLAS.\ncuRAND: A parallel and efficient GPU implementation of random number generators.\nThe different components of the implementation are designed in a modular and maintainable framework using object-oriented programming. All floating-point operations were conducted in IEEE-754 double precision (64 bits). The implementation was developed using the various tools provided by NVIDIA to help debug and profile CUDA code:\nNVIDIA Nsight Systems for timeline profiling\nNVIDIA Nsight Compute for kernel profiling\nNVIDIA Compute Sanitizer and CUDA-GDB for debugging", "Solution for risk HPC and AI convergence\nNVIDIA H100 GPUs are an integral part of the NVIDIA data center platform. Built for AI, HPC, and data analytics, the platform accelerates over 4,000 applications. It is available everywhere, from data center to edge, delivering both dramatic performance gains and cost-saving opportunities with the aim of accelerating \u201cevery workload, everywhere.\u201d\nThe NVIDIA H100 PCIe GPU incorporates groundbreaking technology, such as the NVIDIA Hopper architecture, with a theoretical peak performance of 51 TFLOPS for single-precision and 26 TFLOPS for double-precision calculations. It uses 14,592 CUDA cores, plus 456 fourth-generation Tensor Core modules, which can deliver a theoretical peak performance of 1,513 TFLOPS for BF16 and 51 TFLOPS for FP64 matrix calculations.\nFor HPC applications, the NVIDIA H100 almost triples the theoretical floating-point operations per second (FLOPS) of FP64 compared to the NVIDIA A100. It also adds dynamic programming instructions (DPX) to help achieve better performance. NVIDIA H100 GPUs feature fourth-generation Tensor Cores and the Transformer Engine with FP8 precision.\nWith second-generation Multi-Instance GPU (MIG), built-in NVIDIA confidential computing, and NVIDIA NVLink, the NVIDIA H100 aims to securely accelerate workloads for every data center, from enterprise to exascale. The NVIDIA GPUs in SXM form share a switched NVLink 4.0 interconnect, providing high-speed GPU-to-GPU communication bandwidth.\nThe H100 PCIe Gen 4 configuration used in this SUT provides most of the specified capabilities of H100 SXM5 GPUs in just 350 watts of thermal design power (TDP). This configuration can optionally use the NVLink bridge for connecting up to two GPUs for applications like deep learning in AI that are coded to take advantage of inter-GPU communications. (The STAC-A2 Pack does not use these fabrics.)", "Summary\nWhether at a single-server scale or in larger scaling systems optimized for today\u2019s most demanding HPC plus AI workloads, NVIDIA is uniquely positioned to accelerate workloads ranging from HPC quantitative financial applications and data processing to analytics and generative AI. In addition to risk calculations, organizations are converging NLP with generative AI, feeding inputs to quantitative calculations.\nThis is an active area of work, wherein the convergence of HPC and AI is happening as financial firms work on big-picture solutions combining various modeling techniques including HPC quantitative finance, machine learning (ML), and AI with neural net as well as NLP with generative AI. This enables firms to drive multiple complex business needs with converged HPC plus AI solutions that are increasingly a result of accelerated AI adoption, combining workloads for unique solution needs in the financial industry.\nFor more information, see HPE and NVIDIA Financial Services Solution, Powered by NVIDIA, Sets New Records in Performance.\nReach out to NVIDIA Financial Services with questions as you evaluate or apply accelerated compute to your critical business problems. Read the full report, CUDA 12.0 with 8x NVIDIA H100 PCIe 80GiB GPUs in an HPE ProLiant XL675d Gen10 Plus Server."], "document_title": "NVIDIA H100 System for HPC and Generative AI Sets Record for Financial Risk Calculations", "document_url": "https://developer.nvidia.com/blog/nvidia-h100-system-sets-records-for-hpc-and-generative-ai-financial-risk-calculations/", "document_date": "2023-09-28T15:25:49", "document_date_modified": "2023-11-24T18:50:56", "document_full_text": "NVIDIA H100 System for HPC and Generative AI Sets Record for Financial Risk Calculations\nGenerative AI is taking the world by storm, from large language models (LLMs) to generative pretrained transformer (GPT) models to diffusion models. NVIDIA is uniquely positioned to accelerate generative AI workloads, but also those for data processing, analytics, high-performance computing (HPC), quantitative financial applications, and more. NVIDIA offers a one-stop solution for diverse workload needs.\nIn quantitative applications for financial risk management, for example, NVIDIA GPUs offer incredible speed with great efficiency. NVIDIA H100 Tensor Core GPUs were featured in a stack that set several records in a recent STAC-A2 audit.\nNew STAC-A2 derivative risk results performed on NVIDIA H100s with HPE ProLiant XL675d\nSTAC-A2 is a risk management benchmark created by the Strategic Technology Analysis Center (STAC) Benchmark Council for the assessment of technology stacks used for compute-intensive analytic workloads in finance. Designed by quantitative analysts and technologists from some of the world\u2019s largest banks, STAC-A2 reports the performance, scaling, quality, and resource efficiency of any technology stack that can handle the workload: Monte Carlo estimation of Heston-based Greeks for path-dependent, multi-asset options with early exercise.\nSTAC-A2 is the technology benchmark standard based on financial market risk analysis. The workload can be a proxy extended to price discovery, market risk calculations such as sensitivity Greeks, profit and loss, and value at risk (VaR) in market risk. The benchmark as a proxy can also be extended to counterparty credit risk (CCR) workloads such as credit valuation adjustment (CVA) and margin that financial institutions calculate for trading as well as risk management.\nCompared to all publicly reported solutions to date, this NVIDIA H100-based solution set numerous performance and efficiency records, including (but not limited to) the following:\nThe first sub-10ms warm time (8.9 ms) in the baseline Greeks benchmark.\nA cold time in the baseline Greeks benchmark of 38 ms, more than 3x faster than any previous benchmark.\nThe fastest warm (0.51 s) and cold (1.85 s) times in the large Greeks benchmarks.\nThe most correlated assets (400) and most paths (310,000,000) simulated in 10 minutes.\nThe best energy efficiency (311,045 options/kWh).\nCompared to a solution using 8x NVIDIA A100 SXM4 80GB GPUs, as well as previous generations of the NVIDIA STAC Pack and NVIDIA CUDA, this solution performed as follows:\n10x faster in the cold run of the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.COLD ).\n1.38x faster in the warm runs of the baseline Greeks benchmark ( STAC-A2.\u03b22.GREEKS.TIME.WARM ).\n1.38x faster in the cold run of the large Greeks benchmark ( STAC-A2.\u03b22.GREEKS.10-100k-1260TIME.COLD ).\n1.4x faster in the warm runs of the large Greeks benchmark ( STAC-A2.\u03b22.GREEKS.10-100k-1260TIME.WARM ).\nWith 10% more energy efficiency for benchmark 7.\nIntegrated hardware plus software solutions\nOne of the industry\u2019s key concerns is risk calculation, which relies heavily on the latest technologies for real-time calculations and instant decision-making for trading and risk management.\nThe HPE ProLiant XL675d Gen10 Plus server supports a dense node strategy for financial HPC, enabling up to 10 NVIDIA GPUs to efficiently populate a single HPC server. With this type of dense node, a compute farm for portfolio risk calculations can be realized with far fewer nodes achieving higher performance with lower operational costs for real-world calculations such as price discovery, market risk (such as VaR), and counterparty risk (such as CVA).\nIn areas such as CVA, such setups have been shown to reduce the number of nodes from 100 to 4 in simulation\u2013 and compute-intensive calculations (separately from STAC benchmarking).\nThis dense node solution enables an exciting strategy of scaling up with NVIDIA GPUs for a reduced number of nodes. It enables the highest performance at the lowest operating cost and the smallest data center footprint. The solutions can be extended to other workloads, such as AI language inference and backtesting as needed for a scaling strategy with such dense servers.\nIn addition to the hardware, NVIDIA provides all the key software component layers. This offers multiple options to developers, including the language of choice such as CUDA C++.\nIn calculations that are typically developed for fast run times, this implementation was developed on CUDA 12.0. The implementation uses the highly optimized libraries delivered with CUDA:\ncuBLAS: The GPU-enabled implementation of the linear algebra package BLAS.\ncuRAND: A parallel and efficient GPU implementation of random number generators.\nThe different components of the implementation are designed in a modular and maintainable framework using object-oriented programming. All floating-point operations were conducted in IEEE-754 double precision (64 bits). The implementation was developed using the various tools provided by NVIDIA to help debug and profile CUDA code:\nNVIDIA Nsight Systems for timeline profiling\nNVIDIA Nsight Compute for kernel profiling\nNVIDIA Compute Sanitizer and CUDA-GDB for debugging\nSolution for risk HPC and AI convergence\nNVIDIA H100 GPUs are an integral part of the NVIDIA data center platform. Built for AI, HPC, and data analytics, the platform accelerates over 4,000 applications. It is available everywhere, from data center to edge, delivering both dramatic performance gains and cost-saving opportunities with the aim of accelerating \u201cevery workload, everywhere.\u201d\nThe NVIDIA H100 PCIe GPU incorporates groundbreaking technology, such as the NVIDIA Hopper architecture, with a theoretical peak performance of 51 TFLOPS for single-precision and 26 TFLOPS for double-precision calculations. It uses 14,592 CUDA cores, plus 456 fourth-generation Tensor Core modules, which can deliver a theoretical peak performance of 1,513 TFLOPS for BF16 and 51 TFLOPS for FP64 matrix calculations.\nFor HPC applications, the NVIDIA H100 almost triples the theoretical floating-point operations per second (FLOPS) of FP64 compared to the NVIDIA A100. It also adds dynamic programming instructions (DPX) to help achieve better performance. NVIDIA H100 GPUs feature fourth-generation Tensor Cores and the Transformer Engine with FP8 precision.\nWith second-generation Multi-Instance GPU (MIG), built-in NVIDIA confidential computing, and NVIDIA NVLink, the NVIDIA H100 aims to securely accelerate workloads for every data center, from enterprise to exascale. The NVIDIA GPUs in SXM form share a switched NVLink 4.0 interconnect, providing high-speed GPU-to-GPU communication bandwidth.\nThe H100 PCIe Gen 4 configuration used in this SUT provides most of the specified capabilities of H100 SXM5 GPUs in just 350 watts of thermal design power (TDP). This configuration can optionally use the NVLink bridge for connecting up to two GPUs for applications like deep learning in AI that are coded to take advantage of inter-GPU communications. (The STAC-A2 Pack does not use these fabrics.)\nSummary\nWhether at a single-server scale or in larger scaling systems optimized for today\u2019s most demanding HPC plus AI workloads, NVIDIA is uniquely positioned to accelerate workloads ranging from HPC quantitative financial applications and data processing to analytics and generative AI. In addition to risk calculations, organizations are converging NLP with generative AI, feeding inputs to quantitative calculations.\nThis is an active area of work, wherein the convergence of HPC and AI is happening as financial firms work on big-picture solutions combining various modeling techniques including HPC quantitative finance, machine learning (ML), and AI with neural net as well as NLP with generative AI. This enables firms to drive multiple complex business needs with converged HPC plus AI solutions that are increasingly a result of accelerated AI adoption, combining workloads for unique solution needs in the financial industry.\nFor more information, see HPE and NVIDIA Financial Services Solution, Powered by NVIDIA, Sets New Records in Performance.\nReach out to NVIDIA Financial Services with questions as you evaluate or apply accelerated compute to your critical business problems. Read the full report, CUDA 12.0 with 8x NVIDIA H100 PCIe 80GiB GPUs in an HPE ProLiant XL675d Gen10 Plus Server."}], "https://developer.nvidia.com/blog/enabling-the-worlds-first-gpu-accelerated-5g-open-ran-for-ntt-docomo-with-nvidia-aerial/": [{"text": "NVIDIA, Fujitsu, and Wind River collaborated to enable NTT DOCOMO to launch the first GPU-accelerated commercial Open RAN 5G service in Japan, making it the first telco to deploy such a network worldwide. This milestone addresses the telecom industry's need to improve performance, TCO, and energy efficiency. The solution leverages Fujitsu's vCU and vDU, the NVIDIA Aerial platform, and Wind River's distributed cloud platform. The deployment reduces TCO by up to 30%, improves network design utilization by up to 50%, and cuts power consumption at base stations by up to 50%. The NVIDIA Aerial platform brings together software, hardware, and support for innovation in wireless networks, delivering high performance, flexibility, and scalability. This partnership aims to promote multi-vendor, Open RAN-compliant 5G networks globally. The deployment in Japan is a step towards widespread adoption of GPU-based acceleration in cellular RANs. NVIDIA continues to support operators worldwide in deploying high-performance, energy-efficient, software-defined 5G vRAN networks.", "text_components": ["Enabling the World\u2019s First GPU-Accelerated 5G Open RAN for NTT DOCOMO with NVIDIA Aerial\nNVIDIA, working with Fujitsu and Wind River, has enabled NTT DOCOMO to launch the first GPU-accelerated commercial Open RAN 5G service in its network in Japan. This makes it the first-ever telco in the world to deploy a GPU-accelerated commercial 5G network.\nThe announcement is a major milestone as the telecom industry strives to address the multi-billion-dollar problem of driving improvements in performance, total cost of ownership (TCO), and energy efficiency. The solution unlocks the flexibility, scalability, and supply chain diversity promise of Open RAN.\nDOCOMO and its partners confirmed that the solution is based on Fujitsu\u2019s virtualized centralized unit (vCU) and virtualized distributed unit (vDU), the NVIDIA Aerial platform, and Wind River\u2019s distributed cloud platform.\nThe 5G Open RAN solution is the first 5G vRAN for telecom commercial deployment using the NVIDIA Aerial platform. The platform brings together the NVIDIA Aerial vRAN stack for 5G, AI frameworks, accelerated compute infrastructure, and long-term software support and maintenance. It delivers innovative and transformational new capabilities for telco operators.", "TCO and energy efficiency benefits\nWorking with vendors Fujitsu and Wind River, the new deployment uses the NVIDIA Aerial platform to deliver high performance, high cell density, and flexibility to DOCOMO\u2019s 5G network, bringing better utilization of its network, lower TCO, and reduced power consumption.\nDOCOMO notes that, when compared to its standard network based on a proprietary solution, this new solution reduces TCO by up to 30%, improves network design utilization by up to 50%, and reduces power consumption at base stations by up to 50%.\nAs of July 2023, DOCOMO serves over 22M 5G subscribers, from 20K+ base stations, with 5G coverage in over 815 cities. It uses 29 types of radio units (RUs) from four vendors and eight types of CUs from three vendors. The introduction of vRAN is expected to expand the capacity and coverage of the 5G network.\nWhile the ability to mix and match products from different vendors promises improvements in flexibility and scalability from Open RAN networks, it poses two major concerns for operators:\nFirst, it makes it challenging to bring out the best performance from the different vendor products.\nSecond, there are often technical issues that are only found at interoperability testing, which any operator deploying Open RAN must deal with.", "NVIDIA, Fujitsu, Wind River 5G Open RAN partnership\nDOCOMO launched OREX as an Open RAN service brand to address the challenges facing Open RAN. After the project was launched in February 2021, Fujitsu, NVIDIA, and Wind River worked together under OREX to develop the 5G vRAN solution, which is based on Fujitsu\u2019s vDU and vCU.\nThe solution uses the following components:\nCommercial-off-the-shelf (COTS) servers\nWind River distributed cloud platform\nFujitsu\u2019s 5G vRAN software\nNVIDIA Aerial vRAN stack\nNVIDIA Converged Accelerator\nThis is the first vendor consortium to deliver a commercial live 5G vRAN that meets NTT DOCOMO\u2019s performance and interoperability requirements.\nThe NVIDIA Aerial platform includes NVIDIA Aerial vRAN stack software for the physical (PHY) layer 1 (L1), and NVIDIA Converged Accelerator with its combined data processing unit (DPU) and GPU for hardware acceleration of the computationally intense L1 workload. This makes it the world\u2019s first DPU and GPU-accelerated (that is, NVIDIA-accelerated) 5G Open RAN to be deployed commercially to deliver a scalable, flexible, and cost-efficient network.\nDiagram shows the software stack of the Fujitsu DU, NVIDIA Aerial L1, and Wind River Studio Cloud platform. The hardware stack includes pictures of a COTS server and NVIDIA Converged Accelerator next to the GPU, DPU, and CPU boxes.\nFigure 1. NVIDIA-accelerated 5G vRAN stack deployed by NTT DOCOMO", "NVIDIA Aerial platform: Building blocks for wireless innovation\nNVIDIA is driving innovation in the telecom industry with a portfolio of wireless frameworks, AI frameworks, and accelerated computing infrastructure. This enables the development of high-performance, fully software-defined, and AI-native networks with cloud economics (Figure 2).\nDiagram lists benefits for high-performance, AI-native, fully software-defined 5G networks with cloud economics: faster and flexible deployment, ease of management, open platform, highest spectral efficiency, and more.\nFigure 2. RAN innovation benefits with the NVIDIA full-stack 5G vRAN\nThe accelerated computing infrastructure is made up of a combination of CPU, DPU, and GPU, together with a range of NVIDIA-certified COTS hardware servers.\nNVIDIA Aerial is the platform with software, hardware, and support for delivering innovation in the wireless market segment. It brings together the NVIDIA Aerial vRAN stack for 5G, AI frameworks, other wireless frameworks, an accelerated compute infrastructure, and long-term software support.\nThanks to this combination of industry-shaping hardware, software, and the long-term support and maintenance typical for a commercial-grade software stack, this enables new performance thresholds for 5G networks.\nThe key components of the platform are as follows:\nSoftware: NVIDIA Aerial vRAN stack\nHardware: NVIDIA Accelerated Computing\nCarrier-grade support", "Software: NVIDIA Aerial vRAN stack\nThis is an application framework for building high-performance, 100% software-defined, cloud-native, 5G vRAN, with O-RAN 7.2-x split. The NVIDIA Aerial vRAN stack is highly flexible and scalable and can deliver high performance for the L1.\nNVIDIA Aerial has adopted a GPU-centric approach and relies on two notable subcomponents:\nNVIDIA cuBB SDK (CUDA baseband)\nNVIDIA DOCA GPUNetIO\ncuBB provides GPU-accelerated 5G L1 processing. It delivers high throughput and efficiency by keeping all PHY layer processing within the high-performance GPU memory. The cuBB SDK also includes the 5G L1 high-PHY acceleration library cuPHY, which is optimized for NVIDIA GPUs. cuPHY offers unparalleled scalability by using the GPU\u2019s massive computing capability and a high degree of parallelism.\nNVIDIA DOCA GPUNetIO improves the performance of inline hardware acceleration. It provides optimized I/O and packet processing by exchanging packets directly between GPU memory and the DPU using direct memory access (DMA) technology.\nThe full stack for NVIDIA Aerial 5G vRAN showing the cuBB SDK, framework libraries, and the toolkit and drivers.\nFigure 3. NVIDIA Aerial vRAN stack for complete L1 PHY acceleration", "Hardware: NVIDIA Accelerated Computing\nThis is the processing engine of the vRAN and is made up of CPUs, DPUs, and GPUs, together with COTS hardware servers.\nThe performance of the NVIDIA Aerial vRAN stack, especially the computationally intensive L1, is dependent on the choice of hardware it is deployed on. NVIDIA offers two hardware options for 5G network deployments. Their comparative performance is shown in Table 1 below.\nx86 and NVIDIA Converged Accelerator (used in the NTT DOCOMO announcement )\nNVIDIA Grace Hopper and NVIDIA BlueField DPU", "x86 and NVIDIA Converged Accelerator\nThis option combines the performance of NVIDIA DPUs and GPUs, together with an x86 CPU server, to deliver maximum performance for the 5G vRAN. This is also the hardware acceleration option in the current DOCOMO deployment.\nThe integration of the GPU and DPU brings all front-haul enhanced common public radio interface (eCPRI) data traffic into the GPU without the CPU in the datapath. This is a full inline L1 offload, so the solution achieves high performance by avoiding the back-and-forth of eCPRI data between the CPU and the hardware accelerator in alternative systems across the host PCIe interface.\nAfter the data is in the GPU, it benefits from the massive parallelism of the GPU architecture to optimize the processing capacity of the base station system. This brings improved RU capacity and processing power, provides a high-quality communications environment, and can handle high-load data processing along with future improvements in antenna technologies.", "NVIDIA Grace Hopper and NVIDIA BlueField DPU\nThe NVIDIA Grace Hopper Superchip brings together the NVIDIA Grace CPU, which is based on the Arm architecture, and the high-performance NVIDIA Hopper GPU. The BlueField DPU helps to achieve the same performance from full inline L1 offload in a similar way to NVIDIA Converged Accelerator.\nHowever, the biggest boost to performance comes from the integration of the CPU and GPU architectures using NVIDIA NVLink-C2C to deliver a CPU+GPU coherent memory model for accelerated workloads such as 5G vRAN.\nNVIDIA NVLink-C2C is the NVIDIA memory-coherent, high-bandwidth, and low-latency interconnect. It delivers up to 900 GB/s total bandwidth: 7x higher bandwidth than the x16 PCIe Gen5 lanes commonly used in accelerated systems.\nWith the NVLink-C2C memory coherency, both CPU and GPU threads can concurrently and transparently access both CPU and GPU resident memory, enabling the RAN to optimize how it handles eCPRI data across CPU and GPU.\nX86 + NVIDIA Converged Accelerator\n(refer to AX800)\nGrace Hopper + BlueField 3\n(refer to GH200)\nUp to 20 peak cells of 4T4T = equivalent to 36 Gbps per 2U server\nConfiguration*\nUp to 40 peak cells of 4T4T = equivalent to 72 Gbps per 2U server\n3.2X\n(36 Gbps)\n5G Performance*\n6.4X\n(72 Gbps)\n76X\n(tokens/sec)\nLLM*\n(LLAMA 65B)\n284X\n(tokens/sec)\n1.3X\n(34 W/Gbps)\n5G Power Efficiency*\n3.3X\n(13 W/Gbps)\n1X\n(PCIE)\nCPU \u2013 GPU Bandwidth*\n7X\n(C2C)\n* Relative performance is estimated vs X86 5G SKU for 100 MHZ ,4T4R, 4DL/2UL. PCIE Gen5, 2U Server\nTable 1: Comparative performance of NVIDIA Converged Accelerators vs NVIDIA Grace Hopper for 5G vRAN. Footnote: DOCOMO\u2019s current deployment is using the X86 + NVIDIA Converged Accelerator option.", "Carrier-grade support\nThe NVIDIA Aerial platform provides a full-stack, carrier-grade, hardened, and mature 5G solution with 10 years of long-life support and maintenance for telecommunications operators. This level of carrier-grade support provides assurances of reliability and resilience for telcos for field or commercial deployment using the platform.", "OREX: Building out from Japan and beyond\nThe commercial deployment of a 5G Open RAN network by NTT DOCOMO, using the NVIDIA 5G platform, is a major milestone for the telecommunications industry. It showcases the capabilities of GPU-based acceleration for computationally intensive L1 PHY processing.\nThis new network comes with improved performance, flexibility, and scalability, plus higher cell density, significant improvements in energy efficiency, and a reduction in TCO. The delivery of this network paves the way for widespread adoption of GPU-based acceleration in cellular RANs.\nDOCOMO and its partners in OREX are working together to promote a multi-vendor, Open RAN\u2013compliant 5G vRAN to the global operator community. The commercial deployment in Japan is in alignment with the vision of OREX, enabling its members to validate their solutions commercially and then promote it to other operators globally.\nNVIDIA continues to work with DOCOMO and other partners to support operators around the world to deploy high-performance, energy-efficient, software-defined, commercial 5G vRAN."], "document_title": "Enabling the World\u2019s First GPU-Accelerated 5G Open RAN for NTT DOCOMO with NVIDIA Aerial", "document_url": "https://developer.nvidia.com/blog/enabling-the-worlds-first-gpu-accelerated-5g-open-ran-for-ntt-docomo-with-nvidia-aerial/", "document_date": "2023-09-27T00:00:00", "document_date_modified": "2023-11-14T18:58:58", "document_full_text": "Enabling the World\u2019s First GPU-Accelerated 5G Open RAN for NTT DOCOMO with NVIDIA Aerial\nNVIDIA, working with Fujitsu and Wind River, has enabled NTT DOCOMO to launch the first GPU-accelerated commercial Open RAN 5G service in its network in Japan. This makes it the first-ever telco in the world to deploy a GPU-accelerated commercial 5G network.\nThe announcement is a major milestone as the telecom industry strives to address the multi-billion-dollar problem of driving improvements in performance, total cost of ownership (TCO), and energy efficiency. The solution unlocks the flexibility, scalability, and supply chain diversity promise of Open RAN.\nDOCOMO and its partners confirmed that the solution is based on Fujitsu\u2019s virtualized centralized unit (vCU) and virtualized distributed unit (vDU), the NVIDIA Aerial platform, and Wind River\u2019s distributed cloud platform.\nThe 5G Open RAN solution is the first 5G vRAN for telecom commercial deployment using the NVIDIA Aerial platform. The platform brings together the NVIDIA Aerial vRAN stack for 5G, AI frameworks, accelerated compute infrastructure, and long-term software support and maintenance. It delivers innovative and transformational new capabilities for telco operators.\nTCO and energy efficiency benefits\nWorking with vendors Fujitsu and Wind River, the new deployment uses the NVIDIA Aerial platform to deliver high performance, high cell density, and flexibility to DOCOMO\u2019s 5G network, bringing better utilization of its network, lower TCO, and reduced power consumption.\nDOCOMO notes that, when compared to its standard network based on a proprietary solution, this new solution reduces TCO by up to 30%, improves network design utilization by up to 50%, and reduces power consumption at base stations by up to 50%.\nAs of July 2023, DOCOMO serves over 22M 5G subscribers, from 20K+ base stations, with 5G coverage in over 815 cities. It uses 29 types of radio units (RUs) from four vendors and eight types of CUs from three vendors. The introduction of vRAN is expected to expand the capacity and coverage of the 5G network.\nWhile the ability to mix and match products from different vendors promises improvements in flexibility and scalability from Open RAN networks, it poses two major concerns for operators:\nFirst, it makes it challenging to bring out the best performance from the different vendor products.\nSecond, there are often technical issues that are only found at interoperability testing, which any operator deploying Open RAN must deal with.\nNVIDIA, Fujitsu, Wind River 5G Open RAN partnership\nDOCOMO launched OREX as an Open RAN service brand to address the challenges facing Open RAN. After the project was launched in February 2021, Fujitsu, NVIDIA, and Wind River worked together under OREX to develop the 5G vRAN solution, which is based on Fujitsu\u2019s vDU and vCU.\nThe solution uses the following components:\nCommercial-off-the-shelf (COTS) servers\nWind River distributed cloud platform\nFujitsu\u2019s 5G vRAN software\nNVIDIA Aerial vRAN stack\nNVIDIA Converged Accelerator\nThis is the first vendor consortium to deliver a commercial live 5G vRAN that meets NTT DOCOMO\u2019s performance and interoperability requirements.\nThe NVIDIA Aerial platform includes NVIDIA Aerial vRAN stack software for the physical (PHY) layer 1 (L1), and NVIDIA Converged Accelerator with its combined data processing unit (DPU) and GPU for hardware acceleration of the computationally intense L1 workload. This makes it the world\u2019s first DPU and GPU-accelerated (that is, NVIDIA-accelerated) 5G Open RAN to be deployed commercially to deliver a scalable, flexible, and cost-efficient network.\nDiagram shows the software stack of the Fujitsu DU, NVIDIA Aerial L1, and Wind River Studio Cloud platform. The hardware stack includes pictures of a COTS server and NVIDIA Converged Accelerator next to the GPU, DPU, and CPU boxes.\nFigure 1. NVIDIA-accelerated 5G vRAN stack deployed by NTT DOCOMO\nNVIDIA Aerial platform: Building blocks for wireless innovation\nNVIDIA is driving innovation in the telecom industry with a portfolio of wireless frameworks, AI frameworks, and accelerated computing infrastructure. This enables the development of high-performance, fully software-defined, and AI-native networks with cloud economics (Figure 2).\nDiagram lists benefits for high-performance, AI-native, fully software-defined 5G networks with cloud economics: faster and flexible deployment, ease of management, open platform, highest spectral efficiency, and more.\nFigure 2. RAN innovation benefits with the NVIDIA full-stack 5G vRAN\nThe accelerated computing infrastructure is made up of a combination of CPU, DPU, and GPU, together with a range of NVIDIA-certified COTS hardware servers.\nNVIDIA Aerial is the platform with software, hardware, and support for delivering innovation in the wireless market segment. It brings together the NVIDIA Aerial vRAN stack for 5G, AI frameworks, other wireless frameworks, an accelerated compute infrastructure, and long-term software support.\nThanks to this combination of industry-shaping hardware, software, and the long-term support and maintenance typical for a commercial-grade software stack, this enables new performance thresholds for 5G networks.\nThe key components of the platform are as follows:\nSoftware: NVIDIA Aerial vRAN stack\nHardware: NVIDIA Accelerated Computing\nCarrier-grade support\nSoftware: NVIDIA Aerial vRAN stack\nThis is an application framework for building high-performance, 100% software-defined, cloud-native, 5G vRAN, with O-RAN 7.2-x split. The NVIDIA Aerial vRAN stack is highly flexible and scalable and can deliver high performance for the L1.\nNVIDIA Aerial has adopted a GPU-centric approach and relies on two notable subcomponents:\nNVIDIA cuBB SDK (CUDA baseband)\nNVIDIA DOCA GPUNetIO\ncuBB provides GPU-accelerated 5G L1 processing. It delivers high throughput and efficiency by keeping all PHY layer processing within the high-performance GPU memory. The cuBB SDK also includes the 5G L1 high-PHY acceleration library cuPHY, which is optimized for NVIDIA GPUs. cuPHY offers unparalleled scalability by using the GPU\u2019s massive computing capability and a high degree of parallelism.\nNVIDIA DOCA GPUNetIO improves the performance of inline hardware acceleration. It provides optimized I/O and packet processing by exchanging packets directly between GPU memory and the DPU using direct memory access (DMA) technology.\nThe full stack for NVIDIA Aerial 5G vRAN showing the cuBB SDK, framework libraries, and the toolkit and drivers.\nFigure 3. NVIDIA Aerial vRAN stack for complete L1 PHY acceleration\nHardware: NVIDIA Accelerated Computing\nThis is the processing engine of the vRAN and is made up of CPUs, DPUs, and GPUs, together with COTS hardware servers.\nThe performance of the NVIDIA Aerial vRAN stack, especially the computationally intensive L1, is dependent on the choice of hardware it is deployed on. NVIDIA offers two hardware options for 5G network deployments. Their comparative performance is shown in Table 1 below.\nx86 and NVIDIA Converged Accelerator (used in the NTT DOCOMO announcement )\nNVIDIA Grace Hopper and NVIDIA BlueField DPU\nx86 and NVIDIA Converged Accelerator\nThis option combines the performance of NVIDIA DPUs and GPUs, together with an x86 CPU server, to deliver maximum performance for the 5G vRAN. This is also the hardware acceleration option in the current DOCOMO deployment.\nThe integration of the GPU and DPU brings all front-haul enhanced common public radio interface (eCPRI) data traffic into the GPU without the CPU in the datapath. This is a full inline L1 offload, so the solution achieves high performance by avoiding the back-and-forth of eCPRI data between the CPU and the hardware accelerator in alternative systems across the host PCIe interface.\nAfter the data is in the GPU, it benefits from the massive parallelism of the GPU architecture to optimize the processing capacity of the base station system. This brings improved RU capacity and processing power, provides a high-quality communications environment, and can handle high-load data processing along with future improvements in antenna technologies.\nNVIDIA Grace Hopper and NVIDIA BlueField DPU\nThe NVIDIA Grace Hopper Superchip brings together the NVIDIA Grace CPU, which is based on the Arm architecture, and the high-performance NVIDIA Hopper GPU. The BlueField DPU helps to achieve the same performance from full inline L1 offload in a similar way to NVIDIA Converged Accelerator.\nHowever, the biggest boost to performance comes from the integration of the CPU and GPU architectures using NVIDIA NVLink-C2C to deliver a CPU+GPU coherent memory model for accelerated workloads such as 5G vRAN.\nNVIDIA NVLink-C2C is the NVIDIA memory-coherent, high-bandwidth, and low-latency interconnect. It delivers up to 900 GB/s total bandwidth: 7x higher bandwidth than the x16 PCIe Gen5 lanes commonly used in accelerated systems.\nWith the NVLink-C2C memory coherency, both CPU and GPU threads can concurrently and transparently access both CPU and GPU resident memory, enabling the RAN to optimize how it handles eCPRI data across CPU and GPU.\nX86 + NVIDIA Converged Accelerator\n(refer to AX800)\nGrace Hopper + BlueField 3\n(refer to GH200)\nUp to 20 peak cells of 4T4T = equivalent to 36 Gbps per 2U server\nConfiguration*\nUp to 40 peak cells of 4T4T = equivalent to 72 Gbps per 2U server\n3.2X\n(36 Gbps)\n5G Performance*\n6.4X\n(72 Gbps)\n76X\n(tokens/sec)\nLLM*\n(LLAMA 65B)\n284X\n(tokens/sec)\n1.3X\n(34 W/Gbps)\n5G Power Efficiency*\n3.3X\n(13 W/Gbps)\n1X\n(PCIE)\nCPU \u2013 GPU Bandwidth*\n7X\n(C2C)\n* Relative performance is estimated vs X86 5G SKU for 100 MHZ ,4T4R, 4DL/2UL. PCIE Gen5, 2U Server\nTable 1: Comparative performance of NVIDIA Converged Accelerators vs NVIDIA Grace Hopper for 5G vRAN. Footnote: DOCOMO\u2019s current deployment is using the X86 + NVIDIA Converged Accelerator option.\nCarrier-grade support\nThe NVIDIA Aerial platform provides a full-stack, carrier-grade, hardened, and mature 5G solution with 10 years of long-life support and maintenance for telecommunications operators. This level of carrier-grade support provides assurances of reliability and resilience for telcos for field or commercial deployment using the platform.\nOREX: Building out from Japan and beyond\nThe commercial deployment of a 5G Open RAN network by NTT DOCOMO, using the NVIDIA 5G platform, is a major milestone for the telecommunications industry. It showcases the capabilities of GPU-based acceleration for computationally intensive L1 PHY processing.\nThis new network comes with improved performance, flexibility, and scalability, plus higher cell density, significant improvements in energy efficiency, and a reduction in TCO. The delivery of this network paves the way for widespread adoption of GPU-based acceleration in cellular RANs.\nDOCOMO and its partners in OREX are working together to promote a multi-vendor, Open RAN\u2013compliant 5G vRAN to the global operator community. The commercial deployment in Japan is in alignment with the vision of OREX, enabling its members to validate their solutions commercially and then promote it to other operators globally.\nNVIDIA continues to work with DOCOMO and other partners to support operators around the world to deploy high-performance, energy-efficient, software-defined, commercial 5G vRAN."}], "https://developer.nvidia.com/blog/new-video-series-cuda-developer-tools-tutorials/": [{"text": "The article introduces a new tutorial video series called CUDA Developer Tools, aimed at helping developers harness CUDA acceleration on NVIDIA GPUs. The series covers debugging, profiling, and optimizing CUDA code using tools like NVIDIA Nsight Systems and Nsight Compute. The videos walk viewers through how to analyze performance reports, offer debugging tips, and optimize CUDA code. Nsight Systems provides system-wide performance traces and metrics, while Nsight Compute offers an interactive profiler for CUDA and NVIDIA OptiX applications. The tutorials explain how to set up and use these tools, including key capabilities and features for workload analysis. The videos also cover how to read and analyze reports generated by Nsight Compute, including runtime information and speedup estimations. Viewers are encouraged to stay tuned for more episodes of CUDA Developer Tools tutorials and to download the Nsight Developer Tools for CUDA.", "text_components": ["New Video Series: CUDA Developer Tools Tutorials\nGPU acceleration is enabling faster and more intelligent applications than ever before, and the CUDA Toolkit is key to harnessing acceleration on NVIDIA GPUs. But debugging, profiling, and optimizing CUDA can be a challenge, especially if you are unable to inspect hardware-level throughput and performance. To help you harness CUDA acceleration, NVIDIA offers Nsight Developer Tools.\nCUDA Developer Tools is a new tutorial video series for getting started with CUDA developer tools. Grow your skills, apply our examples to your own development environment, and stay updated on features and functionalities. The videos walk you through how to analyze performance reports, offer debugging tips and tricks, and show you the best ways to optimize your CUDA code.\nWatch the first three tutorials in the series now.\nCUDA Developer Tools | NVIDIA Nsight Tools Ecosystem introduces you to the suite of tools NVIDIA offers. Learn how each is used, and how they\u2019re built to work together.\nNVIDIA Nsight Systems offers system-wide performance traces and metrics, visualization of CPU and GPU utilization, API calls, memory copies, and more.\nWith NVIDIA Nsight Compute, you can dive deeper with an interactive profiler for CUDA and NVIDIA OptiX applications. It provides detailed performance metrics and API debugging. Guided analysis simplifies the performance tuning process with a built-in rule set for CUDA optimization designed by NVIDIA engineers.\nCUDA Developer Tools | Intro to NVIDIA Nsight Systems walks you through how to trace performance and hardware activity to better tune your CUDA application. Learn the profiling process, including project setup and configuration, and how to specify profiling targets, launch an application to trace, and view the results.\nYou\u2019ll also learn how to read and analyze an Nsight Systems report. The built-in timeline view provides an intuitive visualization of system events, making it easy to understand your application\u2019s behavior. With the timeline view, you can see CPU threads, CUDA API calls, GPU activity, and more.\nCUDA Developer Tools | Intro to NVIDIA Nsight Compute explains how to use Nsight Compute to analyze CUDA kernels. You\u2019ll learn how to set up Nsight Compute, including key capabilities and features for workload analysis. Discover how Nsight Compute collects performance metrics, and how to configure permissions for accessing GPU counters and source-level details.\nThe video also covers the detailed reports Nsight Compute generates, and how to read information like runtime information, speedup estimations, and more.\nStay tuned for more episodes of CUDA Developer Tools tutorials.\nLearn more about NVIDIA Nsight Developer Tools for CUDA.\nDownload NVIDIA Nsight Systems.\nDownload NVIDIA Nsight Compute.\nAsk questions and dive deeper in the CUDA Developer Tools forum."], "document_title": "New Video Series: CUDA Developer Tools Tutorials", "document_url": "https://developer.nvidia.com/blog/new-video-series-cuda-developer-tools-tutorials/", "document_date": "2023-09-25T17:00:00", "document_date_modified": "2023-10-19T19:06:02", "document_full_text": "New Video Series: CUDA Developer Tools Tutorials\nGPU acceleration is enabling faster and more intelligent applications than ever before, and the CUDA Toolkit is key to harnessing acceleration on NVIDIA GPUs. But debugging, profiling, and optimizing CUDA can be a challenge, especially if you are unable to inspect hardware-level throughput and performance. To help you harness CUDA acceleration, NVIDIA offers Nsight Developer Tools.\nCUDA Developer Tools is a new tutorial video series for getting started with CUDA developer tools. Grow your skills, apply our examples to your own development environment, and stay updated on features and functionalities. The videos walk you through how to analyze performance reports, offer debugging tips and tricks, and show you the best ways to optimize your CUDA code.\nWatch the first three tutorials in the series now.\nCUDA Developer Tools | NVIDIA Nsight Tools Ecosystem introduces you to the suite of tools NVIDIA offers. Learn how each is used, and how they\u2019re built to work together.\nNVIDIA Nsight Systems offers system-wide performance traces and metrics, visualization of CPU and GPU utilization, API calls, memory copies, and more.\nWith NVIDIA Nsight Compute, you can dive deeper with an interactive profiler for CUDA and NVIDIA OptiX applications. It provides detailed performance metrics and API debugging. Guided analysis simplifies the performance tuning process with a built-in rule set for CUDA optimization designed by NVIDIA engineers.\nCUDA Developer Tools | Intro to NVIDIA Nsight Systems walks you through how to trace performance and hardware activity to better tune your CUDA application. Learn the profiling process, including project setup and configuration, and how to specify profiling targets, launch an application to trace, and view the results.\nYou\u2019ll also learn how to read and analyze an Nsight Systems report. The built-in timeline view provides an intuitive visualization of system events, making it easy to understand your application\u2019s behavior. With the timeline view, you can see CPU threads, CUDA API calls, GPU activity, and more.\nCUDA Developer Tools | Intro to NVIDIA Nsight Compute explains how to use Nsight Compute to analyze CUDA kernels. You\u2019ll learn how to set up Nsight Compute, including key capabilities and features for workload analysis. Discover how Nsight Compute collects performance metrics, and how to configure permissions for accessing GPU counters and source-level details.\nThe video also covers the detailed reports Nsight Compute generates, and how to read information like runtime information, speedup estimations, and more.\nStay tuned for more episodes of CUDA Developer Tools tutorials.\nLearn more about NVIDIA Nsight Developer Tools for CUDA.\nDownload NVIDIA Nsight Systems.\nDownload NVIDIA Nsight Compute.\nAsk questions and dive deeper in the CUDA Developer Tools forum."}], "https://developer.nvidia.com/blog/validating-nvidia-drive-sim-radar-models/": [{"text": "The article discusses the importance of validating sensor models in autonomous vehicle development, specifically focusing on radar sensors. The validation process involves comparing simulated radar data to real-world radar data in various scenarios to ensure accuracy and fidelity. The study includes tests for detection capabilities, separation capability, and dynamic conditions such as Doppler measurements. Results show a high level of correlation between simulated and real-world radar data, with the model accurately capturing complex interactions. The study aims to enhance the model's fidelity by incorporating more complex objects and scenarios. Validating radar sensor behavior in simulated environments can improve system development efficiency, reduce reliance on real-world data collection, and enhance the safety and performance of autonomous vehicles. Future experiments will focus on further improving the model's accuracy and bridging the gap between simulation and reality.", "text_components": ["Validating NVIDIA DRIVE Sim Radar Models\nSensor simulation is a critical tool to address the gaps in real-world data for autonomous vehicle (AV) development. However, it is only effective if sensor models accurately reflect the physical world.\nSensors can be either passive, such as cameras\u2014or active, sending out either an electromagnetic wave (lidar, radar) or an acoustic wave (ultrasonic) to generate the sensor output. When modeled in simulation, each modality must be validated against its real-world counterpart.\nIn previous posts, we detailed the validation process for camera and lidar models in NVIDIA DRIVE Sim. See Validating NVIDIA DRIVE Sim Camera Models and Validating NVIDIA DRIVE Sim Lidar Models. This post will cover radar, an essential sensor for obstacle detection and avoidance.\nThere are multiple ways to approach radar validation. You can compare how an AV stack trained on real-world data behaves when encountering synthetic radar data, for example. Or, you can compare synthetic radar data to its physical counterpart in real-world experiments.\nValidating the model with an AV stack only evaluates its ability to the extent of triggering the AV function, which tests for a lower fidelity ceiling. For this reason, we will focus on the second approach.", "Radar sensor pipeline\nRadar sensors emit radio waves that reflect off objects in the scene and return to the sensor. The received signal then undergoes multiple processing stages that identify the returns from real objects and filter noises. These returns are then presented as a point cloud of the environment.\nSuch postprocessing methods are typically part of a sensor maker\u2019s intellectual property, and thus NVIDIA DRIVE Sim sensor models aim to approximate them. Sensor suppliers in the DRIVE Sim ecosystem can include the exact implementations of their entire pipelines, including postprocessing.\nDRIVE Sim uses ray tracing to model active sensors. Rays that embed the radar radiation pattern are fired into the scene. For each ray that hits an object in the 3D scene, secondary rays are created for reflections and transmissions based on the hit material\u2019s wavelength-dependent properties. The materials in DRIVE Sim use bidirectional scattering distribution functions (BSDF). This enables simulating multipath effects.\nDRIVE Sim ray tracing is time-aware. Each ray has its own timestamp and sees a different environment and sensor position to match that time. This enables simulating time-based effects, such as rolling shutter and Doppler.\nFor radar, after the stopping criteria for ray tracing are met, the sensor model consolidates the returns and processes them. Our radar model accounts for the sensor\u2019s field of view (FOV), antenna directivity, resolutions, ambiguities, and the radar\u2019s sensitivity pattern. A constant false alarm rate (CFAR) algorithm is used to extract valid detections over a simulated noise baseline. The detections are then encoded with the exact communication protocol as the real sensor to serve hardware-in-the-loop use cases.\nBlock diagram for active sensors in DRIVE Sim, starting with capturing the world state, ray-tracing with the NVIDIA Omniverse RTX Renderer, RTX sensor model, postprocessing, then integration with the AV stack.\nFigure 1. Active sensor pipeline", "Radar validation\nTo validate the DRIVE Sim radar model, we designed three scenarios based on the technical product specification (TPS) of the real radar. The goal was to test various components of the radar sensor\u2019s performance, including its detection capability over its FOV, separation capability, and accuracy in dynamic conditions. Then, we created a digital twin environment in simulation, collecting the equivalent data in DRIVE Sim for detailed analysis.", "Data collection environment\nFor the data collection environment, we opted for an open spacious area\u2014the Transportation Research Center in California. In this environment, we could minimize noise and unwanted reflections to simplify digital twin construction in DRIVE Sim.\nZoomed-out aerial image of the test site.\nFigure 2. Test site for data collection. Image courtesy of TRC", "Vehicle setup\nWe used radar sensors in the NVIDIA DRIVE Hyperion AV reference architecture, so developers building on NVIDIA DRIVE can easily transition between simulation and the real world. The sensors were mounted on a development vehicle (Figure 3). For this case, the front center radar (FCR) was the focus for evaluation.\nOperating at a frequency of 77GHz, the radar under test included two scans: a near scan, with a wide FOV but limited range, and a far scan with extended range but a narrow FOV. Additionally, a 360\u00b0 rotating lidar sensor (LD1) was mounted on top of the car to provide pseudo ground truth data.\nSide-by-side images of the real-world test vehicle outfitted with sensors from two different angles (top). Two diagram sketches of where the sensors are placed (bottom).\nFigure 3. Sensor mounting positions on vehicle under test", "Model validation process\nCentral to our three validation experiments were two trihedral corner reflectors. These are standard radar targets that reflect energy back in the incident direction. They are characterized by a radar cross-section (RCS) value, which is a measure of an object\u2019s ability to reflect radar energy back to the receiver.\nWe used one with a \u201chigh\u201d RCS of 15.71 decibels relative to a square meter (dBsm), and another with a \u201clow\u201d RCS of 4.79 dBsm to characterize the model\u2019s behavior across a wide RCS range.\nThree side-by-side images, the first showing a sketch of how the corner reflector reflects energy, followed by an image of the high radar cross section corner reflector and the low radar cross section corner reflector.\nFigure 4. Corner reflector principle (left); high RCS CR (center); low RCS CR (right) The lidar\u2019s pseudo-ground truth measurements were used to replicate the test setup virtually in DRIVE Sim with accurate material assignments.\nAfter collecting the virtual data, we compared the radar model outputs to the real radar. Results of the comparison are presented for the three scenarios below.", "Scenario 1: FOV sampling with corner reflector\nIn the first scenario, we assessed the radar\u2019s detection capabilities across its FOV and verified its range and azimuth accuracy.\nWe placed a corner reflector at multiple grid positions within the radar\u2019s FOV, as shown in Figure 5. We assumed the sensor\u2019s behavior to be symmetric, and thus we only sampled half of the FOV to increase the sampling density.\nAltogether, we recorded a total of 579 positions for the high RCS corner reflector and 632 for the low RCS corner reflector.\nA diagram showing the different positions where the corner reflectors were placed, shown as dots, with side-by-side images below of the real-world vehicle and corner reflector next to the simulated version.\nFigure 5. Example grid positions (left); real-world environment (center); digital twin (right) Figure 6 depicts a top-down view of both real and simulated radar detections across all 1,211 high and low RCS corner reflector positions. We used this as a coherence check to start. Although we observed differences in FOV coverages above 80m, the overall coverage presented a noticeable similarity that is sufficient for the cross check.\nSide-by-side scatter plots showing the radar detections of the real sensor and the simulated sensor model, with similar patterns displayed on each.\nFigure 6. Top-down view of real and simulated radar detections The histograms in Figure 7 present the error distribution in range, azimuth, and RCS relative to the ground truth for both the high and low RCS corner reflectors, combined. Where applicable, we quantified the results by fitting a Gaussian distribution to the data. Results for the real radar are displayed on the left, while the DRIVE Sim data is shown on the right.\nHistograms showing the error distributions for range compared between the real and simulated radar.\nHistograms showing the error distributions for azimuth, compared between the real and simulated radar.\nHistograms showing the error distributions for RCS, compared between the real and simulated radar.\nFigure 7. Error distribution histograms for both CRs in Scenario 1\nWe observed a high level of agreement between real and simulation data over various positions in the radar\u2019s FOV, with both means and standard deviations sharing the same order of magnitude.\nThe discrepancies are primarily attributable to uncertainties in the ground truth. While the lidar sensor has millimeter-level accuracy, identifying the position and orientation of an object like a pole-mounted corner reflector can introduce errors in the centimeter range. Furthermore, while we calibrated the sensor positions prior to data collection, there might still be minor misalignments.\nOverall, the agreement observed in RCS values, detection pattern, and the accuracies of the various detection properties validated the radar\u2019s fidelity, wave propagation, and material modeling.", "Scenario 2: Corner reflector separation capability test\nIn scenarios where road objects are near each other (stationary vehicles under a bridge, pedestrians or motorcyclists next to a vehicle or guard rail, or two closely parked cars, for example) radars can encounter difficulties in distinguishing individual objects. For this reason, it is crucial to accurately simulate this characteristic, known as separation capability.\nWe assessed this capability by placing the two corner reflectors in close proximity to each other. The data was sampled at four different distances from the sensor. For each distance, the corner reflectors were positioned as shown in Figure 8.\nWe selected different positions for the radar\u2019s near and far scans, dependent on their corresponding FOV, to analyze their range and azimuth separation capabilities.\nA simulated image of two corner reflectors and a vehicle in the background (left) and a sketched diagram of the corner reflectors\u2019 position in relation to the vehicle (right).\nFigure 8. Digital twin of the test vehicle and corner reflectors (left) and example positions for both corner reflectors (right) Tables 1 and 2 summarize the results for the near and far scans. The left column represents positions where CRs are close and we expect one detection per the TPS. The center and right columns represent positions where we expect two detections per the TPS. Each cell details the exact location of the CRs, and the number of detections observed for simulation and the real world. The percentage denotes the proportion of detections that followed our expectations in all considered scans. We define success when the simulated and real percentages are less than 20% apart.\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\n0\u00b0 and 50m\nCR1 (50, 0), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (100%)\nCR1 (50, 0), CR2 (50.5, 0)\nReal: Two detections (10%)\nSim: Two detections (0%)\nCR1 (50, 0), CR2 (50, -3)\nReal: Two detections (0%)\nSim: Two detections (0%)\n-45\u00b0 and 20m\nCR1 (14.14, -14.14), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (100%)\nCR1 (14.14, -14.14),CR2 (15.14, -14.14)\nReal: Two detections (100%)\nSim: Two detections (100%)\nCR1 (14.14, -14.14), CR2 (16.44, -11.84)\nReal: Two detections (100%)\nSim: Two detections (100%)\n-45\u00b0 and 50m\nCR1 (35.36, -35.36), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (100%)\nCR1 (35.36, -35.36),CR2 (36.36, -35.36)\nReal: Two detections (5%)\nSim: Two detections (100%)\nCR1 (35.36, -35.36), CR2 (41, -29)\nReal: Two detections (80%)\nSim: Two detections (100%)\nTable 1. Number of detections per scan for near scan\nResults for all positions at 0\u00b0 and 50m, and -45\u00b0 and 20m, demonstrated a high degree of similarity between real and simulated. We observed a minor discrepancy at 0\u00b0 and 50m where CR2 (50.5, 0). In this scenario, the real radar returned two detections instead of one in 10% of the scans.\nComparisons made at -45\u00b0 and 50m were mostly consistent, except for CR2 (36.36, -35.36), where the simulated radar returned two detections.\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\n0\u00b0 and 50m\nCR1 (50, 0), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (95%)\nCR1 (50, 0), CR2 (50.5, 0)\nReal: One detection (100%)\nSim: One detection (5%)\nCR1 (50, 0), CR2 (50, -3)\nReal: Two detections (0%)\nSim: Two detections (5%)\n0\u00b0 and 100m\nCR1 (100,0), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (60%)\nCR1 (100, 0), CR2 (104, 0)\nReal: Two detections (100%)\nSim: Two detections (95%)\nCR1 (100, 0), CR2 (100, -6)\nReal: Two detections (0%)\nSim: Two detections (0%)\nTable 2. Number of detections per scan for far scan As shown in Table 2, the results from the simulated and real-world sensors are largely in correlation. Significant deviations are noted at 0\u00b0 and 50m where CR2 (50.5, 0). Furthermore, for 0\u00b0 and 100m where CR1=CR2, the simulated radar returns two detections in 40% of scans, while real world never returns two detections.\nUpon further analysis of the deviations, we attribute them to the fact that the technical product specification only described the radar\u2019s separation capability at a few angles. This made it difficult for us to estimate the exact layout of range and azimuth bins.\nIn addition, our parameterization and implementation for the CFAR thresholding algorithm is an estimate, as it is a supplier\u2019s intellectual property. The separation capability of the radar is expected to be quite sensitive to the CFAR behavior.\nOverall, across both the near and far scans, we found the simulated separation capability to be close enough to the real sensor.", "Scenario 3: Driving toward a corner reflector with constant speed\nDoppler measurement enables radars to accurately detect the speed of moving targets. We evaluated the model\u2019s performance in dynamic conditions, where the test vehicle drove directly toward high and low RCS corner reflectors, taking separate trips at constant speeds of 10kph, 40kph, and 80kph, as shown below.\nAn image with a corner reflector in the foreground and a test vehicle at a far distance in the background./\nFigure 9. Real-world environment\nA GIF of a simulated vehicle driving toward a simulated corner reflector.\nFigure 10. Digital twin environment The histograms in Figure 11 present the Doppler error results for both the high and low RCS corner reflectors.\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nHigh RCS, 10kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nHigh RCS, 40kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nHigh RCS, 80kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nLow RCS, 10kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nLow RCS, 40kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nLow RCS, 80kph\nFigure 11. Doppler error histograms for Scenario 3\nWe observed a remarkably high correlation in Doppler across all tested speeds. For 10kph, both real and simulated distributions exhibited similar peaks at approximately -3mps, -2mps, and 0mps. For 40kph, the peaks aligned around -10mps. For 80kph, peaks were observed at -20mps and 10mps. This high degree of accuracy was further demonstrated when plotting Doppler against range.\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nHigh RCS, 10kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nHigh RCS, 40kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nHigh RCS, 80kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nLow RCS, 10kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nLow RCS, 40kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nLow RCS, 80kph\nFigure 12. Doppler mean eror and standard deviation over the range for Scenario 3\nFigure 13 illustrates a top-down view of real and simulated radar detections across high and low RCS corner reflector positions and vehicle speeds.\nScatter plot showing both the real and simulated radar detections compared with the ground truth. The real and simulated detections follow similar patterns.\nFigure 13. Top-down view of real and simulated radar detections Both the real and simulated Doppler measurements demonstrated substantial agreement in their mean and standard deviation values. However, we noticed deviations at higher speeds.\nWe attribute these deviations to uncertainties during the creation of the digital twin. The position, speed, and orientation of the ego vehicle were estimated using lidar without the aid of differential GPS. The errors in these estimations are amplified at higher speeds as can be seen in 80kph.\nIn addition, we observed that DRIVE Sim is able to replicate the Radar Aliasing phenomenon, which occurs when an object\u2019s radial velocity surpasses the radar\u2019s maximum measurable unambiguous velocity, resulting in ambiguous velocity values. Real-world radars subtly shift the maximum measurable unambiguous velocity range with each cycle, enabling subsequent perception algorithms to disambiguate the velocities.\nOur simulation accurately replicated this behavior, as demonstrated by the alignment of the peaks in both the real and simulated data. Particularly, at a speed of 80kph, both the real and simulated radar exhibited similar velocity wrapping.", "Conclusion\nThis study presents our first iteration for an in-depth validation of our simulated radar model using real-world data, including static and dynamic conditions. The analysis was designed to assess the model\u2019s fidelity and accuracy across a variety of performance metrics.\nOur results demonstrate a high degree of correlation between the simulated and real-world radar data with the model adeptly handling complex interactions such as multibounce effects.\nUpcoming experiments will focus on capturing radar data from more complex objects (vehicles, pedestrians, motorbikes) mimicking real-world scenarios. These objects not only have more complicated geometries, but are also composed of a variety of materials, thus introducing further complexities in radar wave interactions. Through these efforts, we aim to continually enhance the model\u2019s fidelity, further bridging the gap between simulation and reality.\nBy validating accurate radar sensor behavior in simulated scenarios, we can improve system development efficiency, reduce dependence on costly and time-consuming real-world data collection, and enhance the safety and performance of AV systems.\nTo learn more, see our previously published posts:\nValidating NVIDIA DRIVE Sim Camera Models\nValidating NVIDIA DRIVE Sim Lidar Models"], "document_title": "Validating NVIDIA DRIVE Sim Radar Models", "document_url": "https://developer.nvidia.com/blog/validating-nvidia-drive-sim-radar-models/", "document_date": "2023-09-26T17:01:11", "document_date_modified": "2023-10-19T19:06:01", "document_full_text": "Validating NVIDIA DRIVE Sim Radar Models\nSensor simulation is a critical tool to address the gaps in real-world data for autonomous vehicle (AV) development. However, it is only effective if sensor models accurately reflect the physical world.\nSensors can be either passive, such as cameras\u2014or active, sending out either an electromagnetic wave (lidar, radar) or an acoustic wave (ultrasonic) to generate the sensor output. When modeled in simulation, each modality must be validated against its real-world counterpart.\nIn previous posts, we detailed the validation process for camera and lidar models in NVIDIA DRIVE Sim. See Validating NVIDIA DRIVE Sim Camera Models and Validating NVIDIA DRIVE Sim Lidar Models. This post will cover radar, an essential sensor for obstacle detection and avoidance.\nThere are multiple ways to approach radar validation. You can compare how an AV stack trained on real-world data behaves when encountering synthetic radar data, for example. Or, you can compare synthetic radar data to its physical counterpart in real-world experiments.\nValidating the model with an AV stack only evaluates its ability to the extent of triggering the AV function, which tests for a lower fidelity ceiling. For this reason, we will focus on the second approach.\nRadar sensor pipeline\nRadar sensors emit radio waves that reflect off objects in the scene and return to the sensor. The received signal then undergoes multiple processing stages that identify the returns from real objects and filter noises. These returns are then presented as a point cloud of the environment.\nSuch postprocessing methods are typically part of a sensor maker\u2019s intellectual property, and thus NVIDIA DRIVE Sim sensor models aim to approximate them. Sensor suppliers in the DRIVE Sim ecosystem can include the exact implementations of their entire pipelines, including postprocessing.\nDRIVE Sim uses ray tracing to model active sensors. Rays that embed the radar radiation pattern are fired into the scene. For each ray that hits an object in the 3D scene, secondary rays are created for reflections and transmissions based on the hit material\u2019s wavelength-dependent properties. The materials in DRIVE Sim use bidirectional scattering distribution functions (BSDF). This enables simulating multipath effects.\nDRIVE Sim ray tracing is time-aware. Each ray has its own timestamp and sees a different environment and sensor position to match that time. This enables simulating time-based effects, such as rolling shutter and Doppler.\nFor radar, after the stopping criteria for ray tracing are met, the sensor model consolidates the returns and processes them. Our radar model accounts for the sensor\u2019s field of view (FOV), antenna directivity, resolutions, ambiguities, and the radar\u2019s sensitivity pattern. A constant false alarm rate (CFAR) algorithm is used to extract valid detections over a simulated noise baseline. The detections are then encoded with the exact communication protocol as the real sensor to serve hardware-in-the-loop use cases.\nBlock diagram for active sensors in DRIVE Sim, starting with capturing the world state, ray-tracing with the NVIDIA Omniverse RTX Renderer, RTX sensor model, postprocessing, then integration with the AV stack.\nFigure 1. Active sensor pipeline\nRadar validation\nTo validate the DRIVE Sim radar model, we designed three scenarios based on the technical product specification (TPS) of the real radar. The goal was to test various components of the radar sensor\u2019s performance, including its detection capability over its FOV, separation capability, and accuracy in dynamic conditions. Then, we created a digital twin environment in simulation, collecting the equivalent data in DRIVE Sim for detailed analysis.\nData collection environment\nFor the data collection environment, we opted for an open spacious area\u2014the Transportation Research Center in California. In this environment, we could minimize noise and unwanted reflections to simplify digital twin construction in DRIVE Sim.\nZoomed-out aerial image of the test site.\nFigure 2. Test site for data collection. Image courtesy of TRC\nVehicle setup\nWe used radar sensors in the NVIDIA DRIVE Hyperion AV reference architecture, so developers building on NVIDIA DRIVE can easily transition between simulation and the real world. The sensors were mounted on a development vehicle (Figure 3). For this case, the front center radar (FCR) was the focus for evaluation.\nOperating at a frequency of 77GHz, the radar under test included two scans: a near scan, with a wide FOV but limited range, and a far scan with extended range but a narrow FOV. Additionally, a 360\u00b0 rotating lidar sensor (LD1) was mounted on top of the car to provide pseudo ground truth data.\nSide-by-side images of the real-world test vehicle outfitted with sensors from two different angles (top). Two diagram sketches of where the sensors are placed (bottom).\nFigure 3. Sensor mounting positions on vehicle under test\nModel validation process\nCentral to our three validation experiments were two trihedral corner reflectors. These are standard radar targets that reflect energy back in the incident direction. They are characterized by a radar cross-section (RCS) value, which is a measure of an object\u2019s ability to reflect radar energy back to the receiver.\nWe used one with a \u201chigh\u201d RCS of 15.71 decibels relative to a square meter (dBsm), and another with a \u201clow\u201d RCS of 4.79 dBsm to characterize the model\u2019s behavior across a wide RCS range.\nThree side-by-side images, the first showing a sketch of how the corner reflector reflects energy, followed by an image of the high radar cross section corner reflector and the low radar cross section corner reflector.\nFigure 4. Corner reflector principle (left); high RCS CR (center); low RCS CR (right) The lidar\u2019s pseudo-ground truth measurements were used to replicate the test setup virtually in DRIVE Sim with accurate material assignments.\nAfter collecting the virtual data, we compared the radar model outputs to the real radar. Results of the comparison are presented for the three scenarios below.\nScenario 1: FOV sampling with corner reflector\nIn the first scenario, we assessed the radar\u2019s detection capabilities across its FOV and verified its range and azimuth accuracy.\nWe placed a corner reflector at multiple grid positions within the radar\u2019s FOV, as shown in Figure 5. We assumed the sensor\u2019s behavior to be symmetric, and thus we only sampled half of the FOV to increase the sampling density.\nAltogether, we recorded a total of 579 positions for the high RCS corner reflector and 632 for the low RCS corner reflector.\nA diagram showing the different positions where the corner reflectors were placed, shown as dots, with side-by-side images below of the real-world vehicle and corner reflector next to the simulated version.\nFigure 5. Example grid positions (left); real-world environment (center); digital twin (right) Figure 6 depicts a top-down view of both real and simulated radar detections across all 1,211 high and low RCS corner reflector positions. We used this as a coherence check to start. Although we observed differences in FOV coverages above 80m, the overall coverage presented a noticeable similarity that is sufficient for the cross check.\nSide-by-side scatter plots showing the radar detections of the real sensor and the simulated sensor model, with similar patterns displayed on each.\nFigure 6. Top-down view of real and simulated radar detections The histograms in Figure 7 present the error distribution in range, azimuth, and RCS relative to the ground truth for both the high and low RCS corner reflectors, combined. Where applicable, we quantified the results by fitting a Gaussian distribution to the data. Results for the real radar are displayed on the left, while the DRIVE Sim data is shown on the right.\nHistograms showing the error distributions for range compared between the real and simulated radar.\nHistograms showing the error distributions for azimuth, compared between the real and simulated radar.\nHistograms showing the error distributions for RCS, compared between the real and simulated radar.\nFigure 7. Error distribution histograms for both CRs in Scenario 1\nWe observed a high level of agreement between real and simulation data over various positions in the radar\u2019s FOV, with both means and standard deviations sharing the same order of magnitude.\nThe discrepancies are primarily attributable to uncertainties in the ground truth. While the lidar sensor has millimeter-level accuracy, identifying the position and orientation of an object like a pole-mounted corner reflector can introduce errors in the centimeter range. Furthermore, while we calibrated the sensor positions prior to data collection, there might still be minor misalignments.\nOverall, the agreement observed in RCS values, detection pattern, and the accuracies of the various detection properties validated the radar\u2019s fidelity, wave propagation, and material modeling.\nScenario 2: Corner reflector separation capability test\nIn scenarios where road objects are near each other (stationary vehicles under a bridge, pedestrians or motorcyclists next to a vehicle or guard rail, or two closely parked cars, for example) radars can encounter difficulties in distinguishing individual objects. For this reason, it is crucial to accurately simulate this characteristic, known as separation capability.\nWe assessed this capability by placing the two corner reflectors in close proximity to each other. The data was sampled at four different distances from the sensor. For each distance, the corner reflectors were positioned as shown in Figure 8.\nWe selected different positions for the radar\u2019s near and far scans, dependent on their corresponding FOV, to analyze their range and azimuth separation capabilities.\nA simulated image of two corner reflectors and a vehicle in the background (left) and a sketched diagram of the corner reflectors\u2019 position in relation to the vehicle (right).\nFigure 8. Digital twin of the test vehicle and corner reflectors (left) and example positions for both corner reflectors (right) Tables 1 and 2 summarize the results for the near and far scans. The left column represents positions where CRs are close and we expect one detection per the TPS. The center and right columns represent positions where we expect two detections per the TPS. Each cell details the exact location of the CRs, and the number of detections observed for simulation and the real world. The percentage denotes the proportion of detections that followed our expectations in all considered scans. We define success when the simulated and real percentages are less than 20% apart.\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\n0\u00b0 and 50m\nCR1 (50, 0), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (100%)\nCR1 (50, 0), CR2 (50.5, 0)\nReal: Two detections (10%)\nSim: Two detections (0%)\nCR1 (50, 0), CR2 (50, -3)\nReal: Two detections (0%)\nSim: Two detections (0%)\n-45\u00b0 and 20m\nCR1 (14.14, -14.14), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (100%)\nCR1 (14.14, -14.14),CR2 (15.14, -14.14)\nReal: Two detections (100%)\nSim: Two detections (100%)\nCR1 (14.14, -14.14), CR2 (16.44, -11.84)\nReal: Two detections (100%)\nSim: Two detections (100%)\n-45\u00b0 and 50m\nCR1 (35.36, -35.36), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (100%)\nCR1 (35.36, -35.36),CR2 (36.36, -35.36)\nReal: Two detections (5%)\nSim: Two detections (100%)\nCR1 (35.36, -35.36), CR2 (41, -29)\nReal: Two detections (80%)\nSim: Two detections (100%)\nTable 1. Number of detections per scan for near scan\nResults for all positions at 0\u00b0 and 50m, and -45\u00b0 and 20m, demonstrated a high degree of similarity between real and simulated. We observed a minor discrepancy at 0\u00b0 and 50m where CR2 (50.5, 0). In this scenario, the real radar returned two detections instead of one in 10% of the scans.\nComparisons made at -45\u00b0 and 50m were mostly consistent, except for CR2 (36.36, -35.36), where the simulated radar returned two detections.\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\nPosition of CRs (x,y) in meters\n0\u00b0 and 50m\nCR1 (50, 0), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (95%)\nCR1 (50, 0), CR2 (50.5, 0)\nReal: One detection (100%)\nSim: One detection (5%)\nCR1 (50, 0), CR2 (50, -3)\nReal: Two detections (0%)\nSim: Two detections (5%)\n0\u00b0 and 100m\nCR1 (100,0), CR2 = CR1\nReal: One detection (100%)\nSim: One detection (60%)\nCR1 (100, 0), CR2 (104, 0)\nReal: Two detections (100%)\nSim: Two detections (95%)\nCR1 (100, 0), CR2 (100, -6)\nReal: Two detections (0%)\nSim: Two detections (0%)\nTable 2. Number of detections per scan for far scan As shown in Table 2, the results from the simulated and real-world sensors are largely in correlation. Significant deviations are noted at 0\u00b0 and 50m where CR2 (50.5, 0). Furthermore, for 0\u00b0 and 100m where CR1=CR2, the simulated radar returns two detections in 40% of scans, while real world never returns two detections.\nUpon further analysis of the deviations, we attribute them to the fact that the technical product specification only described the radar\u2019s separation capability at a few angles. This made it difficult for us to estimate the exact layout of range and azimuth bins.\nIn addition, our parameterization and implementation for the CFAR thresholding algorithm is an estimate, as it is a supplier\u2019s intellectual property. The separation capability of the radar is expected to be quite sensitive to the CFAR behavior.\nOverall, across both the near and far scans, we found the simulated separation capability to be close enough to the real sensor.\nScenario 3: Driving toward a corner reflector with constant speed\nDoppler measurement enables radars to accurately detect the speed of moving targets. We evaluated the model\u2019s performance in dynamic conditions, where the test vehicle drove directly toward high and low RCS corner reflectors, taking separate trips at constant speeds of 10kph, 40kph, and 80kph, as shown below.\nAn image with a corner reflector in the foreground and a test vehicle at a far distance in the background./\nFigure 9. Real-world environment\nA GIF of a simulated vehicle driving toward a simulated corner reflector.\nFigure 10. Digital twin environment The histograms in Figure 11 present the Doppler error results for both the high and low RCS corner reflectors.\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nHigh RCS, 10kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nHigh RCS, 40kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nHigh RCS, 80kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nLow RCS, 10kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nLow RCS, 40kph\nHistograms comparing the error distributions in Doppler effect between real and simulated radar.\nLow RCS, 80kph\nFigure 11. Doppler error histograms for Scenario 3\nWe observed a remarkably high correlation in Doppler across all tested speeds. For 10kph, both real and simulated distributions exhibited similar peaks at approximately -3mps, -2mps, and 0mps. For 40kph, the peaks aligned around -10mps. For 80kph, peaks were observed at -20mps and 10mps. This high degree of accuracy was further demonstrated when plotting Doppler against range.\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nHigh RCS, 10kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nHigh RCS, 40kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nHigh RCS, 80kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nLow RCS, 10kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nLow RCS, 40kph\nMean error and standard deviation for Doppler effect between real and simulated radar sensors.\nLow RCS, 80kph\nFigure 12. Doppler mean eror and standard deviation over the range for Scenario 3\nFigure 13 illustrates a top-down view of real and simulated radar detections across high and low RCS corner reflector positions and vehicle speeds.\nScatter plot showing both the real and simulated radar detections compared with the ground truth. The real and simulated detections follow similar patterns.\nFigure 13. Top-down view of real and simulated radar detections Both the real and simulated Doppler measurements demonstrated substantial agreement in their mean and standard deviation values. However, we noticed deviations at higher speeds.\nWe attribute these deviations to uncertainties during the creation of the digital twin. The position, speed, and orientation of the ego vehicle were estimated using lidar without the aid of differential GPS. The errors in these estimations are amplified at higher speeds as can be seen in 80kph.\nIn addition, we observed that DRIVE Sim is able to replicate the Radar Aliasing phenomenon, which occurs when an object\u2019s radial velocity surpasses the radar\u2019s maximum measurable unambiguous velocity, resulting in ambiguous velocity values. Real-world radars subtly shift the maximum measurable unambiguous velocity range with each cycle, enabling subsequent perception algorithms to disambiguate the velocities.\nOur simulation accurately replicated this behavior, as demonstrated by the alignment of the peaks in both the real and simulated data. Particularly, at a speed of 80kph, both the real and simulated radar exhibited similar velocity wrapping.\nConclusion\nThis study presents our first iteration for an in-depth validation of our simulated radar model using real-world data, including static and dynamic conditions. The analysis was designed to assess the model\u2019s fidelity and accuracy across a variety of performance metrics.\nOur results demonstrate a high degree of correlation between the simulated and real-world radar data with the model adeptly handling complex interactions such as multibounce effects.\nUpcoming experiments will focus on capturing radar data from more complex objects (vehicles, pedestrians, motorbikes) mimicking real-world scenarios. These objects not only have more complicated geometries, but are also composed of a variety of materials, thus introducing further complexities in radar wave interactions. Through these efforts, we aim to continually enhance the model\u2019s fidelity, further bridging the gap between simulation and reality.\nBy validating accurate radar sensor behavior in simulated scenarios, we can improve system development efficiency, reduce dependence on costly and time-consuming real-world data collection, and enhance the safety and performance of AV systems.\nTo learn more, see our previously published posts:\nValidating NVIDIA DRIVE Sim Camera Models\nValidating NVIDIA DRIVE Sim Lidar Models"}], "https://developer.nvidia.com/blog/new-video-representing-data-with-openusd-custom-schemas/": [{"text": "The article discusses the importance of custom schemas in Universal Scene Description (OpenUSD or USD) for representing and encoding sophisticated virtual worlds. Custom schemas formalize data models, populate the USD schema registry with definitions, and make data queryable and interpretable by USD-compliant runtimes. The article covers topics such as data formalization, data hierarchies, data modeling compared to runtime behaviors, schema types, and the standardization process for new schemas. Custom schemas in USD allow for the creation of more complex virtual worlds, with continuous development of custom schemas to expand the digital landscape. The article also mentions resources for learning more about OpenUSD and encourages developers to start building OpenUSD-based apps and tools on the Omniverse platform. Stay updated on the platform by subscribing to the newsletter and following NVIDIA Omniverse on social media platforms.", "text_components": ["New Video: Representing Data with OpenUSD Custom Schemas\nCustom schemas in Universal Scene Description, known as OpenUSD or USD, are pivotal for developers seeking to represent and encode sophisticated virtual worlds. By formalizing data models, schemas enable the interpretation of raw data by USD-compliant runtimes.\nWhether underpinning physics simulations or expanding digital twins, custom schemas provide the foundation for creativity, fidelity, and innovation in virtual environments.\nIn the third installment of this OpenUSD series, I share what developers must know about custom schemas.\nSpecifically, we dive into:\nData formalization: Custom schemas formalize data models, such as geometric meshes. These schemas populate the USD schema registry with canonical definitions.\nData hierarchies: USD scenes, or stages, consist of prims \u2014hierarchies of primitives\u2014with each prim serving as a data container. Schemas formalize prim data, making them queryable and interpretable by USD-compliant runtimes.\nData modeling compared to runtime behaviors: Schemas define data structure but not runtime functionality, highlighting the separation between the two aspects in USD development.\nSchema types: USD supports various schema types, including IsA (typed) schemas like UsdGeomMesh, and API schemas such as UsdPhysicsRigidBodyAPI to additionally annotate already-typed prims.\nStandardization process: Standardizing new schemas involves prototyping, formalization, internal reviews, whitepaper publication, and broad industry review, leading to adoption as a USD standard.\nCustom schemas in USD open up numerous pathways for crafting more complex virtual worlds. OpenUSD includes core schemas like geometry and shading, with continuous development of custom schemas to broaden the digital landscape.\nVideo 1. Universal Scene Description (OpenUSD): Custom Schemas\nFor the latest USD resources and tutorials, visit our OpenUSD resources page. Try the free RunUSD Validation Service to validate the compatibility of your OpenUSD assets and applications against a range of OpenUSD versions and configurations.\nIf you\u2019re a developer, start building OpenUSD-based apps and tools on the Omniverse platform. Stay up to date on the platform by subscribing to the newsletter, and following NVIDIA Omniverse on Instagram, Medium, and Twitter. Check out our forums, Discord server, Twitch, and YouTube channels."], "document_title": "New Video: Representing Data with OpenUSD Custom Schemas", "document_url": "https://developer.nvidia.com/blog/new-video-representing-data-with-openusd-custom-schemas/", "document_date": "2023-09-20T16:20:00", "document_date_modified": "2023-10-19T19:06:03", "document_full_text": "New Video: Representing Data with OpenUSD Custom Schemas\nCustom schemas in Universal Scene Description, known as OpenUSD or USD, are pivotal for developers seeking to represent and encode sophisticated virtual worlds. By formalizing data models, schemas enable the interpretation of raw data by USD-compliant runtimes.\nWhether underpinning physics simulations or expanding digital twins, custom schemas provide the foundation for creativity, fidelity, and innovation in virtual environments.\nIn the third installment of this OpenUSD series, I share what developers must know about custom schemas.\nSpecifically, we dive into:\nData formalization: Custom schemas formalize data models, such as geometric meshes. These schemas populate the USD schema registry with canonical definitions.\nData hierarchies: USD scenes, or stages, consist of prims \u2014hierarchies of primitives\u2014with each prim serving as a data container. Schemas formalize prim data, making them queryable and interpretable by USD-compliant runtimes.\nData modeling compared to runtime behaviors: Schemas define data structure but not runtime functionality, highlighting the separation between the two aspects in USD development.\nSchema types: USD supports various schema types, including IsA (typed) schemas like UsdGeomMesh, and API schemas such as UsdPhysicsRigidBodyAPI to additionally annotate already-typed prims.\nStandardization process: Standardizing new schemas involves prototyping, formalization, internal reviews, whitepaper publication, and broad industry review, leading to adoption as a USD standard.\nCustom schemas in USD open up numerous pathways for crafting more complex virtual worlds. OpenUSD includes core schemas like geometry and shading, with continuous development of custom schemas to broaden the digital landscape.\nVideo 1. Universal Scene Description (OpenUSD): Custom Schemas\nFor the latest USD resources and tutorials, visit our OpenUSD resources page. Try the free RunUSD Validation Service to validate the compatibility of your OpenUSD assets and applications against a range of OpenUSD versions and configurations.\nIf you\u2019re a developer, start building OpenUSD-based apps and tools on the Omniverse platform. Stay up to date on the platform by subscribing to the newsletter, and following NVIDIA Omniverse on Instagram, Medium, and Twitter. Check out our forums, Discord server, Twitch, and YouTube channels."}], "https://developer.nvidia.com/blog/how-to-train-an-object-detection-model-for-visual-inspection-with-synthetic-data/": [{"text": "AI is transforming industrial visual inspection by detecting defects and missing parts during assembly. Developing accurate object detection models is challenging, as real-world datasets often lack complexity and diversity. Synthetic data can bridge this gap, providing diverse, randomized data that closely resemble real-world scenarios. Edge Impulse and NVIDIA Omniverse Replicator allow users to generate synthetically annotated images for training object detection models. By creating virtual environments with movable and immovable objects, adjusting lighting, and using multiple cameras, realistic images can be generated for training models. The process includes annotating data, building the model, and testing with real objects to optimize performance. The iterative training process can be enhanced with synthetic data variations. Using a data-centric approach, developers can create more data around failure points to improve model generalization. Overall, training object detection models with synthetic data offers a practical solution for industrial automation, healthcare, and manufacturing applications.", "text_components": ["How to Train an Object Detection Model for Visual Inspection with Synthetic Data\nAI is rapidly changing industrial visual inspection. In a factory setting, visual inspection is used for many issues, including detecting defects and missing or incorrect parts during assembly. Computer vision can help identify problems with products early on, reducing the chances of them being delivered to customers.\nHowever, developing accurate and versatile object detection models remains challenging for edge AI developers. Robust object detection models require access to comprehensive and representative datasets. In many manufacturing scenarios, real-world datasets fall short when capturing the complexity and diversity of actual scenarios. The constraints of narrow environments and limited variations pose challenges in training models to adapt to a range of situations effectively.\nTeams can harness synthetic data for training models on diverse, randomized data that closely resemble real-world scenarios and address dataset gaps. The result is more accurate and adaptable AI models that can be deployed for a wide range of edge AI applications in industrial automation, healthcare, and manufacturing, to name a few.", "From synthetic data generation to AI training\nEdge Impulse is an integrated development platform that empowers developers to create and deploy AI models for edge devices. It supports data collection, preprocessing, model training, and deployment, helping users integrate AI capabilities into their applications effectively.\nWith NVIDIA Omniverse Replicator, a core extension of NVIDIA Omniverse, users can produce physically accurate and photorealistic, synthetically generated annotated images in Universal Scene Description, known as OpenUSD. These images can then be used for training an object detection model on the Edge Impulse platform.\nNVIDIA Omniverse is a computing platform that enables individuals and teams to develop Universal Scene Description (OpenUSD)-based 3D workflows and applications.\nOpenUSD is a highly versatile and interoperable 3D interchangeable and format that excels in synthetic data generation due to its scalability, performance, versioning, and asset management capabilities, making it an ideal choice for creating complex and realistic datasets. There\u2019s a vast ecosystem of 3D content tools that connect to OpenUSD and USD-based SimReady assets that make it easy to integrate physically-based objects into scenes and accelerate our synthetic data generation workflows.\nOmniverse Replicator enables randomization for USD data across several domains to represent scenarios that reflect real-world possibilities object detection models may encounter.\nA diagram showing the workflow from content in Omniverse being used to generate synthetic datasets in Replicator, which can then be used for AI training with Edge Impulse.\nFigure 1. The end-to-end data to training pipeline, starting with Omniverse Replicator, and ending in the Edge Impulse platform\nUsing the synthetically generated images in USD to train models in Edge Impulse only takes a few clicks with the new Edge Impulse Omniverse extension.\nUsing the extension, which was developed using the Omniverse Kit Python extension template, users can connect to the Edge Impulse API and select the dataset to upload their synthetic data. The Kit Python extension template is a simple and self-explanatory resource for code snippet options and developing an extension quickly.", "Generating synthetic data for an object detection model\nTo understand the workflow for generating synthetic data with Omniverse Replicator and using it to train a model in Edge Impulse, follow our example detecting soda cans model.\nTwo images showing cans being detected at different camera angles and distances.\nFigure 2. An object detection model that detects soda cans after being trained on synthetic datasets\nThe first step in the process is building a virtual replica or a digital twin of the environment that represents the real scenario. The scene for generating synthetic images \u200cconsists of movable and immovable objects. The immovable set includes lights, a conveyor belt, and two cameras, while the movable objects consist of soda cans. Employing domain randomization, you can alter many properties, including location, lighting, colors, texture, background, and foreground of select immovable and movable objects.\nThese assets are represented in Omniverse Replicator through OpenUSD. 3D model files can be converted into USD and imported into Omniverse Replicator using the Omniverse CAD Importer extension.\nLighting plays a pivotal role in realistic image generation. Rectangular lights can emulate light generated from a panel and a dome light brightens the entire scene. You can randomize various parameters for the lights like temperature, intensity, scale, position, and rotation of the lights.\nThe following script shows temperature and intensity randomized through sampling from normal distributions, with scales randomized by a uniform distribution. The position and rotation of lights are fixed to remain constant.\npython\n# Lightning setup for Rectangular light and Dome light\ndef rect_lights(num=1):\nlights = rep.create.light(\nlight_type=\"rect\",\ntemperature=rep.distribution.normal(6500, 500),\nintensity=rep.distribution.normal(0, 5000),\nposition=(45, 110, 0),\nrotation=(-90, 0, 0),\nscale=rep.distribution.uniform(50, 100),\ncount=num\n)\nreturn lights.node\nrep.randomizer.register(rect_lights)\ndef dome_lights(num=3):\nlights = rep.create.light(\nlight_type=\"dome\",\ntemperature=rep.distribution.normal(6500, 500),\nintensity=rep.distribution.normal(0, 1000),\nposition=(45, 120, 18),\nrotation=(225, 0, 0),\ncount=num\n)\nreturn lights.node\nrep.randomizer.register(dome_lights) Most scenes have immovable objects important to the environment, like a table or in this case, a conveyor belt. The position of these objects can be fixed, while the material of the objects can be randomized to reflect real-world possibilities.\nThe following script generates a conveyor belt in USD that the cans will be placed upon. It also fixes its position and rotation. In this example, we don\u2019t randomize the material of the conveyor belt.\npython\n# Import and position the conveyor belt\nconveyor = rep.create.from_usd(CONVEYOR_USD, semantics=[('class', 'conveyor')])\nwith conveyor:\nrep.modify.pose(\nposition=(0, 0, 0),\nrotation=(0, -90, -90),\n) To guarantee a high-quality dataset, it\u2019s a good idea to use multiple cameras with different resolutions, and position them strategically in the scene. The position of the cameras can also be randomized. This script sets up two cameras of different resolutions strategically placed at various locations in the scene.\n# Multiple setup cameras and attach to render products\ncamera = rep.create.camera(focus_distance=focus_distance, focal_length=focal_length,\nposition=cam_position, rotation=cam_rotation, f_stop=f_stop)\ncamera2 = rep.create.camera(focus_distance=focus_distance2, focal_length=focal_length2,\nposition=cam_position2, rotation=cam_rotation, f_stop=f_stop)\n# Render images\nrender_product = rep.create.render_product(camera, (1024, 1024))\nrender_product2 = rep.create.render_product(camera2, (1024, 1024))\nImage on left shows a close-up of cans on the conveyor belt, while image on right shows camera positioned further away from conveyor belt.\nFigure 3. Generating synthetic images from multiple camera positions\nThe last step is randomizing the position of the movable objects while also keeping them in the relevant area. In this script, we initialize five instances of 3D-can models, randomly selected from a collection of available can assets.\ncans = list()\nfor i in range(TOTAL_CANS):\nrandom_can = random.choice(cans_list)\nrandom_can_name = random_can.split(\".\")[0].split(\"/\")[-1]\nthis_can = rep.create.from_usd(random_can, semantics=[('class', 'can')])\nwith this_can:\nrep.modify.pose(\nposition=(0, 0, 0),\nrotation=(0, -90, -90)\n)\ncans.append(this_can) Then, the pose of the cans is randomized and scattered across two planes, keeping the cans on the conveyor belt while avoiding collisions.\nwith rep.trigger.on_frame(num_frames=50, rt_subframes=55):\nplanesList=[('class','plane1'),('class','plane2')]\nwith rep.create.group(cans):\nplanes=rep.get.prims(semantics=planesList)\nrep.modify.pose(\nrotation=rep.distribution.uniform(\n(-90, -180, 0), (-90, 180, 0)\n)\n)\nrep.randomizer.scatter_2d(planes, check_for_collisions=True)\nMultple cans on a conveyor belt.\nFigure 4. Cans scattered across the conveyor belt in random poses", "Annotating data, building the model, and testing with real objects\nAfter being generated, the images can be uploaded to Edge Impulse Studio in a few clicks with the Edge Impulse Omniverse extension. In Edge Impulse Studio, datasets can be annotated and trained using models, such as the Yolov5 object detection model. The version control system enables model performance tracking across different dataset versions and hyperparameters, to optimize precision.\nTo test model accuracy with real-world objects, you can stream live video and run the model locally using the Edge Impulse CLI tool.\nUser interface for Edge Impulse, showing how someone can assess the performance of their model based on the classification results when testing on different dataset versions.\nFigure 5. Testing model precision in Edge Impulse\nIf the model does not detect the objects accurately, the model must be trained on additional datasets. This iterative process is the norm when it comes to AI model training. An added benefit of synthetic data is that required variations in subsequent iterations can be done programmatically.\nIn this example, an additional synthetic dataset was generated and used to train the model to improve performance. The additional dataset used a camera distance further from the conveyor. Other parameters like the angle of the camera and materials can be modified in additional datasets to improve performance.\nTaking a data-centric approach, where you create more data around the failure points of the model, is crucial to solving ML problems. Additional training and fine-tuning of parameters can enable a model to generalize well across different orientations, materials, and other relevant conditions.", "Get started training and deploying edge AI with synthetic data\nGenerating physically accurate synthetic data is easy in Omniverse Replicator. Simply download Omniverse free and follow the instructions for getting started with Replicator in Omniverse Code.\nWith Edge Impulse, you can use synthetic data generated in Omniverse to train your ML models. Sign up and begin using embedded machine learning models today.\nJoin Amit Goel, director of product management at NVIDIA, at the Imagine 2023 keynote. Learn about industry insights on AI and machine learning, as well as use cases made possible by NVIDIA Omniverse and Omniverse Replicator.\nStay up to date with NVIDIA Omniverse by subscribing to the newsletter, and following us on Instagram, Medium, and Twitter. For more resources, check out our forums, Discord server, Twitch, and YouTube channels."], "document_title": "How to Train an Object Detection Model for Visual Inspection with Synthetic Data", "document_url": "https://developer.nvidia.com/blog/how-to-train-an-object-detection-model-for-visual-inspection-with-synthetic-data/", "document_date": "2023-09-18T22:30:00", "document_date_modified": "2023-10-25T23:51:21", "document_full_text": "How to Train an Object Detection Model for Visual Inspection with Synthetic Data\nAI is rapidly changing industrial visual inspection. In a factory setting, visual inspection is used for many issues, including detecting defects and missing or incorrect parts during assembly. Computer vision can help identify problems with products early on, reducing the chances of them being delivered to customers.\nHowever, developing accurate and versatile object detection models remains challenging for edge AI developers. Robust object detection models require access to comprehensive and representative datasets. In many manufacturing scenarios, real-world datasets fall short when capturing the complexity and diversity of actual scenarios. The constraints of narrow environments and limited variations pose challenges in training models to adapt to a range of situations effectively.\nTeams can harness synthetic data for training models on diverse, randomized data that closely resemble real-world scenarios and address dataset gaps. The result is more accurate and adaptable AI models that can be deployed for a wide range of edge AI applications in industrial automation, healthcare, and manufacturing, to name a few.\nFrom synthetic data generation to AI training\nEdge Impulse is an integrated development platform that empowers developers to create and deploy AI models for edge devices. It supports data collection, preprocessing, model training, and deployment, helping users integrate AI capabilities into their applications effectively.\nWith NVIDIA Omniverse Replicator, a core extension of NVIDIA Omniverse, users can produce physically accurate and photorealistic, synthetically generated annotated images in Universal Scene Description, known as OpenUSD. These images can then be used for training an object detection model on the Edge Impulse platform.\nNVIDIA Omniverse is a computing platform that enables individuals and teams to develop Universal Scene Description (OpenUSD)-based 3D workflows and applications.\nOpenUSD is a highly versatile and interoperable 3D interchangeable and format that excels in synthetic data generation due to its scalability, performance, versioning, and asset management capabilities, making it an ideal choice for creating complex and realistic datasets. There\u2019s a vast ecosystem of 3D content tools that connect to OpenUSD and USD-based SimReady assets that make it easy to integrate physically-based objects into scenes and accelerate our synthetic data generation workflows.\nOmniverse Replicator enables randomization for USD data across several domains to represent scenarios that reflect real-world possibilities object detection models may encounter.\nA diagram showing the workflow from content in Omniverse being used to generate synthetic datasets in Replicator, which can then be used for AI training with Edge Impulse.\nFigure 1. The end-to-end data to training pipeline, starting with Omniverse Replicator, and ending in the Edge Impulse platform\nUsing the synthetically generated images in USD to train models in Edge Impulse only takes a few clicks with the new Edge Impulse Omniverse extension.\nUsing the extension, which was developed using the Omniverse Kit Python extension template, users can connect to the Edge Impulse API and select the dataset to upload their synthetic data. The Kit Python extension template is a simple and self-explanatory resource for code snippet options and developing an extension quickly.\nGenerating synthetic data for an object detection model\nTo understand the workflow for generating synthetic data with Omniverse Replicator and using it to train a model in Edge Impulse, follow our example detecting soda cans model.\nTwo images showing cans being detected at different camera angles and distances.\nFigure 2. An object detection model that detects soda cans after being trained on synthetic datasets\nThe first step in the process is building a virtual replica or a digital twin of the environment that represents the real scenario. The scene for generating synthetic images \u200cconsists of movable and immovable objects. The immovable set includes lights, a conveyor belt, and two cameras, while the movable objects consist of soda cans. Employing domain randomization, you can alter many properties, including location, lighting, colors, texture, background, and foreground of select immovable and movable objects.\nThese assets are represented in Omniverse Replicator through OpenUSD. 3D model files can be converted into USD and imported into Omniverse Replicator using the Omniverse CAD Importer extension.\nLighting plays a pivotal role in realistic image generation. Rectangular lights can emulate light generated from a panel and a dome light brightens the entire scene. You can randomize various parameters for the lights like temperature, intensity, scale, position, and rotation of the lights.\nThe following script shows temperature and intensity randomized through sampling from normal distributions, with scales randomized by a uniform distribution. The position and rotation of lights are fixed to remain constant.\npython\n# Lightning setup for Rectangular light and Dome light\ndef rect_lights(num=1):\nlights = rep.create.light(\nlight_type=\"rect\",\ntemperature=rep.distribution.normal(6500, 500),\nintensity=rep.distribution.normal(0, 5000),\nposition=(45, 110, 0),\nrotation=(-90, 0, 0),\nscale=rep.distribution.uniform(50, 100),\ncount=num\n)\nreturn lights.node\nrep.randomizer.register(rect_lights)\ndef dome_lights(num=3):\nlights = rep.create.light(\nlight_type=\"dome\",\ntemperature=rep.distribution.normal(6500, 500),\nintensity=rep.distribution.normal(0, 1000),\nposition=(45, 120, 18),\nrotation=(225, 0, 0),\ncount=num\n)\nreturn lights.node\nrep.randomizer.register(dome_lights) Most scenes have immovable objects important to the environment, like a table or in this case, a conveyor belt. The position of these objects can be fixed, while the material of the objects can be randomized to reflect real-world possibilities.\nThe following script generates a conveyor belt in USD that the cans will be placed upon. It also fixes its position and rotation. In this example, we don\u2019t randomize the material of the conveyor belt.\npython\n# Import and position the conveyor belt\nconveyor = rep.create.from_usd(CONVEYOR_USD, semantics=[('class', 'conveyor')])\nwith conveyor:\nrep.modify.pose(\nposition=(0, 0, 0),\nrotation=(0, -90, -90),\n) To guarantee a high-quality dataset, it\u2019s a good idea to use multiple cameras with different resolutions, and position them strategically in the scene. The position of the cameras can also be randomized. This script sets up two cameras of different resolutions strategically placed at various locations in the scene.\n# Multiple setup cameras and attach to render products\ncamera = rep.create.camera(focus_distance=focus_distance, focal_length=focal_length,\nposition=cam_position, rotation=cam_rotation, f_stop=f_stop)\ncamera2 = rep.create.camera(focus_distance=focus_distance2, focal_length=focal_length2,\nposition=cam_position2, rotation=cam_rotation, f_stop=f_stop)\n# Render images\nrender_product = rep.create.render_product(camera, (1024, 1024))\nrender_product2 = rep.create.render_product(camera2, (1024, 1024))\nImage on left shows a close-up of cans on the conveyor belt, while image on right shows camera positioned further away from conveyor belt.\nFigure 3. Generating synthetic images from multiple camera positions\nThe last step is randomizing the position of the movable objects while also keeping them in the relevant area. In this script, we initialize five instances of 3D-can models, randomly selected from a collection of available can assets.\ncans = list()\nfor i in range(TOTAL_CANS):\nrandom_can = random.choice(cans_list)\nrandom_can_name = random_can.split(\".\")[0].split(\"/\")[-1]\nthis_can = rep.create.from_usd(random_can, semantics=[('class', 'can')])\nwith this_can:\nrep.modify.pose(\nposition=(0, 0, 0),\nrotation=(0, -90, -90)\n)\ncans.append(this_can) Then, the pose of the cans is randomized and scattered across two planes, keeping the cans on the conveyor belt while avoiding collisions.\nwith rep.trigger.on_frame(num_frames=50, rt_subframes=55):\nplanesList=[('class','plane1'),('class','plane2')]\nwith rep.create.group(cans):\nplanes=rep.get.prims(semantics=planesList)\nrep.modify.pose(\nrotation=rep.distribution.uniform(\n(-90, -180, 0), (-90, 180, 0)\n)\n)\nrep.randomizer.scatter_2d(planes, check_for_collisions=True)\nMultple cans on a conveyor belt.\nFigure 4. Cans scattered across the conveyor belt in random poses\nAnnotating data, building the model, and testing with real objects\nAfter being generated, the images can be uploaded to Edge Impulse Studio in a few clicks with the Edge Impulse Omniverse extension. In Edge Impulse Studio, datasets can be annotated and trained using models, such as the Yolov5 object detection model. The version control system enables model performance tracking across different dataset versions and hyperparameters, to optimize precision.\nTo test model accuracy with real-world objects, you can stream live video and run the model locally using the Edge Impulse CLI tool.\nUser interface for Edge Impulse, showing how someone can assess the performance of their model based on the classification results when testing on different dataset versions.\nFigure 5. Testing model precision in Edge Impulse\nIf the model does not detect the objects accurately, the model must be trained on additional datasets. This iterative process is the norm when it comes to AI model training. An added benefit of synthetic data is that required variations in subsequent iterations can be done programmatically.\nIn this example, an additional synthetic dataset was generated and used to train the model to improve performance. The additional dataset used a camera distance further from the conveyor. Other parameters like the angle of the camera and materials can be modified in additional datasets to improve performance.\nTaking a data-centric approach, where you create more data around the failure points of the model, is crucial to solving ML problems. Additional training and fine-tuning of parameters can enable a model to generalize well across different orientations, materials, and other relevant conditions.\nGet started training and deploying edge AI with synthetic data\nGenerating physically accurate synthetic data is easy in Omniverse Replicator. Simply download Omniverse free and follow the instructions for getting started with Replicator in Omniverse Code.\nWith Edge Impulse, you can use synthetic data generated in Omniverse to train your ML models. Sign up and begin using embedded machine learning models today.\nJoin Amit Goel, director of product management at NVIDIA, at the Imagine 2023 keynote. Learn about industry insights on AI and machine learning, as well as use cases made possible by NVIDIA Omniverse and Omniverse Replicator.\nStay up to date with NVIDIA Omniverse by subscribing to the newsletter, and following us on Instagram, Medium, and Twitter. For more resources, check out our forums, Discord server, Twitch, and YouTube channels."}], "https://developer.nvidia.com/blog/power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai/": [{"text": "NVIDIA AI Enterprise 4.0 is a solution designed to help enterprises scale generative AI for production-ready applications. The latest version includes features such as NVIDIA NeMo for training and deploying large language models, AI workflows for building applications like chatbots and spear phishing detection, and tools like NVIDIA Triton Management Service for managing AI workloads. The platform also offers new frameworks, tools, and pretrained models to advance AI development. With enterprise-grade security and support, NVIDIA AI Enterprise 4.0 aims to provide reliable AI uptime and uninterrupted excellence. Enterprises can access the platform through NVIDIA LaunchPad for hands-on labs, a free 90-day evaluation, or by purchasing through the NVIDIA Partner Network or major Cloud Service Providers.", "text_components": ["Power Your Business with NVIDIA AI Enterprise 4.0 for Production-Ready Generative AI\nCrossing the chasm and reaching its iPhone moment, generative AI must scale to fulfill exponentially increasing demands. Reliability and uptime are critical for building generative AI at the enterprise level, especially when AI is core to conducting business operations. NVIDIA is investing its expertise into building a solution for those enterprises ready to take the leap.", "Introducing NVIDIA AI Enterprise 4.0\nThe latest version of NVIDIA AI Enterprise accelerates development through multiple facets with production-ready support, manageability, security, and reliability for enterprises innovating with generative AI.", "Quickly train, customize, and deploy LLMs at scale with NVIDIA NeMo\nGenerative AI models have billions of parameters and require an efficient data training pipeline. The complexity of training models, customization for domain-specific tasks, and deployment of models at scale require expertise and compute resources.\nNVIDIA AI Enterprise 4.0 now includes NVIDIA NeMo, an end-to-end, cloud-native framework for data curation at scale, accelerated training and customization of large language models (LLMs), and optimized inference on user-preferred platforms. From cloud to desktop workstations, NVIDIA NeMo provides easy-to-use recipes and optimized performance with accelerated infrastructure, greatly reducing time to solution and increasing ROI.", "Build generative AI applications faster with AI workflows\nNVIDIA AI Enterprise 4.0 introduces two new AI workflows for building generative AI applications: AI chatbot with retrieval augmented generation and spear phishing detection.\nThe generative AI knowledge base chatbot workflow, leveraging Retrieval Augmented Generation, accelerates the development and deployment of generative AI chatbots tuned on your data. These chatbots accurately answer domain-specific questions, retrieving information from a company\u2019s knowledge base and generating real-time responses in natural language. It uses pretrained LLMs, NeMo, NVIDIA Triton Inference Server, along with third-party tools including Langchain and vector database, for training and deploying the knowledge base question-answering system.\nThe spear phishing detection AI workflow uses NVIDIA Morpheus and generative AI with NVIDIA NeMo to train a model that can detect up to 90% of spear phishing e-mails before they hit your inbox.\nDefending against spear-phishing e-mails is a challenge. Spear phishing e-mails are indistinguishable from benign e-mails, with the only difference between the scam and legitimate e-mail being the intent of the sender. This is why traditional mechanisms for detecting spear phishing fall short.", "Develop AI anywhere\nEnterprise adoption of AI can require additional skilled AI developers and data scientists. Organizations will need a flexible high-performance infrastructure consisting of optimized hardware and software to maximize productivity and accelerate AI development. Together with NVIDIA RTX 6000 Ada Generation GPUs for workstations, NVIDIA AI Enterprise 4.0 provides AI developers a single platform for developing AI applications and deploying them in production.\nBeyond the desktop, NVIDIA offers a complete infrastructure portfolio for AI workloads including NVIDIA H100, L40S, L4 GPUs, and accelerated networking with NVIDIA BlueField data processing units. With HPE Machine Learning Data Management, HPE Machine Learning Development Environment, Ubuntu KVM and Nutanix AHV virtualization support, organizations can use on-prem infrastructure to power AI workloads.", "Manage AI workloads and infrastructure\nNVIDIA Triton Management Service, an exclusive addition to NVIDIA AI Enterprise 4.0, automates the deployment of multiple Triton Inference Servers in Kubernetes with GPU resource-efficient model orchestration. It simplifies deployment by loading models from multiple sources and allocating compute resources. Triton Management Service is available for lab experience on NVIDIA LaunchPad.\nNVIDIA AI Enterprise 4.0 also includes cluster management software, NVIDIA Base Command Manager Essentials, for streamlining cluster provisioning, workload management, infrastructure monitoring, and usage reporting. It facilitates the deployment of AI workload management with dynamic scaling and policy-based resource allocation, providing cluster integrity.", "New AI software, tools, and pretrained foundation models\nNVIDIA AI Enterprise 4.0 brings more frameworks and tools to advance AI development. NVIDIA Modulus is a framework for building, training, and fine-tuning physics-machine learning models with a simple Python interface.\nUsing Modulus, users can bolster engineering simulations with AI and build models for enterprise-scale digital twin applications across multiple physics domains, from CFD and Structural to Electromagnetics. The \u200b\u200b Deep Graph Library container is designed to implement and train Graph Neural Networks that can help scientists research the graph structure of molecules or financial services to detect fraud.\nLastly, three exclusive pretrained foundation models, part of NVIDIA TAO, speed time to production for industry applications such as vision AI, defect detection, and retail loss prevention.\nNVIDIA AI Enterprise 4.0 is the most comprehensive upgrade to the platform to date. With enterprise-grade security, stability, manageability, and support, enterprises can expect reliable AI uptime and uninterrupted AI excellence.", "Get started with NVIDIA AI Enterprise\nThree ways to get accelerated with NVIDIA AI Enterprise:\nSign up for NVIDIA LaunchPad for \u200cshort-term access to sets of hands-on labs.\nSign up for a free 90-day evaluation for existing on-prem or cloud infrastructure.\nPurchase through NVIDIA Partner Network or major Cloud Service Providers including AWS, Microsoft Azure, and Google Cloud."], "document_title": "Power Your Business with NVIDIA AI Enterprise 4.0 for Production-Ready Generative AI", "document_url": "https://developer.nvidia.com/blog/power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai/", "document_date": "2023-09-12T21:00:00", "document_date_modified": "2023-10-05T18:18:12", "document_full_text": "Power Your Business with NVIDIA AI Enterprise 4.0 for Production-Ready Generative AI\nCrossing the chasm and reaching its iPhone moment, generative AI must scale to fulfill exponentially increasing demands. Reliability and uptime are critical for building generative AI at the enterprise level, especially when AI is core to conducting business operations. NVIDIA is investing its expertise into building a solution for those enterprises ready to take the leap.\nIntroducing NVIDIA AI Enterprise 4.0\nThe latest version of NVIDIA AI Enterprise accelerates development through multiple facets with production-ready support, manageability, security, and reliability for enterprises innovating with generative AI.\nQuickly train, customize, and deploy LLMs at scale with NVIDIA NeMo\nGenerative AI models have billions of parameters and require an efficient data training pipeline. The complexity of training models, customization for domain-specific tasks, and deployment of models at scale require expertise and compute resources.\nNVIDIA AI Enterprise 4.0 now includes NVIDIA NeMo, an end-to-end, cloud-native framework for data curation at scale, accelerated training and customization of large language models (LLMs), and optimized inference on user-preferred platforms. From cloud to desktop workstations, NVIDIA NeMo provides easy-to-use recipes and optimized performance with accelerated infrastructure, greatly reducing time to solution and increasing ROI.\nBuild generative AI applications faster with AI workflows\nNVIDIA AI Enterprise 4.0 introduces two new AI workflows for building generative AI applications: AI chatbot with retrieval augmented generation and spear phishing detection.\nThe generative AI knowledge base chatbot workflow, leveraging Retrieval Augmented Generation, accelerates the development and deployment of generative AI chatbots tuned on your data. These chatbots accurately answer domain-specific questions, retrieving information from a company\u2019s knowledge base and generating real-time responses in natural language. It uses pretrained LLMs, NeMo, NVIDIA Triton Inference Server, along with third-party tools including Langchain and vector database, for training and deploying the knowledge base question-answering system.\nThe spear phishing detection AI workflow uses NVIDIA Morpheus and generative AI with NVIDIA NeMo to train a model that can detect up to 90% of spear phishing e-mails before they hit your inbox.\nDefending against spear-phishing e-mails is a challenge. Spear phishing e-mails are indistinguishable from benign e-mails, with the only difference between the scam and legitimate e-mail being the intent of the sender. This is why traditional mechanisms for detecting spear phishing fall short.\nDevelop AI anywhere\nEnterprise adoption of AI can require additional skilled AI developers and data scientists. Organizations will need a flexible high-performance infrastructure consisting of optimized hardware and software to maximize productivity and accelerate AI development. Together with NVIDIA RTX 6000 Ada Generation GPUs for workstations, NVIDIA AI Enterprise 4.0 provides AI developers a single platform for developing AI applications and deploying them in production.\nBeyond the desktop, NVIDIA offers a complete infrastructure portfolio for AI workloads including NVIDIA H100, L40S, L4 GPUs, and accelerated networking with NVIDIA BlueField data processing units. With HPE Machine Learning Data Management, HPE Machine Learning Development Environment, Ubuntu KVM and Nutanix AHV virtualization support, organizations can use on-prem infrastructure to power AI workloads.\nManage AI workloads and infrastructure\nNVIDIA Triton Management Service, an exclusive addition to NVIDIA AI Enterprise 4.0, automates the deployment of multiple Triton Inference Servers in Kubernetes with GPU resource-efficient model orchestration. It simplifies deployment by loading models from multiple sources and allocating compute resources. Triton Management Service is available for lab experience on NVIDIA LaunchPad.\nNVIDIA AI Enterprise 4.0 also includes cluster management software, NVIDIA Base Command Manager Essentials, for streamlining cluster provisioning, workload management, infrastructure monitoring, and usage reporting. It facilitates the deployment of AI workload management with dynamic scaling and policy-based resource allocation, providing cluster integrity.\nNew AI software, tools, and pretrained foundation models\nNVIDIA AI Enterprise 4.0 brings more frameworks and tools to advance AI development. NVIDIA Modulus is a framework for building, training, and fine-tuning physics-machine learning models with a simple Python interface.\nUsing Modulus, users can bolster engineering simulations with AI and build models for enterprise-scale digital twin applications across multiple physics domains, from CFD and Structural to Electromagnetics. The \u200b\u200b Deep Graph Library container is designed to implement and train Graph Neural Networks that can help scientists research the graph structure of molecules or financial services to detect fraud.\nLastly, three exclusive pretrained foundation models, part of NVIDIA TAO, speed time to production for industry applications such as vision AI, defect detection, and retail loss prevention.\nNVIDIA AI Enterprise 4.0 is the most comprehensive upgrade to the platform to date. With enterprise-grade security, stability, manageability, and support, enterprises can expect reliable AI uptime and uninterrupted AI excellence.\nGet started with NVIDIA AI Enterprise\nThree ways to get accelerated with NVIDIA AI Enterprise:\nSign up for NVIDIA LaunchPad for \u200cshort-term access to sets of hands-on labs.\nSign up for a free 90-day evaluation for existing on-prem or cloud infrastructure.\nPurchase through NVIDIA Partner Network or major Cloud Service Providers including AWS, Microsoft Azure, and Google Cloud."}], "https://developer.nvidia.com/blog/software-defined-broadcast-with-nvidia-holoscan-for-media/": [{"text": "The broadcast industry is shifting towards software-defined systems with the introduction of NVIDIA Holoscan for Media, a platform for developing and deploying media applications on-prem, in the cloud, and at the edge. This IP-based solution incorporates industry standards and APIs, breaking away from traditional SDI and FPGA-based systems. Holoscan for Media enables the integration of the latest technologies, such as generative AI, without additional infrastructure investments. The platform offers benefits such as repurposability, lower TCO, flexibility, and sustainability. The hardware basis of Holoscan for Media consists of NVIDIA-certified systems using Ampere architecture or later GPUs and BlueField-2 or later DPUs. The software stack includes Kubernetes, Red Hat OpenShift Container Platform, and various operators and plug-ins to manage hardware and services. Developers can build applications using NVIDIA SDKs supported on the platform, including real-time video encoding, computer vision, AI capabilities, and more. Holoscan for Media is now available for early access to developers registered in the NVIDIA Developer Program.", "text_components": ["Software-Defined Broadcast with NVIDIA Holoscan for Media\nThe broadcast industry is undergoing a transformation in how content is created, managed, distributed, and consumed. This transformation includes a shift from traditional linear workflows bound by fixed-function devices to flexible and hybrid, software-defined systems that enable the future of live streaming.\nDevelopers can now apply to join the early access program for NVIDIA Holoscan for Media, a software-defined platform for developing and deploying media applications on-prem, in the cloud, and at the edge.\nUsing Holoscan for Media, broadcasters and solution providers can leverage the latest IT and provisioning technologies and a modern container-based approach to development, orchestration, and delivery.\nHoloscan for Media is an IP-based solution built on industry standards and APIs including SMPTE ST 2110, AMWA NMOS, RIST, SRT, and NDI.\nThe platform integrates open-source and ubiquitous technologies, breaking from the proprietary and inflexible nature of SDI and FPGA-based systems. It also enables incorporation of the latest capabilities in production\u2014such as generative AI \u2014without additional infrastructure investments. With Holoscan for Media, countless NVIDIA application frameworks and SDKs are made accessible \u200b\u200bto the industry for development.\nThis framework provides several benefits to both broadcasters and solution providers, including:\nRepurposability: Use a single platform for many applications.\nLower TCO: Benefit from the cyclical cost reductions.\nFlexibility: The platform is cloud-native and independent of location. An application can be developed once and deployed everywhere.\nSustainability: Provisioning technologies that drive resource sharing means that overall less equipment is required. This means lower power and cooling costs and reduced impact from shipping to and from events. Ultimately, this leads to CO2 reductions.", "IP-based platform architecture\nNVIDIA Holoscan targets sensor data and media processing applications deployed at-scale across countless industries, in the cloud, on premises, and at the edge. Holoscan for Media tightens the focus on broadcast and live production workflows, with the first target being on-premises deployments.\nImage of NVIDIA Holoscan for Media platform architecture pieces described in the next section.\nFigure 1. Holoscan for Media platform architecture\nThe hardware basis of the platform is therefore NVIDIA-certified systems from our partners, using NVIDIA Ampere architecture or later GPUs and NVIDIA BlueField-2 or later DPUs. The first systems are x86, but the entire software stack is multi-architecture to enable a wide range of systems and use cases with lower power consumption. In production, a minimal Holoscan for Media cluster consists of three nodes, and scales from there.\nThe software stack begins with Kubernetes, the open-source container orchestration system for automating software deployment, scaling, and management. Partnering with the Red Hat OpenShift Container Platform brings enterprise-grade operation and support.\nThe inclusion of Kubernetes plug-ins, known as operators, which provide and manage the hardware and underlay services, frees software developers to focus on their unique functionality. The open-source OpenShift Node Tuning Operator, NVIDIA GPU Operator, and NVIDIA Network Operator provide system, GPU, and high-speed secondary networking, tuned for performance and made available to every application that needs them. The GPU Operator can be used to assign one or more entire GPUs to an application.\nSupport for MIG (Multi-Instance GPU) and vGPU (virtual GPU) enables GPUs to be securely shared between applications. The PTP Operator uses the PTP Hardware Clock on NVIDIA DPUs to provide precise timing from the secondary network to each application through a simple \u201cget time\u201d API. Other operators and plug-ins take care of IP address management (IPAM), DNS zone management, and more.\nHoloscan for Media also includes services such as an NMOS Registry and an easy-to-use graph-builder-based NMOS Controller user interface. These can be installed to support development and deployment of applications that act as media nodes and simplify integration with broadcast facility networks.\nApplications on the platform are packaged with Helm for simple, consistent deployment. A developer can indicate each container\u2019s required capabilities and resources, including GPU, CPU, memory, and storage. This enables the platform to schedule and monitor applications to ensure each one is appropriately isolated, their requirements are met, and that best use is made of the available hardware.\nDevelopers can build applications using the growing list of NVIDIA SDKs supported on the Holoscan for Media platform. Traditional real-time video encoding and decoding with the Video Codec SDK, GPU-accelerated computer vision by CV-CUDA library, and any parallel compute algorithm using the CUDA toolkit. On top of GPU-accelerated inference through TensorRT SDK or NVIDIA Triton Inference Server, new AI capabilities are offered by SDK and Cloud APIs like Maxine or NVIDIA Avatar Cloud Engine (ACE). Foundational SMPTE 2110 support and optimization of large media transfer is provided through NVIDIA Rivermax SDK. Developers can natively leverage Rivermax on the platform or through the DeepStream SDK, a complete streaming analytics toolkit based on GStreamer for AI-based media processing. Additionally, if developers have wider use cases beyond media, and want to consume and control other sensor types, NVIDIA provides the Holoscan SDK for creating real-time, AI-enabled sensor processing pipelines that meet latency requirements and scale from the data center to the edge.\nFull source for a containerized reference application is available to Holoscan for Media developers. This uses NVIDIA DeepStream and can be configured as an NMOS-capable ST 2110 transmitter, receiver or transcoder gateway.\nAltogether, this open platform architecture provides the building blocks for the Dynamic Media Facility, using the latest scalable IT and provisioning technologies and open standards to benefit both broadcasters and software vendors.", "Get started with Holoscan for Media\nHoloscan for Media is now available for early access. Note that you must be registered in the NVIDIA Developer Program to apply for the early access release. You must also be logged in using your organization\u2019s email address. We cannot accept applications from accounts using Gmail, Yahoo, QQ, or other personal email accounts.\nTo participate, fill out the short application form and provide details about your use case."], "document_title": "Software-Defined Broadcast with NVIDIA Holoscan for Media", "document_url": "https://developer.nvidia.com/blog/software-defined-broadcast-with-nvidia-holoscan-for-media/", "document_date": "2023-09-14T19:00:00", "document_date_modified": "2023-10-25T23:51:21", "document_full_text": "Software-Defined Broadcast with NVIDIA Holoscan for Media\nThe broadcast industry is undergoing a transformation in how content is created, managed, distributed, and consumed. This transformation includes a shift from traditional linear workflows bound by fixed-function devices to flexible and hybrid, software-defined systems that enable the future of live streaming.\nDevelopers can now apply to join the early access program for NVIDIA Holoscan for Media, a software-defined platform for developing and deploying media applications on-prem, in the cloud, and at the edge.\nUsing Holoscan for Media, broadcasters and solution providers can leverage the latest IT and provisioning technologies and a modern container-based approach to development, orchestration, and delivery.\nHoloscan for Media is an IP-based solution built on industry standards and APIs including SMPTE ST 2110, AMWA NMOS, RIST, SRT, and NDI.\nThe platform integrates open-source and ubiquitous technologies, breaking from the proprietary and inflexible nature of SDI and FPGA-based systems. It also enables incorporation of the latest capabilities in production\u2014such as generative AI \u2014without additional infrastructure investments. With Holoscan for Media, countless NVIDIA application frameworks and SDKs are made accessible \u200b\u200bto the industry for development.\nThis framework provides several benefits to both broadcasters and solution providers, including:\nRepurposability: Use a single platform for many applications.\nLower TCO: Benefit from the cyclical cost reductions.\nFlexibility: The platform is cloud-native and independent of location. An application can be developed once and deployed everywhere.\nSustainability: Provisioning technologies that drive resource sharing means that overall less equipment is required. This means lower power and cooling costs and reduced impact from shipping to and from events. Ultimately, this leads to CO2 reductions.\nIP-based platform architecture\nNVIDIA Holoscan targets sensor data and media processing applications deployed at-scale across countless industries, in the cloud, on premises, and at the edge. Holoscan for Media tightens the focus on broadcast and live production workflows, with the first target being on-premises deployments.\nImage of NVIDIA Holoscan for Media platform architecture pieces described in the next section.\nFigure 1. Holoscan for Media platform architecture\nThe hardware basis of the platform is therefore NVIDIA-certified systems from our partners, using NVIDIA Ampere architecture or later GPUs and NVIDIA BlueField-2 or later DPUs. The first systems are x86, but the entire software stack is multi-architecture to enable a wide range of systems and use cases with lower power consumption. In production, a minimal Holoscan for Media cluster consists of three nodes, and scales from there.\nThe software stack begins with Kubernetes, the open-source container orchestration system for automating software deployment, scaling, and management. Partnering with the Red Hat OpenShift Container Platform brings enterprise-grade operation and support.\nThe inclusion of Kubernetes plug-ins, known as operators, which provide and manage the hardware and underlay services, frees software developers to focus on their unique functionality. The open-source OpenShift Node Tuning Operator, NVIDIA GPU Operator, and NVIDIA Network Operator provide system, GPU, and high-speed secondary networking, tuned for performance and made available to every application that needs them. The GPU Operator can be used to assign one or more entire GPUs to an application.\nSupport for MIG (Multi-Instance GPU) and vGPU (virtual GPU) enables GPUs to be securely shared between applications. The PTP Operator uses the PTP Hardware Clock on NVIDIA DPUs to provide precise timing from the secondary network to each application through a simple \u201cget time\u201d API. Other operators and plug-ins take care of IP address management (IPAM), DNS zone management, and more.\nHoloscan for Media also includes services such as an NMOS Registry and an easy-to-use graph-builder-based NMOS Controller user interface. These can be installed to support development and deployment of applications that act as media nodes and simplify integration with broadcast facility networks.\nApplications on the platform are packaged with Helm for simple, consistent deployment. A developer can indicate each container\u2019s required capabilities and resources, including GPU, CPU, memory, and storage. This enables the platform to schedule and monitor applications to ensure each one is appropriately isolated, their requirements are met, and that best use is made of the available hardware.\nDevelopers can build applications using the growing list of NVIDIA SDKs supported on the Holoscan for Media platform. Traditional real-time video encoding and decoding with the Video Codec SDK, GPU-accelerated computer vision by CV-CUDA library, and any parallel compute algorithm using the CUDA toolkit. On top of GPU-accelerated inference through TensorRT SDK or NVIDIA Triton Inference Server, new AI capabilities are offered by SDK and Cloud APIs like Maxine or NVIDIA Avatar Cloud Engine (ACE). Foundational SMPTE 2110 support and optimization of large media transfer is provided through NVIDIA Rivermax SDK. Developers can natively leverage Rivermax on the platform or through the DeepStream SDK, a complete streaming analytics toolkit based on GStreamer for AI-based media processing. Additionally, if developers have wider use cases beyond media, and want to consume and control other sensor types, NVIDIA provides the Holoscan SDK for creating real-time, AI-enabled sensor processing pipelines that meet latency requirements and scale from the data center to the edge.\nFull source for a containerized reference application is available to Holoscan for Media developers. This uses NVIDIA DeepStream and can be configured as an NMOS-capable ST 2110 transmitter, receiver or transcoder gateway.\nAltogether, this open platform architecture provides the building blocks for the Dynamic Media Facility, using the latest scalable IT and provisioning technologies and open standards to benefit both broadcasters and software vendors.\nGet started with Holoscan for Media\nHoloscan for Media is now available for early access. Note that you must be registered in the NVIDIA Developer Program to apply for the early access release. You must also be logged in using your organization\u2019s email address. We cannot accept applications from accounts using Gmail, Yahoo, QQ, or other personal email accounts.\nTo participate, fill out the short application form and provide details about your use case."}], "https://developer.nvidia.com/blog/generative-ai-and-accelerated-computing-for-spear-phishing-detection/": [{"text": "Spear phishing is a significant cyber threat, costing businesses billions of dollars. Traditional methods of detecting spear phishing emails are challenging due to their similarity to legitimate emails. A new approach using generative AI and accelerated computing with NVIDIA Morpheus is proposed to enhance detection by focusing on sender intent and behavior. By generating new phishing intent emails, training intent models, and analyzing sender behavior, the system can quickly adapt to new attacks and provide real-time protection against spear phishing threats. Results show a significant improvement in detection accuracy when incorporating new attack intents into the model. By leveraging NVIDIA Morpheus and NeMo, organizations can enhance their spear phishing detection capabilities efficiently. Interested parties can learn more about NVIDIA Morpheus and request a free trial to experience its benefits firsthand.", "text_components": ["Generative AI and Accelerated Computing for Spear Phishing Detection\nSpear phishing is the largest and most costly form of cyber threat, with an estimated 300,000 reported victims in 2021 representing $44 million in reported losses in the United States alone. Business e-mail compromises led to $2.4 billion in costs in 2021, according to the FBI Internet Crime Report. In the period from June 2016 to December 2021, costs related to phishing and spear phishing totaled $43 billion for businesses, according to IBM Security Cost of a Data Breach.\nSpear phishing e-mails are indistinguishable from a benign e-mail that a victim would receive. This is also why traditional classification of spear phishing e-mails is so difficult. The content difference between a scam and a legitimate e-mail can be minuscule. Often, the only difference between the two is the intent of the sender: is the invoice legitimate, or is it a scam?\nThis post details a two-fold approach to improve spear phishing detection by boosting the signals of intent using NVIDIA Morpheus to run data processing and inferencing.", "Generating e-mails with new phishing intent\nThe first step involves using generative AI to create large, varied corpora of e-mails with various intents associated with spear phishing and scams. As new threats emerge, the NVIDIA Morpheus team uses the NVIDIA NeMo framework to generate a new corpus of e-mails with such threats. Following the generation of new e-mails with the new type of phishing intent, the team trains a new language model to recognize the intent. In traditional phishing detection mechanisms, such models would require a significant number of human-labeled e-mails.\nDiagram showing an overview of the spear phishing detection methodology. AI- generated e-mails with specific intents are used to train intent models that label incoming user e-mails. These labels are joined with past sender behavior (if any) and e-mail metadata to classify the e-mail as spear phishing or not.\nFigure 1. Overview of the spear phishing detection methodology", "Detecting sender intent\nThe first step targets the intent behind the e-mail. The next step targets the intent of the sender. To defend against spear phishing attacks that use spoofing, known senders, or longer cons that do not express their true intent immediately, we construct additional signals by building up behavioral sketches from senders or groups of senders.\nBuilding on the intent work described above, known senders\u2019 past observed intents are recorded. For example, the first time a known sender asks for money can be a signal to alert the user.\nSyntax usage is also observed and recorded. The syntax of new e-mails is compared to the syntax history of the sender. A deviation from the observed syntax could indicate a possible spoofing attack.\nFinally, the temporal patterns of a sender\u2019s e-mails are collected and cross-referenced when a new e-mail arrives to check for out-of-pattern behavior. Is the sender sending an e-mail for the first time at midnight on a Saturday? If so, that becomes a signal in the final prediction. These signals in aggregate are used to classify e-mails. They are also presented to the end user as an explanation for why an e-mail may be malicious.", "Adapting to new attacks and improving protection\nExisting machine learning (ML) methods rely nearly entirely on human-labeled data and cannot adapt to emerging threats quickly. The biggest benefit to detecting spear phishing e-mails using the approach presented here is how quickly the model can be adapted to new attacks. When a new attack emerges, generative AI is leveraged to create a training corpus for the attack. Intent models are trained to detect its presence in received e-mails.\nUsing models built with NeMo generates thousands of high-quality, on-topic e-mails in just a few hours. The new intents are added to the existing spear phishing detector. The entire end-to-end workflow of creating new phishing attack e-mails and updating the existing models happens in less than 24 hours. Once the models are in place, \u200ce-mail processing and inferencing become a Morpheus pipeline to provide near real-time protection against spear phishing threats.", "Results\nTo illustrate the flexibility of this approach, a model was trained using only money, banking, and personal identifying information (PII) intents. Next, cryptocurrency-flavored phishing e-mails were generated using models built with NeMo. These e-mails were incorporated into the original training and validation subsets.\nThe validation set, now containing the new crypto attacks, was then passed into the original model. Then a second model was trained incorporating the crypto attack intents. Figure 2 shows how the models compare in their detection.\nAfter training for the attack, the F1 score increased from 0.54 to 0.89 (Figure 3). This illustrates how quickly new attacks can be trained for and adapted to using NVIDIA Morpheus and NeMo.\nChart showing how the models compare in their detection.\nFigure 2. Differences in detection between an untrained model and the model trained for a cryptocurrency-based spear phishing attack\nChart showing the F1-score difference between the models\nFigure 3. F1-score difference between an untrained model and the model trained for a cryptocurrency-based spear phishing attack", "Get started with NVIDIA Morpheus\nWatch the video, Improve Spear Phishing Detection with Generative AI for more details. Learn more about how to use NVIDIA Morpheus to detect spear phishing e-mails faster and with greater accuracy using the NVIDIA AI workflow example. You can also apply to try NVIDIA Morpheus in LaunchPad and request a 90-day free trial to test drive NVIDIA Morpheus, part of the NVIDIA AI Enterprise software family."], "document_title": "Generative AI and Accelerated Computing for Spear Phishing Detection", "document_url": "https://developer.nvidia.com/blog/generative-ai-and-accelerated-computing-for-spear-phishing-detection/", "document_date": "2023-09-12T18:00:00", "document_date_modified": "2023-10-05T18:18:13", "document_full_text": "Generative AI and Accelerated Computing for Spear Phishing Detection\nSpear phishing is the largest and most costly form of cyber threat, with an estimated 300,000 reported victims in 2021 representing $44 million in reported losses in the United States alone. Business e-mail compromises led to $2.4 billion in costs in 2021, according to the FBI Internet Crime Report. In the period from June 2016 to December 2021, costs related to phishing and spear phishing totaled $43 billion for businesses, according to IBM Security Cost of a Data Breach.\nSpear phishing e-mails are indistinguishable from a benign e-mail that a victim would receive. This is also why traditional classification of spear phishing e-mails is so difficult. The content difference between a scam and a legitimate e-mail can be minuscule. Often, the only difference between the two is the intent of the sender: is the invoice legitimate, or is it a scam?\nThis post details a two-fold approach to improve spear phishing detection by boosting the signals of intent using NVIDIA Morpheus to run data processing and inferencing.\nGenerating e-mails with new phishing intent\nThe first step involves using generative AI to create large, varied corpora of e-mails with various intents associated with spear phishing and scams. As new threats emerge, the NVIDIA Morpheus team uses the NVIDIA NeMo framework to generate a new corpus of e-mails with such threats. Following the generation of new e-mails with the new type of phishing intent, the team trains a new language model to recognize the intent. In traditional phishing detection mechanisms, such models would require a significant number of human-labeled e-mails.\nDiagram showing an overview of the spear phishing detection methodology. AI- generated e-mails with specific intents are used to train intent models that label incoming user e-mails. These labels are joined with past sender behavior (if any) and e-mail metadata to classify the e-mail as spear phishing or not.\nFigure 1. Overview of the spear phishing detection methodology\nDetecting sender intent\nThe first step targets the intent behind the e-mail. The next step targets the intent of the sender. To defend against spear phishing attacks that use spoofing, known senders, or longer cons that do not express their true intent immediately, we construct additional signals by building up behavioral sketches from senders or groups of senders.\nBuilding on the intent work described above, known senders\u2019 past observed intents are recorded. For example, the first time a known sender asks for money can be a signal to alert the user.\nSyntax usage is also observed and recorded. The syntax of new e-mails is compared to the syntax history of the sender. A deviation from the observed syntax could indicate a possible spoofing attack.\nFinally, the temporal patterns of a sender\u2019s e-mails are collected and cross-referenced when a new e-mail arrives to check for out-of-pattern behavior. Is the sender sending an e-mail for the first time at midnight on a Saturday? If so, that becomes a signal in the final prediction. These signals in aggregate are used to classify e-mails. They are also presented to the end user as an explanation for why an e-mail may be malicious.\nAdapting to new attacks and improving protection\nExisting machine learning (ML) methods rely nearly entirely on human-labeled data and cannot adapt to emerging threats quickly. The biggest benefit to detecting spear phishing e-mails using the approach presented here is how quickly the model can be adapted to new attacks. When a new attack emerges, generative AI is leveraged to create a training corpus for the attack. Intent models are trained to detect its presence in received e-mails.\nUsing models built with NeMo generates thousands of high-quality, on-topic e-mails in just a few hours. The new intents are added to the existing spear phishing detector. The entire end-to-end workflow of creating new phishing attack e-mails and updating the existing models happens in less than 24 hours. Once the models are in place, \u200ce-mail processing and inferencing become a Morpheus pipeline to provide near real-time protection against spear phishing threats.\nResults\nTo illustrate the flexibility of this approach, a model was trained using only money, banking, and personal identifying information (PII) intents. Next, cryptocurrency-flavored phishing e-mails were generated using models built with NeMo. These e-mails were incorporated into the original training and validation subsets.\nThe validation set, now containing the new crypto attacks, was then passed into the original model. Then a second model was trained incorporating the crypto attack intents. Figure 2 shows how the models compare in their detection.\nAfter training for the attack, the F1 score increased from 0.54 to 0.89 (Figure 3). This illustrates how quickly new attacks can be trained for and adapted to using NVIDIA Morpheus and NeMo.\nChart showing how the models compare in their detection.\nFigure 2. Differences in detection between an untrained model and the model trained for a cryptocurrency-based spear phishing attack\nChart showing the F1-score difference between the models\nFigure 3. F1-score difference between an untrained model and the model trained for a cryptocurrency-based spear phishing attack\nGet started with NVIDIA Morpheus\nWatch the video, Improve Spear Phishing Detection with Generative AI for more details. Learn more about how to use NVIDIA Morpheus to detect spear phishing e-mails faster and with greater accuracy using the NVIDIA AI workflow example. You can also apply to try NVIDIA Morpheus in LaunchPad and request a 90-day free trial to test drive NVIDIA Morpheus, part of the NVIDIA AI Enterprise software family."}], "https://developer.nvidia.com/blog/selecting-the-right-camera-for-the-nvidia-jetson-and-other-embedded-systems/": [{"text": "Selecting the right camera for an AI-based embedded system, such as the NVIDIA Jetson, is crucial for optimal performance. The selection process involves considering key aspects like sensor type (CCD or CMOS), electronic shutter type (global or rolling), color vs. monochrome, dynamic range, resolution, frame rate, interface (USB, Ethernet, MIPI CSI-2), and optics (focal length, sensor format, field of view, aperture). Factors like sensor resolution, pixel size, and back-illuminated sensors can impact image quality. NVIDIA partners with camera module makers to provide a wide range of options for Jetson applications. Understanding constraints and focusing on relevant characteristics for the specific use case can help simplify the selection process. Considerations for deployment environment, lighting conditions, and motion applications can guide the choice of camera features. For more information, resources like Jetson Modules and NVIDIA's camera module partners can provide detailed specifications and guidance for optimizing edge applications.", "text_components": ["Selecting the Right Camera for the NVIDIA Jetson and Other Embedded Systems\nThe camera module is the most integral part of an AI-based embedded system. With so many camera module choices on the market, the selection process may seem overwhelming. This post breaks down the process to help make the right selection for an embedded application, including the NVIDIA Jetson.", "Camera selection considerations\nCamera module selection involves consideration of three key aspects: sensor, interface (connector), and optics.", "Sensor\nThe two main types of electronic image sensors are the charge-coupled device (CCD) and the active-pixel sensor (CMOS). For a CCD sensor, pixel values can only be read on a per-row basis. Each row of pixels is shifted, one by one, into a readout register. For a CMOS sensor, each pixel can be read individually and in parallel.\nCMOS is less expensive and consumes less energy without sacrificing image quality, in most cases. It can also achieve higher frame rates due to the parallel readout of pixel values. However, there are some specific scenarios in which CCD sensors still prevail\u2014for example, when long exposure is necessary and very low-noise images are required, such as in astronomy.", "Electronic shutter\nThere are two options for the electronic shutter: global or rolling. A global shutter exposes each pixel to incoming light at the same time. A rolling shutter exposes the pixel rows in a certain order (top to bottom, for example) and can cause distortion (Figure 1).\nTwo images of a helicopter showing distortion of moving blades caused by rolling shutter.\nFigure 1. Distortion of rotor blades caused by rolling shutter The global shutter is not impacted by motion blur and distortion due to object movement. It is much easier to sync multiple cameras with a global shutter because there is a single point in time when exposure starts. However, sensors with a global shutter are much more expensive than those with a rolling shutter.", "Color or monochrome\nIn most cases, a monochrome image sensor is sufficient for typical machine vision tasks like fault detection, presence monitoring, and recording measurements.\nWith a monochrome sensor, each pixel is usually described by eight bits. With a color sensor, each pixel has eight bits for the red channel, eight bits for the green channel, and eight bits for the blue channel. The color sensor requires processing three times the amount of data, resulting in a higher processing time and, consequently, a slower frame rate.", "Dynamic range\nDynamic range is the ratio between the maximum and minimum signal that is acquired by the sensor. At the upper limit, pixels appear white for higher values of intensity (saturation), while pixels appear black at the lower limit and below. An HDR of at least 80db is needed for indoor application and up to 140db is needed for outdoor application.", "Resolution\nResolution is a sensor\u2019s ability to reproduce object details. It can be influenced by factors such as the type of lighting used, the sensor pixel size, and the capabilities of the optics. The smaller the object detail, the higher the required resolution.\nPixel resolution translates to how many millimeters each pixel is equal to on the image. The higher the resolution, the sharper your image will be. The camera or sensor\u2019s resolution should enable coverage of a feature\u2019s area of at least two pixels.\nCMOS sensors with high resolutions tend to have low frame rates. While a sensor may achieve the resolution you need, it will not capture the quality images you need without achieving enough frames per second. It is important to evaluate the speed of the sensor.\nA general rule of thumb to determine the resolution needed for the use case is shown below and in Figure 2. \u202fThe multiplier (2) represents the typical desire to have a minimum two pixels on an object in order to successfully detect it.\nResolution = 2\\times \\frac{Field \\ of \\ View (FOV)}{Size \\ of \\ feature \\ of \\ interest}\nDiagram showing the representation of a person and the working distance from an object as an example of minimum object feature size of interest in the field of view.\nFigure 2. Sensor resolution required is determined by lens field of view and feature of interest size For example, suppose you have an image of an injury around the eye of a boxer.\nResolution= 2\\times \\frac{2000}{4}\nFOV, mm = 2000mm\nSize of feature of interest (the eye), mm = 4mm\nBased on the calculation, 1000 x 1000, a one-megapixel camera should be sufficient to detect the eye using a CV or AI algorithm.\nNote that a sensor is made up of multiple rows of pixels. These pixels are also called photosites. The number of photons collected by a pixel is directly proportional to the size of the pixel. Selecting a larger pixel may seem tempting but may not be the optimal choice in all the cases.\nSmall pixel\nSensitive to noise (-)\nHigher spatial resolution for same sensor size (+)\nLarge pixel\nLess sensitive to noise (+)\nLess spatial resolution for same sensor size (-)\nTable 1. Pros and cons of small and large pixel size Back-illuminated sensors maximize the amount of light being captured and converted by each photodiode. In front-illuminated sensors, metal wiring above the photodiodes blocks off some photons, hence reducing the amount of light captured.\nOn the left, a diagram of a front-illuminated structure with substrate, photodiodes, metal wiring, and microlenses. On the right, a diagram of a back-illuminated structure with metal wiring, photodiodes, and microlenses.\nFigure 3. Cross-section of a front-illuminated structure (left) and a back-illuminated structure (right)", "Frame rate and shutter speed\nThe frame rate\u202frefers to the number of frames (or images captured) per second (FPS). The frame rate should be determined based on the number of inspections required per second. This correlates with the shutter speed (or exposure time), which is the time that the camera sensor is exposed to capture the image.\nTheoretically, the maximum frame rate is equal to the inverse of the exposure time. But achievable FPS is lower because of latency introduced by frame readout, sensor resolution, and the data transfer rate of the interface including cabling.\nFPS can be increased by reducing the need for large exposure times by adding additional lighting, binning the pixels.\nCMOS sensors can achieve higher FPS, as the process of reading out each pixel can be done more quickly than with the charge transfer in a CCD sensor\u2019s shift register.", "Interface\nThere are multiple ways to connect the camera module to an embedded system. Typically, for evaluation purposes, cameras with USB and Ethernet interfaces are used because custom driver development is not needed.\nOther important parameters for interface selection are transmission length, data rate, and operating conditions. Table 2 lists the most popular interfaces. Each option has its pros and cons.\nFeatures\nUSB 3.2\nEthernet (1 GbE)\nMIPI CSI-2\nGMSL2\nFPDLINK III\nBandwidth\n10Gbps\n1Gbps\nDPHY 2.5 Gbps/lane CPHY 5.71 Gbps/lane\n6Gbps\n4.2Gbps\nCable length supported\n< 5m\nUp to 100m\n<30cm\n<15m\n<15m\nPlug-and-play\nSupported\nSupported\nNot supported\nNot supported\nNot supported\nDevelopment costs\nLow\nLow\nMedium to high\nMedium to high\nMedium to high\nOperating environment\nIndoor\nIndoor\nIndoor\nIndoor and outdoor\nIndoor and outdoor\nTable 2. Comparison of various camera interfac es", "Optics\nThe basic purpose of an optical lens is to collect the light scattered by an object and recreate an image of the object on a light-sensitive image sensor (CCD or CMOS). The following factors should be considered when selecting an optimized lens-focal length, sensor format, field of view, aperture, chief ray angle, resolving power, and distortion.\nLenses are manufactured with a limited number of standard focal lengths. Common lens focal lengths include 6mm, 8mm, 12.5mm, 25mm, and 50mm.\nOnce you choose a lens with a focal length closest to the focal length required by your imaging system, you need to adjust the working distance to get the object under inspection in focus. Lenses with short focal lengths (less than 12mm) produce images with a significant amount of distortion.\nIf your application is sensitive to image distortion, try to increase the working distance and use a lens with a higher focal length. If you cannot change the working distance, you are somewhat limited in choosing an optimized lens.\nWide-angle lens\nNormal lens\nTelephoto lens\nFocal length\n<=35mm\n50mm\n>=70mm\nUse case\nNearby scenes\nSame as human eye\nFar-away scenes\nTable 3. Main types of camera lenses To attach a lens to a camera requires some type of mounting system. Both mechanical stability (a loose lens will deliver an out-of-focus image) and the distance to the sensor must be defined.\nTo ensure compatibility between different lenses and cameras, the following standard lens mounts are defined.\nMost popular\nFor industrial applications\nLens mount\nM12/S mount\nC-mount\nFlange focal length\nNon-standard\n17.526mm\nThreads (per mm)\n0.5\n0.75\nSensor size accommodated (inches)\nUp to \u2154\nUp to 1\nTable 4. Common lens mounts used in embedded space", "NVIDIA camera module partners\nNVIDIA maintains a rich ecosystem of partnerships with highly competent camera module makers all over the world. See Jetson Partner Supported Cameras for details. These partners can help you design imaging systems for your application from concept to production for the NVIDIA Jetson.\nGraphic showing NVIDIA Jetson with camera modules for various use cases and industries.\nFigure 4. NVIDIA Jetson in combination with camera modules can be used across industries for various needs", "Summary\nThis post has explained the most important camera characteristics to consider when selecting a camera for an embedded application. Although the selection process may seem daunting, the first step is to understand your key constraints based on design, performance, environment, and cost.\nOnce you understand the constraints, then focus on the characteristics most relevant to your use case. For example, if the camera will be deployed away from the compute or in a rugged environment, consider using the GMSL interface. If the camera will be used in low-light conditions, consider a camera module with larger pixel and sensor sizes. If the camera will be used in a motion application, consider using a camera with a global shutter.\nTo learn more, watch Optimize Your Edge Application: Unveiling the Right Combination of Jetson Processors and Cameras. For detailed specs on AI performance, GPU, CPU, and more for both Xavier and Orin-based Jetson modules, visit Jetson Modules."], "document_title": "Selecting the Right Camera for the NVIDIA Jetson and Other Embedded Systems", "document_url": "https://developer.nvidia.com/blog/selecting-the-right-camera-for-the-nvidia-jetson-and-other-embedded-systems/", "document_date": "2023-09-12T15:00:00", "document_date_modified": "2023-10-05T18:18:14", "document_full_text": "Selecting the Right Camera for the NVIDIA Jetson and Other Embedded Systems\nThe camera module is the most integral part of an AI-based embedded system. With so many camera module choices on the market, the selection process may seem overwhelming. This post breaks down the process to help make the right selection for an embedded application, including the NVIDIA Jetson.\nCamera selection considerations\nCamera module selection involves consideration of three key aspects: sensor, interface (connector), and optics.\nSensor\nThe two main types of electronic image sensors are the charge-coupled device (CCD) and the active-pixel sensor (CMOS). For a CCD sensor, pixel values can only be read on a per-row basis. Each row of pixels is shifted, one by one, into a readout register. For a CMOS sensor, each pixel can be read individually and in parallel.\nCMOS is less expensive and consumes less energy without sacrificing image quality, in most cases. It can also achieve higher frame rates due to the parallel readout of pixel values. However, there are some specific scenarios in which CCD sensors still prevail\u2014for example, when long exposure is necessary and very low-noise images are required, such as in astronomy.\nElectronic shutter\nThere are two options for the electronic shutter: global or rolling. A global shutter exposes each pixel to incoming light at the same time. A rolling shutter exposes the pixel rows in a certain order (top to bottom, for example) and can cause distortion (Figure 1).\nTwo images of a helicopter showing distortion of moving blades caused by rolling shutter.\nFigure 1. Distortion of rotor blades caused by rolling shutter The global shutter is not impacted by motion blur and distortion due to object movement. It is much easier to sync multiple cameras with a global shutter because there is a single point in time when exposure starts. However, sensors with a global shutter are much more expensive than those with a rolling shutter.\nColor or monochrome\nIn most cases, a monochrome image sensor is sufficient for typical machine vision tasks like fault detection, presence monitoring, and recording measurements.\nWith a monochrome sensor, each pixel is usually described by eight bits. With a color sensor, each pixel has eight bits for the red channel, eight bits for the green channel, and eight bits for the blue channel. The color sensor requires processing three times the amount of data, resulting in a higher processing time and, consequently, a slower frame rate.\nDynamic range\nDynamic range is the ratio between the maximum and minimum signal that is acquired by the sensor. At the upper limit, pixels appear white for higher values of intensity (saturation), while pixels appear black at the lower limit and below. An HDR of at least 80db is needed for indoor application and up to 140db is needed for outdoor application.\nResolution\nResolution is a sensor\u2019s ability to reproduce object details. It can be influenced by factors such as the type of lighting used, the sensor pixel size, and the capabilities of the optics. The smaller the object detail, the higher the required resolution.\nPixel resolution translates to how many millimeters each pixel is equal to on the image. The higher the resolution, the sharper your image will be. The camera or sensor\u2019s resolution should enable coverage of a feature\u2019s area of at least two pixels.\nCMOS sensors with high resolutions tend to have low frame rates. While a sensor may achieve the resolution you need, it will not capture the quality images you need without achieving enough frames per second. It is important to evaluate the speed of the sensor.\nA general rule of thumb to determine the resolution needed for the use case is shown below and in Figure 2. \u202fThe multiplier (2) represents the typical desire to have a minimum two pixels on an object in order to successfully detect it.\nResolution = 2\\times \\frac{Field \\ of \\ View (FOV)}{Size \\ of \\ feature \\ of \\ interest}\nDiagram showing the representation of a person and the working distance from an object as an example of minimum object feature size of interest in the field of view.\nFigure 2. Sensor resolution required is determined by lens field of view and feature of interest size For example, suppose you have an image of an injury around the eye of a boxer.\nResolution= 2\\times \\frac{2000}{4}\nFOV, mm = 2000mm\nSize of feature of interest (the eye), mm = 4mm\nBased on the calculation, 1000 x 1000, a one-megapixel camera should be sufficient to detect the eye using a CV or AI algorithm.\nNote that a sensor is made up of multiple rows of pixels. These pixels are also called photosites. The number of photons collected by a pixel is directly proportional to the size of the pixel. Selecting a larger pixel may seem tempting but may not be the optimal choice in all the cases.\nSmall pixel\nSensitive to noise (-)\nHigher spatial resolution for same sensor size (+)\nLarge pixel\nLess sensitive to noise (+)\nLess spatial resolution for same sensor size (-)\nTable 1. Pros and cons of small and large pixel size Back-illuminated sensors maximize the amount of light being captured and converted by each photodiode. In front-illuminated sensors, metal wiring above the photodiodes blocks off some photons, hence reducing the amount of light captured.\nOn the left, a diagram of a front-illuminated structure with substrate, photodiodes, metal wiring, and microlenses. On the right, a diagram of a back-illuminated structure with metal wiring, photodiodes, and microlenses.\nFigure 3. Cross-section of a front-illuminated structure (left) and a back-illuminated structure (right)\nFrame rate and shutter speed\nThe frame rate\u202frefers to the number of frames (or images captured) per second (FPS). The frame rate should be determined based on the number of inspections required per second. This correlates with the shutter speed (or exposure time), which is the time that the camera sensor is exposed to capture the image.\nTheoretically, the maximum frame rate is equal to the inverse of the exposure time. But achievable FPS is lower because of latency introduced by frame readout, sensor resolution, and the data transfer rate of the interface including cabling.\nFPS can be increased by reducing the need for large exposure times by adding additional lighting, binning the pixels.\nCMOS sensors can achieve higher FPS, as the process of reading out each pixel can be done more quickly than with the charge transfer in a CCD sensor\u2019s shift register.\nInterface\nThere are multiple ways to connect the camera module to an embedded system. Typically, for evaluation purposes, cameras with USB and Ethernet interfaces are used because custom driver development is not needed.\nOther important parameters for interface selection are transmission length, data rate, and operating conditions. Table 2 lists the most popular interfaces. Each option has its pros and cons.\nFeatures\nUSB 3.2\nEthernet (1 GbE)\nMIPI CSI-2\nGMSL2\nFPDLINK III\nBandwidth\n10Gbps\n1Gbps\nDPHY 2.5 Gbps/lane CPHY 5.71 Gbps/lane\n6Gbps\n4.2Gbps\nCable length supported\n< 5m\nUp to 100m\n<30cm\n<15m\n<15m\nPlug-and-play\nSupported\nSupported\nNot supported\nNot supported\nNot supported\nDevelopment costs\nLow\nLow\nMedium to high\nMedium to high\nMedium to high\nOperating environment\nIndoor\nIndoor\nIndoor\nIndoor and outdoor\nIndoor and outdoor\nTable 2. Comparison of various camera interfac es\nOptics\nThe basic purpose of an optical lens is to collect the light scattered by an object and recreate an image of the object on a light-sensitive image sensor (CCD or CMOS). The following factors should be considered when selecting an optimized lens-focal length, sensor format, field of view, aperture, chief ray angle, resolving power, and distortion.\nLenses are manufactured with a limited number of standard focal lengths. Common lens focal lengths include 6mm, 8mm, 12.5mm, 25mm, and 50mm.\nOnce you choose a lens with a focal length closest to the focal length required by your imaging system, you need to adjust the working distance to get the object under inspection in focus. Lenses with short focal lengths (less than 12mm) produce images with a significant amount of distortion.\nIf your application is sensitive to image distortion, try to increase the working distance and use a lens with a higher focal length. If you cannot change the working distance, you are somewhat limited in choosing an optimized lens.\nWide-angle lens\nNormal lens\nTelephoto lens\nFocal length\n<=35mm\n50mm\n>=70mm\nUse case\nNearby scenes\nSame as human eye\nFar-away scenes\nTable 3. Main types of camera lenses To attach a lens to a camera requires some type of mounting system. Both mechanical stability (a loose lens will deliver an out-of-focus image) and the distance to the sensor must be defined.\nTo ensure compatibility between different lenses and cameras, the following standard lens mounts are defined.\nMost popular\nFor industrial applications\nLens mount\nM12/S mount\nC-mount\nFlange focal length\nNon-standard\n17.526mm\nThreads (per mm)\n0.5\n0.75\nSensor size accommodated (inches)\nUp to \u2154\nUp to 1\nTable 4. Common lens mounts used in embedded space\nNVIDIA camera module partners\nNVIDIA maintains a rich ecosystem of partnerships with highly competent camera module makers all over the world. See Jetson Partner Supported Cameras for details. These partners can help you design imaging systems for your application from concept to production for the NVIDIA Jetson.\nGraphic showing NVIDIA Jetson with camera modules for various use cases and industries.\nFigure 4. NVIDIA Jetson in combination with camera modules can be used across industries for various needs\nSummary\nThis post has explained the most important camera characteristics to consider when selecting a camera for an embedded application. Although the selection process may seem daunting, the first step is to understand your key constraints based on design, performance, environment, and cost.\nOnce you understand the constraints, then focus on the characteristics most relevant to your use case. For example, if the camera will be deployed away from the compute or in a rugged environment, consider using the GMSL interface. If the camera will be used in low-light conditions, consider a camera module with larger pixel and sensor sizes. If the camera will be used in a motion application, consider using a camera with a global shutter.\nTo learn more, watch Optimize Your Edge Application: Unveiling the Right Combination of Jetson Processors and Cameras. For detailed specs on AI performance, GPU, CPU, and more for both Xavier and Orin-based Jetson modules, visit Jetson Modules."}], "https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/": [{"text": "The article explores GPU-accelerated indexes for vector search algorithms, focusing on RAFT's IVF-Flat, IVF-PQ, and CAGRA methods. IVF-Flat is a simple algorithm that organizes vectors into clusters for faster search times, while IVF-PQ compresses vectors to fit larger datasets on the GPU. CAGRA is a high-performance graph-based method optimized for small-batch cases. Parameters such as n_lists and n_probes can be fine-tuned to optimize search performance and recall levels. The article provides a detailed overview of each algorithm's operation and parameters, as well as a comparison of performance against the CPU-based HNSW method. Pylibraft, a Python library for RAFT, is introduced for easy implementation of vector search in Python. Benchmarks show that using GPU acceleration with RAFT's algorithms leads to higher throughput and better performance, especially at higher batch sizes. Overall, the article emphasizes the benefits of GPU-accelerated vector search indexes and provides resources for further exploration and implementation.", "text_components": ["Accelerating Vector Search: Fine-Tuning GPU Index Algorithms\nIn this post, we dive deeper into each of the GPU-accelerated indexes mentioned in part 1 and give a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior.\nWe then go through a simple end-to-end example to demonstrate RAFT\u2019s Python APIs on a question-and-answer problem with a pretrained large language model and provide a performance comparison of RAFT\u2019s algorithms against HNSW for a few different scenarios involving different numbers of query vectors being passed to the search algorithm concurrently.\nThis post provides:\nAn overview of vector search index algorithms that can be used with GPUs\nAn end-to-end example demonstrating how easy it can be to run vector search on the GPU with Python\nPerformance comparison of vector search on the GPU against HNSW, the current state-of-the-art method on the CPU\nThe first post in this series introduced vector search indexes, explained the role they play in enabling a widespread range of important applications, and provided a brief overview of vector search on the GPU with the RAFT library. For more information, see Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT.\nThe third post in this series focuses on IVF-Flat, an ANN algorithm found in RAPIDS RAFT. We discuss how the algorithm works, and demonstrate the usage of both the Python and C++ APIs in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. For more information, see Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat.", "Vector search indexes\nWhen working with vector search, the vectors are often converted to an indexed format that is optimized for fast lookups. Choosing the right indexing algorithm is important as it can affect both index build and search times. Furthermore, each different index type comes with its own set of knobs for fine-tuning the behavior, trading off index construction time, storage cost, search quality, and search speed.\nWhen the right indexing algorithm is paired with the correct parameter settings, vector search on the GPU provides both faster build and search times for all levels of recall.", "IVF-Flat\nAs it\u2019s the simplest index type, start with the IVF-Flat algorithm. In this algorithm, a set of training vectors are first split into some clusters and then stored in the GPU memory organized by their closest cluster centers. The index-building step is faster than that of other algorithms presented in this post, even at high numbers of clusters.\nTo search an IVF-Flat index, the closest clusters to each query vector are selected, and the k-nearest neighbors (k-NN) are computed from each of those closest clusters. Because IVF-Flat stores the vectors in an exact, or flat format, meaning without compression, it has the advantage of computing exact distances within each of the clusters it searches. As we describe later in this post, this provides an advantage that often has a higher recall than IVF-PQ when the same number of closest clusters are searched. IVF-Flat index is a good choice when the full index can fit in GPU memory.\nRAFT\u2019s IVF-Flat index contains a couple of parameters to help trade off the query performance and accuracy:\nWhen training the index, the ```n_lists``` parameter determines the number of clusters to partition the training dataset.\nThe search parameter ```n_probes``` determines the number of closest clusters to search through to compute the nearest neighbors for a set of query points.\nIn general, a smaller number of probes leads to a faster search at the expense of recall. When the number of probes is set to the number of lists, exact results are computed. However, in that case, a call to RAFT\u2019s brute-force search is more performant.", "IVF-PQ\nWhen your dataset becomes too large to fit on the GPU, you gain some mileage by compressing the vectors using the IVF-PQ index type. Like IVF-Flat, IVF-PQ splits the points into a number of clusters (also specified by a parameter called ```n_lists``` ) and searches the closest clusters to compute the nearest neighbors (also specified by a parameter called ```n_probes``` ), but it shrinks the sizes of the vectors using a technique called product quantization.\nCompressing the index ultimately allows for more vectors to be stored on the GPU. The amount of compression can be controlled with tuning parameters, which we describe later in this post, but higher levels of compression can provide a faster lookup time at the cost of recall. IVF-PQ is currently RAFT\u2019s most memory-efficient vector index.\nRAFT\u2019s IVF-PQ provides two parameters that control memory usage:\n```pq_dim``` sets the target dimensionality of the compressed vector.\n```pq_bits``` sets the number of bits for each vector element after compression.\nWe recommend setting the former to a multiple of 32 while the latter is limited to a range of 4-8 bits. By default, RAFT selects a dimensionality value that minimizes quantization loss according to ```pq_bits```, but this value can be adjusted to lower the memory footprint for each vector. It is useful to play with these parameters to see which settings work best for you.\nWhen using large amounts of compression, an additional refinement step can be performed by querying the IVF-PQ index for a larger number of neighbors than needed and computing an exact search over the resulting neighbors to reduce the set down to the final desired number. The refinement step requires the original uncompressed dataset on the host memory.\nFor more information about building an IVF-PQ index, with in-depth details and recommendations, see the complete guide to RAFT IVF-PQ notebook on our GitHub repo.", "CAGRA\nCAGRA is RAFT\u2019s new state-of-the-art ANN index. It is a high-performance, GPU-accelerated, graph-based method that has been specifically optimized for small-batch cases, where each lookup contains only one or a few query vectors. Like other popular graph-based methods, such as hierarchical navigable small-world graphs (HNSW) and SONG, an optimized k-NN graph is built at index training time with various qualities that yield efficient search at reasonable levels of recall.\nCAGRA performs a search by first randomly selecting candidate vertices from the graph and then expanding, or traversing, those vertices to compute distances to their children, storing off the nearest neighbors along the way (Figure 1). Each time it traverses a set of vertices, it has performed one iteration.\nDiagram shows how CAGRA can map subgraphs to separate thread blocks, enabling parallelism even for a single query.\nFigure 1. CAGRA using multiple thread blocks\nIn Figure 1, CAGRA is using multiple thread blocks to visit more graph nodes in parallel. This is maximizing GPU utilization for single-query searches.\nBecause CAGRA returns the approximate nearest neighbors like the algorithms described earlier, it also provides a few parameters to control the recall and the speed.\nThe main parameter that can be adjusted to trade off search speed is ```itopk_size```, which specifies the size of an internal sorted list that stores the nodes that can be explored in the next iteration. Higher values of ```itopk_size``` keep a larger search context in memory that improves recall at the cost of more time spent in maintaining the queue.\nThe parameter ```search_width``` defines the number of the closest parent vertices that are traversed to expand their children in each search iteration.\nAnother useful parameter is the number of iterations to perform. The setting is selected automatically by default, but this can be changed to a higher or lower value to trade off recall for a faster search.\nCAGRA\u2019s optimized graph is fixed-degree, which is tuned using the parameter ```graph_degree```. The fixed-degree makes better use of GPU resources by keeping the number of computations uniform when searching the graph. It builds the initial k-NN graph by computing an actual k-NN, for example by using IVF-PQ explained earlier, to compute the nearest neighbors of all the points in the training dataset.\nThe number of k-nearest neighbors ( k ) of this intermediate k-NN graph can be tuned using a parameter called ```intermediate_graph_degree``` to trade off the quality of the final searchable CAGRA graph.\nA higher quality graph can be built with a larger ```intermediate_graph_degree``` value, which means that the final optimized graph is more likely to find nearest neighbors that yield a high recall. RAFT provides several useful parameters to tune the CAGRA algorithm. For more information, see the CAGRA API documentation.\nAgain, this parameter can be used to control how thoroughly the overall space is covered by the search but again this comes at the cost of having to search more to find the nearest neighbors, which reduces the search performance.", "Getting started with pylibraft\nPylibraft is the lightweight Python library of RAFT and enables you to use RAFT\u2019s ANN algorithms for vector search right in Python. Pylibraft can accept any object that supports ```__cuda_array_interface__```, such as a Torch or CuPy array.\nThe following example briefly demonstrates how you can build and query a RAFT CAGRA index with Pylibraft.\n```\nfrom pylibraft.neighbors import cagra\nimport cupy as cp\n\n# On small batch sizes, using \"multi_cta\" algorithm is efficient\nindex_params = cagra.IndexParams(graph_degree=32)\nsearch_params = cagra.SearchParams(algo=\"multi_cta\")\n\ncorpus_embeddings = cp.random.random((1500,96), dtype=cp.float32)\nquery_embeddings = cp.random.random((1,96), dtype=cp.float32)\n\ncagra_index = cagra.build(index_params, corpus_embeddings)\n# Find the 10 closest vectors\nhits = cagra.search(search_params, cagra_index, query_embeddings, k=10)\n```\nWith the recent success of LLMs, semantic search is a perfect way to showcase vector similarity search in action using RAFT. In the following example, a DistilBERT transformer model combined with each of the three ANN indexes is used to solve a simple question retrieval problem. The Simple English Wikipedia dataset is used to answer the user\u2019s search query.\nThe language model first transforms the training sentences into vector embeddings that are inserted into a RAFT ANN index. The inference is done by encoding the query and using our trained ANN index to find vectors similar to the encoded query vector. The answer that you return to the user is the nearest article in Simple Wikipedia, which you fetch using the closest vector from the similarity search.\nYou can get started with RAFT by using pylibraft and this notebook for a question-retrieval task:\nSorry, something went wrong. Reload?\nSorry, we cannot display this file.\nSorry, this file is invalid so it cannot be displayed.\nview raw\nVectorSearch_QuestionRetrieval.ipynb\nhosted with \u2764 by GitHub", "Benchmarks\nUsing GPU as a hardware accelerator for your vector search application can lead to an increase in performance, and it is best showcased on large datasets. The benchmarks can be fully reproduced by following RAFT\u2019s end-to-end benchmark documentation. Our benchmarks consider that the data is already available for computation, which means that data transfer is not taken into consideration, although this should not be a significant difference thanks to the high transfer speed of recent NVIDIA hardware (over 25 GB/s).\nWe used the DEEP-100M dataset on an H100 GPU to compare RAFT indexes with HNSW running on an Intel Xeon Platinum 8480CL CPU.\nBar chart compares the throughput of HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a single query at a time at various levels of recall.\nFigure 2. Comparing ANN algorithms at various levels of recall and throughput, batch of 1\nFigure 2 compares ANN algorithms at various levels of recall and throughput for a single query. At high levels of recall, RAFT\u2019s methods demonstrate higher throughput than other alternative libraries.\nWe ran a performance comparison on queries for a single vector at a time, called online search. It\u2019s one of the main use cases for vector search. RAFT-based indexes provide a higher throughput, measured in queries-per-second (QPS), than other libraries that are using CPU or GPU.\nBar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a query of 10 vectors at a time.\nFigure 3. Comparing ANN algorithms at various levels of recall and throughput, batch of 10\nFigure 3 compares ANN algorithms at various levels of recall and throughput with a batch size of 10 queries. RAFT\u2019s methods demonstrate higher throughput than HNSW for all experiments.\nThe benefits of using GPU for vector search applications are most prevalent at higher batch sizes. The performance gap between CPU and GPU is significant and can scale up easily. Figure 3 shows that for a batch size of 10, only RAFT-based indexes are relevant when comparing the number of queries per second. For a batch size of 10k (Figure 4), CAGRA outperforms all other indexes by far.\nBar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a query of 10k vectors at a time.\nFigure 4. Comparing ANN algorithms at various levels of recall and throughput\nFigure 4 compares ANN algorithms at various levels of recall and throughput with a batch size of 10K query. RAFT\u2019s methods demonstrate higher throughput than HNSW for all experiments.", "Summary\nEach different vector search index type has benefits and drawbacks which ultimately depend on your needs. This post outlined some of those benefits and drawbacks, providing a brief explanation of how each different algorithm works, along with a few of the most important parameters that can be tuned to trade off storage costs, build times, search quality, and search performance. In all cases, GPUs can improve both index construction and search performance.\nRAPIDS RAFT is fully open source and available on the /rapidsai/raft GitHub repo. You can get started with RAFT by reading through the docs, running the reproducible benchmarking suite, or building upon the example vector search template project. Also be sure to look for options to enable RAFT indexes in Milvus, Redis, and FAISS. Finally, you can follow us on Twitter at @rapidsai."], "document_title": "Accelerating Vector Search: Fine-Tuning GPU Index Algorithms", "document_url": "https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/", "document_date": "2023-09-11T16:00:00", "document_date_modified": "2023-10-13T05:56:24", "document_full_text": "Accelerating Vector Search: Fine-Tuning GPU Index Algorithms\nIn this post, we dive deeper into each of the GPU-accelerated indexes mentioned in part 1 and give a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior.\nWe then go through a simple end-to-end example to demonstrate RAFT\u2019s Python APIs on a question-and-answer problem with a pretrained large language model and provide a performance comparison of RAFT\u2019s algorithms against HNSW for a few different scenarios involving different numbers of query vectors being passed to the search algorithm concurrently.\nThis post provides:\nAn overview of vector search index algorithms that can be used with GPUs\nAn end-to-end example demonstrating how easy it can be to run vector search on the GPU with Python\nPerformance comparison of vector search on the GPU against HNSW, the current state-of-the-art method on the CPU\nThe first post in this series introduced vector search indexes, explained the role they play in enabling a widespread range of important applications, and provided a brief overview of vector search on the GPU with the RAFT library. For more information, see Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT.\nThe third post in this series focuses on IVF-Flat, an ANN algorithm found in RAPIDS RAFT. We discuss how the algorithm works, and demonstrate the usage of both the Python and C++ APIs in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. For more information, see Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat.\nVector search indexes\nWhen working with vector search, the vectors are often converted to an indexed format that is optimized for fast lookups. Choosing the right indexing algorithm is important as it can affect both index build and search times. Furthermore, each different index type comes with its own set of knobs for fine-tuning the behavior, trading off index construction time, storage cost, search quality, and search speed.\nWhen the right indexing algorithm is paired with the correct parameter settings, vector search on the GPU provides both faster build and search times for all levels of recall.\nIVF-Flat\nAs it\u2019s the simplest index type, start with the IVF-Flat algorithm. In this algorithm, a set of training vectors are first split into some clusters and then stored in the GPU memory organized by their closest cluster centers. The index-building step is faster than that of other algorithms presented in this post, even at high numbers of clusters.\nTo search an IVF-Flat index, the closest clusters to each query vector are selected, and the k-nearest neighbors (k-NN) are computed from each of those closest clusters. Because IVF-Flat stores the vectors in an exact, or flat format, meaning without compression, it has the advantage of computing exact distances within each of the clusters it searches. As we describe later in this post, this provides an advantage that often has a higher recall than IVF-PQ when the same number of closest clusters are searched. IVF-Flat index is a good choice when the full index can fit in GPU memory.\nRAFT\u2019s IVF-Flat index contains a couple of parameters to help trade off the query performance and accuracy:\nWhen training the index, the ```n_lists``` parameter determines the number of clusters to partition the training dataset.\nThe search parameter ```n_probes``` determines the number of closest clusters to search through to compute the nearest neighbors for a set of query points.\nIn general, a smaller number of probes leads to a faster search at the expense of recall. When the number of probes is set to the number of lists, exact results are computed. However, in that case, a call to RAFT\u2019s brute-force search is more performant.\nIVF-PQ\nWhen your dataset becomes too large to fit on the GPU, you gain some mileage by compressing the vectors using the IVF-PQ index type. Like IVF-Flat, IVF-PQ splits the points into a number of clusters (also specified by a parameter called ```n_lists``` ) and searches the closest clusters to compute the nearest neighbors (also specified by a parameter called ```n_probes``` ), but it shrinks the sizes of the vectors using a technique called product quantization.\nCompressing the index ultimately allows for more vectors to be stored on the GPU. The amount of compression can be controlled with tuning parameters, which we describe later in this post, but higher levels of compression can provide a faster lookup time at the cost of recall. IVF-PQ is currently RAFT\u2019s most memory-efficient vector index.\nRAFT\u2019s IVF-PQ provides two parameters that control memory usage:\n```pq_dim``` sets the target dimensionality of the compressed vector.\n```pq_bits``` sets the number of bits for each vector element after compression.\nWe recommend setting the former to a multiple of 32 while the latter is limited to a range of 4-8 bits. By default, RAFT selects a dimensionality value that minimizes quantization loss according to ```pq_bits```, but this value can be adjusted to lower the memory footprint for each vector. It is useful to play with these parameters to see which settings work best for you.\nWhen using large amounts of compression, an additional refinement step can be performed by querying the IVF-PQ index for a larger number of neighbors than needed and computing an exact search over the resulting neighbors to reduce the set down to the final desired number. The refinement step requires the original uncompressed dataset on the host memory.\nFor more information about building an IVF-PQ index, with in-depth details and recommendations, see the complete guide to RAFT IVF-PQ notebook on our GitHub repo.\nCAGRA\nCAGRA is RAFT\u2019s new state-of-the-art ANN index. It is a high-performance, GPU-accelerated, graph-based method that has been specifically optimized for small-batch cases, where each lookup contains only one or a few query vectors. Like other popular graph-based methods, such as hierarchical navigable small-world graphs (HNSW) and SONG, an optimized k-NN graph is built at index training time with various qualities that yield efficient search at reasonable levels of recall.\nCAGRA performs a search by first randomly selecting candidate vertices from the graph and then expanding, or traversing, those vertices to compute distances to their children, storing off the nearest neighbors along the way (Figure 1). Each time it traverses a set of vertices, it has performed one iteration.\nDiagram shows how CAGRA can map subgraphs to separate thread blocks, enabling parallelism even for a single query.\nFigure 1. CAGRA using multiple thread blocks\nIn Figure 1, CAGRA is using multiple thread blocks to visit more graph nodes in parallel. This is maximizing GPU utilization for single-query searches.\nBecause CAGRA returns the approximate nearest neighbors like the algorithms described earlier, it also provides a few parameters to control the recall and the speed.\nThe main parameter that can be adjusted to trade off search speed is ```itopk_size```, which specifies the size of an internal sorted list that stores the nodes that can be explored in the next iteration. Higher values of ```itopk_size``` keep a larger search context in memory that improves recall at the cost of more time spent in maintaining the queue.\nThe parameter ```search_width``` defines the number of the closest parent vertices that are traversed to expand their children in each search iteration.\nAnother useful parameter is the number of iterations to perform. The setting is selected automatically by default, but this can be changed to a higher or lower value to trade off recall for a faster search.\nCAGRA\u2019s optimized graph is fixed-degree, which is tuned using the parameter ```graph_degree```. The fixed-degree makes better use of GPU resources by keeping the number of computations uniform when searching the graph. It builds the initial k-NN graph by computing an actual k-NN, for example by using IVF-PQ explained earlier, to compute the nearest neighbors of all the points in the training dataset.\nThe number of k-nearest neighbors ( k ) of this intermediate k-NN graph can be tuned using a parameter called ```intermediate_graph_degree``` to trade off the quality of the final searchable CAGRA graph.\nA higher quality graph can be built with a larger ```intermediate_graph_degree``` value, which means that the final optimized graph is more likely to find nearest neighbors that yield a high recall. RAFT provides several useful parameters to tune the CAGRA algorithm. For more information, see the CAGRA API documentation.\nAgain, this parameter can be used to control how thoroughly the overall space is covered by the search but again this comes at the cost of having to search more to find the nearest neighbors, which reduces the search performance.\nGetting started with pylibraft\nPylibraft is the lightweight Python library of RAFT and enables you to use RAFT\u2019s ANN algorithms for vector search right in Python. Pylibraft can accept any object that supports ```__cuda_array_interface__```, such as a Torch or CuPy array.\nThe following example briefly demonstrates how you can build and query a RAFT CAGRA index with Pylibraft.\n```\nfrom pylibraft.neighbors import cagra\nimport cupy as cp\n\n# On small batch sizes, using \"multi_cta\" algorithm is efficient\nindex_params = cagra.IndexParams(graph_degree=32)\nsearch_params = cagra.SearchParams(algo=\"multi_cta\")\n\ncorpus_embeddings = cp.random.random((1500,96), dtype=cp.float32)\nquery_embeddings = cp.random.random((1,96), dtype=cp.float32)\n\ncagra_index = cagra.build(index_params, corpus_embeddings)\n# Find the 10 closest vectors\nhits = cagra.search(search_params, cagra_index, query_embeddings, k=10)\n```\nWith the recent success of LLMs, semantic search is a perfect way to showcase vector similarity search in action using RAFT. In the following example, a DistilBERT transformer model combined with each of the three ANN indexes is used to solve a simple question retrieval problem. The Simple English Wikipedia dataset is used to answer the user\u2019s search query.\nThe language model first transforms the training sentences into vector embeddings that are inserted into a RAFT ANN index. The inference is done by encoding the query and using our trained ANN index to find vectors similar to the encoded query vector. The answer that you return to the user is the nearest article in Simple Wikipedia, which you fetch using the closest vector from the similarity search.\nYou can get started with RAFT by using pylibraft and this notebook for a question-retrieval task:\nSorry, something went wrong. Reload?\nSorry, we cannot display this file.\nSorry, this file is invalid so it cannot be displayed.\nview raw\nVectorSearch_QuestionRetrieval.ipynb\nhosted with \u2764 by GitHub\nBenchmarks\nUsing GPU as a hardware accelerator for your vector search application can lead to an increase in performance, and it is best showcased on large datasets. The benchmarks can be fully reproduced by following RAFT\u2019s end-to-end benchmark documentation. Our benchmarks consider that the data is already available for computation, which means that data transfer is not taken into consideration, although this should not be a significant difference thanks to the high transfer speed of recent NVIDIA hardware (over 25 GB/s).\nWe used the DEEP-100M dataset on an H100 GPU to compare RAFT indexes with HNSW running on an Intel Xeon Platinum 8480CL CPU.\nBar chart compares the throughput of HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a single query at a time at various levels of recall.\nFigure 2. Comparing ANN algorithms at various levels of recall and throughput, batch of 1\nFigure 2 compares ANN algorithms at various levels of recall and throughput for a single query. At high levels of recall, RAFT\u2019s methods demonstrate higher throughput than other alternative libraries.\nWe ran a performance comparison on queries for a single vector at a time, called online search. It\u2019s one of the main use cases for vector search. RAFT-based indexes provide a higher throughput, measured in queries-per-second (QPS), than other libraries that are using CPU or GPU.\nBar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a query of 10 vectors at a time.\nFigure 3. Comparing ANN algorithms at various levels of recall and throughput, batch of 10\nFigure 3 compares ANN algorithms at various levels of recall and throughput with a batch size of 10 queries. RAFT\u2019s methods demonstrate higher throughput than HNSW for all experiments.\nThe benefits of using GPU for vector search applications are most prevalent at higher batch sizes. The performance gap between CPU and GPU is significant and can scale up easily. Figure 3 shows that for a batch size of 10, only RAFT-based indexes are relevant when comparing the number of queries per second. For a batch size of 10k (Figure 4), CAGRA outperforms all other indexes by far.\nBar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a query of 10k vectors at a time.\nFigure 4. Comparing ANN algorithms at various levels of recall and throughput\nFigure 4 compares ANN algorithms at various levels of recall and throughput with a batch size of 10K query. RAFT\u2019s methods demonstrate higher throughput than HNSW for all experiments.\nSummary\nEach different vector search index type has benefits and drawbacks which ultimately depend on your needs. This post outlined some of those benefits and drawbacks, providing a brief explanation of how each different algorithm works, along with a few of the most important parameters that can be tuned to trade off storage costs, build times, search quality, and search performance. In all cases, GPUs can improve both index construction and search performance.\nRAPIDS RAFT is fully open source and available on the /rapidsai/raft GitHub repo. You can get started with RAFT by reading through the docs, running the reproducible benchmarking suite, or building upon the example vector search template project. Also be sure to look for options to enable RAFT indexes in Milvus, Redis, and FAISS. Finally, you can follow us on Twitter at @rapidsai."}], "https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/": [{"text": "The article discusses the importance of vector search in the AI landscape, particularly in applications such as large language models and generative AI. It highlights the use of approximate methods for vector search to handle large datasets efficiently. The article introduces the RAFT library for accelerating vector search on GPUs and provides a performance comparison of GPU-accelerated indexes against CPU-based methods. It also explores the applications of vector similarity search in various industries, such as recommender systems, finance, cybersecurity, genomics, and chemistry. The article emphasizes the use of retrieval-augmented language models for semantic search and content generation. It discusses different algorithms for approximate nearest neighbor search, such as IVF-Flat, IVF-PQ, and CAGRA, and their trade-offs between recall and speed. The article concludes by encouraging database providers to consider integrating RAFT into their data sources for improved performance in vector search.", "text_components": ["Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT\nIn the AI landscape of 2023, vector search is one of the hottest topics due to its applications in large language models (LLM) and generative AI. Semantic vector search enables a broad range of important tasks like detecting fraudulent transactions, recommending products to users, using contextual information to augment full-text searches, and finding actors that pose potential security risks.\nData volumes continue to soar and traditional methods for comparing items one by one have become computationally infeasible. Vector search methods use approximate lookups, which are more scalable and can handle massive amounts of data more efficiently. As we show in this post, accelerating vector search on the GPU provides not only faster search times, but the index building times can also be substantially faster.\nThis post provides:\nAn introduction to vector search with a brief review of popular applications\nAn overview of the RAFT library for accelerating vector search on the GPU\nPerformance comparison of GPU-accelerated vectors search indexes against the state-of-the-art on the CPU\nThe second post in this series dives deeper into each of the GPU-accelerated indexes mentioned in this post and gives a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. For more information, see Accelerating Vector Search: Fine-Tuning GPU Index Algorithms.\nThe third post in this series focuses on IVF-Flat, an ANN algorithm found in RAPIDS RAFT. We discuss how the algorithm works, and demonstrate the usage of both the Python and C++ APIs in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. For more information, see Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat.", "What is vector search?\nDiagram shows a list of vectors that may have been encoded from sources like images, documents, or videos and a query vector for which you would like to find the closest vectors from the list.\nFigure 1. Vector search process\nFigure 1 shows that vector search entails creating an index of vectors and performing lookups to find some number of vectors in the index that are closest to a query vector. The vectors could be as small as three-dimensional points from a lidar point cloud or larger embeddings from text documents, images, or videos.\nVector search is the process of querying a database to find the most similar vectors. This similarity search is done on numerical vectors that can represent any type of object (Figure 2). These vectors are often embeddings created from multimedia like images, video, and text fragments or entire documents that went through a deep learning model to encode their semantic characteristics into a vector form.\nEmbedding vectors typically have the advantage of being a smaller object than the original document (lower dimensionality), while maintaining as much information about the source as possible. Therefore, two documents that are similar often have similar embeddings.\nImage of a 3D point cloud such as one created from LIDAR.\nFigure 2. Vectors represent data points in higher dimensions\nThe points in Figure 2 are 3D but they could be 500 dimensions or even higher.\nThis makes it easier to compare objects, as the embedding vectors are smaller and retain most of the information. When two documents share similar characteristics, their embedding vectors are often spatially close, or similar.", "Approximate methods for vector search\nTo handle larger datasets efficiently, approximate nearest neighbor (ANN) methods are often used for vector search. ANN methods speed up the search by approximating the closest vectors. This avoids the exhaustive distance computation often required by an exact brute-force approach, which requires comparing the query against every single vector in the database.\nIn addition to the search compute cost, storing many vectors can also consume a large amount of memory. To ensure both fast searches and low memory usage, you must index vectors in an efficient way. As we outline a bit later, this can sometimes benefit from compression. A vector index is a space-efficient data structure built on mathematical models that is used for efficiently querying several vectors at a time.\nUpdating the indexes, such as from inserting and deleting vectors, can cause problems when indexes take hours or even days to build. It turns out that these indexes can often be built much faster on the GPU. We showcase this performance later in the post.", "Vector search in LLMs\nLLMs have become popular for capturing and preserving the semantic meaning and context of the original documents. This means that the vectors resulting from LLM models can be searched using vector similarity search. This search finds items that happen to contain similar words, shapes, or moving objects. It also finds vectors that contextually and semantically mean similar things.\nThis semantic search doesn\u2019t rely on exact word matching. For example, searching for the term, \u201cI would like to buy a muscle car\u201d in an image database should be able to contextualize the sentence to understand the following:\nBuying a car is different from renting a car, so you\u2019d expect to find vectors closer to car dealerships and reviews from car purchasers, rather than car rental companies.\nA muscle car is different from a bodybuilder so you\u2019d expect to find vectors about Dodge Chargers and not Arnold Schwarzenegger.\nBuying a muscle car is different from buying muscle relaxers or economy vehicles.\nMore recently, large language transformer-based models like ChatGPT, LLaMa, NeMo, and BERT have provided significant technical leaps that are increasing the contextual awareness of the models and making them even more useful and applicable to more industries.\nIn addition to creating embedding vectors that can be stored and later searched, these new LLM models use semantic search in pipelines that generate new content from context gleaned by finding similar vectors. This content generation process, shown in Figure 3, is known as retrieval-augmented generative AI.", "Using vector search in a vector database\nA vector database stores high-dimensional vectors (for example, embeddings), and facilitates fast and accurate search and retrieval based on vector similarity (for example, ANN algorithms). Some databases are purpose-built for vector search (for example, Milvus). Other databases include vector search capabilities as an additional feature (for example, Redis).\nChoosing which vector database to use depends on the requirements of your workflow.\nRetrieval-augmented language models allow pretrained models to be customized for specific products, services, or other domain-specific use cases by augmenting a search with additional context that has been encoded into vectors by the LLM and stored in a vector database.\nMore specifically, a search is encoded into vector form and similar vectors are found in the vector database to augment the search. The vectors are then used with the LLM to formulate an appropriate response. Retrieval-augmented LLMs are a form of generative AI and they have revolutionized the industry of chatbots and semantic text search.\nWorkflow diagram shows how vector search is often combined with LLMs to perform semantic search.\nFigure 3. Example workflow of a text retrieval application using RAPIDS RAFT for vector search", "Other applications of vector similarity search\nIn addition to retrieval-augmented LLMs for generative AI, vector embeddings have been around for some time and have found many useful applications in the real world:\nRecommender systems: Provide personalized suggestions according to what a user has shown interest in or interacted with.\nFinance: Fraud detection models vectorize user transactions, making it possible to determine whether those transactions are similar to typical fraudulent activities.\nCybersecurity: Uses embeddings to model and search behaviors of bad actors and anomalous activities.\nGenomics: Finds similar genes and cell structures in genomics analysis, such as single-cell RNA analysis.\nChemistry: Models molecular descriptors or fingerprints of chemical structures to compare them or find similar structures in a database.\nWe are always interested in learning about your use cases so don\u2019t hesitate to leave a comment if you either use vector search already or would like to discuss how it could benefit your application.", "RAPIDS RAFT library for vector search\nRAFT is a library of composable building blocks for accelerating machine learning algorithms on the GPU, such as those used in nearest neighbors and vector search. ANN algorithms are among the core building blocks that comprise vector search libraries. Most importantly, these algorithms can greatly benefit from GPU acceleration.\nFor more information about RAFT\u2019s core APIs and the various accelerated building blocks that it contains, see Reusable Computational Patterns for Machine Learning and Data Analytics with RAPIDS RAFT.", "ANN for fast searches\nIn addition to brute-force for exact search, RAFT currently provides three different algorithms for ANN search:\nIVF-Flat\nIVF-PQ\nCAGRA\nThe choice of the algorithm can depend upon your needs, as they each offer different advantages. Sometimes, brute force can even be the better option. More are being added in upcoming releases.\nBecause these algorithms are not doing an exact search, it is possible that some highly similar vectors are missed. The recall metric can be used to represent how many neighbors in the results are actual nearest neighbors of the query. Most of our benchmarks target recall levels of 85% and higher, meaning 85% (or more) of the relevant vectors were retrieved.\nTo tune the resulting indexes for different levels of recall, use various settings, or hyperparameters, when training approximate nearest-neighbors algorithms. Reducing the recall score often increases the speed of your searches and increasing the recall decreases the speed. This is known as the recall-speed tradeoff.\nFor more information, see Accelerating Vector Search: Fine-Tuning GPU Index Algorithms.", "Performance comparison\nGPUs excel at processing a lot of data at one time. All the algorithms just mentioned can outperform corresponding algorithms on the CPU when computing the nearest neighbors for thousands or tens of thousands of points at a time.\nHowever, CAGRA was specifically engineered with online search in mind, which means that it outperforms the CPU even when only querying the nearest neighbors for a few data points at a time.\nFigure 4 and Figure 5 show benchmarks that we performed by building an index on 100M vectors and querying only 10 vectors at a time. In Figure 4, CAGRA outperforms HNSW, which is one of the most popular indexes for vector search on CPU, in raw search performance even for an extremely small batch size of 10 vectors. This speed comes at a memory cost, however. In Figure 5, you can see that CAGRA\u2019s memory footprint is a bit higher than the other nearest neighbors methods.\nBar chart compares throughput performance (queries per second) for RAFT\u2019s GPU algorithms against HNSW on the CPU.\nFigure 4. Vector search throughput at 95% recall on DEEP-100M dataset, batch size of 10\nIn Figure 5, the host memory of IVF-PQ is for the optional refinement step.\nBar chart compares memory usage for RAFT\u2019s GPU algorithms against HNSW on the CPU.\nFigure 5. GPU memory usage\nFigure 6 presents a comparison of the index build times and shows that indexes can often be built faster on the GPU.\nBar chart compares index build time for RAFT\u2019s GPU algorithms against HNSW on the CPU.\nFigure 6. Index build time for the best-performing index at 95% recall", "Summary\nFrom feature stores to generative AI, vector similarity search can be applied in every industry. Vector search on the GPU performs at lower latency and achieves higher throughput for every level of recall for both online and batch processing.\nRAFT is a set of composable building blocks that can be used to accelerate vector search in any data source. It has pre-built APIs for Python and C++. Integration for RAFT is underway for Milvus, Redis, and FAISS. We encourage database providers to try RAFT and consider integrating it into their data sources.\nIn addition to state-of-the-art ANN algorithms, RAFT contains other GPU-accelerated building blocks, such as matrix and vector operations, iterative solvers, and clustering algorithms. The second post in this series dives deeper into each of the GPU-accelerated indexes mentioned in this post and gives a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. For more information, see Accelerating Vector Search: Fine-Tuning GPU Index Algorithms.\nRAPIDS RAFT is fully open source and available on the /rapidsai/raft GitHub repo. You can also follow us on Twitter at @rapidsai."], "document_title": "Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT", "document_url": "https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/", "document_date": "2023-09-11T15:59:00", "document_date_modified": "2023-12-05T18:56:13", "document_full_text": "Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT\nIn the AI landscape of 2023, vector search is one of the hottest topics due to its applications in large language models (LLM) and generative AI. Semantic vector search enables a broad range of important tasks like detecting fraudulent transactions, recommending products to users, using contextual information to augment full-text searches, and finding actors that pose potential security risks.\nData volumes continue to soar and traditional methods for comparing items one by one have become computationally infeasible. Vector search methods use approximate lookups, which are more scalable and can handle massive amounts of data more efficiently. As we show in this post, accelerating vector search on the GPU provides not only faster search times, but the index building times can also be substantially faster.\nThis post provides:\nAn introduction to vector search with a brief review of popular applications\nAn overview of the RAFT library for accelerating vector search on the GPU\nPerformance comparison of GPU-accelerated vectors search indexes against the state-of-the-art on the CPU\nThe second post in this series dives deeper into each of the GPU-accelerated indexes mentioned in this post and gives a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. For more information, see Accelerating Vector Search: Fine-Tuning GPU Index Algorithms.\nThe third post in this series focuses on IVF-Flat, an ANN algorithm found in RAPIDS RAFT. We discuss how the algorithm works, and demonstrate the usage of both the Python and C++ APIs in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. For more information, see Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat.\nWhat is vector search?\nDiagram shows a list of vectors that may have been encoded from sources like images, documents, or videos and a query vector for which you would like to find the closest vectors from the list.\nFigure 1. Vector search process\nFigure 1 shows that vector search entails creating an index of vectors and performing lookups to find some number of vectors in the index that are closest to a query vector. The vectors could be as small as three-dimensional points from a lidar point cloud or larger embeddings from text documents, images, or videos.\nVector search is the process of querying a database to find the most similar vectors. This similarity search is done on numerical vectors that can represent any type of object (Figure 2). These vectors are often embeddings created from multimedia like images, video, and text fragments or entire documents that went through a deep learning model to encode their semantic characteristics into a vector form.\nEmbedding vectors typically have the advantage of being a smaller object than the original document (lower dimensionality), while maintaining as much information about the source as possible. Therefore, two documents that are similar often have similar embeddings.\nImage of a 3D point cloud such as one created from LIDAR.\nFigure 2. Vectors represent data points in higher dimensions\nThe points in Figure 2 are 3D but they could be 500 dimensions or even higher.\nThis makes it easier to compare objects, as the embedding vectors are smaller and retain most of the information. When two documents share similar characteristics, their embedding vectors are often spatially close, or similar.\nApproximate methods for vector search\nTo handle larger datasets efficiently, approximate nearest neighbor (ANN) methods are often used for vector search. ANN methods speed up the search by approximating the closest vectors. This avoids the exhaustive distance computation often required by an exact brute-force approach, which requires comparing the query against every single vector in the database.\nIn addition to the search compute cost, storing many vectors can also consume a large amount of memory. To ensure both fast searches and low memory usage, you must index vectors in an efficient way. As we outline a bit later, this can sometimes benefit from compression. A vector index is a space-efficient data structure built on mathematical models that is used for efficiently querying several vectors at a time.\nUpdating the indexes, such as from inserting and deleting vectors, can cause problems when indexes take hours or even days to build. It turns out that these indexes can often be built much faster on the GPU. We showcase this performance later in the post.\nVector search in LLMs\nLLMs have become popular for capturing and preserving the semantic meaning and context of the original documents. This means that the vectors resulting from LLM models can be searched using vector similarity search. This search finds items that happen to contain similar words, shapes, or moving objects. It also finds vectors that contextually and semantically mean similar things.\nThis semantic search doesn\u2019t rely on exact word matching. For example, searching for the term, \u201cI would like to buy a muscle car\u201d in an image database should be able to contextualize the sentence to understand the following:\nBuying a car is different from renting a car, so you\u2019d expect to find vectors closer to car dealerships and reviews from car purchasers, rather than car rental companies.\nA muscle car is different from a bodybuilder so you\u2019d expect to find vectors about Dodge Chargers and not Arnold Schwarzenegger.\nBuying a muscle car is different from buying muscle relaxers or economy vehicles.\nMore recently, large language transformer-based models like ChatGPT, LLaMa, NeMo, and BERT have provided significant technical leaps that are increasing the contextual awareness of the models and making them even more useful and applicable to more industries.\nIn addition to creating embedding vectors that can be stored and later searched, these new LLM models use semantic search in pipelines that generate new content from context gleaned by finding similar vectors. This content generation process, shown in Figure 3, is known as retrieval-augmented generative AI.\nUsing vector search in a vector database\nA vector database stores high-dimensional vectors (for example, embeddings), and facilitates fast and accurate search and retrieval based on vector similarity (for example, ANN algorithms). Some databases are purpose-built for vector search (for example, Milvus). Other databases include vector search capabilities as an additional feature (for example, Redis).\nChoosing which vector database to use depends on the requirements of your workflow.\nRetrieval-augmented language models allow pretrained models to be customized for specific products, services, or other domain-specific use cases by augmenting a search with additional context that has been encoded into vectors by the LLM and stored in a vector database.\nMore specifically, a search is encoded into vector form and similar vectors are found in the vector database to augment the search. The vectors are then used with the LLM to formulate an appropriate response. Retrieval-augmented LLMs are a form of generative AI and they have revolutionized the industry of chatbots and semantic text search.\nWorkflow diagram shows how vector search is often combined with LLMs to perform semantic search.\nFigure 3. Example workflow of a text retrieval application using RAPIDS RAFT for vector search\nOther applications of vector similarity search\nIn addition to retrieval-augmented LLMs for generative AI, vector embeddings have been around for some time and have found many useful applications in the real world:\nRecommender systems: Provide personalized suggestions according to what a user has shown interest in or interacted with.\nFinance: Fraud detection models vectorize user transactions, making it possible to determine whether those transactions are similar to typical fraudulent activities.\nCybersecurity: Uses embeddings to model and search behaviors of bad actors and anomalous activities.\nGenomics: Finds similar genes and cell structures in genomics analysis, such as single-cell RNA analysis.\nChemistry: Models molecular descriptors or fingerprints of chemical structures to compare them or find similar structures in a database.\nWe are always interested in learning about your use cases so don\u2019t hesitate to leave a comment if you either use vector search already or would like to discuss how it could benefit your application.\nRAPIDS RAFT library for vector search\nRAFT is a library of composable building blocks for accelerating machine learning algorithms on the GPU, such as those used in nearest neighbors and vector search. ANN algorithms are among the core building blocks that comprise vector search libraries. Most importantly, these algorithms can greatly benefit from GPU acceleration.\nFor more information about RAFT\u2019s core APIs and the various accelerated building blocks that it contains, see Reusable Computational Patterns for Machine Learning and Data Analytics with RAPIDS RAFT.\nANN for fast searches\nIn addition to brute-force for exact search, RAFT currently provides three different algorithms for ANN search:\nIVF-Flat\nIVF-PQ\nCAGRA\nThe choice of the algorithm can depend upon your needs, as they each offer different advantages. Sometimes, brute force can even be the better option. More are being added in upcoming releases.\nBecause these algorithms are not doing an exact search, it is possible that some highly similar vectors are missed. The recall metric can be used to represent how many neighbors in the results are actual nearest neighbors of the query. Most of our benchmarks target recall levels of 85% and higher, meaning 85% (or more) of the relevant vectors were retrieved.\nTo tune the resulting indexes for different levels of recall, use various settings, or hyperparameters, when training approximate nearest-neighbors algorithms. Reducing the recall score often increases the speed of your searches and increasing the recall decreases the speed. This is known as the recall-speed tradeoff.\nFor more information, see Accelerating Vector Search: Fine-Tuning GPU Index Algorithms.\nPerformance comparison\nGPUs excel at processing a lot of data at one time. All the algorithms just mentioned can outperform corresponding algorithms on the CPU when computing the nearest neighbors for thousands or tens of thousands of points at a time.\nHowever, CAGRA was specifically engineered with online search in mind, which means that it outperforms the CPU even when only querying the nearest neighbors for a few data points at a time.\nFigure 4 and Figure 5 show benchmarks that we performed by building an index on 100M vectors and querying only 10 vectors at a time. In Figure 4, CAGRA outperforms HNSW, which is one of the most popular indexes for vector search on CPU, in raw search performance even for an extremely small batch size of 10 vectors. This speed comes at a memory cost, however. In Figure 5, you can see that CAGRA\u2019s memory footprint is a bit higher than the other nearest neighbors methods.\nBar chart compares throughput performance (queries per second) for RAFT\u2019s GPU algorithms against HNSW on the CPU.\nFigure 4. Vector search throughput at 95% recall on DEEP-100M dataset, batch size of 10\nIn Figure 5, the host memory of IVF-PQ is for the optional refinement step.\nBar chart compares memory usage for RAFT\u2019s GPU algorithms against HNSW on the CPU.\nFigure 5. GPU memory usage\nFigure 6 presents a comparison of the index build times and shows that indexes can often be built faster on the GPU.\nBar chart compares index build time for RAFT\u2019s GPU algorithms against HNSW on the CPU.\nFigure 6. Index build time for the best-performing index at 95% recall\nSummary\nFrom feature stores to generative AI, vector similarity search can be applied in every industry. Vector search on the GPU performs at lower latency and achieves higher throughput for every level of recall for both online and batch processing.\nRAFT is a set of composable building blocks that can be used to accelerate vector search in any data source. It has pre-built APIs for Python and C++. Integration for RAFT is underway for Milvus, Redis, and FAISS. We encourage database providers to try RAFT and consider integrating it into their data sources.\nIn addition to state-of-the-art ANN algorithms, RAFT contains other GPU-accelerated building blocks, such as matrix and vector operations, iterative solvers, and clustering algorithms. The second post in this series dives deeper into each of the GPU-accelerated indexes mentioned in this post and gives a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. For more information, see Accelerating Vector Search: Fine-Tuning GPU Index Algorithms.\nRAPIDS RAFT is fully open source and available on the /rapidsai/raft GitHub repo. You can also follow us on Twitter at @rapidsai."}], "https://developer.nvidia.com/blog/creating-immersive-events-with-openusd-and-digital-twins/": [{"text": "Moment Factory is a multimedia entertainment studio known for creating immersive experiences using video, lighting, sound, and interactivity. They have worked on projects like Billie Eilish's world tour and Lumina Night Walks. To streamline their production process, Moment Factory implemented OpenUSD, an open data format, and NVIDIA Omniverse, a platform for developing 3D workflows. They use digital twins to simulate immersive events before deploying them in the real world, allowing for testing and collaboration among team members. Moment Factory also develops extensions for Omniverse, such as an NDI extension for video projection and an MPCDI converter for multiprojector rigs. These extensions enhance their digital twins and allow for more realistic simulations of their immersive experiences. Developers can access Moment Factory's extensions on GitHub and utilize Omniverse resources to build their own extensions for 3D workflows. Overall, Moment Factory's use of OpenUSD and Omniverse has revolutionized their process for creating immersive events and digital twins.", "text_components": ["Creating Immersive Events with OpenUSD and Digital Twins\nMoment Factory is a global multimedia entertainment studio that combines specializations in video, lighting, architecture, sound, software, and interactivity to create immersive experiences for audiences around the world.\nFrom live performances and multimedia shows to interactive installations, Moment Factory is known for some of the most awe-inspiring and entertaining experiences that bring people together in the real world. These include dazzling visuals at Billie Eilish\u2019s Happier Than Ever world tour, Lumina Night Walks at natural sites around the world, and digital placemaking at the AT&T Discovery District.\nWith a team of over 400 professionals and offices in Montreal, Tokyo, Paris, New York City, and Singapore, Moment Factory has become a global leader in the entertainment industry.\nBillie Eilish on stage during her Happier Than Ever world tour\nFigure 1. Billie Eilish engaged Moment Factory to oversee creative direction, stage design, and content creation for her Happier Than Ever world tour", "Streamlining immersive experience development with OpenUSD\nBringing these experiences to life requires large teams of highly skilled experts with diverse specialties, all using unique tools. To achieve optimal efficiency in their highly complex production processes, Moment Factory looked to implement an interoperable open data format and development platform that could seamlessly integrate all aspects, from concept to operation.\nMoment Factory chose Universal Scene Description, also known as OpenUSD, as the solution. OpenUSD is an extensible framework and ecosystem for describing, composing, simulating, and collaborating within 3D worlds. NVIDIA Omniverse is a software platform that enables teams to develop OpenUSD-based 3D workflows and applications. It provides the unified environment to visualize and collaborate on digital twins in real time with live connections to Moment Factory\u2019s tools.\nUsing OpenUSD with Omniverse enables Moment Factory to unify data from their diverse digital content creation (DCC) tools to form a digital twin of a real-world environment. Every member of the team can interact with this digital twin and iterate on their aspect of the project without affecting other elements\nFor example, a scenographer can work on a base set and unique scene pieces using Vectorworks, 3D design software. At the same time in the same scene, an AV (audio visual) and lighting designer can take care of lighting and projectors with Moment Factory\u2019s proprietary live entertainment operating system and virtual projection mapping software, X-Agora.\nSimultaneously, artists and designers can render and create eye-catching visuals in the scene using tools like Epic Games Unreal Engine, Blender, and Adobe Photoshop\u2014without affecting layers of the project still in progress.\n\u201cUSD is unique in that it can be fragmented into smaller pieces that enable people to work on their own unique parts of a project while staying connected,\u201d said Arnaud Grosjean, solution architect and project lead for Moment Factory\u2019s Innovation Team. \u201cIts flexibility and interoperability allows us to create powerful, custom 3D pipelines.\u201d\nDiagram of USD scenes composition, including nondestructive layers such as venue, scenography, AV, and sensor data from diverse data sources.\nFigure 2. USD scenes are composed of nondestructive layers such as venue, scenography, AV, and sensor data from diverse data sources", "Digital twins simulate real-world experiences\nTo simulate immersive events before deploying them in the real world, Moment Factory is developing digital twins of their installations in NVIDIA Omniverse. Omniverse, a computing platform that enables teams to develop OpenUSD-based 3D workflows and applications, provides the unified environment to visualize and collaborate on digital twins in real time with live connections to DCC tools.\nThe first digital twin they\u2019ve created is that of Blackbox, which serves as an experimentation and prototyping space where they can preview fragments of immersive experiences before real-world deployment. It is a critical space for nearly every phase of the project lifecycle, from conception and design to integration and operation.\nTo build the digital twin of the Blackbox, Moment Factory used USD Composer, a fully customizable foundation application built on NVIDIA Omniverse.\nMoment Factory digital twin\nFigure 3. Live video projection and lighting in the Blackbox is reflected in real time in the digital twin of the Blackbox, shown in the screen on the right\nThe virtual replica of the installation enables the team to run innumerable iterations on the project to test for various factors. They can also better sell concepts for immersive experiences to prospective customers, who can see the show before live production in a virtual environment.\nOne of the key challenges in the process for building large-scale immersive experiences is reaching a consensus among various stakeholders and managing changes.\n\u201cEveryone has their own idea of how a scene should be structured, so we needed a way to align everyone contributing to the project in a unified, dynamic environment\u201d explained Grosjean. \u201cWith the digital twin, potential ideas can be tested and simulated with stakeholders across every core expertise.\u201d\nAs CAD drafters, AV designers, interactive designers, and others contribute to the digital twin of the Blackbox, artists and 2D/3D designers can render and experiment with beauty shots of the immersive experience in action.\nTo see the digital twin of the Blackbox in action, join the Omniverse Livestream with Moment Factory on Wednesday, September 13.", "Developing Omniverse Connectors and extensions\nMoment Factory is continuously building and testing extensions for Omniverse to bring new functionalities and possibilities into their digital twins.\nThey developed an Omniverse Connector for X-Agora, their proprietary multi-display software that allows you to design, plan and operate shows. The software now has a working implementation of a Nucleus connection, USD import/export, and an early live mode implementation.\nVideo projection is a key element of immersive events. The team will often experiment with mapping and projecting visual content onto architectural surfaces, scenic elements, and sometimes even moving objects, transforming static spaces into dynamic and captivating environments.\nNDI, which stands for Network Design Interface, is a popular IP video protocol developed by NewTek that allows for efficient live video production and streaming across interconnected devices and systems. In their immersive experiences, Moment Factory typically connects a media system to physical projectors using video cables. With NDI, they can replicate this connection within a virtual venue, effectively simulating the entire experience digitally.\nTo enable seamless connectivity between the Omniverse RTX Renderer and their creative content, Moment Factory developed an NDI extension for Omniverse. The extension supports more than just video projection and allows the team to simulate LED walls, screens, and pixel fields to mirror their real-world setup in the digital twin.\nThe extension, which was developed with Omniverse Kit, also enables users to use video feeds as dynamic textures. Developers at Moment Factory used the kit-cv-video-example and kit-dynamic texture-example to develop the extension.\nAnyone can access and use Moment Factory\u2019s Omniverse-NDI-extension on GitHub, and install it on the Omniverse Launcher or launch with:\n$ ./link_app.bat --app create\n$ ./app/omni.create.bat --/rtx/ecoMode/enabled=false --ext-folder exts --enable mf.ov.ndi\nExtensions in Omniverse serve as reusable components or tools that developers can build to accelerate and add new functionalities for 3D workflows. They can be built for simple tasks like randomizing objects or used to enable more complex workflows like visual scripting.\nThe team also developed an extension for converting MPDCI, a VESA standard describing multiprojector rigs, to USD called the Omniverse- MPCDI-converter. They are currently testing extensions for MVR (My Virtual Rig) and GDTF (General Device Type Format) Converters to import lighting fixtures and rigs into their digital twins.\nEven more compelling is a lidar UDP simulator extension, which is being developed to enable sensor simulation in Omniverse and connect synthetic data to lidar-compatible software.\nYou can use Moment Factory\u2019s NDI and MPDCI extensions today in your workflows. Stay tuned for new extensions coming soon.\nTo build extensions like Moment Factory, get started with all the Omniverse Developer Resources you\u2019ll need, like documentation, tutorials, USD resources, GitHub samples, and more.\nGet started with NVIDIA Omniverse by downloading the standard license free, or learn how Omniverse Enterprise can connect your team.\nDevelopers can check out these Omniverse resources to begin building on the platform.\nStay up to date on the platform by subscribing to the newsletter and following NVIDIA Omniverse on Instagram, LinkedIn, Medium, Threads, and Twitter.\nFor more, check out our forums, Discord server, Twitch, and YouTube channels."], "document_title": "Creating Immersive Events with OpenUSD and Digital Twins", "document_url": "https://developer.nvidia.com/blog/creating-immersive-events-with-openusd-and-digital-twins/", "document_date": "2023-09-11T15:57:41", "document_date_modified": "2023-10-25T23:51:22", "document_full_text": "Creating Immersive Events with OpenUSD and Digital Twins\nMoment Factory is a global multimedia entertainment studio that combines specializations in video, lighting, architecture, sound, software, and interactivity to create immersive experiences for audiences around the world.\nFrom live performances and multimedia shows to interactive installations, Moment Factory is known for some of the most awe-inspiring and entertaining experiences that bring people together in the real world. These include dazzling visuals at Billie Eilish\u2019s Happier Than Ever world tour, Lumina Night Walks at natural sites around the world, and digital placemaking at the AT&T Discovery District.\nWith a team of over 400 professionals and offices in Montreal, Tokyo, Paris, New York City, and Singapore, Moment Factory has become a global leader in the entertainment industry.\nBillie Eilish on stage during her Happier Than Ever world tour\nFigure 1. Billie Eilish engaged Moment Factory to oversee creative direction, stage design, and content creation for her Happier Than Ever world tour\nStreamlining immersive experience development with OpenUSD\nBringing these experiences to life requires large teams of highly skilled experts with diverse specialties, all using unique tools. To achieve optimal efficiency in their highly complex production processes, Moment Factory looked to implement an interoperable open data format and development platform that could seamlessly integrate all aspects, from concept to operation.\nMoment Factory chose Universal Scene Description, also known as OpenUSD, as the solution. OpenUSD is an extensible framework and ecosystem for describing, composing, simulating, and collaborating within 3D worlds. NVIDIA Omniverse is a software platform that enables teams to develop OpenUSD-based 3D workflows and applications. It provides the unified environment to visualize and collaborate on digital twins in real time with live connections to Moment Factory\u2019s tools.\nUsing OpenUSD with Omniverse enables Moment Factory to unify data from their diverse digital content creation (DCC) tools to form a digital twin of a real-world environment. Every member of the team can interact with this digital twin and iterate on their aspect of the project without affecting other elements\nFor example, a scenographer can work on a base set and unique scene pieces using Vectorworks, 3D design software. At the same time in the same scene, an AV (audio visual) and lighting designer can take care of lighting and projectors with Moment Factory\u2019s proprietary live entertainment operating system and virtual projection mapping software, X-Agora.\nSimultaneously, artists and designers can render and create eye-catching visuals in the scene using tools like Epic Games Unreal Engine, Blender, and Adobe Photoshop\u2014without affecting layers of the project still in progress.\n\u201cUSD is unique in that it can be fragmented into smaller pieces that enable people to work on their own unique parts of a project while staying connected,\u201d said Arnaud Grosjean, solution architect and project lead for Moment Factory\u2019s Innovation Team. \u201cIts flexibility and interoperability allows us to create powerful, custom 3D pipelines.\u201d\nDiagram of USD scenes composition, including nondestructive layers such as venue, scenography, AV, and sensor data from diverse data sources.\nFigure 2. USD scenes are composed of nondestructive layers such as venue, scenography, AV, and sensor data from diverse data sources\nDigital twins simulate real-world experiences\nTo simulate immersive events before deploying them in the real world, Moment Factory is developing digital twins of their installations in NVIDIA Omniverse. Omniverse, a computing platform that enables teams to develop OpenUSD-based 3D workflows and applications, provides the unified environment to visualize and collaborate on digital twins in real time with live connections to DCC tools.\nThe first digital twin they\u2019ve created is that of Blackbox, which serves as an experimentation and prototyping space where they can preview fragments of immersive experiences before real-world deployment. It is a critical space for nearly every phase of the project lifecycle, from conception and design to integration and operation.\nTo build the digital twin of the Blackbox, Moment Factory used USD Composer, a fully customizable foundation application built on NVIDIA Omniverse.\nMoment Factory digital twin\nFigure 3. Live video projection and lighting in the Blackbox is reflected in real time in the digital twin of the Blackbox, shown in the screen on the right\nThe virtual replica of the installation enables the team to run innumerable iterations on the project to test for various factors. They can also better sell concepts for immersive experiences to prospective customers, who can see the show before live production in a virtual environment.\nOne of the key challenges in the process for building large-scale immersive experiences is reaching a consensus among various stakeholders and managing changes.\n\u201cEveryone has their own idea of how a scene should be structured, so we needed a way to align everyone contributing to the project in a unified, dynamic environment\u201d explained Grosjean. \u201cWith the digital twin, potential ideas can be tested and simulated with stakeholders across every core expertise.\u201d\nAs CAD drafters, AV designers, interactive designers, and others contribute to the digital twin of the Blackbox, artists and 2D/3D designers can render and experiment with beauty shots of the immersive experience in action.\nTo see the digital twin of the Blackbox in action, join the Omniverse Livestream with Moment Factory on Wednesday, September 13.\nDeveloping Omniverse Connectors and extensions\nMoment Factory is continuously building and testing extensions for Omniverse to bring new functionalities and possibilities into their digital twins.\nThey developed an Omniverse Connector for X-Agora, their proprietary multi-display software that allows you to design, plan and operate shows. The software now has a working implementation of a Nucleus connection, USD import/export, and an early live mode implementation.\nVideo projection is a key element of immersive events. The team will often experiment with mapping and projecting visual content onto architectural surfaces, scenic elements, and sometimes even moving objects, transforming static spaces into dynamic and captivating environments.\nNDI, which stands for Network Design Interface, is a popular IP video protocol developed by NewTek that allows for efficient live video production and streaming across interconnected devices and systems. In their immersive experiences, Moment Factory typically connects a media system to physical projectors using video cables. With NDI, they can replicate this connection within a virtual venue, effectively simulating the entire experience digitally.\nTo enable seamless connectivity between the Omniverse RTX Renderer and their creative content, Moment Factory developed an NDI extension for Omniverse. The extension supports more than just video projection and allows the team to simulate LED walls, screens, and pixel fields to mirror their real-world setup in the digital twin.\nThe extension, which was developed with Omniverse Kit, also enables users to use video feeds as dynamic textures. Developers at Moment Factory used the kit-cv-video-example and kit-dynamic texture-example to develop the extension.\nAnyone can access and use Moment Factory\u2019s Omniverse-NDI-extension on GitHub, and install it on the Omniverse Launcher or launch with:\n$ ./link_app.bat --app create\n$ ./app/omni.create.bat --/rtx/ecoMode/enabled=false --ext-folder exts --enable mf.ov.ndi\nExtensions in Omniverse serve as reusable components or tools that developers can build to accelerate and add new functionalities for 3D workflows. They can be built for simple tasks like randomizing objects or used to enable more complex workflows like visual scripting.\nThe team also developed an extension for converting MPDCI, a VESA standard describing multiprojector rigs, to USD called the Omniverse- MPCDI-converter. They are currently testing extensions for MVR (My Virtual Rig) and GDTF (General Device Type Format) Converters to import lighting fixtures and rigs into their digital twins.\nEven more compelling is a lidar UDP simulator extension, which is being developed to enable sensor simulation in Omniverse and connect synthetic data to lidar-compatible software.\nYou can use Moment Factory\u2019s NDI and MPDCI extensions today in your workflows. Stay tuned for new extensions coming soon.\nTo build extensions like Moment Factory, get started with all the Omniverse Developer Resources you\u2019ll need, like documentation, tutorials, USD resources, GitHub samples, and more.\nGet started with NVIDIA Omniverse by downloading the standard license free, or learn how Omniverse Enterprise can connect your team.\nDevelopers can check out these Omniverse resources to begin building on the platform.\nStay up to date on the platform by subscribing to the newsletter and following NVIDIA Omniverse on Instagram, LinkedIn, Medium, Threads, and Twitter.\nFor more, check out our forums, Discord server, Twitch, and YouTube channels."}], "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/": [{"text": "NVIDIA has introduced TensorRT-LLM, an open-source library that accelerates and optimizes Large Language Model (LLM) inference on NVIDIA GPUs. This software, integrated into the NVIDIA NeMo framework, offers peak performance and customization capabilities for LLMs without requiring deep knowledge of C++ or NVIDIA CUDA. By using TensorRT-LLM, organizations like Meta, Grammarly, and OctoML have seen significant performance improvements in LLM inference. The software includes optimized kernels, pre/post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance. In addition, TensorRT-LLM incorporates in-flight batching, quantization, and other features to improve efficiency and reduce TCO and energy consumption in data centers. The software supports various LLMs, including Meta Llama 2, OpenAI GPT-2, and Falcon, making it easier to create customized LLMs for different industries. With the ability to convert model weights to FP8 format and take advantage of optimized kernels, TensorRT-LLM is a valuable tool for enhancing LLM inference performance and efficiency.", "text_components": ["NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\nLarge language models (LLMs) offer incredible new capabilities, expanding the frontier of what is possible with AI. However, their large size and unique execution characteristics can make them difficult to use in cost-effective ways.\nNVIDIA has been working closely with leading companies, including Meta, Anyscale, Cohere, Deci, Grammarly, Mistral AI, MosaicML (now a part of Databricks ), OctoML, Perplexity, Tabnine, and Together AI, to accelerate and optimize LLM inference.\nimg\nAs of October 19, 2023, NVIDIA TensorRT-LLM is now public and free to use for all as an open-source library on the /NVIDIA/TensorRT-LLM GitHub repo and as part of the NVIDIA NeMo framework.\nThose innovations have been integrated into the open-source NVIDIA TensorRT-LLM software, available for NVIDIA Ampere, NVIDIA Lovelace, and NVIDIA Hopper GPUs. TensorRT-LLM consists of the TensorRT deep learning compiler and includes optimized kernels, pre\u2013 and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs. It enables you to experiment with new LLMs, offering peak performance and quick customization capabilities, without requiring a deep knowledge of C++ or NVIDIA CUDA.\nTensorRT-LLM improves ease of use and extensibility through an open-source modular Python API for defining, optimizing, and executing new architectures and enhancements as LLMs evolve, and can be customized easily.\nFor example, MosaicML has seamlessly added specific features that it needs on top of TensorRT-LLM and integrated them into inference serving. Naveen Rao, vice president of engineering at Databricks, said, \u201cIt has been an absolute breeze.\u201d\n\u201cTensorRT-LLM is easy to use, feature-packed with streaming of tokens, in-flight batching, paged-attention, quantization, and more, and is efficient,\u201d Rao said. \u201cIt delivers state-of-the-art performance for LLM serving using NVIDIA GPUs and allows us to pass on the cost savings to our customers.\u201d", "Performance comparison\nSummarizing articles is just one of the many applications of LLMs. The following benchmarks show performance improvements brought by TensorRT-LLM on the latest NVIDIA Hopper architecture.\nThe following figures reflect article summarization using an NVIDIA A100 and NVIDIA H100 GPUs with CNN/Daily Mail, a well-known dataset for evaluating summarization performance.\nIn Figure 1, the NVIDIA H100 GPU alone is 4x faster than the A100 GPU. Adding TensorRT-LLM and its benefits, including in-flight batching, results in an 8x total increase to deliver the highest throughput.\nGPT-J performance comparison between A100 and H100 with and without TensorRT-LLM.\nFigure 1. GPT-J-6B A100 compared to H100 with and without TensorRT-LLM Text summarization, variable I/O length, CNN / DailyMail dataset | A100 FP16 PyTorch eager mode | H100 FP8 | H100 FP8, in-flight batching, TensorRT-LLM\nOn Llama 2\u2014a popular language model released recently by Meta and used widely by organizations looking to incorporate generative AI\u2014TensorRT-LLM can accelerate inference performance by 4.6x compared to A100 GPUs.\nLlama 2 70B performance comparison between A100 and H100 with and without TensorRT-LLM.\nFigure 2. Llama 2 70B, A100 compared to H100 with and without TensorRT-LLM Text summarization, variable I/O length, CNN / DailyMail dataset | A100 FP16 PyTorch eager mode| H100 FP8 | H100 FP8, in-flight batching, TensorRT-LLM", "TCO and energy efficiency improvements\nMinimizing total cost of ownership (TCO) and energy consumption in the data center are key goals for customers adopting AI and LLMs in particular, given their explosive increase in computational requirements. Customers don\u2019t just look at the cost of a single server when it comes to AI platform expenditures. Rather, they have to look at aggregate capital and operational costs:\nCost of GPU servers\nManagement head nodes (CPU servers to coordinate all the GPU servers)\nNetworking equipment (fabric, Ethernet, and cabling)\nStorage\nData center IT staff and software\nEquipment maintenance\nData center rent and electricity\nTaken at a holistic level of the actual costs incurred by a data center, significant performance speedups reduce the equipment and maintenance requirements, leading to sizable capital and operational expense savings.\nFigure 3 shows that an 8x performance speedup on small language models like GPT-J 6B leads to a 5.3x reduction in TCO and a 5.6x reduction in energy (electricity bill savings) over the A100 GPU baseline.\nThis bar chart graph shows a GPT-J-6B performance comparison between A100 and H100 TCO and energy benefits.\nFigure 3. A100 compared to H100 with TensorRT-LLM showing TCO and energy cost benefits\nSimilarly, on state-of-the-art LLMs like Llama2, even with 70B parameters, you can realize a 4.6x performance speedup, which results in a 3x reduction in TCO and a 3.2x reduction in energy consumed compared to the A100 baseline.\nThis bar chart graph shows a Llama 2 70B performance comparison between A100 and H100 TCO and energy benefits\nFigure 4. A100 compared to H100 with TensorRT-LLM TCO and energy cost benefits\nIn addition to TCO, there are substantial labor costs associated with software development that can easily exceed infrastructure costs themselves. Investments made by NVIDIA in TensorRT, TensortRT-LLM, Triton Inference Server, and the NVIDIA NeMo framework save you a great deal of time as well as reduce time to market. You must factor in these labor costs, which can easily exceed capital and operational costs, to develop a true picture of your aggregate AI expenditures.", "LLM ecosystem explosion\nThe ecosystem is innovating rapidly, developing new and diverse model architectures. Larger models unleash new capabilities and use cases. Some of the largest, most advanced language models, like Meta\u2019s 70B-parameter Llama 2, require multiple GPUs working in concert to deliver responses in real time. Previously, developers looking to achieve the best performance for LLM inference had to rewrite and manually split the AI model into fragments and coordinate execution across GPUs.\nTensorRT-LLM uses tensor parallelism, a type of model parallelism in which individual weight matrices are split across devices. This enables efficient inference at scale\u2014with each model running in parallel across multiple GPUs connected through NVLink and across multiple servers\u2014without developer intervention or model changes.\nAs new models and model architectures are introduced, you can optimize models with the latest NVIDIA AI kernels available as open source in TensorRT-LLM. The supported kernel fusions include cutting-edge implementations of ```FlashAttention``` and masked multi-head attention for the context and generation phases of GPT model execution, along with many others.\nTensorRT-LLM also includes fully optimized, ready-to-run versions of many LLMs widely used in production today, all of which can be implemented with the simple-to-use TensorRT-LLM Python API:\nMeta Llama 2\nOpenAI GPT-2 and GPT-3\nFalcon\nMosaic MPT\nBLOOM\n\u2026and a dozen others\nThese capabilities help you create customized LLMs faster and more accurately to meet the needs of virtually any industry.", "In-flight batching\nToday\u2019s LLMs are extremely versatile. A single model can be used simultaneously for a variety of different tasks. From a simple question-and-answer response in a chatbot to the summarization of a document or the generation of a long chunk of code, workloads are highly dynamic, with outputs varying in size by several orders of magnitude.\nThis versatility can make it difficult to batch requests and execute them in parallel effectively\u2014a common optimization for serving neural networks, which could result in some requests finishing much earlier than others.\nTo manage these dynamic loads, TensorRT-LLM includes an optimized scheduling technique called in-flight batching. This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model.\nWith in-flight batching, rather than waiting for the whole batch to finish before moving on to the next set of requests, the TensorRT-LLM runtime immediately evicts finished sequences from the batch. It then begins executing new requests while other requests are still in flight.\nIn-flight batching and the additional kernel-level optimizations enable improved GPU usage and minimally double the throughput on a benchmark of real-world LLM requests on NVIDIA H100 Tensor Core GPUs, helping to reduce energy costs and minimize TCO.", "H100 Transformer Engine with FP8\nLLMs contain billions of model weights and activations, typically trained and represented with 16-bit floating point (FP16 or BF16) values where each value occupies 16 bits of memory. At inference time, however, most models can be effectively represented at lower precision, like 8-bit or even 4-bit integers (INT8 or INT4), using modern quantization techniques.\nQuantization is the process of reducing the precision of a model\u2019s weights and activations without sacrificing accuracy. Using lower precision means that each parameter is smaller, and the model takes up less space in GPU memory. This enables inference on larger models with the same hardware while spending less time on memory operations during execution.\nNVIDIA H100 GPUs with TensorRT-LLM give you the ability to convert model weights into a new FP8 format easily and compile models to take advantage of optimized FP8 kernels automatically. This is made possible through NVIDIA Hopper Transformer Engine technology and done without having to change any model code.\nThe FP8 data format introduced by the H100 enables you to quantize your models and radically reduce \u200cmemory consumption without degrading model accuracy. FP8 quantization retains higher accuracy compared to other data formats like INT8 or INT4 while achieving the fastest performance and offering the simplest implementation.", "Summary\nLLMs are advancing rapidly. Diverse model architectures are being developed daily and contribute to a growing ecosystem. In turn, larger models unleash new capabilities and use cases, driving adoption across all industries.\nLLM inference is also reshaping the data center. Higher performance with increased accuracy yields better TCO for enterprises. Model innovations enable better customer experiences, translating into higher revenue and earnings.\nWhen planning inference deployment projects, there are still many other considerations to achieve peak performance using state-of-the-art LLMs. Optimization rarely happens automatically. You must consider fine-tuning factors such as parallelism, end-to-end pipelines, and advanced scheduling techniques. Those factors require a computing platform that can handle mixed precision without diminishing accuracy.", "Get started with TensorRT-LLM\nNVIDIA TensorRT-LLM is now available as the open-source library /NVIDIA/TensorRT-LLM on GitHub and the NVIDIA NeMo framework\u2014part of NVIDIA AI Enterprise, an enterprise-grade AI software platform with security, stability, manageability, and support."], "document_title": "NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs", "document_url": "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/", "document_date": "2023-09-09T17:00:00", "document_date_modified": "2023-11-07T22:27:14", "document_full_text": "NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\nLarge language models (LLMs) offer incredible new capabilities, expanding the frontier of what is possible with AI. However, their large size and unique execution characteristics can make them difficult to use in cost-effective ways.\nNVIDIA has been working closely with leading companies, including Meta, Anyscale, Cohere, Deci, Grammarly, Mistral AI, MosaicML (now a part of Databricks ), OctoML, Perplexity, Tabnine, and Together AI, to accelerate and optimize LLM inference.\nimg\nAs of October 19, 2023, NVIDIA TensorRT-LLM is now public and free to use for all as an open-source library on the /NVIDIA/TensorRT-LLM GitHub repo and as part of the NVIDIA NeMo framework.\nThose innovations have been integrated into the open-source NVIDIA TensorRT-LLM software, available for NVIDIA Ampere, NVIDIA Lovelace, and NVIDIA Hopper GPUs. TensorRT-LLM consists of the TensorRT deep learning compiler and includes optimized kernels, pre\u2013 and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs. It enables you to experiment with new LLMs, offering peak performance and quick customization capabilities, without requiring a deep knowledge of C++ or NVIDIA CUDA.\nTensorRT-LLM improves ease of use and extensibility through an open-source modular Python API for defining, optimizing, and executing new architectures and enhancements as LLMs evolve, and can be customized easily.\nFor example, MosaicML has seamlessly added specific features that it needs on top of TensorRT-LLM and integrated them into inference serving. Naveen Rao, vice president of engineering at Databricks, said, \u201cIt has been an absolute breeze.\u201d\n\u201cTensorRT-LLM is easy to use, feature-packed with streaming of tokens, in-flight batching, paged-attention, quantization, and more, and is efficient,\u201d Rao said. \u201cIt delivers state-of-the-art performance for LLM serving using NVIDIA GPUs and allows us to pass on the cost savings to our customers.\u201d\nPerformance comparison\nSummarizing articles is just one of the many applications of LLMs. The following benchmarks show performance improvements brought by TensorRT-LLM on the latest NVIDIA Hopper architecture.\nThe following figures reflect article summarization using an NVIDIA A100 and NVIDIA H100 GPUs with CNN/Daily Mail, a well-known dataset for evaluating summarization performance.\nIn Figure 1, the NVIDIA H100 GPU alone is 4x faster than the A100 GPU. Adding TensorRT-LLM and its benefits, including in-flight batching, results in an 8x total increase to deliver the highest throughput.\nGPT-J performance comparison between A100 and H100 with and without TensorRT-LLM.\nFigure 1. GPT-J-6B A100 compared to H100 with and without TensorRT-LLM Text summarization, variable I/O length, CNN / DailyMail dataset | A100 FP16 PyTorch eager mode | H100 FP8 | H100 FP8, in-flight batching, TensorRT-LLM\nOn Llama 2\u2014a popular language model released recently by Meta and used widely by organizations looking to incorporate generative AI\u2014TensorRT-LLM can accelerate inference performance by 4.6x compared to A100 GPUs.\nLlama 2 70B performance comparison between A100 and H100 with and without TensorRT-LLM.\nFigure 2. Llama 2 70B, A100 compared to H100 with and without TensorRT-LLM Text summarization, variable I/O length, CNN / DailyMail dataset | A100 FP16 PyTorch eager mode| H100 FP8 | H100 FP8, in-flight batching, TensorRT-LLM\nTCO and energy efficiency improvements\nMinimizing total cost of ownership (TCO) and energy consumption in the data center are key goals for customers adopting AI and LLMs in particular, given their explosive increase in computational requirements. Customers don\u2019t just look at the cost of a single server when it comes to AI platform expenditures. Rather, they have to look at aggregate capital and operational costs:\nCost of GPU servers\nManagement head nodes (CPU servers to coordinate all the GPU servers)\nNetworking equipment (fabric, Ethernet, and cabling)\nStorage\nData center IT staff and software\nEquipment maintenance\nData center rent and electricity\nTaken at a holistic level of the actual costs incurred by a data center, significant performance speedups reduce the equipment and maintenance requirements, leading to sizable capital and operational expense savings.\nFigure 3 shows that an 8x performance speedup on small language models like GPT-J 6B leads to a 5.3x reduction in TCO and a 5.6x reduction in energy (electricity bill savings) over the A100 GPU baseline.\nThis bar chart graph shows a GPT-J-6B performance comparison between A100 and H100 TCO and energy benefits.\nFigure 3. A100 compared to H100 with TensorRT-LLM showing TCO and energy cost benefits\nSimilarly, on state-of-the-art LLMs like Llama2, even with 70B parameters, you can realize a 4.6x performance speedup, which results in a 3x reduction in TCO and a 3.2x reduction in energy consumed compared to the A100 baseline.\nThis bar chart graph shows a Llama 2 70B performance comparison between A100 and H100 TCO and energy benefits\nFigure 4. A100 compared to H100 with TensorRT-LLM TCO and energy cost benefits\nIn addition to TCO, there are substantial labor costs associated with software development that can easily exceed infrastructure costs themselves. Investments made by NVIDIA in TensorRT, TensortRT-LLM, Triton Inference Server, and the NVIDIA NeMo framework save you a great deal of time as well as reduce time to market. You must factor in these labor costs, which can easily exceed capital and operational costs, to develop a true picture of your aggregate AI expenditures.\nLLM ecosystem explosion\nThe ecosystem is innovating rapidly, developing new and diverse model architectures. Larger models unleash new capabilities and use cases. Some of the largest, most advanced language models, like Meta\u2019s 70B-parameter Llama 2, require multiple GPUs working in concert to deliver responses in real time. Previously, developers looking to achieve the best performance for LLM inference had to rewrite and manually split the AI model into fragments and coordinate execution across GPUs.\nTensorRT-LLM uses tensor parallelism, a type of model parallelism in which individual weight matrices are split across devices. This enables efficient inference at scale\u2014with each model running in parallel across multiple GPUs connected through NVLink and across multiple servers\u2014without developer intervention or model changes.\nAs new models and model architectures are introduced, you can optimize models with the latest NVIDIA AI kernels available as open source in TensorRT-LLM. The supported kernel fusions include cutting-edge implementations of ```FlashAttention``` and masked multi-head attention for the context and generation phases of GPT model execution, along with many others.\nTensorRT-LLM also includes fully optimized, ready-to-run versions of many LLMs widely used in production today, all of which can be implemented with the simple-to-use TensorRT-LLM Python API:\nMeta Llama 2\nOpenAI GPT-2 and GPT-3\nFalcon\nMosaic MPT\nBLOOM\n\u2026and a dozen others\nThese capabilities help you create customized LLMs faster and more accurately to meet the needs of virtually any industry.\nIn-flight batching\nToday\u2019s LLMs are extremely versatile. A single model can be used simultaneously for a variety of different tasks. From a simple question-and-answer response in a chatbot to the summarization of a document or the generation of a long chunk of code, workloads are highly dynamic, with outputs varying in size by several orders of magnitude.\nThis versatility can make it difficult to batch requests and execute them in parallel effectively\u2014a common optimization for serving neural networks, which could result in some requests finishing much earlier than others.\nTo manage these dynamic loads, TensorRT-LLM includes an optimized scheduling technique called in-flight batching. This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model.\nWith in-flight batching, rather than waiting for the whole batch to finish before moving on to the next set of requests, the TensorRT-LLM runtime immediately evicts finished sequences from the batch. It then begins executing new requests while other requests are still in flight.\nIn-flight batching and the additional kernel-level optimizations enable improved GPU usage and minimally double the throughput on a benchmark of real-world LLM requests on NVIDIA H100 Tensor Core GPUs, helping to reduce energy costs and minimize TCO.\nH100 Transformer Engine with FP8\nLLMs contain billions of model weights and activations, typically trained and represented with 16-bit floating point (FP16 or BF16) values where each value occupies 16 bits of memory. At inference time, however, most models can be effectively represented at lower precision, like 8-bit or even 4-bit integers (INT8 or INT4), using modern quantization techniques.\nQuantization is the process of reducing the precision of a model\u2019s weights and activations without sacrificing accuracy. Using lower precision means that each parameter is smaller, and the model takes up less space in GPU memory. This enables inference on larger models with the same hardware while spending less time on memory operations during execution.\nNVIDIA H100 GPUs with TensorRT-LLM give you the ability to convert model weights into a new FP8 format easily and compile models to take advantage of optimized FP8 kernels automatically. This is made possible through NVIDIA Hopper Transformer Engine technology and done without having to change any model code.\nThe FP8 data format introduced by the H100 enables you to quantize your models and radically reduce \u200cmemory consumption without degrading model accuracy. FP8 quantization retains higher accuracy compared to other data formats like INT8 or INT4 while achieving the fastest performance and offering the simplest implementation.\nSummary\nLLMs are advancing rapidly. Diverse model architectures are being developed daily and contribute to a growing ecosystem. In turn, larger models unleash new capabilities and use cases, driving adoption across all industries.\nLLM inference is also reshaping the data center. Higher performance with increased accuracy yields better TCO for enterprises. Model innovations enable better customer experiences, translating into higher revenue and earnings.\nWhen planning inference deployment projects, there are still many other considerations to achieve peak performance using state-of-the-art LLMs. Optimization rarely happens automatically. You must consider fine-tuning factors such as parallelism, end-to-end pipelines, and advanced scheduling techniques. Those factors require a computing platform that can handle mixed precision without diminishing accuracy.\nGet started with TensorRT-LLM\nNVIDIA TensorRT-LLM is now available as the open-source library /NVIDIA/TensorRT-LLM on GitHub and the NVIDIA NeMo framework\u2014part of NVIDIA AI Enterprise, an enterprise-grade AI software platform with security, stability, manageability, and support."}], "https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/": [{"text": "The article discusses the importance of AI inference in deploying AI capabilities in various applications. MLPerf Inference v3.1, an industry-standard benchmark suite, measures performance across different workloads and scenarios. NVIDIA submitted results using various products, including the GH200 Grace Hopper Superchip, which showed up to 17% better performance than previous GPUs. The article also highlights optimizations for GPT-J 6B models, DLRMv2, and BERT to improve performance. It introduces the use of the Programmable Vision Accelerator on Jetson Orin modules for parallel computations, resulting in significant performance gains. The article also discusses how algorithmic optimizations can enhance BERT performance. Overall, NVIDIA's accelerated computing platform demonstrated exceptional performance in both closed and open divisions, showcasing leadership in AI inference and training workloads. The results of MLPerf Inference v3.1 highlight the innovation and versatility of NVIDIA's technology stack in delivering high-performance AI solutions across different deployment scenarios.", "text_components": ["Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut\nAI is transforming computing, and inference is how the capabilities of AI are deployed in the world\u2019s applications. Intelligent chatbots, image and video synthesis from simple text prompts, personalized content recommendations, and medical imaging are just a few examples of AI-powered applications.\nInference workloads are both computationally demanding and diverse, requiring that platforms be able to process many predictions on never-seen-before data quickly as well as run inference on a breadth of AI models. Organizations looking to deploy AI need a way to evaluate the performance of infrastructure objectively across a breadth of workloads, environments, and deployment scenarios. This is true for both AI training and inference.\nMLPerf Inference v3.1, developed by the MLCommons consortium, is the latest edition of an industry-standard AI inference benchmark suite. It complements MLPerf Training and MLPerf HPC. MLPerf Inference v3.1 measures inference performance across a variety of important workloads, including image classification, object detection, natural language processing, speech recognition, and recommender systems, across common data center and edge deployment scenarios.\nMLPerf Inference v3.1 includes two important updates to better reflect modern AI use cases:\nThe addition of a large language model (LLM) test based on GPT-J\u2013an open source, 6B-parameter LLM\u2013to represent text summarization, a form of generative AI.\nAn updated DLRM test with a new model architecture and a substantially larger dataset that mirrors the DLRM update introduced in MLPerf Training v3.0. The update better reflects the scale and complexity of modern recommender systems.\nPowered by the full NVIDIA AI Inference software stack, including the latest TensorRT 9.0, NVIDIA made submissions in MLPerf Inference v3.1 using a wide array of products. These included the debut submission of the NVIDIA GH200 Grace Hopper Superchip, which extended the great per-accelerator performance delivered by the NVIDIA H100 Tensor Core GPU. NVIDIA also submitted the NVIDIA L4 Tensor Core GPU for mainstream servers, as well as both the NVIDIA Jetson AGX Orin and Jetson Orin NX platforms for edge AI and robotics.\nThe rest of this post provides highlights of the NVIDIA submissions as well as a peek into how these exceptional results were achieved.", "Grace Hopper Superchip extends NVIDIA Hopper inference performance\nThe NVIDIA GH200 Grace Hopper Superchip combines the NVIDIA Hopper GPU and the NVIDIA Grace CPU through the coherent NVLink-C2C at 900 GB/s to create a single superchip. That\u2019s 7x higher than PCIe Gen5 at 5x lower power. It also incorporates up to 576 GB of fast access memory through the combination of 96 GB of HBM3 GPU memory and up to 480 GB of low-power, high-bandwidth LPDDR5X memory.\nThe GH200 Grace Hopper Superchip has integrated power management features that enable the GH200 to take advantage of the energy efficiency of the Grace CPU to balance efficiency and performance. For more information, see NVIDIA Grace Hopper Superchip Architecture In-Depth and the NVIDIA Grace Hopper Superchip Architecture whitepaper.\nDiagram shows the GH200 with 96 GB HBM3 was used for MLPerf Inference v3.1 submission.\nFigure 1. Logical overview of the NVIDIA GH200 Grace Hopper Superchip\nThe NVIDIA GH200 Grace Hopper Superchip is designed for the versatility required to deliver leading performance across compute and memory-intensive workloads. It also delivers substantially higher performance on the most demanding frontier workloads, such as large transformer-based models with hundreds of billions or trillions of parameters, recommender systems with multi-terabyte embedding tables, and vector databases.\nIn addition to being built for the most intensive AI workloads, the GH200 Grace Hopper Superchip also shines on the popular, mainstream workloads tested by MLPerf Inference. It ran every test, demonstrating its seamless support for the full NVIDIA software stack. It extended the exceptional performance achieved by NVIDIA\u2019s single H100 SXM submission on every workload.\nBar chart shows that NVIDIA Grace Hopper delivered up to 17% better performance than H100 SXM with the help of larger memory capacity, wider memory bandwidth, and sustaining higher GPU clock frequency.\nFigure 2. NVIDIA Grace Hopper MLPerf Inference data center performance results compared to DGX H100 SXM\nMLPerf Inference: Datacenter v3.1, Closed. Submission IDs: NVIDIA 3.1-0107(1xH100 SXM), 3.1-0110(1xGH200 Grace Hopper Superchip)\nThe MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. For more information, see www.mlcommons.org.\nThe GH200 Grace Hopper Superchip incorporates 96 GB of HBM3 and provides up to four TB/s of HBM3 memory bandwidth, compared to 80 GB and 3.35 TB/s for H100 SXM, respectively. This larger memory capacity, as well as greater memory bandwidth, enabled the use of larger batch sizes for workloads on the NVIDIA GH200 Grace Hopper Superchip compared to the NVIDIA H100 SXM. For example, both RetinaNet and DLRMv2 ran with up to double the batch sizes in the Server scenario and 50% greater batch sizes in the Offline scenario.\nThe GH200 Grace Hopper Superchip\u2019s high-bandwidth NVLink-C2C link between the NVIDIA Hopper GPU and the Grace CPU enables fast communication between the CPU and GPU, which can help boost performance.\nFor example, in the MLPerf DLRMv2 workload, transferring a batch of tensors over PCIe takes approximately 22% of the batch inference time on H100 SXM. The GH200 Grace Hopper Superchip, however, performed the same transfer using just 3% of the inference time as a result of NVLink-C2C.\nThanks to higher memory bandwidth and larger memory capacity, the Grace Hopper Superchip delivered up to 17% higher per-chip performance advantage compared to the H100 GPU on MLPerf Inference v3.1 workloads. These results showcase the performance and versatility of both the GH200 Grace Hopper Superchip and the NVIDIA software stack.", "Optimizing GPT-J 6B for LLM inference\nTo represent LLM inference workloads, MLPerf Inference v3.1 introduces a new test based on the GPT-J 6B model: an LLM with 6B parameters. The task tested by the new benchmark is text summarization using the CNN/DailyMail dataset.\nThe NVIDIA platform delivered strong results on the GPT-J workload, with GH200 Grace Hopper Superchip delivered the highest per-accelerator performance on both the Offline and Server scenarios on a per-accelerator basis. The NVIDIA L4 GPU also delivered strong performance, outpacing the best CPU-only result up to 6x in a 1-slot PCIe card with a thermal design power (TDP) of just 72 Watts.\nTo achieve these results, NVIDIA software for LLM inference intelligently applies both FP8 and FP16 precisions to increase performance while also meeting target accuracy requirements.\nA key challenge for performing GPT-J inference is the high memory consumption of the key-value (KV) cache in the transformer block. By storing the KV cache in the FP8 data format, the NVIDIA submission significantly increased the batch size used. This boosted GPU memory utilization and enabled better use of the immense compute performance of NVIDIA GPUs.\nDiagram shows the architecture of the GPT-J model, including input, output, and internal mechanism.\nFigure 3. GPT-J architecture", "Enabling DLRM-DCNv2 submissions\nMLPerf Inference v3.1 introduced an update to the DLRMv1 model used in prior versions of the benchmark. This DLRMv2 model replaces the interactions layer with a three-layer DCNv2 cross network. DLRMv2 also uses multi-hot categorical inputs rather than one-hot, which are synthetically generated from the Criteo Terabyte Click Logs Dataset.\nOne of the challenges of recommender inference arises from fitting the embedding tables on the system. By converting the model to FP16 precision, including the embedding table, we could both improve performance and halve the memory footprint of the embedding table, reducing it to 49 GB. This enables the entire embedding table to fit within a single H100 GPU.\nTo enable our submission on the L4 GPU, which has 24 GB of memory, NVIDIA software intelligently splits the embedding table between GPU and host memory using row-frequency data obtained by analyzing the training dataset. Using this data, NVIDIA software can minimize memory transfers between the host CPU and GPU by storing the most frequently used embedding table rows on the GPU.\nThe NVIDIA platform demonstrated exceptional results on DLRMv2, with GH200 showing up to a 17% increase compared to the great performance delivered by H100 SXM.", "Maximizing parallelism on NVIDIA Jetson Orin with Programmable Vision Accelerator\nThe Jetson AGX Orin series and Jetson Orin NX series are embedded modules for edge AI and robotics, based on the NVIDIA Orin system-on-chip (SoC). To deliver exceptional AI performance and efficiency across a range of use cases, Jetson Orin incorporates many compute engines:\nA GPU based on the NVIDIA Ampere Architecture, with third-generation Tensor Cores\nTwo second-generation, fixed-function NVIDIA Deep Learning Accelerators (NVDLA v2.0)\nOne second-generation Programmable Vision Accelerator (PVA v2.0).\nThese accelerators can be used to offload the GPU and enable additional AI inference performance on the Jetson Orin modules.\nNVDLA is a fixed-function accelerator optimized for deep learning operations and is designed to do full hardware acceleration of convolutional neural network inferencing.\nDiagram of the NVIDIA Orin SoC shows the individual blocks, including CPU, GPU, dedicated accelerators, cache, and memory interface.\nFigure 4. NVIDIA Orin system-on-chip\nFor the first time in MLPerf Inference v3.1, we demonstrate the concurrent use of the PVA alongside GPU and DLA for inference. The second-generation PVA provides dedicated hardware for various computer vision kernels such as filtering, warping, and fast Fourier transforms (FFT). It also supports advanced programmed kernels, which can serve as the backend runtime of TensorRT custom plug-ins.\nWith the 23.08 Jetson CUDA-X AI Developer Preview, we\u2019ve included a sample PVA SDK. This package provides runtime support for a non-maximum suppression (NMS) layer. It demonstrates that the PVA can serve as a highly capable accelerator, complementing the powerful Jetson Orin GPU.\nNVIDIA has developed a TensorRT custom NMS PVA plug-in as a reference for Jetson Orin users and it was included as part of the NVIDIA MLPerf Inference v3.1 submission.\nIn the NVIDIA MLPerf Inference v3.0 RetinaNet submission on NVIDIA Orin platforms, the GPU handled all outputs from the ResNext + FPN backbone from the GPU as well as from the two DLAs.\nDiagram shows how inference queries are sent to the GPU and DLAs, and then the GPU handles the outputs from the DLAs.\nFigure 5. GPU responsible for GPU and DLA outputs in MLPerf Inference v3.0\nFigure 5 shows how, in MLPerf Inference v3.0 submissions, the GPU was responsible for outputs from the ResNext+FPN backbone from both the GPU and the DLAs.\nBy using the NMS PVA plug-in, the NMS operator is now offloaded from GPU to PVA, enabling three fully parallel inference flows on Jetson Orin AGX and Jetson Orin NX. The output from the ResNext and the FPN backbone running on the two DLAs is now consumed by the two PVAs running the NMS PVA plug-in inside the end-to-end RetinaNet TensorRT engine.\nDiagram shows how inference queries are sent to the GPU and DLAs, the PVAs consume the outputs from the DLAs, and then the GPU and two PVAs create output.\nFigure 6. Fully parallel computations in MLPerf Inference v3.1\nIn Figure 6, the NVIDIA MLPerf Inference v3.1 submission enables computations to run fully in parallel through optimized use of Jetson Orin PVAs.\nThis careful use of PVA along with GPU and DLA boosts performance by 30% on both the Jetson AGX Orin 64GB and the Jetson Orin NX 16GB modules. When this use of PVA is coupled with a newly optimized NMS Opt GPU plug-in, Jetson AGX Orin delivers 61% higher performance and 38% better power efficiency on the RetinaNet workload. The Jetson Orin NX 16GB showed an even larger gain, with an 84% performance boost on the same test.", "Algorithmic optimizations further improve BERT performance\nIn MLPerf Inference v3.1, NVIDIA made a submission on the BERT Large workload using the L4 GPU in the open division using techniques developed by the OmniML team. OmniML is a startup acquired by NVIDIA in early 2023 that brought expertise in machine learning algorithmic model optimization for use cases spanning cloud platforms to edge devices.\nThe open division submission on BERT applied structured pruning with distillation techniques to improve the performance by up to 4.7x while maintaining 99% accuracy. This submission demonstrates the potential of algorithmic optimizations for enhancing significantly the already exceptional performance of the NVIDIA platform.\nNVIDIA deployed a proprietary, automatic, structured pruning tool that uses a gradient-based sensitivity analysis to prune the model to the given target FLOPs and fine-tune it with distillation to recover most of the accuracy. The number of transformer layers, attention heads, and linear layer dimensions were pruned in all the transformer layers in the model while the embedding dimension was kept unchanged.\nCompared to the original MLPerf Inference BERT INT8 model, our pruned model reduced the number of parameters by 4x and the number of FLOPs by 5.6x. This model has a varying number of heads and linear layer dimensions in each layer. The resulting TensorRT engine built from the pruned model is 3.4x smaller, 177 MB compared to 607 MB.\nThe fine-tuned model is quantized to INT8 precision using the same technique employed in the NVIDIA closed division submission. The submission also employed distillation during quantization-aware training (QAT) to achieve an accuracy that is 99% or higher.\nScenario\nClosed Division\nOpen Division\nSpeedup\nOffline samples/sec\n1029\n4609\n4.5x\nServer samples/sec\n899\n4265\n4.7x\nSingle Stream p90 Latency (ms)\n2.58\n0.82\n3.1x\nTable 1. BERT Large performance metrics for both closed division and open division To understand better how each of the model optimizations affects performance, NVIDIA performed a stacking analysis and applied different model optimization methods individually (Figure 8).\nDiagram stacks quantization performance on optimization and pruning (Closed Division) and then on distillation (Open Division). The accuracy baseline is the FP32 model (not listed).\nFigure 7. Stacking performance analysis\nFigure 7 shows that, through model pruning and distillation, the NVIDIA open division submission on the BERT workload using L4 provides a 4.5x speedup compared to the same GPU running the closed division workload in the offline scenario.\nEach model optimization method applied can be easily integrated with each other. Together, they yielded a substantial performance improvement compared to the baseline model.", "NVIDIA accelerated computing boosts performance for inference and AI training workloads\nIn its MLPerf debut, the GH200 Grace Hopper Superchip turned in exceptional performance on all workloads and scenarios in the closed division of the data center category, boosting performance by up to 17% on the NVIDIA single-chip H100 SXM submission. The NVIDIA software stack fully supports the GH200 Grace Hopper Superchip today.\nFor mainstream servers, the L4 GPU showed delivery of a large performance leap over CPU-only offerings in a compact, low-power, PCIe add-in card.\nFor edge AI and robotics applications, the Jetson AGX Orin and Jetson Orin NX modules achieved great performance. Software optimizations helped to further unlock the potential of the powerful NVIDIA Orin SoC that powers those modules. It boosted performance on RetinaNet, a popular AI network for object detection, by up to 84%.\nIn this round, NVIDIA also submitted results in the open division, providing a first look at the potential for model optimizations to speed inference performance dramatically while still achieving excellent accuracy.\nThe latest MLPerf Inference v3.1 benchmarks show that the NVIDIA accelerated computing platform continues to deliver leadership performance and versatility. There\u2019s innovation at every layer of the technology stack, from cloud to edge, at the speed of light."], "document_title": "Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut", "document_url": "https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/", "document_date": "2023-09-09T16:00:00", "document_date_modified": "2023-09-22T16:17:33", "document_full_text": "Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut\nAI is transforming computing, and inference is how the capabilities of AI are deployed in the world\u2019s applications. Intelligent chatbots, image and video synthesis from simple text prompts, personalized content recommendations, and medical imaging are just a few examples of AI-powered applications.\nInference workloads are both computationally demanding and diverse, requiring that platforms be able to process many predictions on never-seen-before data quickly as well as run inference on a breadth of AI models. Organizations looking to deploy AI need a way to evaluate the performance of infrastructure objectively across a breadth of workloads, environments, and deployment scenarios. This is true for both AI training and inference.\nMLPerf Inference v3.1, developed by the MLCommons consortium, is the latest edition of an industry-standard AI inference benchmark suite. It complements MLPerf Training and MLPerf HPC. MLPerf Inference v3.1 measures inference performance across a variety of important workloads, including image classification, object detection, natural language processing, speech recognition, and recommender systems, across common data center and edge deployment scenarios.\nMLPerf Inference v3.1 includes two important updates to better reflect modern AI use cases:\nThe addition of a large language model (LLM) test based on GPT-J\u2013an open source, 6B-parameter LLM\u2013to represent text summarization, a form of generative AI.\nAn updated DLRM test with a new model architecture and a substantially larger dataset that mirrors the DLRM update introduced in MLPerf Training v3.0. The update better reflects the scale and complexity of modern recommender systems.\nPowered by the full NVIDIA AI Inference software stack, including the latest TensorRT 9.0, NVIDIA made submissions in MLPerf Inference v3.1 using a wide array of products. These included the debut submission of the NVIDIA GH200 Grace Hopper Superchip, which extended the great per-accelerator performance delivered by the NVIDIA H100 Tensor Core GPU. NVIDIA also submitted the NVIDIA L4 Tensor Core GPU for mainstream servers, as well as both the NVIDIA Jetson AGX Orin and Jetson Orin NX platforms for edge AI and robotics.\nThe rest of this post provides highlights of the NVIDIA submissions as well as a peek into how these exceptional results were achieved.\nGrace Hopper Superchip extends NVIDIA Hopper inference performance\nThe NVIDIA GH200 Grace Hopper Superchip combines the NVIDIA Hopper GPU and the NVIDIA Grace CPU through the coherent NVLink-C2C at 900 GB/s to create a single superchip. That\u2019s 7x higher than PCIe Gen5 at 5x lower power. It also incorporates up to 576 GB of fast access memory through the combination of 96 GB of HBM3 GPU memory and up to 480 GB of low-power, high-bandwidth LPDDR5X memory.\nThe GH200 Grace Hopper Superchip has integrated power management features that enable the GH200 to take advantage of the energy efficiency of the Grace CPU to balance efficiency and performance. For more information, see NVIDIA Grace Hopper Superchip Architecture In-Depth and the NVIDIA Grace Hopper Superchip Architecture whitepaper.\nDiagram shows the GH200 with 96 GB HBM3 was used for MLPerf Inference v3.1 submission.\nFigure 1. Logical overview of the NVIDIA GH200 Grace Hopper Superchip\nThe NVIDIA GH200 Grace Hopper Superchip is designed for the versatility required to deliver leading performance across compute and memory-intensive workloads. It also delivers substantially higher performance on the most demanding frontier workloads, such as large transformer-based models with hundreds of billions or trillions of parameters, recommender systems with multi-terabyte embedding tables, and vector databases.\nIn addition to being built for the most intensive AI workloads, the GH200 Grace Hopper Superchip also shines on the popular, mainstream workloads tested by MLPerf Inference. It ran every test, demonstrating its seamless support for the full NVIDIA software stack. It extended the exceptional performance achieved by NVIDIA\u2019s single H100 SXM submission on every workload.\nBar chart shows that NVIDIA Grace Hopper delivered up to 17% better performance than H100 SXM with the help of larger memory capacity, wider memory bandwidth, and sustaining higher GPU clock frequency.\nFigure 2. NVIDIA Grace Hopper MLPerf Inference data center performance results compared to DGX H100 SXM\nMLPerf Inference: Datacenter v3.1, Closed. Submission IDs: NVIDIA 3.1-0107(1xH100 SXM), 3.1-0110(1xGH200 Grace Hopper Superchip)\nThe MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. For more information, see www.mlcommons.org.\nThe GH200 Grace Hopper Superchip incorporates 96 GB of HBM3 and provides up to four TB/s of HBM3 memory bandwidth, compared to 80 GB and 3.35 TB/s for H100 SXM, respectively. This larger memory capacity, as well as greater memory bandwidth, enabled the use of larger batch sizes for workloads on the NVIDIA GH200 Grace Hopper Superchip compared to the NVIDIA H100 SXM. For example, both RetinaNet and DLRMv2 ran with up to double the batch sizes in the Server scenario and 50% greater batch sizes in the Offline scenario.\nThe GH200 Grace Hopper Superchip\u2019s high-bandwidth NVLink-C2C link between the NVIDIA Hopper GPU and the Grace CPU enables fast communication between the CPU and GPU, which can help boost performance.\nFor example, in the MLPerf DLRMv2 workload, transferring a batch of tensors over PCIe takes approximately 22% of the batch inference time on H100 SXM. The GH200 Grace Hopper Superchip, however, performed the same transfer using just 3% of the inference time as a result of NVLink-C2C.\nThanks to higher memory bandwidth and larger memory capacity, the Grace Hopper Superchip delivered up to 17% higher per-chip performance advantage compared to the H100 GPU on MLPerf Inference v3.1 workloads. These results showcase the performance and versatility of both the GH200 Grace Hopper Superchip and the NVIDIA software stack.\nOptimizing GPT-J 6B for LLM inference\nTo represent LLM inference workloads, MLPerf Inference v3.1 introduces a new test based on the GPT-J 6B model: an LLM with 6B parameters. The task tested by the new benchmark is text summarization using the CNN/DailyMail dataset.\nThe NVIDIA platform delivered strong results on the GPT-J workload, with GH200 Grace Hopper Superchip delivered the highest per-accelerator performance on both the Offline and Server scenarios on a per-accelerator basis. The NVIDIA L4 GPU also delivered strong performance, outpacing the best CPU-only result up to 6x in a 1-slot PCIe card with a thermal design power (TDP) of just 72 Watts.\nTo achieve these results, NVIDIA software for LLM inference intelligently applies both FP8 and FP16 precisions to increase performance while also meeting target accuracy requirements.\nA key challenge for performing GPT-J inference is the high memory consumption of the key-value (KV) cache in the transformer block. By storing the KV cache in the FP8 data format, the NVIDIA submission significantly increased the batch size used. This boosted GPU memory utilization and enabled better use of the immense compute performance of NVIDIA GPUs.\nDiagram shows the architecture of the GPT-J model, including input, output, and internal mechanism.\nFigure 3. GPT-J architecture\nEnabling DLRM-DCNv2 submissions\nMLPerf Inference v3.1 introduced an update to the DLRMv1 model used in prior versions of the benchmark. This DLRMv2 model replaces the interactions layer with a three-layer DCNv2 cross network. DLRMv2 also uses multi-hot categorical inputs rather than one-hot, which are synthetically generated from the Criteo Terabyte Click Logs Dataset.\nOne of the challenges of recommender inference arises from fitting the embedding tables on the system. By converting the model to FP16 precision, including the embedding table, we could both improve performance and halve the memory footprint of the embedding table, reducing it to 49 GB. This enables the entire embedding table to fit within a single H100 GPU.\nTo enable our submission on the L4 GPU, which has 24 GB of memory, NVIDIA software intelligently splits the embedding table between GPU and host memory using row-frequency data obtained by analyzing the training dataset. Using this data, NVIDIA software can minimize memory transfers between the host CPU and GPU by storing the most frequently used embedding table rows on the GPU.\nThe NVIDIA platform demonstrated exceptional results on DLRMv2, with GH200 showing up to a 17% increase compared to the great performance delivered by H100 SXM.\nMaximizing parallelism on NVIDIA Jetson Orin with Programmable Vision Accelerator\nThe Jetson AGX Orin series and Jetson Orin NX series are embedded modules for edge AI and robotics, based on the NVIDIA Orin system-on-chip (SoC). To deliver exceptional AI performance and efficiency across a range of use cases, Jetson Orin incorporates many compute engines:\nA GPU based on the NVIDIA Ampere Architecture, with third-generation Tensor Cores\nTwo second-generation, fixed-function NVIDIA Deep Learning Accelerators (NVDLA v2.0)\nOne second-generation Programmable Vision Accelerator (PVA v2.0).\nThese accelerators can be used to offload the GPU and enable additional AI inference performance on the Jetson Orin modules.\nNVDLA is a fixed-function accelerator optimized for deep learning operations and is designed to do full hardware acceleration of convolutional neural network inferencing.\nDiagram of the NVIDIA Orin SoC shows the individual blocks, including CPU, GPU, dedicated accelerators, cache, and memory interface.\nFigure 4. NVIDIA Orin system-on-chip\nFor the first time in MLPerf Inference v3.1, we demonstrate the concurrent use of the PVA alongside GPU and DLA for inference. The second-generation PVA provides dedicated hardware for various computer vision kernels such as filtering, warping, and fast Fourier transforms (FFT). It also supports advanced programmed kernels, which can serve as the backend runtime of TensorRT custom plug-ins.\nWith the 23.08 Jetson CUDA-X AI Developer Preview, we\u2019ve included a sample PVA SDK. This package provides runtime support for a non-maximum suppression (NMS) layer. It demonstrates that the PVA can serve as a highly capable accelerator, complementing the powerful Jetson Orin GPU.\nNVIDIA has developed a TensorRT custom NMS PVA plug-in as a reference for Jetson Orin users and it was included as part of the NVIDIA MLPerf Inference v3.1 submission.\nIn the NVIDIA MLPerf Inference v3.0 RetinaNet submission on NVIDIA Orin platforms, the GPU handled all outputs from the ResNext + FPN backbone from the GPU as well as from the two DLAs.\nDiagram shows how inference queries are sent to the GPU and DLAs, and then the GPU handles the outputs from the DLAs.\nFigure 5. GPU responsible for GPU and DLA outputs in MLPerf Inference v3.0\nFigure 5 shows how, in MLPerf Inference v3.0 submissions, the GPU was responsible for outputs from the ResNext+FPN backbone from both the GPU and the DLAs.\nBy using the NMS PVA plug-in, the NMS operator is now offloaded from GPU to PVA, enabling three fully parallel inference flows on Jetson Orin AGX and Jetson Orin NX. The output from the ResNext and the FPN backbone running on the two DLAs is now consumed by the two PVAs running the NMS PVA plug-in inside the end-to-end RetinaNet TensorRT engine.\nDiagram shows how inference queries are sent to the GPU and DLAs, the PVAs consume the outputs from the DLAs, and then the GPU and two PVAs create output.\nFigure 6. Fully parallel computations in MLPerf Inference v3.1\nIn Figure 6, the NVIDIA MLPerf Inference v3.1 submission enables computations to run fully in parallel through optimized use of Jetson Orin PVAs.\nThis careful use of PVA along with GPU and DLA boosts performance by 30% on both the Jetson AGX Orin 64GB and the Jetson Orin NX 16GB modules. When this use of PVA is coupled with a newly optimized NMS Opt GPU plug-in, Jetson AGX Orin delivers 61% higher performance and 38% better power efficiency on the RetinaNet workload. The Jetson Orin NX 16GB showed an even larger gain, with an 84% performance boost on the same test.\nAlgorithmic optimizations further improve BERT performance\nIn MLPerf Inference v3.1, NVIDIA made a submission on the BERT Large workload using the L4 GPU in the open division using techniques developed by the OmniML team. OmniML is a startup acquired by NVIDIA in early 2023 that brought expertise in machine learning algorithmic model optimization for use cases spanning cloud platforms to edge devices.\nThe open division submission on BERT applied structured pruning with distillation techniques to improve the performance by up to 4.7x while maintaining 99% accuracy. This submission demonstrates the potential of algorithmic optimizations for enhancing significantly the already exceptional performance of the NVIDIA platform.\nNVIDIA deployed a proprietary, automatic, structured pruning tool that uses a gradient-based sensitivity analysis to prune the model to the given target FLOPs and fine-tune it with distillation to recover most of the accuracy. The number of transformer layers, attention heads, and linear layer dimensions were pruned in all the transformer layers in the model while the embedding dimension was kept unchanged.\nCompared to the original MLPerf Inference BERT INT8 model, our pruned model reduced the number of parameters by 4x and the number of FLOPs by 5.6x. This model has a varying number of heads and linear layer dimensions in each layer. The resulting TensorRT engine built from the pruned model is 3.4x smaller, 177 MB compared to 607 MB.\nThe fine-tuned model is quantized to INT8 precision using the same technique employed in the NVIDIA closed division submission. The submission also employed distillation during quantization-aware training (QAT) to achieve an accuracy that is 99% or higher.\nScenario\nClosed Division\nOpen Division\nSpeedup\nOffline samples/sec\n1029\n4609\n4.5x\nServer samples/sec\n899\n4265\n4.7x\nSingle Stream p90 Latency (ms)\n2.58\n0.82\n3.1x\nTable 1. BERT Large performance metrics for both closed division and open division To understand better how each of the model optimizations affects performance, NVIDIA performed a stacking analysis and applied different model optimization methods individually (Figure 8).\nDiagram stacks quantization performance on optimization and pruning (Closed Division) and then on distillation (Open Division). The accuracy baseline is the FP32 model (not listed).\nFigure 7. Stacking performance analysis\nFigure 7 shows that, through model pruning and distillation, the NVIDIA open division submission on the BERT workload using L4 provides a 4.5x speedup compared to the same GPU running the closed division workload in the offline scenario.\nEach model optimization method applied can be easily integrated with each other. Together, they yielded a substantial performance improvement compared to the baseline model.\nNVIDIA accelerated computing boosts performance for inference and AI training workloads\nIn its MLPerf debut, the GH200 Grace Hopper Superchip turned in exceptional performance on all workloads and scenarios in the closed division of the data center category, boosting performance by up to 17% on the NVIDIA single-chip H100 SXM submission. The NVIDIA software stack fully supports the GH200 Grace Hopper Superchip today.\nFor mainstream servers, the L4 GPU showed delivery of a large performance leap over CPU-only offerings in a compact, low-power, PCIe add-in card.\nFor edge AI and robotics applications, the Jetson AGX Orin and Jetson Orin NX modules achieved great performance. Software optimizations helped to further unlock the potential of the powerful NVIDIA Orin SoC that powers those modules. It boosted performance on RetinaNet, a popular AI network for object detection, by up to 84%.\nIn this round, NVIDIA also submitted results in the open division, providing a first look at the potential for model optimizations to speed inference performance dramatically while still achieving excellent accuracy.\nThe latest MLPerf Inference v3.1 benchmarks show that the NVIDIA accelerated computing platform continues to deliver leadership performance and versatility. There\u2019s innovation at every layer of the technology stack, from cloud to edge, at the speed of light."}], "https://developer.nvidia.com/blog/cuda-toolkit-symbol-server/": [{"text": "NVIDIA has introduced a repository of CUDA Toolkit symbols for Linux to enhance application development by providing obfuscated symbols for NVIDIA libraries being debugged or profiled. Initially, symbols are available for the CUDA Driver and CUDA Runtime, with more libraries to come. By providing a symbolized call stack, developers can speed up the debug process when facing issues related to CUDA APIs. The symbol files contain obfuscated symbol names and do not include debug data. Two recommended ways to use the symbols are by unstripping the library or deploying the .sym file as a symbol file. By symbolizing the call stack, developers can provide a more detailed bug description to NVIDIA for analysis. Overall, using the CUDA symbol server can make profiling and debugging applications using CUDA faster and easier. For further assistance, developers can visit the forum at Developer Tools.", "text_components": ["NVIDIA CUDA Toolkit Symbol Server\nNVIDIA has already made available a GPU driver binary symbols server for Windows. Now, NVIDIA is making available a repository of CUDA Toolkit symbols for Linux.", "What are we providing?\nNVIDIA is introducing CUDA Toolkit symbols for Linux for an application development enhancement. During application development, you can now download obfuscated symbols for NVIDIA libraries that are being debugged or profiled in your application. This is shipping initially for the CUDA Driver ( ```libcuda.so``` ) and the CUDA Runtime ( ```libcudart.so``` ), with more libraries to be added.\nFor instance, when an issue appears to relate to a CUDA API, it may not always be possible to provide NVIDIA with a reproducing example, core dump, or unsymbolized stack traces with all library load information. Providing a symbolized call stack can help speed up the debug process.\nWe are only hosting symbol files, so debug data will not be distributed. The symbol files contain obfuscated symbol names.", "Quickstart guide\nThere are two recommended ways to use the obfuscated symbols for each library:\nBy unstripping the library\nBy deploying the .sym file as a symbol file for the library\n```\n# Determine the symbol file to fetch and obtain it\n$ readelf -n /usr/local/cuda/lib64/libcudart.so\n\n# ... Build ID: 70f26eb93e24216ffc0e93ccd8da31612d277030\n# Browse to https://cudatoolkit-symbols.nvidia.com/libcudart.so/70f26eb93e24216ffc0e93ccd8da31612d277030/index.html to determine filename to download\n$ wget https://cudatoolkit-symbols.nvidia.com/libcudart.so/70f26eb93e24216ffc0e93ccd8da31612d277030/libcudart.so.12.2.128.sym\n\n# Then with appropriate permissions, either unstrip,\n$ eu-unstrip /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.128 libcudart.so.12.2.128.sym \u2013o /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.128\n\n# Or, with appropriate permissions, deploy as symbol file\n# By splitting the Build ID into first two characters as directory, then remaining with \".debug\" extension\n$ cp libcudart.so.12.2.128.sym /usr/lib/debug/.build-id/70/f26eb93e24216ffc0e93ccd8da31612d277030.debug\n```", "Example: Symbolizing\nHere is a simplified example to show the uses of symbolizing. The sample application test_shared has a data corruption that leads to an invalid handle being passed to the CUDA Runtime API cudaStreamDestroy. With a default install of CUDA Toolkit and no obfuscated symbols, the output in gdb might look like the following:\n```\nThread 1 \"test_shared\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff65f9468 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n(gdb) bt\n#0  0x00007ffff65f9468 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n#1  0x00007ffff6657e1f in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n#2  0x00007ffff6013845 in ?? () from /usr/local/cuda/lib64/libcudart.so.12\n#3  0x00007ffff604e698 in cudaStreamDestroy () from /usr/local/cuda/lib64/libcudart.so.12\n#4  0x00005555555554e3 in main ()\n```\nAfter applying the obfuscated symbols in one of the ways described earlier, it would give a stack trace like the following example:\n```\nThread 1 \"test_shared\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff65f9468 in libcuda_8e2eae48ba8eb68460582f76460557784d48a71a () from /lib/x86_64-linux-gnu/libcuda.so.1\n(gdb) bt\n#0  0x00007ffff65f9468 in libcuda_8e2eae48ba8eb68460582f76460557784d48a71a () from /lib/x86_64-linux-gnu/libcuda.so.1\n#1  0x00007ffff6657e1f in libcuda_10c0735c5053f532d0a8bdb0959e754c2e7a4e3d () from /lib/x86_64-linux-gnu/libcuda.so.1\n#2  0x00007ffff6013845 in libcudart_43d9a0d553511aed66b6c644856e24b360d81d0c () from /usr/local/cuda/lib64/libcudart.so.12\n#3  0x00007ffff604e698 in cudaStreamDestroy () from /usr/local/cuda/lib64/libcudart.so.12\n#4  0x00005555555554e3 in main ()\n```\nThe symbolized call stack can then be documented as part of the bug description provided to NVIDIA for analysis.", "Conclusion\nWhen you have to profile and debug applications using CUDA and want to share a call stack with NVIDIA for analysis, use the CUDA symbol server. Profiling and debugging will be faster and easier.\nFor questions or issues, dive into the forum at Developer Tools."], "document_title": "NVIDIA CUDA Toolkit Symbol Server", "document_url": "https://developer.nvidia.com/blog/cuda-toolkit-symbol-server/", "document_date": "2023-09-07T19:10:21", "document_date_modified": "2023-09-21T17:56:27", "document_full_text": "NVIDIA CUDA Toolkit Symbol Server\nNVIDIA has already made available a GPU driver binary symbols server for Windows. Now, NVIDIA is making available a repository of CUDA Toolkit symbols for Linux.\nWhat are we providing?\nNVIDIA is introducing CUDA Toolkit symbols for Linux for an application development enhancement. During application development, you can now download obfuscated symbols for NVIDIA libraries that are being debugged or profiled in your application. This is shipping initially for the CUDA Driver ( ```libcuda.so``` ) and the CUDA Runtime ( ```libcudart.so``` ), with more libraries to be added.\nFor instance, when an issue appears to relate to a CUDA API, it may not always be possible to provide NVIDIA with a reproducing example, core dump, or unsymbolized stack traces with all library load information. Providing a symbolized call stack can help speed up the debug process.\nWe are only hosting symbol files, so debug data will not be distributed. The symbol files contain obfuscated symbol names.\nQuickstart guide\nThere are two recommended ways to use the obfuscated symbols for each library:\nBy unstripping the library\nBy deploying the .sym file as a symbol file for the library\n```\n# Determine the symbol file to fetch and obtain it\n$ readelf -n /usr/local/cuda/lib64/libcudart.so\n\n# ... Build ID: 70f26eb93e24216ffc0e93ccd8da31612d277030\n# Browse to https://cudatoolkit-symbols.nvidia.com/libcudart.so/70f26eb93e24216ffc0e93ccd8da31612d277030/index.html to determine filename to download\n$ wget https://cudatoolkit-symbols.nvidia.com/libcudart.so/70f26eb93e24216ffc0e93ccd8da31612d277030/libcudart.so.12.2.128.sym\n\n# Then with appropriate permissions, either unstrip,\n$ eu-unstrip /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.128 libcudart.so.12.2.128.sym \u2013o /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.128\n\n# Or, with appropriate permissions, deploy as symbol file\n# By splitting the Build ID into first two characters as directory, then remaining with \".debug\" extension\n$ cp libcudart.so.12.2.128.sym /usr/lib/debug/.build-id/70/f26eb93e24216ffc0e93ccd8da31612d277030.debug\n```\nExample: Symbolizing\nHere is a simplified example to show the uses of symbolizing. The sample application test_shared has a data corruption that leads to an invalid handle being passed to the CUDA Runtime API cudaStreamDestroy. With a default install of CUDA Toolkit and no obfuscated symbols, the output in gdb might look like the following:\n```\nThread 1 \"test_shared\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff65f9468 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n(gdb) bt\n#0  0x00007ffff65f9468 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n#1  0x00007ffff6657e1f in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n#2  0x00007ffff6013845 in ?? () from /usr/local/cuda/lib64/libcudart.so.12\n#3  0x00007ffff604e698 in cudaStreamDestroy () from /usr/local/cuda/lib64/libcudart.so.12\n#4  0x00005555555554e3 in main ()\n```\nAfter applying the obfuscated symbols in one of the ways described earlier, it would give a stack trace like the following example:\n```\nThread 1 \"test_shared\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff65f9468 in libcuda_8e2eae48ba8eb68460582f76460557784d48a71a () from /lib/x86_64-linux-gnu/libcuda.so.1\n(gdb) bt\n#0  0x00007ffff65f9468 in libcuda_8e2eae48ba8eb68460582f76460557784d48a71a () from /lib/x86_64-linux-gnu/libcuda.so.1\n#1  0x00007ffff6657e1f in libcuda_10c0735c5053f532d0a8bdb0959e754c2e7a4e3d () from /lib/x86_64-linux-gnu/libcuda.so.1\n#2  0x00007ffff6013845 in libcudart_43d9a0d553511aed66b6c644856e24b360d81d0c () from /usr/local/cuda/lib64/libcudart.so.12\n#3  0x00007ffff604e698 in cudaStreamDestroy () from /usr/local/cuda/lib64/libcudart.so.12\n#4  0x00005555555554e3 in main ()\n```\nThe symbolized call stack can then be documented as part of the bug description provided to NVIDIA for analysis.\nConclusion\nWhen you have to profile and debug applications using CUDA and want to share a call stack with NVIDIA for analysis, use the CUDA symbol server. Profiling and debugging will be faster and easier.\nFor questions or issues, dive into the forum at Developer Tools."}], "https://developer.nvidia.com/blog/unlocking-multi-gpu-model-training-with-dask-xgboost/": [{"text": "The article discusses how to optimize training large XGBoost models on huge datasets using multiple GPUs with Dask XGBoost. It addresses common challenges like out of memory errors and provides solutions like setting environment variables, installing the latest RAPIDS version, and using DaskQuantileDMatrix for reduced memory usage. The article also covers advanced optimizations such as enabling memory spilling and using Unified Communication X (UCX) for optimal data transfer. By following these techniques, users can train XGBoost models on large datasets with fewer GPUs and faster training times. The walkthrough includes steps for setting up the environment, loading data, and training the model. Results show that by implementing UCX and spilling optimizations, training times can be significantly reduced, even with fewer GPUs and memory resources. Overall, leveraging Dask and XGBoost with multiple GPUs can help data scientists overcome memory challenges and achieve faster model training on large datasets.", "text_components": ["Unlocking Multi-GPU Model Training with Dask XGBoost\nAs data scientists, we often face the challenging task of training large models on huge datasets. One commonly used tool, XGBoost, is a robust and efficient gradient-boosting framework that\u2019s been widely adopted due to its speed and performance for large tabular data.\nUsing multiple GPUs should theoretically provide a significant boost in computational power, resulting in faster model training. Yet, many users have found it challenging when attempting to leverage this power through Dask XGBoost. Dask is a flexible open-source Python library for parallel computing and XGBoost provides Dask APIs to train CPU or GPU Dask DataFrames.\nA common hurdle of training Dask XGBoost is handling out of memory (OOM) errors at different stages, including\nLoading the training data\nConverting the DataFrame into XGBoost\u2019s DMatrix format\nDuring the actual model training\nAddressing these memory issues can be challenging, but very rewarding because the potential benefits of multi-GPU training are enticing.", "Top takeaways\nThis post explores how you can optimize Dask XGBoost on multiple GPUs and manage memory errors. Training XGBoost on large datasets presents a variety of challenges. I use the Otto Group Product Classification Challenge dataset to demonstrate the OOM problem and how to fix it. The dataset has 180 million rows and 152 columns, totaling 110 GB when loaded into memory.\nThe key issues we tackle include:\nInstallation using the latest version of RAPIDS and the correct version of XGBoost.\nSetting environment variables.\nDealing with OOM errors.\nUtilizing UCX-py for more speedup.\nBe sure to follow along with the accompanying Notebooks for each section.", "Prerequisites\nAn initial step in leveraging the power of RAPIDS for multi-GPU training is the correct installation of RAPIDS libraries. It\u2019s critical to note that there are several ways to install these libraries\u2014pip, conda, docker, and building from source, each compatible with Linux and Windows Subsystem for Linux.\nEach method has unique considerations. For this guide, I recommend using Mamba, while adhering to the conda install instructions. Mamba provides similar functionalities as conda but is much faster, especially for dependency resolution. Specifically, I opted for a fresh installation of mamba.", "Install the latest RAPIDS version\nAs a best practice, always install the latest RAPIDS libraries available to use the latest features. You can find up-to-date install instructions in the RAPIDS Installation Guide.\nThis post uses version 23.04, which can be installed with the following command:\nmamba create -n rapids-23.04 -c rapidsai -c conda-forge -c nvidia \\\nrapids=23.04 python=3.10 cudatoolkit=11.8 This instruction installs all the libraries required including Dask, Dask-cuDF, XGBoost, and more. In particular, you\u2019ll want to check the XGBoost library installed using the command:\n```mamba list xgboost```\nThe output is listed in Table 1:\nName\nVersion\nBuild\nChannel\nXGBoost\n1.7.1dev.rapidsai23.04\ncuda_11_py310_3\nrapidsai-nightly\nTabel 1. Install the correct XGBoost whose channel should be rapidsai or rapidsai-nightly", "Avoid manual updates for XGBoost\nSome users might notice that the version of XGBoost is not the latest, which is 1.7.5. Manually updating or installing XGBoost using pip or conda-forge is\u200c problematic when training XGBoost together with UCX.\nThe error message will read something like the following:\nException: \u201cXGBoostError(\u2018[14:14:27] /opt/conda/conda-bld/work/rabit/include/rabit/internal/utils.h:86: Allreduce failed\u2019)\u201d\nInstead, use the XGBoost installed from RAPIDS. A quick way to verify the correctness of the XGBoost version is ```mamba list xgboost``` and check the \u201cchannel\u201d of the xgboost, which should be \u201crapidsai\u201d or \u201crapidsai-nightly\u201d.\nXGBoost in the rapidsai channel is built with the RMM plug-in enabled and delivers the best performance regarding multi-GPU training.", "Multi-GPU training walkthrough\nFirst, I\u2019ll walk through a multi-GPU training notebook for the Otto dataset and cover the steps to make it work. Later on, we will talk about some advanced optimizations including UCX and spilling.\nYou can also find the XGB-186-CLICKS-DASK Notebook on GitHub. Alternatively, we provide a python script with full command line configurability.\nThe main libraries we are going to use are xgboost, dask, dask_cuda, and dask-cudf.\nimport os\nimport dask\nimport dask_cudf\nimport xgboost as xgb\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster", "Environment set up\nFirst, let\u2019s set our environment variables to make sure our GPUs are visible. This example uses eight GPUs with 32 GB of memory on each GPU, which is the minimum requirement to run this notebook without OOM complications. In Section Enable memory spilling below we will discuss techniques to lower this requirement to 4 GPUs.\nGPUs = ','.join([str(i) for i in range(0,8)])\nos.environ['CUDA_VISIBLE_DEVICES'] = GPUs Next, define a helper function to create a local GPU cluster for a mutli-GPU single node.\ndef get_cluster():\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nreturn client Then, create a Dask client for your computations.\nclient = get_cluster()", "Loading data\nNow, let\u2019s load the Otto dataset. Use ```dask_cudf read_parquet``` function, which uses multiple GPUs to read the parquet files into a dask_cudf.DataFrame.\nusers = dask_cudf.read_parquet('/raid/otto/Otto-Comp/pqs/train_v152_*.pq').persist() The dataset consists of 152 columns that represent engineered features, providing information about the frequency with which specific product pairs are viewed or purchased together. The objective is to predict which product the user will click next based on their browsing history. The details of this dataset can be found in this writeup.\nEven at this early stage, out of memory errors can occur. This issue often arises due to excessively large row groups in \u200cparquet files. To resolve this, we recommend rewriting the parquet files with smaller row groups. For a more in-depth explanation, refer to the Parquet Large Row Group Demo Notebook.\nAfter loading the data, we can check its shape and memory usage.\nusers.shape[0].compute()\nusers.memory_usage().sum().compute()/2**30 The \u2018clicks\u2019 column is our target, which means if the recommended item was clicked by the user. We ignore the ID columns and use the rest columns as features.\nFEATURES = users.columns[2:]\nTARS = ['clicks']\nFEATURES = [f for f in FEATURES if f not in TARS] Next, we create a DaskQuantileDMatrix which is the input data format for training xgboost models. DaskQuantileDMatrix is a drop-in replacement for the DaskDMatrix when the histogram tree method is used. It helps reduce overall memory usage.\nThis step is critical to avoid OOM errors. If we use the DaskDMatrix OOM occurs even with 16 GPUs. In contrast, DaskQuantileDMatrix enables training xgboot with eight GPUs or less without OOM errors.\ndtrain = xgb.dask.DaskQuantileDMatrix(client, users[FEATURES], users['clicks'])", "XGBoost model training\nWe then set our XGBoost model parameters and start the training process. Given the target column \u2018clicks\u2019 is binary, we use the binary classification objective.\nxgb_parms = {\n'max_depth':4,\n'learning_rate':0.1,\n'subsample':0.7,\n'colsample_bytree':0.5,\n'eval_metric':'map',\n'objective':'binary:logistic',\n'scale_pos_weight':8,\n'tree_method':'gpu_hist',\n'random_state':42\n} Now, you\u2019re ready to train the XGBoost model using all eight GPUs.\nOutput:\n[99] train-map:0.20168\nCPU times: user 7.45 s, sys: 1.93 s, total: 9.38 s\nWall time: 1min 10s That\u2019s it! You\u2019re done with training the XGBoost model using multiple GPUs.", "Enable memory spilling\nIn the previous XGB-186-CLICKS-DASK Notebook, training the XGBoost model on the Otto dataset required a minimum of eight GPUs. Given that this dataset occupies 110GB in memory, and each V100 GPU offers 32GB, the data-to-GPU-memory ratio amounts to a mere 43% (calculated as 110/(32*8)).\nOptimally, we\u2019d halve this by using just four GPUs. Yet, a straightforward reduction of GPUs in our previous setup invariably leads to OOM errors. This issue arises from the creation of temporary variables needed to generate the ```DaskQuantileDMatrix``` from the Dask cuDF dataframe and in other steps of training XGBoost. These variables themselves consume a substantial share of the GPU memory.", "Optimize the same GPU resources to train larger datasets\nIn the XGB-186-CLICKS-DASK-SPILL Notebook, I introduce minor tweaks to the previous setup. By enabling spilling, you can now train on the same dataset using just four GPUs. This technique allows you to train much larger data with the same GPU resources.\nSpilling is the technique that moves data automatically when an operation that would otherwise succeed runs out of memory due to other dataframes or series taking up needed space in GPU memory. It enables out-of-core computations on datasets that don\u2019t fit into memory. RAPIDS cuDF and dask-cudf now support spilling from GPU to CPU memory\nEnabling spilling is surprisingly easy, where we just need to reconfigure the cluster with two new parameters, ```device_memory_limit``` and ```jit_unspil``` l:\ndef get_cluster():\nip = get_ip()\ncluster = LocalCUDACluster(ip=ip,\ndevice_memory_limit='10GB',\njit_unspill=True)\nclient = Client(cluster)\nreturn client```device_memory_limit='10GB\u2019``` sets a limit on the amount of GPU memory that can be used by each GPU before spilling is triggered. Our configuration intentionally assigns a ```device_memory_limit``` of 10GB, substantially less than the total 32GB of the GPU. This is a deliberate strategy designed to preempt OOM errors during XGBoost training.\nIt\u2019s also important to understand that memory usage by XGBoost isn\u2019t managed directly by Dask-CUDA or Dask-cuDF. Therefore, to prevent memory overflow, Dask-CUDA and Dask-cuDF need to initiate the spilling process before the memory limit is reached by XGBoost operations.\n```Jit_unspill``` enables Just-In-Time un-spilling, which means that the cluster will automatically spill data from GPU memory to main memory when GPU memory is running low, and unspill it back just in time for a computation.\nAnd that\u2019s it! The rest of the notebook is identical to the previous notebook. Now it can train with just four GPUs, saving 50% of computing resources.\nRefer to the XGB-186-CLICKS-DASK-SPILL Notebook for details.", "Use Unified Communication X (UCX) for optimal data transfer\nUCX-py is a high-performance communication protocol that provides optimized data transfer capabilities, which is particularly useful for GPU-to-GPU communication.\nTo use UCX effectively, we need to set another environment variable ```RAPIDS_NO_INITIALIZE```:\nos.environ[\"RAPIDS_NO_INITIALIZE\"] = \"1\" It stops cuDF from running various diagnostics on import which requires the creation of an NVIDIA CUDA context. When running distributed and using UCX, we have to bring up the networking stack before a CUDA context is created (for various reasons). By setting that environment variable, any child processes that import cuDF do not create a CUDA context before UCX has a chance to do so.\nReconfigure the cluster:\ndef get_cluster():\nip = get_ip()\ncluster = LocalCUDACluster(ip=ip,\ndevice_memory_limit='10GB',\njit_unspill=True,\nprotocol=\"ucx\",\nrmm_pool_size=\"29GB\"\n)\nclient = Client(cluster)\nreturn client The protocol=\u2019ucx\u2019 parameter specifies UCX to be the communication protocol used for transferring data between the workers in the cluster.\nUse the prmm_pool_size=\u201929GB\u2019 parameter to set the size of the RAPIDS Memory Manager (RMM) pool for each worker. RMM allows for efficient use of GPU memory. In this case, the pool size is set to 29GB which is less than the total GPU memory size of 32GB. This adjustment is crucial as it accounts for the fact that XGBoost creates certain intermediate variables that exist outside the control of the RMM pool.\nBy simply enabling UCX, we experienced a substantial acceleration in our training times\u2014a significant speed boost of 20% with spilling, and an impressive 40.7% speedup when spilling was not needed. Refer to the XGB-186-CLICKS-DASK-UCX-SPILL Notebook for details.", "Configure local_directory\nThere are times when warning messages emerge, such as, \u201cUserWarning: Creating scratch directories is taking a surprisingly long time.\u201d This is a signal indicating that \u200cdisk performance is becoming a bottleneck.\nTo circumvent this issue, we could set ```local_directory``` of ```dask-cuda```, which specifies the path on the local machine to store temporary files. These temporary files are used during Dask\u2019s spill-to-disk operations.\nA recommended practice is to set the ```local_directory``` to a location on a fast storage device. For instance, we could set ```local_directory``` to ```/raid/dask_dir ``` if it is on a high-speed local SSD. Making this simple change can significantly reduce the time it takes for scratch directory operations, optimizing your overall workflow.\nThe final cluster configuration is as follows:\ndef get_cluster():\nip = get_ip()\ncluster = LocalCUDACluster(ip=ip,\nlocal_directory=\u2019/raid/dask_dir\u2019\ndevice_memory_limit='10GB',\njit_unspill=True,\nprotocol=\"ucx\",\nrmm_pool_size=\"29GB\"\n)\nclient = Client(cluster)\nreturn client", "Results\nAs shown in Table 2, the two main optimization techniques are UCX and spilling. We managed to train XGBoost with just four GPUs and 128GB of memory. We will also show the performance scales nicely to more GPUs.\nSpilling off\nSpilling on\nUCX off\n135s / 8GPUs / 256 GB\n270s / 4GPUs / 128 GB\nUCX on\n80s / 8GPUs / 256 GB\n217s / 4GPUs / 128 GB\nTable 2. Overview of four combinations of optimizations In each cell, the numbers represent end-to-end execution time, the minimum number of GPUs required, and the total GPU memory available. All four demos accomplish the same task of loading and training 110 GB of Otto data.", "Summary\nIn conclusion, leveraging Dask and XGBoost with multiple GPUs can be an exciting adventure, despite the occasional bumps like out of memory errors.\nYou can mitigate these memory challenges and tap into the potential of multi-GPU model training by:\nCarefully configuring parameters such as row group size in the input parquet files\nEnsuring the correct installation of RAPIDS and XGBoost\nUtilizing Dask Quantile DMatrix\nEnabling spilling\nFurthermore, by applying advanced features such as UCX-Py, you can significantly speed up training times.\nimg\nSign up for the latest Data Science news. Get the latest announcements, notebooks, hands-on tutorials, events, and more in your inbox once a month from NVIDIA."], "document_title": "Unlocking Multi-GPU Model Training with Dask XGBoost", "document_url": "https://developer.nvidia.com/blog/unlocking-multi-gpu-model-training-with-dask-xgboost/", "document_date": "2023-09-07T17:09:18", "document_date_modified": "2023-09-07T18:30:59", "document_full_text": "Unlocking Multi-GPU Model Training with Dask XGBoost\nAs data scientists, we often face the challenging task of training large models on huge datasets. One commonly used tool, XGBoost, is a robust and efficient gradient-boosting framework that\u2019s been widely adopted due to its speed and performance for large tabular data.\nUsing multiple GPUs should theoretically provide a significant boost in computational power, resulting in faster model training. Yet, many users have found it challenging when attempting to leverage this power through Dask XGBoost. Dask is a flexible open-source Python library for parallel computing and XGBoost provides Dask APIs to train CPU or GPU Dask DataFrames.\nA common hurdle of training Dask XGBoost is handling out of memory (OOM) errors at different stages, including\nLoading the training data\nConverting the DataFrame into XGBoost\u2019s DMatrix format\nDuring the actual model training\nAddressing these memory issues can be challenging, but very rewarding because the potential benefits of multi-GPU training are enticing.\nTop takeaways\nThis post explores how you can optimize Dask XGBoost on multiple GPUs and manage memory errors. Training XGBoost on large datasets presents a variety of challenges. I use the Otto Group Product Classification Challenge dataset to demonstrate the OOM problem and how to fix it. The dataset has 180 million rows and 152 columns, totaling 110 GB when loaded into memory.\nThe key issues we tackle include:\nInstallation using the latest version of RAPIDS and the correct version of XGBoost.\nSetting environment variables.\nDealing with OOM errors.\nUtilizing UCX-py for more speedup.\nBe sure to follow along with the accompanying Notebooks for each section.\nPrerequisites\nAn initial step in leveraging the power of RAPIDS for multi-GPU training is the correct installation of RAPIDS libraries. It\u2019s critical to note that there are several ways to install these libraries\u2014pip, conda, docker, and building from source, each compatible with Linux and Windows Subsystem for Linux.\nEach method has unique considerations. For this guide, I recommend using Mamba, while adhering to the conda install instructions. Mamba provides similar functionalities as conda but is much faster, especially for dependency resolution. Specifically, I opted for a fresh installation of mamba.\nInstall the latest RAPIDS version\nAs a best practice, always install the latest RAPIDS libraries available to use the latest features. You can find up-to-date install instructions in the RAPIDS Installation Guide.\nThis post uses version 23.04, which can be installed with the following command:\nmamba create -n rapids-23.04 -c rapidsai -c conda-forge -c nvidia \\\nrapids=23.04 python=3.10 cudatoolkit=11.8 This instruction installs all the libraries required including Dask, Dask-cuDF, XGBoost, and more. In particular, you\u2019ll want to check the XGBoost library installed using the command:\n```mamba list xgboost```\nThe output is listed in Table 1:\nName\nVersion\nBuild\nChannel\nXGBoost\n1.7.1dev.rapidsai23.04\ncuda_11_py310_3\nrapidsai-nightly\nTabel 1. Install the correct XGBoost whose channel should be rapidsai or rapidsai-nightly\nAvoid manual updates for XGBoost\nSome users might notice that the version of XGBoost is not the latest, which is 1.7.5. Manually updating or installing XGBoost using pip or conda-forge is\u200c problematic when training XGBoost together with UCX.\nThe error message will read something like the following:\nException: \u201cXGBoostError(\u2018[14:14:27] /opt/conda/conda-bld/work/rabit/include/rabit/internal/utils.h:86: Allreduce failed\u2019)\u201d\nInstead, use the XGBoost installed from RAPIDS. A quick way to verify the correctness of the XGBoost version is ```mamba list xgboost``` and check the \u201cchannel\u201d of the xgboost, which should be \u201crapidsai\u201d or \u201crapidsai-nightly\u201d.\nXGBoost in the rapidsai channel is built with the RMM plug-in enabled and delivers the best performance regarding multi-GPU training.\nMulti-GPU training walkthrough\nFirst, I\u2019ll walk through a multi-GPU training notebook for the Otto dataset and cover the steps to make it work. Later on, we will talk about some advanced optimizations including UCX and spilling.\nYou can also find the XGB-186-CLICKS-DASK Notebook on GitHub. Alternatively, we provide a python script with full command line configurability.\nThe main libraries we are going to use are xgboost, dask, dask_cuda, and dask-cudf.\nimport os\nimport dask\nimport dask_cudf\nimport xgboost as xgb\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\nEnvironment set up\nFirst, let\u2019s set our environment variables to make sure our GPUs are visible. This example uses eight GPUs with 32 GB of memory on each GPU, which is the minimum requirement to run this notebook without OOM complications. In Section Enable memory spilling below we will discuss techniques to lower this requirement to 4 GPUs.\nGPUs = ','.join([str(i) for i in range(0,8)])\nos.environ['CUDA_VISIBLE_DEVICES'] = GPUs Next, define a helper function to create a local GPU cluster for a mutli-GPU single node.\ndef get_cluster():\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nreturn client Then, create a Dask client for your computations.\nclient = get_cluster()\nLoading data\nNow, let\u2019s load the Otto dataset. Use ```dask_cudf read_parquet``` function, which uses multiple GPUs to read the parquet files into a dask_cudf.DataFrame.\nusers = dask_cudf.read_parquet('/raid/otto/Otto-Comp/pqs/train_v152_*.pq').persist() The dataset consists of 152 columns that represent engineered features, providing information about the frequency with which specific product pairs are viewed or purchased together. The objective is to predict which product the user will click next based on their browsing history. The details of this dataset can be found in this writeup.\nEven at this early stage, out of memory errors can occur. This issue often arises due to excessively large row groups in \u200cparquet files. To resolve this, we recommend rewriting the parquet files with smaller row groups. For a more in-depth explanation, refer to the Parquet Large Row Group Demo Notebook.\nAfter loading the data, we can check its shape and memory usage.\nusers.shape[0].compute()\nusers.memory_usage().sum().compute()/2**30 The \u2018clicks\u2019 column is our target, which means if the recommended item was clicked by the user. We ignore the ID columns and use the rest columns as features.\nFEATURES = users.columns[2:]\nTARS = ['clicks']\nFEATURES = [f for f in FEATURES if f not in TARS] Next, we create a DaskQuantileDMatrix which is the input data format for training xgboost models. DaskQuantileDMatrix is a drop-in replacement for the DaskDMatrix when the histogram tree method is used. It helps reduce overall memory usage.\nThis step is critical to avoid OOM errors. If we use the DaskDMatrix OOM occurs even with 16 GPUs. In contrast, DaskQuantileDMatrix enables training xgboot with eight GPUs or less without OOM errors.\ndtrain = xgb.dask.DaskQuantileDMatrix(client, users[FEATURES], users['clicks'])\nXGBoost model training\nWe then set our XGBoost model parameters and start the training process. Given the target column \u2018clicks\u2019 is binary, we use the binary classification objective.\nxgb_parms = {\n'max_depth':4,\n'learning_rate':0.1,\n'subsample':0.7,\n'colsample_bytree':0.5,\n'eval_metric':'map',\n'objective':'binary:logistic',\n'scale_pos_weight':8,\n'tree_method':'gpu_hist',\n'random_state':42\n} Now, you\u2019re ready to train the XGBoost model using all eight GPUs.\nOutput:\n[99] train-map:0.20168\nCPU times: user 7.45 s, sys: 1.93 s, total: 9.38 s\nWall time: 1min 10s That\u2019s it! You\u2019re done with training the XGBoost model using multiple GPUs.\nEnable memory spilling\nIn the previous XGB-186-CLICKS-DASK Notebook, training the XGBoost model on the Otto dataset required a minimum of eight GPUs. Given that this dataset occupies 110GB in memory, and each V100 GPU offers 32GB, the data-to-GPU-memory ratio amounts to a mere 43% (calculated as 110/(32*8)).\nOptimally, we\u2019d halve this by using just four GPUs. Yet, a straightforward reduction of GPUs in our previous setup invariably leads to OOM errors. This issue arises from the creation of temporary variables needed to generate the ```DaskQuantileDMatrix``` from the Dask cuDF dataframe and in other steps of training XGBoost. These variables themselves consume a substantial share of the GPU memory.\nOptimize the same GPU resources to train larger datasets\nIn the XGB-186-CLICKS-DASK-SPILL Notebook, I introduce minor tweaks to the previous setup. By enabling spilling, you can now train on the same dataset using just four GPUs. This technique allows you to train much larger data with the same GPU resources.\nSpilling is the technique that moves data automatically when an operation that would otherwise succeed runs out of memory due to other dataframes or series taking up needed space in GPU memory. It enables out-of-core computations on datasets that don\u2019t fit into memory. RAPIDS cuDF and dask-cudf now support spilling from GPU to CPU memory\nEnabling spilling is surprisingly easy, where we just need to reconfigure the cluster with two new parameters, ```device_memory_limit``` and ```jit_unspil``` l:\ndef get_cluster():\nip = get_ip()\ncluster = LocalCUDACluster(ip=ip,\ndevice_memory_limit='10GB',\njit_unspill=True)\nclient = Client(cluster)\nreturn client```device_memory_limit='10GB\u2019``` sets a limit on the amount of GPU memory that can be used by each GPU before spilling is triggered. Our configuration intentionally assigns a ```device_memory_limit``` of 10GB, substantially less than the total 32GB of the GPU. This is a deliberate strategy designed to preempt OOM errors during XGBoost training.\nIt\u2019s also important to understand that memory usage by XGBoost isn\u2019t managed directly by Dask-CUDA or Dask-cuDF. Therefore, to prevent memory overflow, Dask-CUDA and Dask-cuDF need to initiate the spilling process before the memory limit is reached by XGBoost operations.\n```Jit_unspill``` enables Just-In-Time un-spilling, which means that the cluster will automatically spill data from GPU memory to main memory when GPU memory is running low, and unspill it back just in time for a computation.\nAnd that\u2019s it! The rest of the notebook is identical to the previous notebook. Now it can train with just four GPUs, saving 50% of computing resources.\nRefer to the XGB-186-CLICKS-DASK-SPILL Notebook for details.\nUse Unified Communication X (UCX) for optimal data transfer\nUCX-py is a high-performance communication protocol that provides optimized data transfer capabilities, which is particularly useful for GPU-to-GPU communication.\nTo use UCX effectively, we need to set another environment variable ```RAPIDS_NO_INITIALIZE```:\nos.environ[\"RAPIDS_NO_INITIALIZE\"] = \"1\" It stops cuDF from running various diagnostics on import which requires the creation of an NVIDIA CUDA context. When running distributed and using UCX, we have to bring up the networking stack before a CUDA context is created (for various reasons). By setting that environment variable, any child processes that import cuDF do not create a CUDA context before UCX has a chance to do so.\nReconfigure the cluster:\ndef get_cluster():\nip = get_ip()\ncluster = LocalCUDACluster(ip=ip,\ndevice_memory_limit='10GB',\njit_unspill=True,\nprotocol=\"ucx\",\nrmm_pool_size=\"29GB\"\n)\nclient = Client(cluster)\nreturn client The protocol=\u2019ucx\u2019 parameter specifies UCX to be the communication protocol used for transferring data between the workers in the cluster.\nUse the prmm_pool_size=\u201929GB\u2019 parameter to set the size of the RAPIDS Memory Manager (RMM) pool for each worker. RMM allows for efficient use of GPU memory. In this case, the pool size is set to 29GB which is less than the total GPU memory size of 32GB. This adjustment is crucial as it accounts for the fact that XGBoost creates certain intermediate variables that exist outside the control of the RMM pool.\nBy simply enabling UCX, we experienced a substantial acceleration in our training times\u2014a significant speed boost of 20% with spilling, and an impressive 40.7% speedup when spilling was not needed. Refer to the XGB-186-CLICKS-DASK-UCX-SPILL Notebook for details.\nConfigure local_directory\nThere are times when warning messages emerge, such as, \u201cUserWarning: Creating scratch directories is taking a surprisingly long time.\u201d This is a signal indicating that \u200cdisk performance is becoming a bottleneck.\nTo circumvent this issue, we could set ```local_directory``` of ```dask-cuda```, which specifies the path on the local machine to store temporary files. These temporary files are used during Dask\u2019s spill-to-disk operations.\nA recommended practice is to set the ```local_directory``` to a location on a fast storage device. For instance, we could set ```local_directory``` to ```/raid/dask_dir ``` if it is on a high-speed local SSD. Making this simple change can significantly reduce the time it takes for scratch directory operations, optimizing your overall workflow.\nThe final cluster configuration is as follows:\ndef get_cluster():\nip = get_ip()\ncluster = LocalCUDACluster(ip=ip,\nlocal_directory=\u2019/raid/dask_dir\u2019\ndevice_memory_limit='10GB',\njit_unspill=True,\nprotocol=\"ucx\",\nrmm_pool_size=\"29GB\"\n)\nclient = Client(cluster)\nreturn client\nResults\nAs shown in Table 2, the two main optimization techniques are UCX and spilling. We managed to train XGBoost with just four GPUs and 128GB of memory. We will also show the performance scales nicely to more GPUs.\nSpilling off\nSpilling on\nUCX off\n135s / 8GPUs / 256 GB\n270s / 4GPUs / 128 GB\nUCX on\n80s / 8GPUs / 256 GB\n217s / 4GPUs / 128 GB\nTable 2. Overview of four combinations of optimizations In each cell, the numbers represent end-to-end execution time, the minimum number of GPUs required, and the total GPU memory available. All four demos accomplish the same task of loading and training 110 GB of Otto data.\nSummary\nIn conclusion, leveraging Dask and XGBoost with multiple GPUs can be an exciting adventure, despite the occasional bumps like out of memory errors.\nYou can mitigate these memory challenges and tap into the potential of multi-GPU model training by:\nCarefully configuring parameters such as row group size in the input parquet files\nEnsuring the correct installation of RAPIDS and XGBoost\nUtilizing Dask Quantile DMatrix\nEnabling spilling\nFurthermore, by applying advanced features such as UCX-Py, you can significantly speed up training times.\nimg\nSign up for the latest Data Science news. Get the latest announcements, notebooks, hands-on tutorials, events, and more in your inbox once a month from NVIDIA."}], "https://developer.nvidia.com/blog/supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions/": [{"text": "Ransomware attacks are increasingly sophisticated and difficult to detect, with some attacks going undetected for over 300 days. To combat this, AI-enhanced cybersecurity solutions using technologies like NVIDIA DPUs and GPUs are crucial. BlueField DPUs provide intrusion detection and host-based protection, while the Morpheus AI framework accelerates AI pipelines for identifying security threats. The AI models in Morpheus, such as the ransomware detection pipeline, leverage DOCA App-Shield data sources to detect attacks in real-time. The collaboration between BlueField DPUs and Morpheus was demonstrated by FinSec Innovation Lab, who detected a ransomware attack in under 12 seconds and saved 80% of data on infected servers. By leveraging these advanced technologies, developers can build faster and more effective ransomware detection solutions to enhance cybersecurity.", "text_components": ["Supercharge Ransomware Detection with AI-Enhanced Cybersecurity Solutions\nRansomware attacks have become increasingly popular, more sophisticated, and harder to detect. For example, in 2022, a destructive ransomware attack took 233 days to identify and 91 days to contain, for a total lifecycle of 324 days. Going undetected for this amount of time can cause irreversible damage. Faster and smarter detection capabilities are critical to addressing these attacks.\nBehavioral ransomware detection with NVIDIA DPUs and GPUs\nAdversaries and malware are evolving faster than defenders, making it hard for security teams to track changes and maintain signatures for known threats. To address this, a combination of AI and advanced security monitoring is needed. Developers can build solutions for detecting ransomware attacks faster using advanced technologies including NVIDIA BlueField Data Processing Units (DPUs), the NVIDIA DOCA SDK with DOCA App Shield, and NVIDIA Morpheus cybersecurity AI framework.", "Intrusion detection with BlueField DPU\nBlueField DPUs are ideal for enabling best-in-class, zero-trust security, and extending that security to include host-based protection. With built-in isolation, this creates a separate trust domain from the host system, where intrusion detection system (IDS) security agents are deployed. If a host is compromised, the isolation layer between the security control agents on the DPU and the host prevents the attack from spreading throughout the data center.\nDOCA App-Shield is one of the libraries provided with the NVIDIA DOCA software framework. It is a security framework for host monitoring, enabling cybersecurity vendors to create IDS solutions that can quickly identify an attack on any physical server or virtual machine.\nDOCA App-Shield runs on the NVIDIA DPU as an out-of-band (OOB) device in a separate domain from the host CPU and OS and is:\nResilient against attacks on a host machine.\nLeast disruptive to the execution of host applications.\nDOCA App Shield exposes an API to users developing security applications. For detecting malicious activities from the DPU Arm processor, it uses DMA without involving the host OS or CPU. In contrast, a standard agent of anti-virus or endpoint-detection-response runs on the host and can be seen or\u200c compromised by an attacker or malware.\nImage of an NVIDIA BlueField-3 DPU.\nFigure 1. NVIDIA BlueField-3 DPU 400 Gb/s infrastructure compute platform", "Morpheus AI framework for cybersecurity\nMorpheus is part of the NVIDIA AI Enterprise software product family and is designed to build complex ML and AI-based pipelines. It provides significant acceleration of AI pipelines to deal with high data volumes, classify data, and identify anomalies, vulnerabilities, phishing, compromised machines, and many other security issues.\nMorpheus can be deployed on-premise with a GPU-accelerated server like the NVIDIA EGX Enterprise Platform, and it is also accessible through cloud deployment.\nA workflow showing Morpheus consisting of a GPU-accelerated server with SmartNic/DPU and software stack of RAPIDS, Cyber Logs Accelerator, NVIDIA Triton, and NVIDIA TensorRT for real-time telemetry from BlueField DPUs.\nFigure 2. NVIDIA Morpheus with BlueField DPU Telemetry", "Addressing ransomware with AI\nOne of the pretrained AI models in Morpheus is the ransomware detection pipeline that leverages NVIDIA DOCA App-Shield as a data source. This brings a new level of security for detecting ransomware attacks that were previously impossible to detect in real time.\nRansomware detection AI pipeline showing a DPU monitoring virtual machines. The Morpheus AI server receives DOCA AppShield events and alerts high anomaly processes.\nFigure 3. Ransomware detection AI pipeline", "Inside BlueField DPU\nBlueField DPU offers the new OS-Inspector app to leverage DOCA App-Shield host monitoring capabilities and enables a constant collection of OS attributes from the monitored host or virtual machine. OS-Inspector app is now available through early access. Contact us for more information.\nThe collected operating system attributes include processes, threads, libraries, handles, and vads (for a complete API list, see the App-Shield programming guide ).\nOS-Inspector App then uses DOCA Telemetry Service to stream the attributes to the Morpheus inference server using the Kafka event streaming platform.", "Inside the Morpheus Inference Framework\nThe Morpheus ransomware detection AI pipeline processes the data using GPU acceleration and feeds the data to the ransomware detection AI model.\nThis tree-based model detects ransomware attacks based on suspicious attributes in the servers. It uses N-gram features to capture the change in attributes through time and detect any suspicious anomaly.\nWhen an attack is detected, Morpheus generates an inference event and triggers a real-time alert to the security team for further mitigation steps.\nA ransomware detection model detects a ransomware process named sample.exe.\nFigure 4. Ransomware detection model", "FinSec lab use case\nNVIDIA partner FinSec Innovation Lab, a joint venture between Mastercard and Enel X, demonstrated their solution for combating ransomware attacks at NVIDIA GTC 2023.\nFinSec ran a POC, which used BlueField DPUs and the Morpheus cybersecurity AI framework to train a model that detected a ransomware attack in less than 12 seconds. This real-time response enabled them to isolate a virtual machine and save 80% of the data on the infected servers.", "Learn more\nBlueField DPU running DOCA App Shield enables OOB host monitoring. Together with Morpheus, developers can quickly build AI models to protect against cyber attacks, better than ever before. OS-Inspector app is now available through early access. Contact us for more information."], "document_title": "Supercharge Ransomware Detection with AI-Enhanced Cybersecurity Solutions", "document_url": "https://developer.nvidia.com/blog/supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions/", "document_date": "2023-09-06T19:08:55", "document_date_modified": "2023-09-07T18:33:23", "document_full_text": "Supercharge Ransomware Detection with AI-Enhanced Cybersecurity Solutions\nRansomware attacks have become increasingly popular, more sophisticated, and harder to detect. For example, in 2022, a destructive ransomware attack took 233 days to identify and 91 days to contain, for a total lifecycle of 324 days. Going undetected for this amount of time can cause irreversible damage. Faster and smarter detection capabilities are critical to addressing these attacks.\nBehavioral ransomware detection with NVIDIA DPUs and GPUs\nAdversaries and malware are evolving faster than defenders, making it hard for security teams to track changes and maintain signatures for known threats. To address this, a combination of AI and advanced security monitoring is needed. Developers can build solutions for detecting ransomware attacks faster using advanced technologies including NVIDIA BlueField Data Processing Units (DPUs), the NVIDIA DOCA SDK with DOCA App Shield, and NVIDIA Morpheus cybersecurity AI framework.\nIntrusion detection with BlueField DPU\nBlueField DPUs are ideal for enabling best-in-class, zero-trust security, and extending that security to include host-based protection. With built-in isolation, this creates a separate trust domain from the host system, where intrusion detection system (IDS) security agents are deployed. If a host is compromised, the isolation layer between the security control agents on the DPU and the host prevents the attack from spreading throughout the data center.\nDOCA App-Shield is one of the libraries provided with the NVIDIA DOCA software framework. It is a security framework for host monitoring, enabling cybersecurity vendors to create IDS solutions that can quickly identify an attack on any physical server or virtual machine.\nDOCA App-Shield runs on the NVIDIA DPU as an out-of-band (OOB) device in a separate domain from the host CPU and OS and is:\nResilient against attacks on a host machine.\nLeast disruptive to the execution of host applications.\nDOCA App Shield exposes an API to users developing security applications. For detecting malicious activities from the DPU Arm processor, it uses DMA without involving the host OS or CPU. In contrast, a standard agent of anti-virus or endpoint-detection-response runs on the host and can be seen or\u200c compromised by an attacker or malware.\nImage of an NVIDIA BlueField-3 DPU.\nFigure 1. NVIDIA BlueField-3 DPU 400 Gb/s infrastructure compute platform\nMorpheus AI framework for cybersecurity\nMorpheus is part of the NVIDIA AI Enterprise software product family and is designed to build complex ML and AI-based pipelines. It provides significant acceleration of AI pipelines to deal with high data volumes, classify data, and identify anomalies, vulnerabilities, phishing, compromised machines, and many other security issues.\nMorpheus can be deployed on-premise with a GPU-accelerated server like the NVIDIA EGX Enterprise Platform, and it is also accessible through cloud deployment.\nA workflow showing Morpheus consisting of a GPU-accelerated server with SmartNic/DPU and software stack of RAPIDS, Cyber Logs Accelerator, NVIDIA Triton, and NVIDIA TensorRT for real-time telemetry from BlueField DPUs.\nFigure 2. NVIDIA Morpheus with BlueField DPU Telemetry\nAddressing ransomware with AI\nOne of the pretrained AI models in Morpheus is the ransomware detection pipeline that leverages NVIDIA DOCA App-Shield as a data source. This brings a new level of security for detecting ransomware attacks that were previously impossible to detect in real time.\nRansomware detection AI pipeline showing a DPU monitoring virtual machines. The Morpheus AI server receives DOCA AppShield events and alerts high anomaly processes.\nFigure 3. Ransomware detection AI pipeline\nInside BlueField DPU\nBlueField DPU offers the new OS-Inspector app to leverage DOCA App-Shield host monitoring capabilities and enables a constant collection of OS attributes from the monitored host or virtual machine. OS-Inspector app is now available through early access. Contact us for more information.\nThe collected operating system attributes include processes, threads, libraries, handles, and vads (for a complete API list, see the App-Shield programming guide ).\nOS-Inspector App then uses DOCA Telemetry Service to stream the attributes to the Morpheus inference server using the Kafka event streaming platform.\nInside the Morpheus Inference Framework\nThe Morpheus ransomware detection AI pipeline processes the data using GPU acceleration and feeds the data to the ransomware detection AI model.\nThis tree-based model detects ransomware attacks based on suspicious attributes in the servers. It uses N-gram features to capture the change in attributes through time and detect any suspicious anomaly.\nWhen an attack is detected, Morpheus generates an inference event and triggers a real-time alert to the security team for further mitigation steps.\nA ransomware detection model detects a ransomware process named sample.exe.\nFigure 4. Ransomware detection model\nFinSec lab use case\nNVIDIA partner FinSec Innovation Lab, a joint venture between Mastercard and Enel X, demonstrated their solution for combating ransomware attacks at NVIDIA GTC 2023.\nFinSec ran a POC, which used BlueField DPUs and the Morpheus cybersecurity AI framework to train a model that detected a ransomware attack in less than 12 seconds. This real-time response enabled them to isolate a virtual machine and save 80% of the data on the infected servers.\nLearn more\nBlueField DPU running DOCA App Shield enables OOB host monitoring. Together with Morpheus, developers can quickly build AI models to protect against cyber attacks, better than ever before. OS-Inspector app is now available through early access. Contact us for more information."}], "https://developer.nvidia.com/blog/gpus-for-etl-optimizing-etl-architecture-for-apache-spark-sql-operations/": [{"text": "The article explores the use of GPUs for ETL operations in Apache Spark SQL, specifically focusing on the NVIDIA RAPIDS Accelerator. The experiment involved creating large, complex datasets to test the performance and cost savings of GPUs compared to CPUs for various Spark SQL operations such as aggregation, cross join, and union. The results showed that GPUs are well suited for highly parallelizable operations like cross join, providing significant time and cost savings. However, for operations like union, the difference between GPUs and CPUs was negligible. The decision to migrate ETL operations to GPUs should be based on factors such as data structure, scale, and technical depth. Overall, GPUs are recommended for large, complex datasets and operations that can benefit from parallelization, while CPUs may be more suitable for simpler, in-memory calculations. The experiment highlights the potential benefits of using GPUs for ETL operations and provides guidance on how to implement RAPIDS Accelerator for Apache Spark.", "text_components": ["GPUs for ETL? Optimizing ETL Architecture for Apache Spark SQL Operations\nExtract-transform-load (ETL) operations with GPUs using the NVIDIA RAPIDS Accelerator for Apache Spark running on large-scale data can produce both cost savings and performance gains. We demonstrated this in our previous post, GPUs for ETL? Run Faster, Less Costly Workloads with NVIDIA RAPIDS Accelerator for Apache Spark and Databricks. In this post, we dive deeper to identify precisely which Apache Spark SQL operations are accelerated for a given processing architecture.\nThis post is part of a series on GPUs and extract-transform-load (ETL) operations.", "Migrating ETL to GPUs\nShould all ETL be migrated to GPUs? Or is there an advantage to evaluating which processing architecture is best suited to specific Spark SQL operations?\nCPUs are optimized for sequential processing with significantly fewer yet faster individual cores. There are clear computational advantages for memory management, handling I/O operations, running operating systems, and so on.\nGPUs are optimized for parallel processing with significantly more yet slower cores. GPUs excel at rendering graphics, training, machine learning and deep learning models, performing matrix calculations, and other operations that benefit from parallelization.", "Experimental design\nWe created three large, complex datasets modeled after real client retail sales data using computationally expensive ETL operations:\nAggregation (SUM + GROUP BY)\nCROSS JOIN\nUNION\nEach dataset was specifically curated to test the limits and value of specific Spark SQL operations. All three datasets were modeled based on a transactional sales dataset from a global retailer. The row size, column count, and type were selected to balance experimental processing costs while performing tests that would demonstrate and evaluate the benefits of both CPU and GPU architectures under specific operating conditions. See Table 1 for data profiles.\nOperation\nRows\n# COLUMNS: Structured data\n# COLUMNS: Unstructured data\nSize (MB)\nAggregation (SUM + GROUP BY)\n94.4 million\n2\n0\n3,200\nCROSS JOIN\n63 billion\n6\n1\n983\nUNION\n447 million\n10\n2\n721\nTable 1. Summary of experimental datasets The following computational configurations were evaluated for this experiment:\nWorker and driver type\nWorkers [minimum and maximum]\nRAPIDS or Photon deployment\nMaximal hourly limits on Databricks units (DBUs)\u2014a proprietary measure of Databricks compute cost\nWorker and driver type\nWorkers [min/max]\nRAPIDS Accelerator / PHOTON\nMax DBUs / hour\nStandard_NC4as_T4_v3\n1/1\nRAPIDS Accelerator\n2\nStandard_NC4as_T4_v3\n2/8\nRAPIDS Accelerator\n9\nStandard_NC8as_T4_v3\n2/2\nRAPIDS Accelerator\n4.5\nStandard_NC8as_T4_v3\n2/8\nRAPIDS Accelerator\n14\nStandard_NC16as_T4_v3\n2/2\nRAPIDS Accelerator\n7.5\nStandard_NC16as_T4_v3\n2/8\nRAPIDS Accelerator\n23\nStandard_E16_v3\n2/2\nPhoton\n24\nStandard_E16_v3\n2/8\nPhoton\n72\nTable 2. Experimental computational configurations", "Other experimental considerations\nIn addition to building industry-representative test datasets, other experimental factors are listed below.\nDatasets are run using several different worker and driver configurations on pay-as-you-go instances\u2013as opposed to spot instances\u2013as their inherent availability establishes pricing consistency across experiments.\nFor GPU testing, we leveraged RAPIDS Accelerator on T4 GPUs, which are optimized for analytics-heavy loads, and carry a substantially lower cost per DBU.\nThe CPU worker type is an in-memory optimized architecture which uses Intel Xeon Platinum 8370C (Ice Lake) CPUs.\nWe also leveraged Databricks Photon, a native CPU accelerator solution and accelerated version of their traditional Java runtime, rewritten in C++.\nThese parameters were chosen to ensure experimental repeatability and applicability to common use cases.", "Results\nTo evaluate experimental results in a consistent fashion, we developed a composite metric named adjusted DBUs per minute (ADBUs). ADBUs are based on DBUs and computed as follows:\n\\text{\\emph{Adjusted DBUs per Minute}} = \\frac{\\text{\\emph{Runtime (mins)}}}{\\text{\\emph{Cluster DBUs Cost per Hour}}}\nExperimental results demonstrate that there is no computational Spark SQL task in which one chipset\u2013GPU or CPU\u2013dominates. As Figure 1 shows, dataset characteristics and the suitability of a cluster configuration have the strongest impact on which framework to choose for a specific task. Although unsurprising, the question remains: which ETL processes should be migrated to GPUs?", "UNION operations\nAlthough RAPIDS Accelerator on T4 GPUs generate results having both lower costs and execution times with UNION operations, the difference when compared with CPUs is negligible. Moving an existing ETL pipeline from CPUs to GPUs seems unwarranted for this combination of dataset and Spark SQL operation. It is likely\u2013albeit untested by this research\u2013that a larger dataset may generate results that warrant a move to GPUs.", "CROSS JOIN operations\nFor the compute-heavy CROSS JOIN operation, we observed an order of magnitude of both time and cost savings by employing RAPIDS Accelerator (GPUs) over Photon (CPUs).\nOne possible explanation for these performance gains is that the CROSS JOIN is a Cartesian product that involves an unstructured data column being multiplied with itself. This leads to exponentially increasing complexity. The performance gains of GPUs are well suited for this type of large-scale parallelizable operation.\nThe main driver of cost differences is that the CPU clusters we experimented with had a much higher DBU rating than the chosen GPU clusters.", "SUM + GROUP BY operations\nFor aggregation operations (SUM + GROUP BY), we observed mixed results. Photon (CPUs) delivered notably faster compute times, whereas RAPIDS Accelerator (GPUs) provided lower overall costs. Looking at individual experimental runs, we observed that the higher Photon costs result in higher DBUs, whereas the costs associated with T4s are significantly lower.\nThis explains the lower overall cost using RAPIDS Accelerator in this part of the experiment. In summary, if speed is the objective, Photon is the clear winner. More price-conscious users may prefer the longer compute times of RAPIDS Accelerator for notable cost savings.\nBar graphs showing the trade-off between compute time and cost for UNION, CROSS JOIN, and SUM + GROUP operations in Spark SQL for both Photon and RAPIDS Accelerator\nFigure 1. Comparison of mean compute time and mean cost", "Deciding which architecture to use\nThe CPU cluster gained performance in execution time in the commonly used aggregation (SUM + GROUP BY) experiment. However, this came at the price of higher associated cluster costs. For CROSS JOINs, a less common high-compute and highly-parallelizable operation, GPUs dominated both in higher speed and lower costs. UNIONs showed negligible comparative differences in compute time and cost.\nWhere GPUs (and by association RAPIDS Accelerator) will excel depends largely on the data structure, the scale of the data, the ETL operation(s) performed, and the user\u2019s technical depth.", "GPUs for ETL\nIn general, GPUs are well suited to large, complex datasets and Spark SQL operations that are highly parallelizable. The experimental results suggest using GPUs for CROSS JOIN situations, as they are amenable to parallelization, and can also scale easily as data grows in size and complexity.\nIt is important to note the scale of data is less important than the complexity of the data and the selected operation, as shown in the SUM + GROUP BY experiment. (This experiment involved more data, but less computational complexity compared to CROSS JOINs.) You can work with NVIDIA free of charge to estimate expected GPU acceleration gains based on analyses of Spark log files.", "CPUs for ETL\nBased on the experiments, certain Spark SQL operations such as UNIONs showed a negligible difference in cost and compute time. A shift to GPUs may not be warranted in this case. Moreover, for aggregations (SUM + GROUP BY), a conscious choice of speed over cost can be made based on situational requirements, where CPUs will execute faster, but at a higher cost.\nIn cases where in-memory calculations are straightforward, staying with an established CPU ETL architecture may be ideal.", "Discussion and future considerations\nThis experiment explored one-step Spark SQL operations. For example, a singular CROSS JOIN, or a singular UNION, omitting more complex ETL jobs that involve multiple steps. An interesting future experiment might include optimizing ETL processing at a granular level, sending individual SparkSQL operations to CPUs or GPUs in a single job or script, and optimizing for both time and compute cost.\nA savvy Spark user might try to focus on implementing scripting strategies to make the most of the default runtime, rather than implementing a more efficient paradigm. Examples include:\nSpark SQL join strategies (broadcast join, shuffle merge, hash join, and so on)\nHigh-performing data structures (storing data in parquet files that are highly performant in a cloud architecture as compared to text files, for example)\nStrategic data caching for reuse\nThe results of our experiment indicate that leveraging GPUs for ETL can supply additional performance sufficient to warrant the effort to implement a GPU architecture.\nAlthough supported, RAPIDS Accelerator for Apache Spark is not available by default on Azure Databricks. This requires the installation of .jar files that may necessitate some debugging. This tech debt was largely paid going forward, as subsequent uses of RAPIDS Accelerator were seamless and straightforward. NVIDIA support was always readily available to help if and when necessary.\nFinally, we opted to keep all created clusters under 100 DBUs per hour to manage experimental costs. We tried only one size of Photon cluster. Experimental results may change by varying the cluster size, number of workers, and other experimental parameters. We feel these results are sufficiently robust and relevant for many typical use cases in organizations running ETL jobs.", "Conclusion\nNVIDIA T4 GPUs, designed specifically for analytics workloads, accomplish a leap in the price/performance ratio associated with leveraging GPU-based compute. NVIDIA RAPIDS Accelerator for Apache Spark, especially when run on NVIDIA T4 GPUs, has the potential to significantly reduce costs and execution times for certain common ETL SparkSQL operations, particularly those that are highly parallelizable.\nTo implement this solution on your own Apache Spark workload with no code changes, visit the NVIDIA/spark-rapids-examples GitHub repo or the Apache Spark tool page for sample code and applications that showcase the performance and benefits of using RAPIDS Accelerator in your data processing or machine learning pipelines."], "document_title": "GPUs for ETL? Optimizing ETL Architecture for Apache Spark SQL Operations", "document_url": "https://developer.nvidia.com/blog/gpus-for-etl-optimizing-etl-architecture-for-apache-spark-sql-operations/", "document_date": "2023-09-06T16:53:28", "document_date_modified": "2023-11-10T01:26:53", "document_full_text": "GPUs for ETL? Optimizing ETL Architecture for Apache Spark SQL Operations\nExtract-transform-load (ETL) operations with GPUs using the NVIDIA RAPIDS Accelerator for Apache Spark running on large-scale data can produce both cost savings and performance gains. We demonstrated this in our previous post, GPUs for ETL? Run Faster, Less Costly Workloads with NVIDIA RAPIDS Accelerator for Apache Spark and Databricks. In this post, we dive deeper to identify precisely which Apache Spark SQL operations are accelerated for a given processing architecture.\nThis post is part of a series on GPUs and extract-transform-load (ETL) operations.\nMigrating ETL to GPUs\nShould all ETL be migrated to GPUs? Or is there an advantage to evaluating which processing architecture is best suited to specific Spark SQL operations?\nCPUs are optimized for sequential processing with significantly fewer yet faster individual cores. There are clear computational advantages for memory management, handling I/O operations, running operating systems, and so on.\nGPUs are optimized for parallel processing with significantly more yet slower cores. GPUs excel at rendering graphics, training, machine learning and deep learning models, performing matrix calculations, and other operations that benefit from parallelization.\nExperimental design\nWe created three large, complex datasets modeled after real client retail sales data using computationally expensive ETL operations:\nAggregation (SUM + GROUP BY)\nCROSS JOIN\nUNION\nEach dataset was specifically curated to test the limits and value of specific Spark SQL operations. All three datasets were modeled based on a transactional sales dataset from a global retailer. The row size, column count, and type were selected to balance experimental processing costs while performing tests that would demonstrate and evaluate the benefits of both CPU and GPU architectures under specific operating conditions. See Table 1 for data profiles.\nOperation\nRows\n# COLUMNS: Structured data\n# COLUMNS: Unstructured data\nSize (MB)\nAggregation (SUM + GROUP BY)\n94.4 million\n2\n0\n3,200\nCROSS JOIN\n63 billion\n6\n1\n983\nUNION\n447 million\n10\n2\n721\nTable 1. Summary of experimental datasets The following computational configurations were evaluated for this experiment:\nWorker and driver type\nWorkers [minimum and maximum]\nRAPIDS or Photon deployment\nMaximal hourly limits on Databricks units (DBUs)\u2014a proprietary measure of Databricks compute cost\nWorker and driver type\nWorkers [min/max]\nRAPIDS Accelerator / PHOTON\nMax DBUs / hour\nStandard_NC4as_T4_v3\n1/1\nRAPIDS Accelerator\n2\nStandard_NC4as_T4_v3\n2/8\nRAPIDS Accelerator\n9\nStandard_NC8as_T4_v3\n2/2\nRAPIDS Accelerator\n4.5\nStandard_NC8as_T4_v3\n2/8\nRAPIDS Accelerator\n14\nStandard_NC16as_T4_v3\n2/2\nRAPIDS Accelerator\n7.5\nStandard_NC16as_T4_v3\n2/8\nRAPIDS Accelerator\n23\nStandard_E16_v3\n2/2\nPhoton\n24\nStandard_E16_v3\n2/8\nPhoton\n72\nTable 2. Experimental computational configurations\nOther experimental considerations\nIn addition to building industry-representative test datasets, other experimental factors are listed below.\nDatasets are run using several different worker and driver configurations on pay-as-you-go instances\u2013as opposed to spot instances\u2013as their inherent availability establishes pricing consistency across experiments.\nFor GPU testing, we leveraged RAPIDS Accelerator on T4 GPUs, which are optimized for analytics-heavy loads, and carry a substantially lower cost per DBU.\nThe CPU worker type is an in-memory optimized architecture which uses Intel Xeon Platinum 8370C (Ice Lake) CPUs.\nWe also leveraged Databricks Photon, a native CPU accelerator solution and accelerated version of their traditional Java runtime, rewritten in C++.\nThese parameters were chosen to ensure experimental repeatability and applicability to common use cases.\nResults\nTo evaluate experimental results in a consistent fashion, we developed a composite metric named adjusted DBUs per minute (ADBUs). ADBUs are based on DBUs and computed as follows:\n\\text{\\emph{Adjusted DBUs per Minute}} = \\frac{\\text{\\emph{Runtime (mins)}}}{\\text{\\emph{Cluster DBUs Cost per Hour}}}\nExperimental results demonstrate that there is no computational Spark SQL task in which one chipset\u2013GPU or CPU\u2013dominates. As Figure 1 shows, dataset characteristics and the suitability of a cluster configuration have the strongest impact on which framework to choose for a specific task. Although unsurprising, the question remains: which ETL processes should be migrated to GPUs?\nUNION operations\nAlthough RAPIDS Accelerator on T4 GPUs generate results having both lower costs and execution times with UNION operations, the difference when compared with CPUs is negligible. Moving an existing ETL pipeline from CPUs to GPUs seems unwarranted for this combination of dataset and Spark SQL operation. It is likely\u2013albeit untested by this research\u2013that a larger dataset may generate results that warrant a move to GPUs.\nCROSS JOIN operations\nFor the compute-heavy CROSS JOIN operation, we observed an order of magnitude of both time and cost savings by employing RAPIDS Accelerator (GPUs) over Photon (CPUs).\nOne possible explanation for these performance gains is that the CROSS JOIN is a Cartesian product that involves an unstructured data column being multiplied with itself. This leads to exponentially increasing complexity. The performance gains of GPUs are well suited for this type of large-scale parallelizable operation.\nThe main driver of cost differences is that the CPU clusters we experimented with had a much higher DBU rating than the chosen GPU clusters.\nSUM + GROUP BY operations\nFor aggregation operations (SUM + GROUP BY), we observed mixed results. Photon (CPUs) delivered notably faster compute times, whereas RAPIDS Accelerator (GPUs) provided lower overall costs. Looking at individual experimental runs, we observed that the higher Photon costs result in higher DBUs, whereas the costs associated with T4s are significantly lower.\nThis explains the lower overall cost using RAPIDS Accelerator in this part of the experiment. In summary, if speed is the objective, Photon is the clear winner. More price-conscious users may prefer the longer compute times of RAPIDS Accelerator for notable cost savings.\nBar graphs showing the trade-off between compute time and cost for UNION, CROSS JOIN, and SUM + GROUP operations in Spark SQL for both Photon and RAPIDS Accelerator\nFigure 1. Comparison of mean compute time and mean cost\nDeciding which architecture to use\nThe CPU cluster gained performance in execution time in the commonly used aggregation (SUM + GROUP BY) experiment. However, this came at the price of higher associated cluster costs. For CROSS JOINs, a less common high-compute and highly-parallelizable operation, GPUs dominated both in higher speed and lower costs. UNIONs showed negligible comparative differences in compute time and cost.\nWhere GPUs (and by association RAPIDS Accelerator) will excel depends largely on the data structure, the scale of the data, the ETL operation(s) performed, and the user\u2019s technical depth.\nGPUs for ETL\nIn general, GPUs are well suited to large, complex datasets and Spark SQL operations that are highly parallelizable. The experimental results suggest using GPUs for CROSS JOIN situations, as they are amenable to parallelization, and can also scale easily as data grows in size and complexity.\nIt is important to note the scale of data is less important than the complexity of the data and the selected operation, as shown in the SUM + GROUP BY experiment. (This experiment involved more data, but less computational complexity compared to CROSS JOINs.) You can work with NVIDIA free of charge to estimate expected GPU acceleration gains based on analyses of Spark log files.\nCPUs for ETL\nBased on the experiments, certain Spark SQL operations such as UNIONs showed a negligible difference in cost and compute time. A shift to GPUs may not be warranted in this case. Moreover, for aggregations (SUM + GROUP BY), a conscious choice of speed over cost can be made based on situational requirements, where CPUs will execute faster, but at a higher cost.\nIn cases where in-memory calculations are straightforward, staying with an established CPU ETL architecture may be ideal.\nDiscussion and future considerations\nThis experiment explored one-step Spark SQL operations. For example, a singular CROSS JOIN, or a singular UNION, omitting more complex ETL jobs that involve multiple steps. An interesting future experiment might include optimizing ETL processing at a granular level, sending individual SparkSQL operations to CPUs or GPUs in a single job or script, and optimizing for both time and compute cost.\nA savvy Spark user might try to focus on implementing scripting strategies to make the most of the default runtime, rather than implementing a more efficient paradigm. Examples include:\nSpark SQL join strategies (broadcast join, shuffle merge, hash join, and so on)\nHigh-performing data structures (storing data in parquet files that are highly performant in a cloud architecture as compared to text files, for example)\nStrategic data caching for reuse\nThe results of our experiment indicate that leveraging GPUs for ETL can supply additional performance sufficient to warrant the effort to implement a GPU architecture.\nAlthough supported, RAPIDS Accelerator for Apache Spark is not available by default on Azure Databricks. This requires the installation of .jar files that may necessitate some debugging. This tech debt was largely paid going forward, as subsequent uses of RAPIDS Accelerator were seamless and straightforward. NVIDIA support was always readily available to help if and when necessary.\nFinally, we opted to keep all created clusters under 100 DBUs per hour to manage experimental costs. We tried only one size of Photon cluster. Experimental results may change by varying the cluster size, number of workers, and other experimental parameters. We feel these results are sufficiently robust and relevant for many typical use cases in organizations running ETL jobs.\nConclusion\nNVIDIA T4 GPUs, designed specifically for analytics workloads, accomplish a leap in the price/performance ratio associated with leveraging GPU-based compute. NVIDIA RAPIDS Accelerator for Apache Spark, especially when run on NVIDIA T4 GPUs, has the potential to significantly reduce costs and execution times for certain common ETL SparkSQL operations, particularly those that are highly parallelizable.\nTo implement this solution on your own Apache Spark workload with no code changes, visit the NVIDIA/spark-rapids-examples GitHub repo or the Apache Spark tool page for sample code and applications that showcase the performance and benefits of using RAPIDS Accelerator in your data processing or machine learning pipelines."}], "https://developer.nvidia.com/blog/advanced-api-performance-shaders/": [{"text": "The article discusses best practices for working with shaders on NVIDIA GPUs to optimize API performance. Shaders are essential in graphics programming for manipulating vertices, pixels, and data on the GPU. Tips are provided for general shaders, compute shaders, pixel shaders, vertex shaders, and geometry, domain, and hull shaders. Recommendations include avoiding warp-divergent constant buffer reads, optimizing control flow, using Root Signature 1.1, preferring structured buffers over raw buffers, and using point filtering in certain circumstances. For compute shaders, suggestions include using wave intrinsics for communication across threads and optimizing thread group size. In pixel shaders, it is recommended to use depth bounds test or stencil and depth testing over manual depth tests and to be mindful of patterns that may disable Early-Z testing. Vertex shaders should use compressed vertex formats and SRVs for skinning data. Additionally, the article recommends replacing geometry, domain, and hull shaders with mesh shading capabilities introduced in NVIDIA Turing for dynamic generation of surfaces and objects. The article acknowledges the contributions of Ryan Prescott, Ana Mihut, Katherine Sun, and Ivan Fedorov.", "text_components": ["Advanced API Performance: Shaders\nThis post covers best practices when working with shaders on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.\nShaders play a critical role in graphics programming by enabling you to control various aspects of the rendering process. They run on the GPU and are responsible for manipulating vertices, pixels, and other data.\nGeneral shaders\nCompute shaders\nPixel shaders\nVertex shaders\nGeometry, domain, and hull shaders", "General shaders\nThese tips apply to all types of shaders.", "Recommended\nAvoid warp-divergent constant buffer view (CBV) and immediate constant buffer (ICB) reads.\nConstant buffer reads are most effective when threads in a warp access data uniformly. If you need divergent reads, use shader resource view (SRVs).\nTypical cases where SRVs should be preferred over CBVs include the following:\nBones or skinning data\nLookup tables, like precomputed random numbers\nTo optimize buffers and group shared memory, use manual bit packing. When creating structures for packing data, consider the range of values a field can hold and choose the smallest datatype that can encompass this range.\nOptimize control flow by providing hints of the expected runtime behavior.\nMake sure to enable compile flag -all-resources-bound for DXC (or D3DCOMPILE_ALL_RESOURCES_BOUND in FXC ) if possible. This enables a larger set of driver-side optimizations.\nConsider using the [FLATTEN] and [BRANCH] keywords where appropriate.\nA conditional branch may prevent the compiler from hoisting long-latency instructions, such as texture fetches.\nThe [FLATTEN] keyword hints that the compiler is free to hoist and start the load operations before the statement has been evaluated.\nUse Root Signature 1.1 to specify static data and descriptors to enable the driver to make the most optimal shader optimizations.\nKeep the register use to a minimum. Register allocation could limit occupancy and may force the driver to spill registers to memory.\nPrefer the use of gather instructions when loading single channel texture quads.\nThis will cut down the expected latency by almost 4x compared to the equivalent operation constructed from consecutive sample instructions.\nPrefer structured buffers over raw buffers.\nStructured buffers have stricter alignment requirements, which enables the driver to schedule more efficient load instructions.\nConsider using numerical approximations or precomputed lookup tables of transcendental functions (exp, log, sin, cos, sqrt) in math-intensive shaders, for instance, physics simulations and denoisers.\nTo promote a fast path in the TEX unit, with up to 2x speedup, use point filtering in certain circumstances:\nLow-resolution textures where point filtering is already an accurate representation.\nTextures that are being accessed at their native resolution.", "Not recommended\nDon\u2019t assume that half-precision floats are always faster than full precision and the reverse.\nOn NVIDIA Ampere GPUs, it\u2019s just as efficient to execute FP32 as FP16 instructions. The overhead of converting between precision formats may just end up with a net loss.\nNVIDIA Turing GPUs may benefit from using FP16 math, as FP16 can be issued at twice the rate of FP32.", "Compute shaders\nCompute shaders are used for general-purpose computations, from data processing and simulations to machine learning.", "Recommended\nConsider using wave intrinsics over group shared memory when possible for communication across threads.\nWave intrinsics don\u2019t require explicit thread synchronization.\nStarting from SM 6.0, HLSL supports warp-wide wave intrinsics natively without the need for vendor-specific HLSL extensions. Consider using vendor-specific APIs only when the expected functionality is missing. For more information, see Unlocking GPU Intrinsics in HLSL.\nTo increase atomic throughput, use wave instructions to coalesce atomic operations across a warp.\nTo maximize cache locality and to improve L1 and L2 hit rate, try thread group ID swizzling for full-screen compute passes.\nA good starting point is to target a thread group size corresponding to between two or eight warps. For instance, thread group size 8x8x1 or 16x16x1 for full-screen passes. Make sure to profile your shader and tune the dimensions based on profiling results.", "Not recommended\nDo not make your thread group size difficult to scale per platform and GPU architecture.\nSpecialization constants can be used in Vulkan to set the dimensions at pipeline creation time whereas HLSL requires the thread group size to be known at shader compile time.\nBe careless of thread group launch latency.\nIf your CS has early-out conditions that are expected to early out in most cases, it might be better to choose larger thread group dimensions and cut down on the total number of thread groups launched.", "Pixel shaders\nPixel shaders, also known as fragment shaders, are used to calculate effects on a per-pixel basis.", "Recommended\nPrefer the use of depth bounds test or stencil and depth testing over manual depth tests in pixel shaders.\nDepth and stencil tests may discard entire 16\u00d716 raster tiles down to individual pixels. Make sure that Early-Z is enabled.\nBe mindful of the use patterns that may force the driver to disable Early-Z testing:\nConditional z-writes such as clip and discard\nAs an alternative consider using null blend ops instead\nPixel shader depth write\nWriting to UAV resources\nConsider converting your full screen pass to a compute shader if there\u2019s a large difference in latency between warps.", "Not recommended\nDon\u2019t use raster order view (ROV) techniques pervasively.\nGuaranteeing order doesn\u2019t come for free.\nAlways compare with alternative approaches like advanced blending ops and atomics.", "Vertex shaders\nVertex shaders are used to calculate effects on a per-vertex basis.", "Recommended\nPrefer the use of compressed vertex formats.\nPrefer the use of SRVs for skinning data over CBVs. This is a typical case of divergent CBV reads.", "Geometry, domain, and hull shaders\nGeometry, domain, and hull shaders are used to control, evaluate, and generate geometry, enabling tessellation to create a dynamic generation of surfaces and objects.", "Recommended\nReplace the geometry, domain, and hull shaders with the mesh shading capabilities introduced in NVIDIA Turing.\nEnable the fast geometry path with the following configuration:\nFixed topology: Neither an expansion or reduction in the number of vertices.\nFixed primitive type: The input primitive type is equal to the output primitive type.\nImmutable per-vertex attributes: The application cannot change the vertex attributes and can only copy them from the input to the output.\nMutable per-primitive attributes: The application can compute a single value for the whole primitive, which then is passed to the fragment shader stage. For example, it can compute the area of the triangle.", "Acknowledgments\nThanks to Ryan Prescott, Ana Mihut, Katherine Sun, and Ivan Fedorov."], "document_title": "Advanced API Performance: Shaders", "document_url": "https://developer.nvidia.com/blog/advanced-api-performance-shaders/", "document_date": "2023-09-01T15:36:30", "document_date_modified": "2023-10-25T23:52:32", "document_full_text": "Advanced API Performance: Shaders\nThis post covers best practices when working with shaders on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.\nShaders play a critical role in graphics programming by enabling you to control various aspects of the rendering process. They run on the GPU and are responsible for manipulating vertices, pixels, and other data.\nGeneral shaders\nCompute shaders\nPixel shaders\nVertex shaders\nGeometry, domain, and hull shaders\nGeneral shaders\nThese tips apply to all types of shaders.\nRecommended\nAvoid warp-divergent constant buffer view (CBV) and immediate constant buffer (ICB) reads.\nConstant buffer reads are most effective when threads in a warp access data uniformly. If you need divergent reads, use shader resource view (SRVs).\nTypical cases where SRVs should be preferred over CBVs include the following:\nBones or skinning data\nLookup tables, like precomputed random numbers\nTo optimize buffers and group shared memory, use manual bit packing. When creating structures for packing data, consider the range of values a field can hold and choose the smallest datatype that can encompass this range.\nOptimize control flow by providing hints of the expected runtime behavior.\nMake sure to enable compile flag -all-resources-bound for DXC (or D3DCOMPILE_ALL_RESOURCES_BOUND in FXC ) if possible. This enables a larger set of driver-side optimizations.\nConsider using the [FLATTEN] and [BRANCH] keywords where appropriate.\nA conditional branch may prevent the compiler from hoisting long-latency instructions, such as texture fetches.\nThe [FLATTEN] keyword hints that the compiler is free to hoist and start the load operations before the statement has been evaluated.\nUse Root Signature 1.1 to specify static data and descriptors to enable the driver to make the most optimal shader optimizations.\nKeep the register use to a minimum. Register allocation could limit occupancy and may force the driver to spill registers to memory.\nPrefer the use of gather instructions when loading single channel texture quads.\nThis will cut down the expected latency by almost 4x compared to the equivalent operation constructed from consecutive sample instructions.\nPrefer structured buffers over raw buffers.\nStructured buffers have stricter alignment requirements, which enables the driver to schedule more efficient load instructions.\nConsider using numerical approximations or precomputed lookup tables of transcendental functions (exp, log, sin, cos, sqrt) in math-intensive shaders, for instance, physics simulations and denoisers.\nTo promote a fast path in the TEX unit, with up to 2x speedup, use point filtering in certain circumstances:\nLow-resolution textures where point filtering is already an accurate representation.\nTextures that are being accessed at their native resolution.\nNot recommended\nDon\u2019t assume that half-precision floats are always faster than full precision and the reverse.\nOn NVIDIA Ampere GPUs, it\u2019s just as efficient to execute FP32 as FP16 instructions. The overhead of converting between precision formats may just end up with a net loss.\nNVIDIA Turing GPUs may benefit from using FP16 math, as FP16 can be issued at twice the rate of FP32.\nCompute shaders\nCompute shaders are used for general-purpose computations, from data processing and simulations to machine learning.\nRecommended\nConsider using wave intrinsics over group shared memory when possible for communication across threads.\nWave intrinsics don\u2019t require explicit thread synchronization.\nStarting from SM 6.0, HLSL supports warp-wide wave intrinsics natively without the need for vendor-specific HLSL extensions. Consider using vendor-specific APIs only when the expected functionality is missing. For more information, see Unlocking GPU Intrinsics in HLSL.\nTo increase atomic throughput, use wave instructions to coalesce atomic operations across a warp.\nTo maximize cache locality and to improve L1 and L2 hit rate, try thread group ID swizzling for full-screen compute passes.\nA good starting point is to target a thread group size corresponding to between two or eight warps. For instance, thread group size 8x8x1 or 16x16x1 for full-screen passes. Make sure to profile your shader and tune the dimensions based on profiling results.\nNot recommended\nDo not make your thread group size difficult to scale per platform and GPU architecture.\nSpecialization constants can be used in Vulkan to set the dimensions at pipeline creation time whereas HLSL requires the thread group size to be known at shader compile time.\nBe careless of thread group launch latency.\nIf your CS has early-out conditions that are expected to early out in most cases, it might be better to choose larger thread group dimensions and cut down on the total number of thread groups launched.\nPixel shaders\nPixel shaders, also known as fragment shaders, are used to calculate effects on a per-pixel basis.\nRecommended\nPrefer the use of depth bounds test or stencil and depth testing over manual depth tests in pixel shaders.\nDepth and stencil tests may discard entire 16\u00d716 raster tiles down to individual pixels. Make sure that Early-Z is enabled.\nBe mindful of the use patterns that may force the driver to disable Early-Z testing:\nConditional z-writes such as clip and discard\nAs an alternative consider using null blend ops instead\nPixel shader depth write\nWriting to UAV resources\nConsider converting your full screen pass to a compute shader if there\u2019s a large difference in latency between warps.\nNot recommended\nDon\u2019t use raster order view (ROV) techniques pervasively.\nGuaranteeing order doesn\u2019t come for free.\nAlways compare with alternative approaches like advanced blending ops and atomics.\nVertex shaders\nVertex shaders are used to calculate effects on a per-vertex basis.\nRecommended\nPrefer the use of compressed vertex formats.\nPrefer the use of SRVs for skinning data over CBVs. This is a typical case of divergent CBV reads.\nGeometry, domain, and hull shaders\nGeometry, domain, and hull shaders are used to control, evaluate, and generate geometry, enabling tessellation to create a dynamic generation of surfaces and objects.\nRecommended\nReplace the geometry, domain, and hull shaders with the mesh shading capabilities introduced in NVIDIA Turing.\nEnable the fast geometry path with the following configuration:\nFixed topology: Neither an expansion or reduction in the number of vertices.\nFixed primitive type: The input primitive type is equal to the output primitive type.\nImmutable per-vertex attributes: The application cannot change the vertex attributes and can only copy them from the input to the output.\nMutable per-primitive attributes: The application can compute a single value for the whole primitive, which then is passed to the fragment shader stage. For example, it can compute the area of the triangle.\nAcknowledgments\nThanks to Ryan Prescott, Ana Mihut, Katherine Sun, and Ivan Fedorov."}], "https://developer.nvidia.com/blog/speeding-up-text-to-speech-diffusion-models-by-distillation/": [{"text": "Three M.Sc. students from the University of Warsaw worked with engineers from NVIDIA Warsaw to reduce latency in a diffusion-based text-to-speech (TTS) model called TorToiSe. They used a combination of classifier-free guidance and progressive distillation to achieve a 5x reduction in diffusion latency without compromising speech quality. Their approach involved two knowledge distillation phases to train a student model to mimic the output of the guided diffusion model. This progressive distillation process reduced the number of inference steps from 4,000 to 31, resulting in a 5x speedup compared to the guided diffusion model. The distilled model matched the quality of the TTS model based on guided distillation. This project exemplifies collaboration between academia and industry and highlights the potential for knowledge distillation techniques in speeding up pretrained TTS models. The students' unique solution using synthetic data generation is applicable to TTS models without access to original training data. This work showcases the partnership between NVIDIA and local universities and encourages exploration of NVIDIA Academic Programs and the NeMo Framework for generative AI solutions.", "text_components": ["Speeding Up Text-To-Speech Diffusion Models by Distillation\nEvery year, as part of their coursework, students from the University of Warsaw, Poland get to work under the supervision of engineers from the NVIDIA Warsaw office on challenging problems in deep learning and accelerated computing. We present the work of three M.Sc. students\u2014Alicja Ziarko, Pawe\u0142 Pawlik, and Micha\u0142 Siennicki\u2014who managed to significantly reduce the latency in TorToiSe, a multi-stage, diffusion-based, text-to-speech (TTS) model.\nAlicja, Pawe\u0142, and Micha\u0142 first learned about the recent advancements in speech synthesis and diffusion models. They chose the combination of classifier-free guidance and progressive distillation, which performs well in computer vision, and adapted it to speech synthesis, achieving a 5x reduction in diffusion latency without a regression in speech quality. Small perceptual speech tests confirmed the results. Notably, this approach does not require costly training from scratch of the original model.", "Why speed up diffusion-based TTS?\nSince the publication of WaveNet in 2016, neural networks have become the primary models for speech synthesis. In simple applications, such as synthesis for AI-based voice assistants, synthetic voices are almost indistinguishable from human speech. Such voices can be synthesized orders of magnitudes faster than real time, for instance with the NVIDIA NeMo AI toolkit.\nHowever, achieving high expressivity or imitating a voice based on a few seconds of recorded speech (few-shot) is still considered challenging.\nDenoising Diffusion Probabilistic Models (DDPMs) emerged as a generative technique that enables the generation of images of great quality and expressivity based on input text. DDPMs can be readily applied to TTS because a frequency-based spectrogram, which graphically represents a speech signal, can be processed like an image.\nFor instance, in TorToiSe, which is a guided diffusion-based TTS model, a spectrogram is generated by combining the results of two diffusion models (Figure 1). The iterative diffusion process involves hundreds of steps to achieve a high-quality output, significantly increasing latency compared to state-of-the-art TTS methods, which severely limits its applications.\nIn Figure 1, the unconditional diffusion model iteratively refines the initial noise until a high-quality spectrogram is obtained. The second diffusion model is further conditioned on the text embeddings produced by the language model.\nDiagram shows a speech spectrogram generated by combining the results of two diffusion models. After numerous iterations, the expected speech spectrogram is obtained.\nFigure 1. Architecture of TorToiSe, a diffusion-based neural network for TTS", "Methods for speeding up diffusion\nExisting latency reduction techniques in diffusion-based TTS can be divided into training-free and training-based methods.\nTraining-free methods do not involve training the network used to generate images by reversing the diffusion process. Instead, they only focus on optimizing the multi-step diffusion process. The diffusion process can be seen as solving ODE/SDE equations, so one way to optimize it is to create a better solver like DDPM, DDIM, and DPM, which lowers the number of diffusion steps. Parallel sampling methods, such as those based on Picard iterations or Normalizing Flows, can parallelize the diffusion process to benefit from parallel computing on GPUs.\nTraining-based methods focus on optimizing the network used in the diffusion process. The network can be pruned, quantized, or sparsified, and then fine-tuned for higher accuracy. Alternatively, its neural architecture can be changed manually or automatically using NAS. Knowledge distillation techniques enable distilling the student network from the teacher network to reduce the number of steps in the diffusion process.", "Distillation in diffusion-based TTS\nAlicja, Pawe\u0142, and Micha\u0142 decided to use the distillation approach based on promising results in computer vision and its potential for an estimated 5x reduction in latency of the diffusion model at inference. They have managed to adapt progressive distillation to the diffusion part of a pretrained TorToiSe model, overcoming problems like the lack of access to the original training data.\nTheir approach consists of two knowledge distillation phases:\nMimicking the guided diffusion model output\nTraining another student model\nIn the first knowledge distillation phase (Figure 2), the student model is trained to mimic the output of the guided diffusion model at each diffusion step. This phase reduces latency by half by combining the two diffusion models into one model.\nTo address the lack of access to the original training data, text embeddings from the language model are passed through the original teacher model to generate synthetic data used in distillation. The use of synthetic data also makes the distillation process more efficient because the entire TTS, guided diffusion pipeline does not have to be invoked at each distillation step.\nDiagram shows a two-step distillation pipeline. First, the student model is trained (distilled) to mimic the output of the guided diffusion model at each diffusion step. In the second phase, the newly trained student model serves as a teacher to another student model, with a reduced number of steps, using progressive distillation.\nFigure 2. Distillation of guided diffusion-based TTS model\nIn the second progressive distillation phase (Figure 3), the newly trained student model serves as a teacher to train another student model. In this technique, the student model is trained to mimic the teacher model while reducing the number of diffusion steps by a factor of two. This process is repeated many times to further reduce the number of steps, while each time, a new student serves as the teacher for the next round of distillation.\nA progressive distillation with seven iterations reduces the number of inference steps 7^2 times, from 4,000 steps on which the model was trained to 31 steps. This reduction results in a 5x speedup compared to the guided diffusion model, excluding the text embedding calculation cost.\nDiagram shows two steps of progressive distillation. In each step, the number of steps required to transform the Gaussian noise to the output speech spectrogram is reduced by a factor of two, from 4 to 2, and then 2 to 1.\nFigure 3. Example of two iterations of progressive distillation\nThe perceptual pairwise speech test shows that the distilled model (after the second phase) matches the quality of speech produced by the TTS model based on guided distillation.\nAs an example, listen to audio samples in Table 1 generated by the progressive distillation-based TTS model. The samples match the quality of the audio samples from the guided diffusion-based TTS model. If we simply reduced the number of distillation steps to 31, instead of using progressive distillation, the quality of the generated speech deteriorates significantly.\nSpeaker\nGuided diffusion-based TTS model\n(2\u00d780 diffusion steps)\nDiffusion-based TTS after progressive distillation\n(31 diffusion steps)\nGuided diffusion-based TTS model\n(naive reduction to 31 diffusion steps)\nFemale\n1\nAudio\nAudio\nAudio\nFemale 2\nAudio\nAudio\nAudio\nFemale 3\nAudio\nAudio\nAudio\nMale 1\nAudio\nAudio\nAudio\nTable 1: Audio samples generated by diffusion-based TTS compared to the two baseline models", "Conclusion\nCollaborating with academia and assisting young students in shaping their future in science and engineering is one of the core NVIDIA values. Alicja, Pawe\u0142, and Micha\u0142\u2019s successful project exemplifies the NVIDIA Warsaw, Poland office partnership with local universities.\nThe students managed to solve the challenging problem of speeding up the pretrained, diffusion-based, text-to-speech (TTS) model. They designed and implemented a knowledge distillation-based solution in the complex field of diffusion-based TTS, achieving a 5x speedup of the diffusion process. Most notably, their unique solution based on synthetic data generation is applicable to pretrained TTS models without access to the original training data.\nWe encourage you to explore NVIDIA Academic Programs and try out the NVIDIA NeMo Framework to create complete conversational AI (TTS, ASR, or NLP/LLM) solutions for the new era of generative AI."], "document_title": "Speeding Up Text-To-Speech Diffusion Models by Distillation", "document_url": "https://developer.nvidia.com/blog/speeding-up-text-to-speech-diffusion-models-by-distillation/", "document_date": "2023-09-01T15:30:11", "document_date_modified": "2023-11-03T07:14:57", "document_full_text": "Speeding Up Text-To-Speech Diffusion Models by Distillation\nEvery year, as part of their coursework, students from the University of Warsaw, Poland get to work under the supervision of engineers from the NVIDIA Warsaw office on challenging problems in deep learning and accelerated computing. We present the work of three M.Sc. students\u2014Alicja Ziarko, Pawe\u0142 Pawlik, and Micha\u0142 Siennicki\u2014who managed to significantly reduce the latency in TorToiSe, a multi-stage, diffusion-based, text-to-speech (TTS) model.\nAlicja, Pawe\u0142, and Micha\u0142 first learned about the recent advancements in speech synthesis and diffusion models. They chose the combination of classifier-free guidance and progressive distillation, which performs well in computer vision, and adapted it to speech synthesis, achieving a 5x reduction in diffusion latency without a regression in speech quality. Small perceptual speech tests confirmed the results. Notably, this approach does not require costly training from scratch of the original model.\nWhy speed up diffusion-based TTS?\nSince the publication of WaveNet in 2016, neural networks have become the primary models for speech synthesis. In simple applications, such as synthesis for AI-based voice assistants, synthetic voices are almost indistinguishable from human speech. Such voices can be synthesized orders of magnitudes faster than real time, for instance with the NVIDIA NeMo AI toolkit.\nHowever, achieving high expressivity or imitating a voice based on a few seconds of recorded speech (few-shot) is still considered challenging.\nDenoising Diffusion Probabilistic Models (DDPMs) emerged as a generative technique that enables the generation of images of great quality and expressivity based on input text. DDPMs can be readily applied to TTS because a frequency-based spectrogram, which graphically represents a speech signal, can be processed like an image.\nFor instance, in TorToiSe, which is a guided diffusion-based TTS model, a spectrogram is generated by combining the results of two diffusion models (Figure 1). The iterative diffusion process involves hundreds of steps to achieve a high-quality output, significantly increasing latency compared to state-of-the-art TTS methods, which severely limits its applications.\nIn Figure 1, the unconditional diffusion model iteratively refines the initial noise until a high-quality spectrogram is obtained. The second diffusion model is further conditioned on the text embeddings produced by the language model.\nDiagram shows a speech spectrogram generated by combining the results of two diffusion models. After numerous iterations, the expected speech spectrogram is obtained.\nFigure 1. Architecture of TorToiSe, a diffusion-based neural network for TTS\nMethods for speeding up diffusion\nExisting latency reduction techniques in diffusion-based TTS can be divided into training-free and training-based methods.\nTraining-free methods do not involve training the network used to generate images by reversing the diffusion process. Instead, they only focus on optimizing the multi-step diffusion process. The diffusion process can be seen as solving ODE/SDE equations, so one way to optimize it is to create a better solver like DDPM, DDIM, and DPM, which lowers the number of diffusion steps. Parallel sampling methods, such as those based on Picard iterations or Normalizing Flows, can parallelize the diffusion process to benefit from parallel computing on GPUs.\nTraining-based methods focus on optimizing the network used in the diffusion process. The network can be pruned, quantized, or sparsified, and then fine-tuned for higher accuracy. Alternatively, its neural architecture can be changed manually or automatically using NAS. Knowledge distillation techniques enable distilling the student network from the teacher network to reduce the number of steps in the diffusion process.\nDistillation in diffusion-based TTS\nAlicja, Pawe\u0142, and Micha\u0142 decided to use the distillation approach based on promising results in computer vision and its potential for an estimated 5x reduction in latency of the diffusion model at inference. They have managed to adapt progressive distillation to the diffusion part of a pretrained TorToiSe model, overcoming problems like the lack of access to the original training data.\nTheir approach consists of two knowledge distillation phases:\nMimicking the guided diffusion model output\nTraining another student model\nIn the first knowledge distillation phase (Figure 2), the student model is trained to mimic the output of the guided diffusion model at each diffusion step. This phase reduces latency by half by combining the two diffusion models into one model.\nTo address the lack of access to the original training data, text embeddings from the language model are passed through the original teacher model to generate synthetic data used in distillation. The use of synthetic data also makes the distillation process more efficient because the entire TTS, guided diffusion pipeline does not have to be invoked at each distillation step.\nDiagram shows a two-step distillation pipeline. First, the student model is trained (distilled) to mimic the output of the guided diffusion model at each diffusion step. In the second phase, the newly trained student model serves as a teacher to another student model, with a reduced number of steps, using progressive distillation.\nFigure 2. Distillation of guided diffusion-based TTS model\nIn the second progressive distillation phase (Figure 3), the newly trained student model serves as a teacher to train another student model. In this technique, the student model is trained to mimic the teacher model while reducing the number of diffusion steps by a factor of two. This process is repeated many times to further reduce the number of steps, while each time, a new student serves as the teacher for the next round of distillation.\nA progressive distillation with seven iterations reduces the number of inference steps 7^2 times, from 4,000 steps on which the model was trained to 31 steps. This reduction results in a 5x speedup compared to the guided diffusion model, excluding the text embedding calculation cost.\nDiagram shows two steps of progressive distillation. In each step, the number of steps required to transform the Gaussian noise to the output speech spectrogram is reduced by a factor of two, from 4 to 2, and then 2 to 1.\nFigure 3. Example of two iterations of progressive distillation\nThe perceptual pairwise speech test shows that the distilled model (after the second phase) matches the quality of speech produced by the TTS model based on guided distillation.\nAs an example, listen to audio samples in Table 1 generated by the progressive distillation-based TTS model. The samples match the quality of the audio samples from the guided diffusion-based TTS model. If we simply reduced the number of distillation steps to 31, instead of using progressive distillation, the quality of the generated speech deteriorates significantly.\nSpeaker\nGuided diffusion-based TTS model\n(2\u00d780 diffusion steps)\nDiffusion-based TTS after progressive distillation\n(31 diffusion steps)\nGuided diffusion-based TTS model\n(naive reduction to 31 diffusion steps)\nFemale\n1\nAudio\nAudio\nAudio\nFemale 2\nAudio\nAudio\nAudio\nFemale 3\nAudio\nAudio\nAudio\nMale 1\nAudio\nAudio\nAudio\nTable 1: Audio samples generated by diffusion-based TTS compared to the two baseline models\nConclusion\nCollaborating with academia and assisting young students in shaping their future in science and engineering is one of the core NVIDIA values. Alicja, Pawe\u0142, and Micha\u0142\u2019s successful project exemplifies the NVIDIA Warsaw, Poland office partnership with local universities.\nThe students managed to solve the challenging problem of speeding up the pretrained, diffusion-based, text-to-speech (TTS) model. They designed and implemented a knowledge distillation-based solution in the complex field of diffusion-based TTS, achieving a 5x speedup of the diffusion process. Most notably, their unique solution based on synthetic data generation is applicable to pretrained TTS models without access to the original training data.\nWe encourage you to explore NVIDIA Academic Programs and try out the NVIDIA NeMo Framework to create complete conversational AI (TTS, ASR, or NLP/LLM) solutions for the new era of generative AI."}], "https://developer.nvidia.com/blog/solving-self-intersection-artifacts-in-directx-raytracing/": [{"text": "The article discusses the issue of self-intersection artifacts in DirectX Raytracing and presents a method to solve this problem by offsetting secondary rays to avoid intersections. The method involves computing spawn points for secondary rays that are safe from self-intersections without modifying traversal and intersection routines. Error bounds are calculated to account for numerical imprecision in ray tracing operations. The offset is determined based on the triangle normal to prevent self-intersections. The article also addresses offsetting for connection rays to avoid self-intersections at both ends. The method is described in detail, including computations in object-space, world-space, and camera space. It is suggested to reposition geometry in object-space or translate the scene into camera space to reduce error magnitudes. The method is robust, has minimal runtime overhead, and can be easily integrated into shading pipelines. While the implementation is in HLSL for DXR, it can be adapted to GLSL for Vulkan and CUDA for OptiX. Overall, the approach offers a practical solution to self-intersection artifacts in ray tracing.", "text_components": ["Solving Self-Intersection Artifacts in DirectX Raytracing\nRay and path tracing algorithms construct light paths by starting at the camera or the light sources and intersecting rays with the scene geometry. As objects are hit, new secondary rays are generated on these surfaces to continue the paths.\nIn theory, these secondary rays will not yield an intersection with the same triangle again, as intersections at a distance of zero are excluded by the intersection algorithm. In practice, however, the finite floating-point precision used in the actual implementation often leads to false-positive results, known as self-intersections (Figure 2). This creates artifacts, such as shadow acne, where the triangle sometimes improperly shadows itself (Figure 1).\nSelf-intersection can be avoided by explicitly excluding the same primitive from intersection using its identifier. In DirectX Raytracing (DXR) this self-intersection check would be implemented in an any-hit shader. However, forcing an any-hit invocation for all triangle hits comes at a significant performance penalty. Furthermore, this method does not deal with false positives against adjacent (near) coplanar triangles.\nThe most widespread solutions to work around the issue use various heuristics to offset the ray along either the ray direction or the normal. These methods are, however, not robust enough to handle a variety of common production content and may even require manual parameter tweaking on a per-scene basis, particularly in scenes with heavily translated, scaled or sheared instanced geometry. For more information, see Ray Tracing Gems: High-Quality and Real-Time Rendering with DXR and Other APIs.\nAlternatively, the sources of the numerical imprecision can be numerically bounded at runtime, giving robust error intervals on the intersection test. However, this comes with considerable performance overhead and requires source access to the underlying implementation of the ray/triangle intersection routine, which is not possible in a hardware-accelerated API like DXR.\nThis post describes a robust offsetting method for secondary rays spawned from triangles in DXR. The method is based on a thorough numerical analysis of the sources of the numerical imprecision. It involves computing spawn points for secondary rays, safe from self-intersections. The method does not require modification of the traversal and ray/triangle intersection routines and can thus be used with closed-source and hardware-accelerated ray tracing APIs like DXR. Finally, the method does not rely on self-intersection rejection using an any-hit shader and has a fixed overhead per shading point.\nTwo renderings of a floating castle. The rendering on the right shows shading artifacts because no self-intersection avoidance is applied.\nFigure 1. Rendering with self-intersection avoidance (left) and without self-intersection avoidance (right). Image credit: Sander van der Meiren", "Method overview\nThe spawn point of a secondary ray coincides with the hit point on a triangle of an incoming ray. The goal is to compute a spawn point as close as possible to the hit point in the triangle plane, while still avoiding self-intersections. Too close to the triangle may result in self-intersection artifacts, but too far away may push the spawn point past nearby geometry, causing light leaking artifacts.\nFigure 2 shows the sources of numerical error for secondary rays. In the user shader, the object-space hit point is reconstructed and transformed into world-space. During DXR ray traversal, the world-space ray is transformed back into object-space and intersected against triangles.\nEach of these operations accumulates numerical errors, possibly resulting in self-intersections. This method computes a minimal uncertainty interval centered around the intended ray origin (red dot in Figure 2) on the triangle at each operation. The approximate ray origin (black dot in Figure 2) lies within this uncertainty interval. The ray origin is offset along the triangle normal beyond the final uncertainty interval to prevent self-intersections.\nA flow diagram of four stages (in green), showing an approximate computed ray origin as it is constructed, transformed, and finally used for a ray-triangle intersection test.\nFigure 2. The sources of numerical error in the user shader (left) and DXR ray traversal and intersection (right)", "Hit point\nStart by reconstructing the hit point and the geometric triangle normal in object-space (Listing 1).\nprecise float3 edge1 = v1 - v0;\nprecise float3 edge2 = v2 - v0;\n// interpolate triangle using barycentrics\n// add in base vertex last to reduce object-space error\nprecise float3 objPosition = v0 + mad(barys.x, edge1, mul(barys.y, edge2));\nfloat3 objNormal = cross(edge1, edge2);\nThe hit point is computed by interpolating the triangle vertices ```v0```, ```v1```, and ```v2``` using the 2D barycentric hit coordinates ```barys```. Although it is possible to compute the interpolated hit point using two fused multiply-add operations, adding the base vertex ```v0``` last reduces the maximum rounding error on the base vertex, which in practice dominates the rounding error in this computation.\nUse the ```precise``` keyword to force the compiler to perform the computations exactly as specified. Enforced precise computation of the normal and the error bounds is not required. The effects of rounding errors on these quantities are vanishingly small and can safely be ignored for self-intersection.\nNext, the object-space position is transformed into world-space (Listing 2).\nconst float3x4 o2w = ObjectToWorld3x4();\n// transform object-space position\n// add in translation last to reduce world-space error\nprecise float3 wldPosition;\nwldPosition.x = o2w._m03 +\nmad(o2w._m00, objPosition.x,\nmad(o2w._m01, objPosition.y,\nmul(o2w._m02, objPosition.z )));\nwldPosition.y = o2w._m13 +\nmad(o2w._m10, objPosition.x,\nmad(o2w._m11, objPosition.y,\nmul(o2w._m12, objPosition.z )));\nwldPosition.z = o2w._m23 +\nmad(o2w._m20, objPosition.x ,\nmad(o2w._m21, objPosition.y ,\nmul(o2w._m22, objPosition.z )));\nInstead of using the HLSL matrix mul intrinsic, write out the transformation. This ensures that the translational part {m_{i,2}}\nof the transformation is added last. This again reduces the rounding error on the translation, which in practice tends to dominate the error in this computation.\nFinally, transform the object-space normal to world-space and normalize it (Listing 3).\nconst float3x4 w2o = WorldToObject3x4();\n// transform normal to world-space using\n// inverse transpose matrix\nfloat3 wldNormal = mul(transpose((float3x3)w2o), objNormal);\n// normalize world-space normal\nconst float wldScale = rsqrt(dot(wldNormal, wldNormal));\nwldNormal = mul(wldScale, wldNormal);\n// flip towards incoming ray\nif(dot(WorldRayDirection(), wldNormal) > 0)\nwldNormal = -wldNormal;\nTo support transformations with uneven scaling or shear, the normals are transformed using the inverse transpose transformation. There is no need to normalize the object-space normal before the transformation. It is necessary to normalize again in world-space anyway. Because the inverse length of the world normal is needed again later to appropriately scale the error bounds, normalize manually instead of using the HLSL ```normalize``` intrinsic.", "Error bounds\nWith an approximate world-space position and triangle normal, continue by computing error bounds on the computed position, bounding the maximum finite precision rounding error. It is necessary to account for the rounding errors in the computations in Listings 1 and 2.\nIt is also necessary to account for rounding errors that may occur during traversal (Figure 2). During traversal, DXR will apply a world-to-object transformation and perform a ray-triangle intersection test. Both of these are performed in finite precision and thus introduce rounding errors.\nStart by computing a combined object-space error bound, accounting both for the rounding errors in Listing 1 and rounding errors due to the DXR ray-triangle intersection test (Listing 4).\nconst float c0 = 5.9604644775390625E-8f;\nconst float c1 = 1.788139769587360206060111522674560546875E-7f;\n// compute twice the maximum extent of the triangle\nconst float3 extent3 = abs(edge1) + abs(edge2) +\nabs(abs(edge1) - abs(edge2));\nconst float extent = max(max(extent3.x, extent3.y), extent3.z);\n// bound object-space error due to reconstruction and intersection\nfloat3 objErr = mad(c0, abs(v0), mul(c1, extent));\nNote that the error on the triangle intersection is bounded by the maximum triangle extent along the three dimensions. A rigorous proof for this bound goes beyond the scope of this post. To provide an intuitive justification, common ray-triangle intersection algorithms reorient the triangle into \u2019ray space\u2019 (by subtracting the ray origin) before performing the intersection test. In the context of self-intersection, the ray origin lies on the triangle. Thus, the magnitude of the remaining triangle vertices in this ray space is bounded by the extent of the triangle along each dimension.\nFurthermore, these intersection algorithms project the triangle into a 2D plane. This projection causes errors along one dimension to bleed over into the other dimensions. Therefore, take the maximum extent along all dimensions, instead of treating the error along the dimensions independently. The exact bound on the ray-triangle intersection test will be hardware-specific. The constant ```c1``` is tuned for NVIDIA RTX hardware, but may require some adjusting on different platforms.\nError bounds for custom intersection primitives depend on the implementation details of their Intersection shader. See Advanced Linear Algebra: Foundations to Frontiers for a thorough introduction to finite precision rounding error analysis.\nNext, compute the world-space error bound due to the transformation of the hit point from object-space to world-space (Listing 5).\n// bound world-space error due to object-to-world transform\nconst float c2 = 1.19209317972490680404007434844970703125E-7f;\nfloat3 wldErr = mad(c1, mul(abs((float3x3)o2w), abs(objPosition)),\nmul(c2, abs(transpose(o2w[3]))));\nThat leaves the rounding errors in the world-to-object transformation performed by DXR during ray traversal (Listing 6).\n// bound object-space error due to world-to-object transform\nobjErr = mad(c2, mul(abs(w2o), float4(abs(wldPosition), 1)), objErr);\nLike the ray-triangle intersection test, the rounding error in the world-to-object transformation depends on the hardware. The constant ```c2``` is conservative and should suffice for the various ways of implementing the vector matrix multiplication.\nThe finite precision representation of the world-to-object transformation matrix and its inverse are not guaranteed to match exactly. In the analysis, the error in the representation can be attributed to one or the other. Because the object-to-world transformation is performed in user code, the errors are best attributed to the object-to-world transformation matrix, enabling tighter bounds.", "Offset\nThe previous section explained how to compute bounds on the rounding errors for secondary ray construction and traversal. These bounds yield an interval around the approximate, finite precision ray origin. The intended, full-precision \u2018true\u2019 ray origin is guaranteed to lie somewhere in this interval.\nThe true triangle passes through the true ray origin, so the triangle also passes through this interval. Figure 3 shows how to offset the approximate origin along the triangle normal to guarantee it lies above the true triangle, thus preventing self-intersections.\nThe approximate computed ray origin, centered inside a 2D uncertainty interval. The true ray origin lies within the uncertainty interval. The true triangle passing through the true ray origin. The triangle normal, passing through the approximate ray origin and a conservatively offset triangle, touching the uncertainty interval. The offset ray origin is the intersection of the triangle normal and the conservatively offset triangle.\nFigure 3. Avoid self-intersection by offsetting the ray origin along the normal to outside the error interval The error bound \u2206 is projected onto the normal n to obtain an offset \u03b4 along the normal\n{\\delta = \\frac{\\Delta \\cdot {abs}\\left(n\\right)}{n \\cdot n}}\nRounding errors on the normal are of similar magnitude as rounding errors on the computation of the error bounds and offset themselves. These are vanishingly small and can in practice be ignored. Combine the object and world-space offsets into a single world-space offset along the world-space normal (Listing 7).\n// compute world-space self-intersection avoidance offset\nfloat objOffset = dot(objErr, abs(objNormal));\nfloat wldOffset = dot(wldErr, abs(wldNormal));\nwldOffset = mad(wldScale, objOffset, wldOffset);\nUse the already normalized world-space normal {\\bar{n}_w}\nfrom Listing 3. The world-space offset {\\delta_w}\nsimplifies to \\delta_w = \\Delta \\cdot {abs}\\left( \\bar{n}_w \\right)\n. The object-space offset {\\delta_o}\nalong the object-space normal {n_o}\nneeds to be transformed into world-space as {\\delta_o M n_o}\n.\nNote, however, that the transformed object-space offset is not necessarily parallel to the world-space normal {n_w}\n. To obtain a single combined offset along the world-space normal, project the transformed object-space offset onto the world-space normal, as {\\delta_o M n_o \\cdot n_w}\n. Using that {n_w = M^{-T} n_o}\nthis simplifies to:\n{\\delta_o M n_o \\cdot M^{-T} n_o = \\delta_o \\left( M n_o \\right)^T M^{-T} n_o = \\delta_o n_o \\cdot M^T M^{-T} n_o = \\delta_o n_o \\cdot n_o = \\Delta \\cdot {abs}\\left(n\\right)}\nFinally, use the computed offset to perturb the hit point along the triangle normal (Listing 8).\n// offset along the normal on either side.\nprecise float3 wldFront = mad( wldOffset, wldNormal, wldPosition);\nprecise float3 wldBack = mad(-wldOffset, wldNormal, wldPosition);\nThis yields front and back spawn points safe from self-intersection. The derived error bounds (and thus offsets) neither depend on the incoming ray direction nor the outgoing secondary ray direction. It is therefore possible to reuse the same spawn points for all secondary rays originating from this hit point. All reflection rays should use the front spawn point while transmission rays should use the back spawn point.\nObject-to-world and world-to-object transformations of the direction also cause rounding errors in the ray direction. At extreme grazing angles, these rounding errors may cause it to flip sides, orienting it back towards the triangle. The offsetting method in this post does not protect against such rounding errors. It is generally advised to filter out secondary rays at extreme angles.\nAlternatively, similar error bounds can be derived on the ray direction transformations. Offsetting the ray direction along the triangle normal (as for the ray origin) can then guarantee its sidedness. However, as the reflectance distribution of common BRDF models tends towards zero at grazing angles, this problem can be safely ignored in many applications.", "Object space\nAs seen in Listing 4, the offset grows linearly in the triangle extent and the magnitude of the triangle base vertex in object-space. For small triangles, the rounding error in the base vertex will dominate the object-space error (Figure 2). It is thus possible to reduce the object-space error by repositioning geometry in object-space, centering it around the object-space origin to minimize the distance to the origin. For geometry with extremely large triangles, such as ground planes, it may be worthwhile to tessellate the geometry and further reduce the rounding errors in the triangle extent.", "Camera space\nAs seen in Listings 5 and 6, the magnitude of the offset will grow linearly with the magnitudes of the world-space position. The proportionality constant ```c2``` is approximately 1 ulps. Instanced geometry at a distance {d}\nfrom the scene origin in world-space will have a maximum rounding error in the order of {d2^{-22}}\n, or 1 mm of offset for every 4 km distance. The offset magnitudes also scale linear with the triangle extent and object-space position.\nFor an example secondary ray in Figure 4 spawned on a leaf of 10 cm, in a tree of 20 m (object-space origin at the root) 1 km away from the world space origin, the offset magnitudes due to the triangle extent, object-space position, and world-space position will be in the order of 45 nm, 4 \u00b5m, and 0.25 mm, respectively. In practice, \u200c rounding errors in the world-space position tend to dominate all rounding errors. This is particularly true for large scenes of relatively small objects.\nA diagram of a tree, placed 1 km from the world-space scene origin. Measurements are shown for a leaf of 10 cm within the tree, placed 20 m from the object-space origin. A camera icon is close to the tree, oriented towards the leaf.\nFigure 4. Offset magnitudes scale linear with the triangle extent, object-space position, and world-space position magnitudes Note that the error is proportional to the world-space distance to the scene origin, not the scene camera. Consequently, if the camera is far away from the scene origin, the offsets for rays spawned from nearby geometry may become prohibitively large, resulting in visual artifacts.\nThis problem can be reduced by translating the entire scene into camera space. All instances are repositioned so the camera origin coincides with the world-space origin. Consequently, the distance {d}\nbecomes the distance to the camera in this camera space and the offset magnitudes will be proportional to the distance to the camera. Rays spawned from geometry near the camera will enjoy relatively small offsets, reducing the likelihood of visual artifacts due to offsetting.", "Connection rays\nThis discussion has so far focused on offsetting of the ray origin to prevent self-intersection at the origin. Ray and path tracing algorithms also trace rays to evaluate visibility between two points on different triangles, such as shadow rays connecting a shading point and a light source.\nThese rays may suffer from self-intersection on either end of the ray. It is necessary to offset both ends to avoid self-intersections. The offset for the endpoint is computed in a similar fashion as for the ray origin, but using the object-to-world and world-to-object transformation matrices, barycentric and triangle vertices of the endpoint and using the connection ray direction as the incoming ray direction.\nContrary to scattering rays, it is necessary to account for rounding errors in the world-to-object ray direction transform during traversal. Theoretically, it is also necessary to account for additional rounding error in the ray-triangle intersection test because the ray origin does not lie on the endpoint triangle. However, this additional error scales sublinearly with the world-to-object error, so for simplicity these errors are implicitly combined.\nFor the endpoint, the world-to-object transformation error computation in Listing 6 is replaced by (Listing 9).\n// connection ray direction\nprecise float3 wldDir = wldEndPosition - wldOrigin;\n// bound endpoint object-space error due to object-to-world transform\nfloat4 absOriginDir = (float4)(abs(wldOrigin) + abs(wldDir), 1);\nobjEndErr = mad(c2, mul(abs(w2oEnd), absOriginDir), objEndErr);\nHere, ```wldOrigin``` is the connection ray origin in world-space. In DXR, rays are defined using an origin and direction. Instead of offsetting the endpoint and recomputing the ray direction, apply the offset directly to the world-space direction. For endpoint offsetting, Listing 8 thus becomes Listing 10.\n// offset ray direction along the endpoint normal towards the ray origin\nwldDir = mad(wldEndOffset, wldEndNormal, wldDir) ;\n// shorten the ray tmax by 1 ulp\nconst float tmax = 0.99999994039f;\nShorten the ray length by 1 ulp to account for rounding errors in the direction computation.\nIn practice, a simpler approach of using a cheap approximate offsetting heuristic in combination with identifier-based self-intersection rejection is often sufficient to avoid endpoint self-intersection.The approximate offsetting will avoid most endpoint self-intersections, with identifier-based hit rejection taking care of the remaining self-intersections.\nFor secondary scatter rays, avoid identifier based self-intersection rejection, as it requires invoking an any-hit shader for every intersection along the ray, adding significant performance overhead. However, for visibility rays, the additional performance overhead of endpoint identifier-based hit rejection is minimal.\nFor visibility rays using the ```RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH``` flag there will always be at most two additional reported hits: the rejected endpoint self-intersection and any occluder terminating traversal.\nFor visibility rays not using the ```RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH``` flag, self-intersections can be rejected in the closest-hit shader instead of the any-hit shader. If the visibility ray invokes the closest-hit shader for the endpoint triangle, no closer hit was found and thus the hit should simply be treated as a miss in the closest-hit shader.", "Conclusion\nThe method presented in this post offers a robust and easy-to-use solution for self-intersections of secondary rays. The method applies a minimal conservative offset, resolving self-intersection artifacts while reducing light leaking artifacts. Moreover, the method has minimal runtime overhead and integrates easily in common shading pipelines. While this post describes an HLSL implementation for DXR, the approach translates easily to GLSL for Vulkan and CUDA for OptiX.\nFor more information, visit NVIDIA/self-intersection-avoidance on GitHub for HLSL and GLSL sample implementations. And check out the OptiX Toolkit ShaderUtil Library for a ready-to-use OptiX header library for self-intersection avoidance."], "document_title": "Solving Self-Intersection Artifacts in DirectX Raytracing", "document_url": "https://developer.nvidia.com/blog/solving-self-intersection-artifacts-in-directx-raytracing/", "document_date": "2023-08-31T17:46:30", "document_date_modified": "2023-10-25T23:52:33", "document_full_text": "Solving Self-Intersection Artifacts in DirectX Raytracing\nRay and path tracing algorithms construct light paths by starting at the camera or the light sources and intersecting rays with the scene geometry. As objects are hit, new secondary rays are generated on these surfaces to continue the paths.\nIn theory, these secondary rays will not yield an intersection with the same triangle again, as intersections at a distance of zero are excluded by the intersection algorithm. In practice, however, the finite floating-point precision used in the actual implementation often leads to false-positive results, known as self-intersections (Figure 2). This creates artifacts, such as shadow acne, where the triangle sometimes improperly shadows itself (Figure 1).\nSelf-intersection can be avoided by explicitly excluding the same primitive from intersection using its identifier. In DirectX Raytracing (DXR) this self-intersection check would be implemented in an any-hit shader. However, forcing an any-hit invocation for all triangle hits comes at a significant performance penalty. Furthermore, this method does not deal with false positives against adjacent (near) coplanar triangles.\nThe most widespread solutions to work around the issue use various heuristics to offset the ray along either the ray direction or the normal. These methods are, however, not robust enough to handle a variety of common production content and may even require manual parameter tweaking on a per-scene basis, particularly in scenes with heavily translated, scaled or sheared instanced geometry. For more information, see Ray Tracing Gems: High-Quality and Real-Time Rendering with DXR and Other APIs.\nAlternatively, the sources of the numerical imprecision can be numerically bounded at runtime, giving robust error intervals on the intersection test. However, this comes with considerable performance overhead and requires source access to the underlying implementation of the ray/triangle intersection routine, which is not possible in a hardware-accelerated API like DXR.\nThis post describes a robust offsetting method for secondary rays spawned from triangles in DXR. The method is based on a thorough numerical analysis of the sources of the numerical imprecision. It involves computing spawn points for secondary rays, safe from self-intersections. The method does not require modification of the traversal and ray/triangle intersection routines and can thus be used with closed-source and hardware-accelerated ray tracing APIs like DXR. Finally, the method does not rely on self-intersection rejection using an any-hit shader and has a fixed overhead per shading point.\nTwo renderings of a floating castle. The rendering on the right shows shading artifacts because no self-intersection avoidance is applied.\nFigure 1. Rendering with self-intersection avoidance (left) and without self-intersection avoidance (right). Image credit: Sander van der Meiren\nMethod overview\nThe spawn point of a secondary ray coincides with the hit point on a triangle of an incoming ray. The goal is to compute a spawn point as close as possible to the hit point in the triangle plane, while still avoiding self-intersections. Too close to the triangle may result in self-intersection artifacts, but too far away may push the spawn point past nearby geometry, causing light leaking artifacts.\nFigure 2 shows the sources of numerical error for secondary rays. In the user shader, the object-space hit point is reconstructed and transformed into world-space. During DXR ray traversal, the world-space ray is transformed back into object-space and intersected against triangles.\nEach of these operations accumulates numerical errors, possibly resulting in self-intersections. This method computes a minimal uncertainty interval centered around the intended ray origin (red dot in Figure 2) on the triangle at each operation. The approximate ray origin (black dot in Figure 2) lies within this uncertainty interval. The ray origin is offset along the triangle normal beyond the final uncertainty interval to prevent self-intersections.\nA flow diagram of four stages (in green), showing an approximate computed ray origin as it is constructed, transformed, and finally used for a ray-triangle intersection test.\nFigure 2. The sources of numerical error in the user shader (left) and DXR ray traversal and intersection (right)\nHit point\nStart by reconstructing the hit point and the geometric triangle normal in object-space (Listing 1).\nprecise float3 edge1 = v1 - v0;\nprecise float3 edge2 = v2 - v0;\n// interpolate triangle using barycentrics\n// add in base vertex last to reduce object-space error\nprecise float3 objPosition = v0 + mad(barys.x, edge1, mul(barys.y, edge2));\nfloat3 objNormal = cross(edge1, edge2);\nThe hit point is computed by interpolating the triangle vertices ```v0```, ```v1```, and ```v2``` using the 2D barycentric hit coordinates ```barys```. Although it is possible to compute the interpolated hit point using two fused multiply-add operations, adding the base vertex ```v0``` last reduces the maximum rounding error on the base vertex, which in practice dominates the rounding error in this computation.\nUse the ```precise``` keyword to force the compiler to perform the computations exactly as specified. Enforced precise computation of the normal and the error bounds is not required. The effects of rounding errors on these quantities are vanishingly small and can safely be ignored for self-intersection.\nNext, the object-space position is transformed into world-space (Listing 2).\nconst float3x4 o2w = ObjectToWorld3x4();\n// transform object-space position\n// add in translation last to reduce world-space error\nprecise float3 wldPosition;\nwldPosition.x = o2w._m03 +\nmad(o2w._m00, objPosition.x,\nmad(o2w._m01, objPosition.y,\nmul(o2w._m02, objPosition.z )));\nwldPosition.y = o2w._m13 +\nmad(o2w._m10, objPosition.x,\nmad(o2w._m11, objPosition.y,\nmul(o2w._m12, objPosition.z )));\nwldPosition.z = o2w._m23 +\nmad(o2w._m20, objPosition.x ,\nmad(o2w._m21, objPosition.y ,\nmul(o2w._m22, objPosition.z )));\nInstead of using the HLSL matrix mul intrinsic, write out the transformation. This ensures that the translational part {m_{i,2}}\nof the transformation is added last. This again reduces the rounding error on the translation, which in practice tends to dominate the error in this computation.\nFinally, transform the object-space normal to world-space and normalize it (Listing 3).\nconst float3x4 w2o = WorldToObject3x4();\n// transform normal to world-space using\n// inverse transpose matrix\nfloat3 wldNormal = mul(transpose((float3x3)w2o), objNormal);\n// normalize world-space normal\nconst float wldScale = rsqrt(dot(wldNormal, wldNormal));\nwldNormal = mul(wldScale, wldNormal);\n// flip towards incoming ray\nif(dot(WorldRayDirection(), wldNormal) > 0)\nwldNormal = -wldNormal;\nTo support transformations with uneven scaling or shear, the normals are transformed using the inverse transpose transformation. There is no need to normalize the object-space normal before the transformation. It is necessary to normalize again in world-space anyway. Because the inverse length of the world normal is needed again later to appropriately scale the error bounds, normalize manually instead of using the HLSL ```normalize``` intrinsic.\nError bounds\nWith an approximate world-space position and triangle normal, continue by computing error bounds on the computed position, bounding the maximum finite precision rounding error. It is necessary to account for the rounding errors in the computations in Listings 1 and 2.\nIt is also necessary to account for rounding errors that may occur during traversal (Figure 2). During traversal, DXR will apply a world-to-object transformation and perform a ray-triangle intersection test. Both of these are performed in finite precision and thus introduce rounding errors.\nStart by computing a combined object-space error bound, accounting both for the rounding errors in Listing 1 and rounding errors due to the DXR ray-triangle intersection test (Listing 4).\nconst float c0 = 5.9604644775390625E-8f;\nconst float c1 = 1.788139769587360206060111522674560546875E-7f;\n// compute twice the maximum extent of the triangle\nconst float3 extent3 = abs(edge1) + abs(edge2) +\nabs(abs(edge1) - abs(edge2));\nconst float extent = max(max(extent3.x, extent3.y), extent3.z);\n// bound object-space error due to reconstruction and intersection\nfloat3 objErr = mad(c0, abs(v0), mul(c1, extent));\nNote that the error on the triangle intersection is bounded by the maximum triangle extent along the three dimensions. A rigorous proof for this bound goes beyond the scope of this post. To provide an intuitive justification, common ray-triangle intersection algorithms reorient the triangle into \u2019ray space\u2019 (by subtracting the ray origin) before performing the intersection test. In the context of self-intersection, the ray origin lies on the triangle. Thus, the magnitude of the remaining triangle vertices in this ray space is bounded by the extent of the triangle along each dimension.\nFurthermore, these intersection algorithms project the triangle into a 2D plane. This projection causes errors along one dimension to bleed over into the other dimensions. Therefore, take the maximum extent along all dimensions, instead of treating the error along the dimensions independently. The exact bound on the ray-triangle intersection test will be hardware-specific. The constant ```c1``` is tuned for NVIDIA RTX hardware, but may require some adjusting on different platforms.\nError bounds for custom intersection primitives depend on the implementation details of their Intersection shader. See Advanced Linear Algebra: Foundations to Frontiers for a thorough introduction to finite precision rounding error analysis.\nNext, compute the world-space error bound due to the transformation of the hit point from object-space to world-space (Listing 5).\n// bound world-space error due to object-to-world transform\nconst float c2 = 1.19209317972490680404007434844970703125E-7f;\nfloat3 wldErr = mad(c1, mul(abs((float3x3)o2w), abs(objPosition)),\nmul(c2, abs(transpose(o2w[3]))));\nThat leaves the rounding errors in the world-to-object transformation performed by DXR during ray traversal (Listing 6).\n// bound object-space error due to world-to-object transform\nobjErr = mad(c2, mul(abs(w2o), float4(abs(wldPosition), 1)), objErr);\nLike the ray-triangle intersection test, the rounding error in the world-to-object transformation depends on the hardware. The constant ```c2``` is conservative and should suffice for the various ways of implementing the vector matrix multiplication.\nThe finite precision representation of the world-to-object transformation matrix and its inverse are not guaranteed to match exactly. In the analysis, the error in the representation can be attributed to one or the other. Because the object-to-world transformation is performed in user code, the errors are best attributed to the object-to-world transformation matrix, enabling tighter bounds.\nOffset\nThe previous section explained how to compute bounds on the rounding errors for secondary ray construction and traversal. These bounds yield an interval around the approximate, finite precision ray origin. The intended, full-precision \u2018true\u2019 ray origin is guaranteed to lie somewhere in this interval.\nThe true triangle passes through the true ray origin, so the triangle also passes through this interval. Figure 3 shows how to offset the approximate origin along the triangle normal to guarantee it lies above the true triangle, thus preventing self-intersections.\nThe approximate computed ray origin, centered inside a 2D uncertainty interval. The true ray origin lies within the uncertainty interval. The true triangle passing through the true ray origin. The triangle normal, passing through the approximate ray origin and a conservatively offset triangle, touching the uncertainty interval. The offset ray origin is the intersection of the triangle normal and the conservatively offset triangle.\nFigure 3. Avoid self-intersection by offsetting the ray origin along the normal to outside the error interval The error bound \u2206 is projected onto the normal n to obtain an offset \u03b4 along the normal\n{\\delta = \\frac{\\Delta \\cdot {abs}\\left(n\\right)}{n \\cdot n}}\nRounding errors on the normal are of similar magnitude as rounding errors on the computation of the error bounds and offset themselves. These are vanishingly small and can in practice be ignored. Combine the object and world-space offsets into a single world-space offset along the world-space normal (Listing 7).\n// compute world-space self-intersection avoidance offset\nfloat objOffset = dot(objErr, abs(objNormal));\nfloat wldOffset = dot(wldErr, abs(wldNormal));\nwldOffset = mad(wldScale, objOffset, wldOffset);\nUse the already normalized world-space normal {\\bar{n}_w}\nfrom Listing 3. The world-space offset {\\delta_w}\nsimplifies to \\delta_w = \\Delta \\cdot {abs}\\left( \\bar{n}_w \\right)\n. The object-space offset {\\delta_o}\nalong the object-space normal {n_o}\nneeds to be transformed into world-space as {\\delta_o M n_o}\n.\nNote, however, that the transformed object-space offset is not necessarily parallel to the world-space normal {n_w}\n. To obtain a single combined offset along the world-space normal, project the transformed object-space offset onto the world-space normal, as {\\delta_o M n_o \\cdot n_w}\n. Using that {n_w = M^{-T} n_o}\nthis simplifies to:\n{\\delta_o M n_o \\cdot M^{-T} n_o = \\delta_o \\left( M n_o \\right)^T M^{-T} n_o = \\delta_o n_o \\cdot M^T M^{-T} n_o = \\delta_o n_o \\cdot n_o = \\Delta \\cdot {abs}\\left(n\\right)}\nFinally, use the computed offset to perturb the hit point along the triangle normal (Listing 8).\n// offset along the normal on either side.\nprecise float3 wldFront = mad( wldOffset, wldNormal, wldPosition);\nprecise float3 wldBack = mad(-wldOffset, wldNormal, wldPosition);\nThis yields front and back spawn points safe from self-intersection. The derived error bounds (and thus offsets) neither depend on the incoming ray direction nor the outgoing secondary ray direction. It is therefore possible to reuse the same spawn points for all secondary rays originating from this hit point. All reflection rays should use the front spawn point while transmission rays should use the back spawn point.\nObject-to-world and world-to-object transformations of the direction also cause rounding errors in the ray direction. At extreme grazing angles, these rounding errors may cause it to flip sides, orienting it back towards the triangle. The offsetting method in this post does not protect against such rounding errors. It is generally advised to filter out secondary rays at extreme angles.\nAlternatively, similar error bounds can be derived on the ray direction transformations. Offsetting the ray direction along the triangle normal (as for the ray origin) can then guarantee its sidedness. However, as the reflectance distribution of common BRDF models tends towards zero at grazing angles, this problem can be safely ignored in many applications.\nObject space\nAs seen in Listing 4, the offset grows linearly in the triangle extent and the magnitude of the triangle base vertex in object-space. For small triangles, the rounding error in the base vertex will dominate the object-space error (Figure 2). It is thus possible to reduce the object-space error by repositioning geometry in object-space, centering it around the object-space origin to minimize the distance to the origin. For geometry with extremely large triangles, such as ground planes, it may be worthwhile to tessellate the geometry and further reduce the rounding errors in the triangle extent.\nCamera space\nAs seen in Listings 5 and 6, the magnitude of the offset will grow linearly with the magnitudes of the world-space position. The proportionality constant ```c2``` is approximately 1 ulps. Instanced geometry at a distance {d}\nfrom the scene origin in world-space will have a maximum rounding error in the order of {d2^{-22}}\n, or 1 mm of offset for every 4 km distance. The offset magnitudes also scale linear with the triangle extent and object-space position.\nFor an example secondary ray in Figure 4 spawned on a leaf of 10 cm, in a tree of 20 m (object-space origin at the root) 1 km away from the world space origin, the offset magnitudes due to the triangle extent, object-space position, and world-space position will be in the order of 45 nm, 4 \u00b5m, and 0.25 mm, respectively. In practice, \u200c rounding errors in the world-space position tend to dominate all rounding errors. This is particularly true for large scenes of relatively small objects.\nA diagram of a tree, placed 1 km from the world-space scene origin. Measurements are shown for a leaf of 10 cm within the tree, placed 20 m from the object-space origin. A camera icon is close to the tree, oriented towards the leaf.\nFigure 4. Offset magnitudes scale linear with the triangle extent, object-space position, and world-space position magnitudes Note that the error is proportional to the world-space distance to the scene origin, not the scene camera. Consequently, if the camera is far away from the scene origin, the offsets for rays spawned from nearby geometry may become prohibitively large, resulting in visual artifacts.\nThis problem can be reduced by translating the entire scene into camera space. All instances are repositioned so the camera origin coincides with the world-space origin. Consequently, the distance {d}\nbecomes the distance to the camera in this camera space and the offset magnitudes will be proportional to the distance to the camera. Rays spawned from geometry near the camera will enjoy relatively small offsets, reducing the likelihood of visual artifacts due to offsetting.\nConnection rays\nThis discussion has so far focused on offsetting of the ray origin to prevent self-intersection at the origin. Ray and path tracing algorithms also trace rays to evaluate visibility between two points on different triangles, such as shadow rays connecting a shading point and a light source.\nThese rays may suffer from self-intersection on either end of the ray. It is necessary to offset both ends to avoid self-intersections. The offset for the endpoint is computed in a similar fashion as for the ray origin, but using the object-to-world and world-to-object transformation matrices, barycentric and triangle vertices of the endpoint and using the connection ray direction as the incoming ray direction.\nContrary to scattering rays, it is necessary to account for rounding errors in the world-to-object ray direction transform during traversal. Theoretically, it is also necessary to account for additional rounding error in the ray-triangle intersection test because the ray origin does not lie on the endpoint triangle. However, this additional error scales sublinearly with the world-to-object error, so for simplicity these errors are implicitly combined.\nFor the endpoint, the world-to-object transformation error computation in Listing 6 is replaced by (Listing 9).\n// connection ray direction\nprecise float3 wldDir = wldEndPosition - wldOrigin;\n// bound endpoint object-space error due to object-to-world transform\nfloat4 absOriginDir = (float4)(abs(wldOrigin) + abs(wldDir), 1);\nobjEndErr = mad(c2, mul(abs(w2oEnd), absOriginDir), objEndErr);\nHere, ```wldOrigin``` is the connection ray origin in world-space. In DXR, rays are defined using an origin and direction. Instead of offsetting the endpoint and recomputing the ray direction, apply the offset directly to the world-space direction. For endpoint offsetting, Listing 8 thus becomes Listing 10.\n// offset ray direction along the endpoint normal towards the ray origin\nwldDir = mad(wldEndOffset, wldEndNormal, wldDir) ;\n// shorten the ray tmax by 1 ulp\nconst float tmax = 0.99999994039f;\nShorten the ray length by 1 ulp to account for rounding errors in the direction computation.\nIn practice, a simpler approach of using a cheap approximate offsetting heuristic in combination with identifier-based self-intersection rejection is often sufficient to avoid endpoint self-intersection.The approximate offsetting will avoid most endpoint self-intersections, with identifier-based hit rejection taking care of the remaining self-intersections.\nFor secondary scatter rays, avoid identifier based self-intersection rejection, as it requires invoking an any-hit shader for every intersection along the ray, adding significant performance overhead. However, for visibility rays, the additional performance overhead of endpoint identifier-based hit rejection is minimal.\nFor visibility rays using the ```RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH``` flag there will always be at most two additional reported hits: the rejected endpoint self-intersection and any occluder terminating traversal.\nFor visibility rays not using the ```RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH``` flag, self-intersections can be rejected in the closest-hit shader instead of the any-hit shader. If the visibility ray invokes the closest-hit shader for the endpoint triangle, no closer hit was found and thus the hit should simply be treated as a miss in the closest-hit shader.\nConclusion\nThe method presented in this post offers a robust and easy-to-use solution for self-intersections of secondary rays. The method applies a minimal conservative offset, resolving self-intersection artifacts while reducing light leaking artifacts. Moreover, the method has minimal runtime overhead and integrates easily in common shading pipelines. While this post describes an HLSL implementation for DXR, the approach translates easily to GLSL for Vulkan and CUDA for OptiX.\nFor more information, visit NVIDIA/self-intersection-avoidance on GitHub for HLSL and GLSL sample implementations. And check out the OptiX Toolkit ShaderUtil Library for a ready-to-use OptiX header library for self-intersection avoidance."}], "https://developer.nvidia.com/blog/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/": [{"text": "Graph neural networks (GNNs) are beneficial for machine learning tasks on graph-structured data, with applications in various fields. The use of cuGraph-DGL, a GPU-accelerated library for graph computations, can enhance the performance of GNNs. GNNs work by propagating and transforming node features across the graph structure in multiple layers, updating the features of each node based on its own features and the features of its neighbors. The bottleneck in GNN sampling and training is the lack of scalability for handling large-scale graphs, which can be addressed by using RAPIDS and cuGraph. cuGraph-DGL, an extension of cuGraph, integrates with DGL to run GNN workflows at high speed on GPUs. Setting up cuGraph-DGL involves installing the necessary libraries and implementing a GNN for node classification. By using cuGraph-DGL on large graphs, significant speedups can be achieved compared to traditional setups. Overall, cuGraph-DGL is a valuable tool for accelerating graph-based machine learning tasks and further exploration is encouraged to leverage its capabilities.", "text_components": ["Introduction to Graph Neural Networks with NVIDIA cuGraph-DGL\nGraph neural networks (GNNs) have emerged as a powerful tool for a variety of machine learning tasks on graph-structured data. These tasks range from node classification and link prediction to graph classification. They also cover a wide range of applications such as social network analysis, drug discovery in healthcare, fraud detection in financial services, and molecular chemistry.\nIn this post, I introduce how to use cuGraph-DGL, a GPU-accelerated library for graph computations. It extends Deep Graph Library (DGL), a popular framework for GNNs that enables large-scale applications.", "Basics of graph neural networks\nBefore I dive into cuGraph-DGL, I want to establish some basics. GNNs are a special kind of neural network designed to work with data structured as graphs. Unlike traditional neural networks that assume independence between samples, which doesn\u2019t fit well with graph data, GNNs effectively exploit the rich and complex interconnections within graph data.\nIn a nutshell, GNNs work by propagating and transforming node features across the graph structure in multiple steps, often referred to as layers (Figure 1). Each layer updates the features of each node based on its own features and the features of its neighbors.\nDiagram shows the initial graph on the left and the update graph on the right. Node features are respected by using information from its neighbors and graph structure.\nFigure 1. Schematic for the message passing layer (source: Distill )\nIn Figure 1, the first step \u201cprepares\u201d a message composed of information from an edge and its connected nodes and then \u201cpasses\u201d the message to the node. This process enables the model to learn high-level representations of nodes, edges, and the graph as a whole, which can be used for various downstream tasks like node classification, link prediction, and graph classification.\nFigure 2 shows how a 2-layer GNN is supposed to compute the output of node 5.\nGIF shows a two-layer GNN updating the seed node\u2019s embeddings. First, immediate neighbors\u2019 embeddings are refreshed using data from neighbors two hops away. Then, these updated embeddings are used to update the seed node\u2019s representation.\nFigure 2. Update of embeddings on a single node in a 2-layer GNN (source: DGL documentation )", "Bottlenecks when handling large-scale graphs\nThe bottleneck in GNN sampling and training is the lack of an existing implementation that can scale to handle billions or even trillions of edges, a scale often seen in real-world graph problems. For example, if you\u2019re handling a graph with trillions of edges, you must be able to run DGL-based GNN workflows quickly.\nOne solution is to use RAPIDS, which already possesses the foundational elements capable of scaling to trillions of edges using GPUs.", "What is RAPIDS cuGraph?\ncuGraph is a part of the RAPIDS AI ecosystem, an open-source suite of software libraries for executing end-to-end data science and analytics pipelines entirely on GPUs. The cuGraph library provides a simple, flexible, and powerful API for graph analytics, enabling you to perform computations on graph data at scale and speed.", "What is DGL?\nDeep Graph Library (DGL) is a Python library designed to simplify the implementation of graph neural networks (GNNs) by providing intuitive interfaces and high-performance computation.\nDGL supports a broad array of graph operations and structures, enhancing the modeling of complex systems and relationships. It also integrates with popular deep learning frameworks like PyTorch and TensorFlow, fostering seamless development and deployment of GNNs.", "What is cuGraph-DGL?\ncuGraph-DGL is an extension of cuGraph that integrates with the Deep Graph Library (DGL) to leverage the power of GPUs to run DGL-based GNN workflows at unprecedented speed. This library is a collaborative effort between DGL developers and cuGraph developers.\nIn addition to cuGraph-DGL, cuGraph also provides the cugraph-ops library, which enables DGL users to get performance boosts using CuGraphSAGEConv, CuGraphGATConv, and CuGraphRelGraphConv in place of the default SAGEConv, GATConv, and RelGraphConv models. You can also import the SAGEConv, GATConv, and RelGraphConv models directly from the ```cugraph_dgl``` library.\nIn GNN sampling and training, the major challenge is the absence of an implementation that can manage real-world graph problems with billions or trillions of edges. To address this, use cuGraph-DGL, with its inherent capability to scale to trillions of edges using GPUs.", "Setting up cuGraph-DGL\nBefore you dive into the code, make sure that you have cuGraph and DGL installed in your Python environment. To install the cuGraph-DGL-enabled environment, run the following command:\n```\nconda install mamba -c conda-forge \n\nmamba create -n cugraph_dgl_23_06 -c pytorch -c dglteam/label/cu118 -c rapidsai-nightly -c nvidia -c conda-forge dgl cugraph-dgl=23.10 pylibcugraphops=23.10 cudatoolkit=11.8 torchmetrics ogb\n```", "Implementing a GNN with cuGraph-DGL\nWith your environment set up, put cuGraph-DGL into action and construct a simple GNN for node classification. Converting an existing DGL workflow to a cuGraph-DGL workflow has the following steps:\nUse cuGraph-ops models such as ```CuGraphSAGECon```, in place of the native DGL model ( ```SAGEConv``` ).\nCreate a ```CuGraphGraph``` object from a DGL graph.\nUse the ```cuGraph``` data loader in place of the native DGL Dataloader.\nUsing cugraph-dgl on a 3.2 billion-edge graph, we observed a 3x speedup when using eight GPUs for sampling and training, compared to a single GPU UVA DGL setup. Additionally, we saw a 2x speedup when using eight GPUs for sampling and one GPU for training.\nAn upcoming blog post will provide more details on the gains and scalability.", "Create a cuGraph-DGL graph\nTo create a ```cugraph_dgl``` graph directly from a DGL graph, run the following code example.\n```\nimport dgl\nimport cugraph_dgl\n\ndataset = dgl.data.CoraGraphDataset()\ndgl_g = dataset[0]\n# Add self loops as cugraph\n# does not support isolated vertices yet \ndgl_g = dgl.add_self_loop(dgl_g)\ncugraph_g = cugraph_dgl.convert.cugraph_storage_from_heterograph(dgl_g, single_gpu=True)\n```\nFor more information about creating a ```cuGraph``` storage object, see CuGraphStorage.", "Create a cuGraph-Ops-based model\nIn this step, the only modification to make is the importation of ```cugraph_ops```-based models. These models are drop-in replacements for upstream models like ```dgl.nn.SAGECon```.\n```\n# Drop in replacement for dgl.nn.SAGEConv\nfrom dgl.nn import CuGraphSAGEConv as SAGEConv\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SAGE(nn.Module):\n    def __init__(self, in_size, hid_size, out_size):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        # three-layer GraphSAGE-mean\n        self.layers.append(SAGEConv(in_size, hid_size, \"mean\"))\n        self.layers.append(SAGEConv(hid_size, hid_size, \"mean\"))\n        self.layers.append(SAGEConv(hid_size, out_size, \"mean\"))\n        self.dropout = nn.Dropout(0.5)\n        self.hid_size = hid_size\n        self.out_size = out_size\n\n    def forward(self, blocks, x):\n        h = x\n        for l_id, (layer, block) in enumerate(zip(self.layers, blocks)):\n            h = layer(block, h)\n            if l_id != len(self.layers) - 1:\n                h = F.relu(h)\n                h = self.dropout(h)\n        return h\n\n\n# Create the model with given dimensions\nfeat_size = cugraph_g.ndata[\"feat\"][\"_N\"].shape[1]\nmodel = SAGE(feat_size, 256, dataset.num_classes).to(\"cuda\")\n```", "Train the model\nIn this step, you opt to use ```cugraph_dgl.dataloading.NeighborSampler``` and ```cugraph_dgl.dataloading.DataLoader```, replacing the conventional data loaders of upstream DGL.\n```\nimport torchmetrics.functional as MF\nimport tempfile\nimport torch\n\ndef train(g, model):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    features = g.ndata[\"feat\"][\"_N\"].to(\"cuda\")\n    labels = g.ndata[\"label\"][\"_N\"].to(\"cuda\")\n    train_nid = torch.tensor(range(g.num_nodes())).type(torch.int64)\n    temp_dir_name = tempfile.TemporaryDirectory().name\n\n    for epoch in range(10):\n        model.train()\n        sampler = cugraph_dgl.dataloading.NeighborSampler([10,10,10])\n        dataloader = cugraph_dgl.dataloading.DataLoader(g, train_nid, sampler,\n                                                batch_size=128,\n                                                shuffle=True,\n                                                drop_last=False,\n                                                num_workers=0,\n                                                sampling_output_dir=temp_dir_name)\n\n        total_loss = 0\n        \n        for step, (input_nodes, seeds, blocks) in enumerate((dataloader)):\n            batch_inputs = features[input_nodes]\n            batch_labels = labels[seeds]\n            batch_pred = model(blocks, batch_inputs)\n            loss = F.cross_entropy(batch_pred, batch_labels)\n            total_loss += loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        sampler = cugraph_dgl.dataloading.NeighborSampler([-1,-1,-1])\n        dataloader = cugraph_dgl.dataloading.DataLoader(g, train_nid, sampler,\n                                                batch_size=1024,\n                                                shuffle=False,\n                                                drop_last=False,\n                                                num_workers=0,\n                                                sampling_output_dir=temp_dir_name)\n\n\n        acc = evaluate(model, features, labels, dataloader)\n        print(\"Epoch {:05d} | Acc {:.4f} | Loss {:.4f} \".format(epoch, acc, total_loss))\n\n\ndef evaluate(model, features, labels, dataloader):\n    with torch.no_grad():\n        model.eval()\n        ys = []\n        y_hats = []\n        for it, (in_nodes, out_nodes, blocks) in enumerate(dataloader):\n            with torch.no_grad():\n                x = features[in_nodes]\n                ys.append(labels[out_nodes])\n                y_hats.append(model(blocks, x))\n        num_classes = y_hats[0].shape[1]\n        return MF.accuracy(\n            torch.cat(y_hats),\n            torch.cat(ys),\n            task=\"multiclass\",\n            num_classes=num_classes,\n        )\n\ntrain(cugraph_g, model)\n\nEpoch 00000 | Acc 0.3401 | Loss 39.3890 \nEpoch 00001 | Acc 0.7164 | Loss 27.8906 \nEpoch 00002 | Acc 0.7888 | Loss 16.9441 \nEpoch 00003 | Acc 0.8589 | Loss 12.5475 \nEpoch 00004 | Acc 0.8863 | Loss 9.9894 \nEpoch 00005 | Acc 0.8948 | Loss 9.0556 \nEpoch 00006 | Acc 0.9029 | Loss 7.3637 \nEpoch 00007 | Acc 0.9055 | Loss 7.2541 \nEpoch 00008 | Acc 0.9132 | Loss 6.6912 \nEpoch 00009 | Acc 0.9121 | Loss 7.0908\n```", "Conclusion\nBy combining the power of GPU-accelerated graph computations with the flexibility of DGL, cuGraph-DGL emerges as an invaluable tool for anyone dealing with graph data.\nThis post has only scratched the surface of what you can do with cuGraph-DGL. I encourage you to explore further, experiment with different GNN architectures, and discover how cuGraph-DGL can accelerate your graph-based, machine-learning tasks.\nRead Intro to Graph Neural Networks with cuGraph-PyG for details on how to implement GNNs in the cuGraph-PyG ecosystem"], "document_title": "Introduction to Graph Neural Networks with NVIDIA cuGraph-DGL", "document_url": "https://developer.nvidia.com/blog/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/", "document_date": "2023-08-31T16:00:00", "document_date_modified": "2023-12-05T18:55:58", "document_full_text": "Introduction to Graph Neural Networks with NVIDIA cuGraph-DGL\nGraph neural networks (GNNs) have emerged as a powerful tool for a variety of machine learning tasks on graph-structured data. These tasks range from node classification and link prediction to graph classification. They also cover a wide range of applications such as social network analysis, drug discovery in healthcare, fraud detection in financial services, and molecular chemistry.\nIn this post, I introduce how to use cuGraph-DGL, a GPU-accelerated library for graph computations. It extends Deep Graph Library (DGL), a popular framework for GNNs that enables large-scale applications.\nBasics of graph neural networks\nBefore I dive into cuGraph-DGL, I want to establish some basics. GNNs are a special kind of neural network designed to work with data structured as graphs. Unlike traditional neural networks that assume independence between samples, which doesn\u2019t fit well with graph data, GNNs effectively exploit the rich and complex interconnections within graph data.\nIn a nutshell, GNNs work by propagating and transforming node features across the graph structure in multiple steps, often referred to as layers (Figure 1). Each layer updates the features of each node based on its own features and the features of its neighbors.\nDiagram shows the initial graph on the left and the update graph on the right. Node features are respected by using information from its neighbors and graph structure.\nFigure 1. Schematic for the message passing layer (source: Distill )\nIn Figure 1, the first step \u201cprepares\u201d a message composed of information from an edge and its connected nodes and then \u201cpasses\u201d the message to the node. This process enables the model to learn high-level representations of nodes, edges, and the graph as a whole, which can be used for various downstream tasks like node classification, link prediction, and graph classification.\nFigure 2 shows how a 2-layer GNN is supposed to compute the output of node 5.\nGIF shows a two-layer GNN updating the seed node\u2019s embeddings. First, immediate neighbors\u2019 embeddings are refreshed using data from neighbors two hops away. Then, these updated embeddings are used to update the seed node\u2019s representation.\nFigure 2. Update of embeddings on a single node in a 2-layer GNN (source: DGL documentation )\nBottlenecks when handling large-scale graphs\nThe bottleneck in GNN sampling and training is the lack of an existing implementation that can scale to handle billions or even trillions of edges, a scale often seen in real-world graph problems. For example, if you\u2019re handling a graph with trillions of edges, you must be able to run DGL-based GNN workflows quickly.\nOne solution is to use RAPIDS, which already possesses the foundational elements capable of scaling to trillions of edges using GPUs.\nWhat is RAPIDS cuGraph?\ncuGraph is a part of the RAPIDS AI ecosystem, an open-source suite of software libraries for executing end-to-end data science and analytics pipelines entirely on GPUs. The cuGraph library provides a simple, flexible, and powerful API for graph analytics, enabling you to perform computations on graph data at scale and speed.\nWhat is DGL?\nDeep Graph Library (DGL) is a Python library designed to simplify the implementation of graph neural networks (GNNs) by providing intuitive interfaces and high-performance computation.\nDGL supports a broad array of graph operations and structures, enhancing the modeling of complex systems and relationships. It also integrates with popular deep learning frameworks like PyTorch and TensorFlow, fostering seamless development and deployment of GNNs.\nWhat is cuGraph-DGL?\ncuGraph-DGL is an extension of cuGraph that integrates with the Deep Graph Library (DGL) to leverage the power of GPUs to run DGL-based GNN workflows at unprecedented speed. This library is a collaborative effort between DGL developers and cuGraph developers.\nIn addition to cuGraph-DGL, cuGraph also provides the cugraph-ops library, which enables DGL users to get performance boosts using CuGraphSAGEConv, CuGraphGATConv, and CuGraphRelGraphConv in place of the default SAGEConv, GATConv, and RelGraphConv models. You can also import the SAGEConv, GATConv, and RelGraphConv models directly from the ```cugraph_dgl``` library.\nIn GNN sampling and training, the major challenge is the absence of an implementation that can manage real-world graph problems with billions or trillions of edges. To address this, use cuGraph-DGL, with its inherent capability to scale to trillions of edges using GPUs.\nSetting up cuGraph-DGL\nBefore you dive into the code, make sure that you have cuGraph and DGL installed in your Python environment. To install the cuGraph-DGL-enabled environment, run the following command:\n```\nconda install mamba -c conda-forge \n\nmamba create -n cugraph_dgl_23_06 -c pytorch -c dglteam/label/cu118 -c rapidsai-nightly -c nvidia -c conda-forge dgl cugraph-dgl=23.10 pylibcugraphops=23.10 cudatoolkit=11.8 torchmetrics ogb\n```\nImplementing a GNN with cuGraph-DGL\nWith your environment set up, put cuGraph-DGL into action and construct a simple GNN for node classification. Converting an existing DGL workflow to a cuGraph-DGL workflow has the following steps:\nUse cuGraph-ops models such as ```CuGraphSAGECon```, in place of the native DGL model ( ```SAGEConv``` ).\nCreate a ```CuGraphGraph``` object from a DGL graph.\nUse the ```cuGraph``` data loader in place of the native DGL Dataloader.\nUsing cugraph-dgl on a 3.2 billion-edge graph, we observed a 3x speedup when using eight GPUs for sampling and training, compared to a single GPU UVA DGL setup. Additionally, we saw a 2x speedup when using eight GPUs for sampling and one GPU for training.\nAn upcoming blog post will provide more details on the gains and scalability.\nCreate a cuGraph-DGL graph\nTo create a ```cugraph_dgl``` graph directly from a DGL graph, run the following code example.\n```\nimport dgl\nimport cugraph_dgl\n\ndataset = dgl.data.CoraGraphDataset()\ndgl_g = dataset[0]\n# Add self loops as cugraph\n# does not support isolated vertices yet \ndgl_g = dgl.add_self_loop(dgl_g)\ncugraph_g = cugraph_dgl.convert.cugraph_storage_from_heterograph(dgl_g, single_gpu=True)\n```\nFor more information about creating a ```cuGraph``` storage object, see CuGraphStorage.\nCreate a cuGraph-Ops-based model\nIn this step, the only modification to make is the importation of ```cugraph_ops```-based models. These models are drop-in replacements for upstream models like ```dgl.nn.SAGECon```.\n```\n# Drop in replacement for dgl.nn.SAGEConv\nfrom dgl.nn import CuGraphSAGEConv as SAGEConv\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SAGE(nn.Module):\n    def __init__(self, in_size, hid_size, out_size):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        # three-layer GraphSAGE-mean\n        self.layers.append(SAGEConv(in_size, hid_size, \"mean\"))\n        self.layers.append(SAGEConv(hid_size, hid_size, \"mean\"))\n        self.layers.append(SAGEConv(hid_size, out_size, \"mean\"))\n        self.dropout = nn.Dropout(0.5)\n        self.hid_size = hid_size\n        self.out_size = out_size\n\n    def forward(self, blocks, x):\n        h = x\n        for l_id, (layer, block) in enumerate(zip(self.layers, blocks)):\n            h = layer(block, h)\n            if l_id != len(self.layers) - 1:\n                h = F.relu(h)\n                h = self.dropout(h)\n        return h\n\n\n# Create the model with given dimensions\nfeat_size = cugraph_g.ndata[\"feat\"][\"_N\"].shape[1]\nmodel = SAGE(feat_size, 256, dataset.num_classes).to(\"cuda\")\n```\nTrain the model\nIn this step, you opt to use ```cugraph_dgl.dataloading.NeighborSampler``` and ```cugraph_dgl.dataloading.DataLoader```, replacing the conventional data loaders of upstream DGL.\n```\nimport torchmetrics.functional as MF\nimport tempfile\nimport torch\n\ndef train(g, model):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    features = g.ndata[\"feat\"][\"_N\"].to(\"cuda\")\n    labels = g.ndata[\"label\"][\"_N\"].to(\"cuda\")\n    train_nid = torch.tensor(range(g.num_nodes())).type(torch.int64)\n    temp_dir_name = tempfile.TemporaryDirectory().name\n\n    for epoch in range(10):\n        model.train()\n        sampler = cugraph_dgl.dataloading.NeighborSampler([10,10,10])\n        dataloader = cugraph_dgl.dataloading.DataLoader(g, train_nid, sampler,\n                                                batch_size=128,\n                                                shuffle=True,\n                                                drop_last=False,\n                                                num_workers=0,\n                                                sampling_output_dir=temp_dir_name)\n\n        total_loss = 0\n        \n        for step, (input_nodes, seeds, blocks) in enumerate((dataloader)):\n            batch_inputs = features[input_nodes]\n            batch_labels = labels[seeds]\n            batch_pred = model(blocks, batch_inputs)\n            loss = F.cross_entropy(batch_pred, batch_labels)\n            total_loss += loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        sampler = cugraph_dgl.dataloading.NeighborSampler([-1,-1,-1])\n        dataloader = cugraph_dgl.dataloading.DataLoader(g, train_nid, sampler,\n                                                batch_size=1024,\n                                                shuffle=False,\n                                                drop_last=False,\n                                                num_workers=0,\n                                                sampling_output_dir=temp_dir_name)\n\n\n        acc = evaluate(model, features, labels, dataloader)\n        print(\"Epoch {:05d} | Acc {:.4f} | Loss {:.4f} \".format(epoch, acc, total_loss))\n\n\ndef evaluate(model, features, labels, dataloader):\n    with torch.no_grad():\n        model.eval()\n        ys = []\n        y_hats = []\n        for it, (in_nodes, out_nodes, blocks) in enumerate(dataloader):\n            with torch.no_grad():\n                x = features[in_nodes]\n                ys.append(labels[out_nodes])\n                y_hats.append(model(blocks, x))\n        num_classes = y_hats[0].shape[1]\n        return MF.accuracy(\n            torch.cat(y_hats),\n            torch.cat(ys),\n            task=\"multiclass\",\n            num_classes=num_classes,\n        )\n\ntrain(cugraph_g, model)\n\nEpoch 00000 | Acc 0.3401 | Loss 39.3890 \nEpoch 00001 | Acc 0.7164 | Loss 27.8906 \nEpoch 00002 | Acc 0.7888 | Loss 16.9441 \nEpoch 00003 | Acc 0.8589 | Loss 12.5475 \nEpoch 00004 | Acc 0.8863 | Loss 9.9894 \nEpoch 00005 | Acc 0.8948 | Loss 9.0556 \nEpoch 00006 | Acc 0.9029 | Loss 7.3637 \nEpoch 00007 | Acc 0.9055 | Loss 7.2541 \nEpoch 00008 | Acc 0.9132 | Loss 6.6912 \nEpoch 00009 | Acc 0.9121 | Loss 7.0908\n```\nConclusion\nBy combining the power of GPU-accelerated graph computations with the flexibility of DGL, cuGraph-DGL emerges as an invaluable tool for anyone dealing with graph data.\nThis post has only scratched the surface of what you can do with cuGraph-DGL. I encourage you to explore further, experiment with different GNN architectures, and discover how cuGraph-DGL can accelerate your graph-based, machine-learning tasks.\nRead Intro to Graph Neural Networks with cuGraph-PyG for details on how to implement GNNs in the cuGraph-PyG ecosystem"}], "https://developer.nvidia.com/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/": [{"text": "The article discusses deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA, focusing on Quantization-Aware Training (QAT) for maximizing deep learning performance. YOLOv5 is an object detection algorithm known for its balance between accuracy and speed. The post provides a detailed technical guide on training a YOLOv5 model with QAT, converting it to a Post-Training Quantization (PTQ) model for deployment on DLA, and running inference using CUDA through TensorRT and cuDLA. The article explores different options for adding Quantization/Dequantization (Q/DQ) nodes to optimize model performance on DLA. It also includes information on on-target validation and profiling to ensure accuracy and performance. The article highlights the benefits of DLA cores on Orin AGX platforms and provides insights into performance improvements with the latest DLA compiler release. Developers are encouraged to explore the YOLOv5 sample provided to replicate the workflow and leverage DLA for efficient object detection tasks. Additional resources and samples are available on GitHub for further exploration of DLA capabilities.", "text_components": ["Deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA: Quantization-Aware Training to Inference\nNVIDIA Jetson Orin is the best-in-class embedded platform for AI workloads. One of the key components of the Orin platform is the second-generation Deep Learning Accelerator (DLA), the dedicated deep learning inference engine that offers one-third of the AI compute on the AGX Orin platforms.\nThis post is a deep technical dive into how embedded developers working with Orin platforms can deploy deep neural networks (DNNs) using YOLOv5 as a reference. To learn more about how DLA can help maximize the performance of your deep learning applications, see Maximizing Deep Learning Performance on NVIDIA Jetson Orin with DLA.\nYOLOv5 is an object detection algorithm. Building on the success of v3 and v4, YOLOv5 aims to provide improved accuracy and speed in real-time object detection tasks. YOLOv5 has gained notoriety due to its excellent trade-off between accuracy and speed, making it a popular choice among researchers and practitioners in the field of computer vision. Its open-source implementation enables developers to leverage pretrained models and customize them according to specific goals.\nThe following sections walk through an end-to-end YOLOv5 cuDLA sample that shows you how to:\nTrain a YOLOv5 model with Quantization-Aware Training (QAT) and export it for deployment on DLA.\nDeploy the network and run inference using CUDA through TensorRT and cuDLA.\nExecute on-target YOLOv5 accuracy validation and performance profiling.\nUsing this sample, we demonstrate how to achieve 37.3 mAP on the COCO dataset with DLA INT8 (official FP32 mAP is 37.4). We also show how to obtain over 400 FPS for YOLOv5 on a single NVIDIA Jetson Orin DLA. (A total of two DLA instances are available on Orin.)", "QAT training and export for DLA\nTo balance the inference performance and accuracy of YOLOv5, it\u2019s essential to apply Quantization-Aware-Training (QAT) on the model. Because DLA does not support QAT through TensorRT at the time of writing, it\u2019s necessary to convert the QAT model to a Post-Training Quantization (PTQ) model before inference. The steps are outlined in Figure 1.\nDiagram of the key steps in taking a QAT model and converting it a model with post training quantization scales.\nFigure 1. Key steps involved in converting a QAT model to a PTQ model", "QAT training workflow\nUse the TensorRT pytorch-quantization toolkit to quantize YOLOv5. The first step is to add quantizer modules to the neural network graph. This toolkit provides a set of quantized layer modules for common DL operations. If a module is not among the provided quantized modules, you can create a custom quantization module for the right place in the model.\nThe second step is to calibrate the model, obtaining the scale values for each Quantization/Dequantization (Q/DQ) module. After the calibration is complete, select a training schedule and fine-tune the calibrated model using the COCO dataset.\nDiagram of the steps involved in quantization aware training of a model, from left to right: pretrained model. insert Q/DA node, calibration, fine-tuning with data, quantized model; below: training data.\nFigure 2. Steps of the QAT training workflow", "Adding Q/DQ nodes\nThere are two options for adding Q/DQ nodes to your network:\nOption 1: Place Q/DQ nodes, as recommended, in TensorRT Processing of Q/DQ Networks. This method follows TensorRT fusion strategy for Q/DQ layers. These TensorRT strategies are mostly tuned for GPU inference. To make this compatible with DLA, add additional Q/DQ nodes, which can be derived using the scales from their neighboring layers with the Q/DQ Translator.\nAny missing scales would otherwise result in certain layers running in FP16. This may result in a slight decrease in mAP and possibly a large performance drop. The Orin DLA is optimized for INT8 convolutions, about 15x over FP16 dense performance (or 30x when comparing dense FP16 to INT8 sparse performance).\nOption 2: Insert Q/DQ nodes at every layer to make sure all tensors have INT8 scales. With this option, all layers\u2019 scales can be obtained during model fine-tuning. However, this method may potentially disrupt TensorRT fusion strategy with Q/DQ layers when running inference on GPU and lead to higher latency on the GPU. For DLA, on the other hand, the rule of thumb with PTQ scales is, \u201cThe more available scales, the lower the latency.\u201d\nAs confirmed by experiment, our YOLOv5 model was verified on the COCO 2017 validation dataset with a resolution of 672 x 672 pixels. Option 1 and Option 2, respectively, achieved mAP scores of 37.1 and 37.0.\nChoose the best option based on your needs. If you already have an existing QAT workflow for GPU and would like to preserve it as much as possible, Option 1 is probably better. (You may need to extend Q/DQ Translator to infer more missing scales to achieve optimal DLA latency as well.)\nOn the other hand, if you are looking for a QAT training method that inserts Q/DQ nodes into all layers and is compatible with DLA, Option 2 may be your most promising.", "Q/DQ Translator workflow\nThe purpose of the Q/DQ Translator is to translate an ONNX graph trained with QAT, to PTQ tensor scales and an ONNX model without Q/DQ nodes.\nFor this YOLOv5 model, extract quantization scales from Q/DQ nodes in the QAT model. Use the information of neighboring layers to infer the input/output scales of other layers such as Sigmoid and Mul in YOLOv5\u2019s SiLU or for Concat nodes. After scales are extracted, export the ONNX model without Q/DQ nodes and the (PTQ) calibration cache file such that TensorRT can use them to build a DLA engine.", "Deploying network to DLA for inference\nThe next step is to deploy the network and run inference using CUDA through TensorRT and cuDLA.", "Loadable build with TensorRT\nUse TensorRT to build the DLA loadable. This provides an easy-to-use interface for DLA loadable building and seamless integration with GPU if needed. For more information about TensorRT-DLA, see Working with DLA in the TensorRT Developer Guide.\ntrtexec is a convenient tool provided by TensorRT for building engines and benchmarking performance. Note that a DLA loadable is the result of successful DLA compilation through the DLA Compiler, and that TensorRT can package DLA loadables inside of serialized engines.\nFirst, prepare the ONNX model and the calibration cache generated in the previous section. The DLA loadable can be built with a single command. Pass the ```--safe``` option and the entire model can run on DLA. This directly saves the compilation result as a serialized DLA loadable (without a TensorRT engine wrapping around it). For more details about this step, see the NVIDIA Deep Learning TensorRT Documentation.\ntrtexec --onnx=model.onnx --useDLACore=0 --safe --saveEngine=model.loadable --inputIOFormats=int8:dla_hwc4 --outputIOFormats=fp16:chw16 --int8 --fp16 --calib=qat2ptq.cache Note that the input format ```dla_hwc4``` is highly recommended from a performance point of view, if your model input qualifies. The input must have at most four input channels and be consumed by a convolution. In INT8, DLA can benefit from a specific hardware and software optimization that is not available if you use ```--inputIOFormats=int8:chw32``` instead, for example.", "Running inference using cuDLA\ncuDLA is the CUDA runtime interface for DLA, an extension of the CUDA programming model that integrates DLA with CUDA. cuDLA enables you to submit DLA tasks using CUDA programming constructs. You can run inference using cuDLA either implicitly through TensorRT runtime or you can explicitly call the cuDLA APIs. This sample demonstrates the latter approach to explicitly call cuDLA APIs to run inference in hybrid mode and standalone mode.\ncuDLA hybrid mode and standalone mode mainly differ in synchronization. In hybrid mode, DLA tasks are submitted to a CUDA stream, so synchronization can be done seamlessly with other CUDA tasks.\nIn standalone mode, the ```cudlaTask``` structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively, as part of ```cudlaSubmitTask```.\nIn short, using cuDLA hybrid mode can give quick integration with other CUDA tasks. Using cuDLA standalone mode can prevent the creation of CUDA context, and thus can save resources if the pipeline has no CUDA context.\nThe primary cuDLA APIs used in this YOLOv5 sample are detailed below.\n```cudlaCreateDevice``` creates the DLA device.\n```cudlaModuleLoadFromMemory``` loads the engine memory for DLA use.\n```cudaMalloc``` and ```cudlaMemRegister``` are called to first allocate memory on GPU, then let the CUDA pointer be registered with the DLA. (Used only for hybrid mode.)\n```cudlaImportExternalMemory``` and ```cudlaImportExternalSemaphore``` are called to import external NvSci buffers and sync objects. (Used only for standalone mode.)\n```cudlaModuleGetAttributes``` gets module attributes from the loaded module.\n```cudlaSubmitTask``` is called to submit the inference task. In hybrid mode, users need to specify the CUDA stream to let cuDLA tasks run on it. In standalone mode, users need to specify the signal event and wait event to let cuDLA wait and signal when the corresponding fence expires.", "On-target validation and profiling\nIt\u2019s important to note the numerical differences between GPU to DLA. The underlying hardware is different, so the computations are not bit-wise accurate. Because training the network is done on the GPU and then deployed to DLA on the target, it\u2019s important to validate on the target. This specifically comes into play when it comes to quantization. It\u2019s also important to compare against a reference baseline.", "YOLOv5 DLA accuracy validation\nWe used the COCO dataset to validate. Figure 3 shows the inference pipeline architecture. First, load the image data and normalize it. Extra reformats on the inference inputs and outputs are needed because DLA only supports INT8/FP16.\nAfter inference, decode the inference result and perform NMS (non-maximum suppression) to get the detection result. Finally, save the result and compute mAP.\nSchematic showing the entire pipeline with tasks mapped to DLA, GPU, and CPU.\nFigure 3. Inference pipeline with tasks mapped to the different compute engines\nIn the case of YOLOv5, the feature maps of the last three convolution layers encode final detection information. When quantized to INT8, the quantization error of the bounding box coordinates becomes noticeable compared to FP16/FP32, thus affecting the final mAP.\nOur experiment shows that running the last three convolution layers in FP16 improves the final mAP from 35.9 to 37.1. Orin DLA has a special hardware design highly optimized for INT8, so we observe a performance drop when these three convolutions run in FP16.\nDiagram showing different mixed precision configurations of the last three convolution layers of the YOLOv5 engine.\nFigure 4. YOLOv5 engine with different precision configurations Configuration 1\nConfiguration 2\nConfiguration 3\nInput tensor format\nINT8:DLA_HWC4\nINT8:DLA_HWC4\nINT8:DLA_HWC4\nOutput tensor format\nINT8:CHW32\nFP16:CHW16\nFP16:CHW16\nCOCO Val mAP\n35.9\n37.1\n37.3\nFPS (DLA 3.14.0, 1x DLA @ 1.33 GHz, EMC @ 3.2 GHz)\n410\n255\n252\nTable 1. Configurations exploring mixed precision for the last three convolution layers Note that the mAP results are based on Option 1 described in the preceding section on adding Q/DQ nodes. You can apply the same principle to Option 2 as well.", "YOLOv5 DLA performance\nDLA offers one-third of AI compute on Orin AGX platforms, thanks to the two DLA cores. For a general baseline of Orin DLA performance, see Deep-Learning-Accelerator-SW on GitHub.\nIn the latest release, DLA 3.14.0 (DOS 6.0.8.0 and JetPack 6.0), several performance optimizations were added to the DLA compiler that specifically apply for INT8 CNN architecture-based models:\nNative INT8 Sigmoid (previously ran in FP16 and had to be cast to and from INT8; also applies to Tanh)\nINT8 SiLU fusion into a single DLA HW operation (instead of standalone Sigmoid plus standalone elementwise Mul)\nFusing the INT8 SiLU HW op with the previous INT8 Conv HW op (also applies to standalone Sigmoid or Tanh)\nThese improvements can provide a 6x speedup for YOLO architectures compared to prior releases. For instance, in the case of YOLOv5, the inference performance jumped from 13 ms to 2.4 ms in INT8 (with a few layers running in FP16), which is a 5.4x improvement. Further, you can use the cuDLA sample to profile your DNN layer-wise, identify bottlenecks, and modify your network to improve its performance.", "Get started with DLA\nThis post explains how to run an entire object detection pipeline on Orin in the most efficient way using YOLOv5 on its dedicated Deep Learning Accelerator. Keep in mind that other SoC components such as the GPU are either idling or running at very small load. If you had a single camera producing inputs at 30 fps, one DLA instance would only be loaded at about 10%. So there is plenty of headroom for adding more bells and whistles to your application.\nReady to dive in? The YOLOv5 sample replicates the entire workflow discussed here. You can use it as a reference point for your own use case.\nFor beginners, the Jetson_dla_tutorial on GitHub demonstrates a basic DLA workflow to help you get started deploying a simple model to DLA.\nFor additional samples and resources on leveraging DLA to get the most out of NVIDIA DRIVE or NVIDIA Jetson, visit Deep-Learning-Accelerator-SW on GitHub. For more information about cuDLA, visit Deep-Learning-Accelerator-SW/samples/cuDLA."], "document_title": "Deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA: Quantization-Aware Training to Inference", "document_url": "https://developer.nvidia.com/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/", "document_date": "2023-08-31T17:00:00", "document_date_modified": "2023-09-07T18:38:16", "document_full_text": "Deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA: Quantization-Aware Training to Inference\nNVIDIA Jetson Orin is the best-in-class embedded platform for AI workloads. One of the key components of the Orin platform is the second-generation Deep Learning Accelerator (DLA), the dedicated deep learning inference engine that offers one-third of the AI compute on the AGX Orin platforms.\nThis post is a deep technical dive into how embedded developers working with Orin platforms can deploy deep neural networks (DNNs) using YOLOv5 as a reference. To learn more about how DLA can help maximize the performance of your deep learning applications, see Maximizing Deep Learning Performance on NVIDIA Jetson Orin with DLA.\nYOLOv5 is an object detection algorithm. Building on the success of v3 and v4, YOLOv5 aims to provide improved accuracy and speed in real-time object detection tasks. YOLOv5 has gained notoriety due to its excellent trade-off between accuracy and speed, making it a popular choice among researchers and practitioners in the field of computer vision. Its open-source implementation enables developers to leverage pretrained models and customize them according to specific goals.\nThe following sections walk through an end-to-end YOLOv5 cuDLA sample that shows you how to:\nTrain a YOLOv5 model with Quantization-Aware Training (QAT) and export it for deployment on DLA.\nDeploy the network and run inference using CUDA through TensorRT and cuDLA.\nExecute on-target YOLOv5 accuracy validation and performance profiling.\nUsing this sample, we demonstrate how to achieve 37.3 mAP on the COCO dataset with DLA INT8 (official FP32 mAP is 37.4). We also show how to obtain over 400 FPS for YOLOv5 on a single NVIDIA Jetson Orin DLA. (A total of two DLA instances are available on Orin.)\nQAT training and export for DLA\nTo balance the inference performance and accuracy of YOLOv5, it\u2019s essential to apply Quantization-Aware-Training (QAT) on the model. Because DLA does not support QAT through TensorRT at the time of writing, it\u2019s necessary to convert the QAT model to a Post-Training Quantization (PTQ) model before inference. The steps are outlined in Figure 1.\nDiagram of the key steps in taking a QAT model and converting it a model with post training quantization scales.\nFigure 1. Key steps involved in converting a QAT model to a PTQ model\nQAT training workflow\nUse the TensorRT pytorch-quantization toolkit to quantize YOLOv5. The first step is to add quantizer modules to the neural network graph. This toolkit provides a set of quantized layer modules for common DL operations. If a module is not among the provided quantized modules, you can create a custom quantization module for the right place in the model.\nThe second step is to calibrate the model, obtaining the scale values for each Quantization/Dequantization (Q/DQ) module. After the calibration is complete, select a training schedule and fine-tune the calibrated model using the COCO dataset.\nDiagram of the steps involved in quantization aware training of a model, from left to right: pretrained model. insert Q/DA node, calibration, fine-tuning with data, quantized model; below: training data.\nFigure 2. Steps of the QAT training workflow\nAdding Q/DQ nodes\nThere are two options for adding Q/DQ nodes to your network:\nOption 1: Place Q/DQ nodes, as recommended, in TensorRT Processing of Q/DQ Networks. This method follows TensorRT fusion strategy for Q/DQ layers. These TensorRT strategies are mostly tuned for GPU inference. To make this compatible with DLA, add additional Q/DQ nodes, which can be derived using the scales from their neighboring layers with the Q/DQ Translator.\nAny missing scales would otherwise result in certain layers running in FP16. This may result in a slight decrease in mAP and possibly a large performance drop. The Orin DLA is optimized for INT8 convolutions, about 15x over FP16 dense performance (or 30x when comparing dense FP16 to INT8 sparse performance).\nOption 2: Insert Q/DQ nodes at every layer to make sure all tensors have INT8 scales. With this option, all layers\u2019 scales can be obtained during model fine-tuning. However, this method may potentially disrupt TensorRT fusion strategy with Q/DQ layers when running inference on GPU and lead to higher latency on the GPU. For DLA, on the other hand, the rule of thumb with PTQ scales is, \u201cThe more available scales, the lower the latency.\u201d\nAs confirmed by experiment, our YOLOv5 model was verified on the COCO 2017 validation dataset with a resolution of 672 x 672 pixels. Option 1 and Option 2, respectively, achieved mAP scores of 37.1 and 37.0.\nChoose the best option based on your needs. If you already have an existing QAT workflow for GPU and would like to preserve it as much as possible, Option 1 is probably better. (You may need to extend Q/DQ Translator to infer more missing scales to achieve optimal DLA latency as well.)\nOn the other hand, if you are looking for a QAT training method that inserts Q/DQ nodes into all layers and is compatible with DLA, Option 2 may be your most promising.\nQ/DQ Translator workflow\nThe purpose of the Q/DQ Translator is to translate an ONNX graph trained with QAT, to PTQ tensor scales and an ONNX model without Q/DQ nodes.\nFor this YOLOv5 model, extract quantization scales from Q/DQ nodes in the QAT model. Use the information of neighboring layers to infer the input/output scales of other layers such as Sigmoid and Mul in YOLOv5\u2019s SiLU or for Concat nodes. After scales are extracted, export the ONNX model without Q/DQ nodes and the (PTQ) calibration cache file such that TensorRT can use them to build a DLA engine.\nDeploying network to DLA for inference\nThe next step is to deploy the network and run inference using CUDA through TensorRT and cuDLA.\nLoadable build with TensorRT\nUse TensorRT to build the DLA loadable. This provides an easy-to-use interface for DLA loadable building and seamless integration with GPU if needed. For more information about TensorRT-DLA, see Working with DLA in the TensorRT Developer Guide.\ntrtexec is a convenient tool provided by TensorRT for building engines and benchmarking performance. Note that a DLA loadable is the result of successful DLA compilation through the DLA Compiler, and that TensorRT can package DLA loadables inside of serialized engines.\nFirst, prepare the ONNX model and the calibration cache generated in the previous section. The DLA loadable can be built with a single command. Pass the ```--safe``` option and the entire model can run on DLA. This directly saves the compilation result as a serialized DLA loadable (without a TensorRT engine wrapping around it). For more details about this step, see the NVIDIA Deep Learning TensorRT Documentation.\ntrtexec --onnx=model.onnx --useDLACore=0 --safe --saveEngine=model.loadable --inputIOFormats=int8:dla_hwc4 --outputIOFormats=fp16:chw16 --int8 --fp16 --calib=qat2ptq.cache Note that the input format ```dla_hwc4``` is highly recommended from a performance point of view, if your model input qualifies. The input must have at most four input channels and be consumed by a convolution. In INT8, DLA can benefit from a specific hardware and software optimization that is not available if you use ```--inputIOFormats=int8:chw32``` instead, for example.\nRunning inference using cuDLA\ncuDLA is the CUDA runtime interface for DLA, an extension of the CUDA programming model that integrates DLA with CUDA. cuDLA enables you to submit DLA tasks using CUDA programming constructs. You can run inference using cuDLA either implicitly through TensorRT runtime or you can explicitly call the cuDLA APIs. This sample demonstrates the latter approach to explicitly call cuDLA APIs to run inference in hybrid mode and standalone mode.\ncuDLA hybrid mode and standalone mode mainly differ in synchronization. In hybrid mode, DLA tasks are submitted to a CUDA stream, so synchronization can be done seamlessly with other CUDA tasks.\nIn standalone mode, the ```cudlaTask``` structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively, as part of ```cudlaSubmitTask```.\nIn short, using cuDLA hybrid mode can give quick integration with other CUDA tasks. Using cuDLA standalone mode can prevent the creation of CUDA context, and thus can save resources if the pipeline has no CUDA context.\nThe primary cuDLA APIs used in this YOLOv5 sample are detailed below.\n```cudlaCreateDevice``` creates the DLA device.\n```cudlaModuleLoadFromMemory``` loads the engine memory for DLA use.\n```cudaMalloc``` and ```cudlaMemRegister``` are called to first allocate memory on GPU, then let the CUDA pointer be registered with the DLA. (Used only for hybrid mode.)\n```cudlaImportExternalMemory``` and ```cudlaImportExternalSemaphore``` are called to import external NvSci buffers and sync objects. (Used only for standalone mode.)\n```cudlaModuleGetAttributes``` gets module attributes from the loaded module.\n```cudlaSubmitTask``` is called to submit the inference task. In hybrid mode, users need to specify the CUDA stream to let cuDLA tasks run on it. In standalone mode, users need to specify the signal event and wait event to let cuDLA wait and signal when the corresponding fence expires.\nOn-target validation and profiling\nIt\u2019s important to note the numerical differences between GPU to DLA. The underlying hardware is different, so the computations are not bit-wise accurate. Because training the network is done on the GPU and then deployed to DLA on the target, it\u2019s important to validate on the target. This specifically comes into play when it comes to quantization. It\u2019s also important to compare against a reference baseline.\nYOLOv5 DLA accuracy validation\nWe used the COCO dataset to validate. Figure 3 shows the inference pipeline architecture. First, load the image data and normalize it. Extra reformats on the inference inputs and outputs are needed because DLA only supports INT8/FP16.\nAfter inference, decode the inference result and perform NMS (non-maximum suppression) to get the detection result. Finally, save the result and compute mAP.\nSchematic showing the entire pipeline with tasks mapped to DLA, GPU, and CPU.\nFigure 3. Inference pipeline with tasks mapped to the different compute engines\nIn the case of YOLOv5, the feature maps of the last three convolution layers encode final detection information. When quantized to INT8, the quantization error of the bounding box coordinates becomes noticeable compared to FP16/FP32, thus affecting the final mAP.\nOur experiment shows that running the last three convolution layers in FP16 improves the final mAP from 35.9 to 37.1. Orin DLA has a special hardware design highly optimized for INT8, so we observe a performance drop when these three convolutions run in FP16.\nDiagram showing different mixed precision configurations of the last three convolution layers of the YOLOv5 engine.\nFigure 4. YOLOv5 engine with different precision configurations Configuration 1\nConfiguration 2\nConfiguration 3\nInput tensor format\nINT8:DLA_HWC4\nINT8:DLA_HWC4\nINT8:DLA_HWC4\nOutput tensor format\nINT8:CHW32\nFP16:CHW16\nFP16:CHW16\nCOCO Val mAP\n35.9\n37.1\n37.3\nFPS (DLA 3.14.0, 1x DLA @ 1.33 GHz, EMC @ 3.2 GHz)\n410\n255\n252\nTable 1. Configurations exploring mixed precision for the last three convolution layers Note that the mAP results are based on Option 1 described in the preceding section on adding Q/DQ nodes. You can apply the same principle to Option 2 as well.\nYOLOv5 DLA performance\nDLA offers one-third of AI compute on Orin AGX platforms, thanks to the two DLA cores. For a general baseline of Orin DLA performance, see Deep-Learning-Accelerator-SW on GitHub.\nIn the latest release, DLA 3.14.0 (DOS 6.0.8.0 and JetPack 6.0), several performance optimizations were added to the DLA compiler that specifically apply for INT8 CNN architecture-based models:\nNative INT8 Sigmoid (previously ran in FP16 and had to be cast to and from INT8; also applies to Tanh)\nINT8 SiLU fusion into a single DLA HW operation (instead of standalone Sigmoid plus standalone elementwise Mul)\nFusing the INT8 SiLU HW op with the previous INT8 Conv HW op (also applies to standalone Sigmoid or Tanh)\nThese improvements can provide a 6x speedup for YOLO architectures compared to prior releases. For instance, in the case of YOLOv5, the inference performance jumped from 13 ms to 2.4 ms in INT8 (with a few layers running in FP16), which is a 5.4x improvement. Further, you can use the cuDLA sample to profile your DNN layer-wise, identify bottlenecks, and modify your network to improve its performance.\nGet started with DLA\nThis post explains how to run an entire object detection pipeline on Orin in the most efficient way using YOLOv5 on its dedicated Deep Learning Accelerator. Keep in mind that other SoC components such as the GPU are either idling or running at very small load. If you had a single camera producing inputs at 30 fps, one DLA instance would only be loaded at about 10%. So there is plenty of headroom for adding more bells and whistles to your application.\nReady to dive in? The YOLOv5 sample replicates the entire workflow discussed here. You can use it as a reference point for your own use case.\nFor beginners, the Jetson_dla_tutorial on GitHub demonstrates a basic DLA workflow to help you get started deploying a simple model to DLA.\nFor additional samples and resources on leveraging DLA to get the most out of NVIDIA DRIVE or NVIDIA Jetson, visit Deep-Learning-Accelerator-SW on GitHub. For more information about cuDLA, visit Deep-Learning-Accelerator-SW/samples/cuDLA."}], "https://developer.nvidia.com/blog/how-to-build-a-distributed-inference-cache-with-nvidia-triton-and-redis/": [{"text": "Caching is a crucial aspect of computing that helps improve performance and efficiency by storing frequently accessed data. NVIDIA Triton Inference Server has implemented a local cache pattern to improve inference speed. Recently, the Triton team integrated a Redis cache for even better performance. Redis, a key-value database, offers fast and efficient caching, making it ideal for use with Triton. Distributed caching with Redis allows for horizontal scalability and high availability, unlike the local cache which is limited to the resources of a single machine. Performance tests showed that Redis provided comparable throughput and latency to the local cache, with some variations depending on the model being used. Managing trade-offs between caching solutions is essential based on the computational intensity of queries and the size of output tensors. Ultimately, leveraging distributed caching with Redis can significantly enhance the performance and efficiency of NVIDIA Triton Inference Server.", "text_components": ["How to Build a Distributed Inference Cache with NVIDIA Triton and Redis\nCaching is as fundamental to computing as arrays, symbols, or strings. Various layers of caching throughout the stack hold instructions from memory while pending on your CPU. They enable you to reload the page quickly and without re-authenticating, should you navigate away. They also dramatically decrease application workloads, and increase throughput by not re-running the same queries repeatedly.\nCaching is not new to NVIDIA Triton Inference Server, which is a system tuned to answering questions in the form of running inferences on tensors. Running inferences is a relatively computationally expensive task that often calls on the same inference to run repeatedly. This naturally lends itself to using a caching pattern.\nThe NVIDIA Triton team recently implemented the Triton response cache using the Triton local cache library. They have also built a cache API to make this caching pattern extensible within Triton. The Redis team then leveraged that API to build the Redis cache for NVIDIA Triton.\nIn this post, the Redis team explores the benefits of the new Redis implementation of the Triton Caching API. We explore how to get started and discuss some of the best practices for using Redis to supercharge your NVIDIA Triton instance.", "What is Redis?\nRedis is an acronym for REmote DIctionary Server. It is a NoSQL database that operates as a key-value data structure store. Redis is memory-first, meaning that the entire dataset in Redis is stored in memory, and optionally persisted to disk, based on configuration. Because it is a key-value database completely held in memory, Redis is blazingly fast. Execution times are measured in microseconds, and throughputs in tens of thousands of operations a second.\nThe remarkable speed and typical access pattern of Redis make it ideal for caching. Redis is synonymous with caching and is consequentially one of the built-in distributed caches of most major application frameworks across a variety of developer communities.", "What is local cache?\nThe local cache is an in-memory derivation of the most common caching pattern out there (cache-aside). It is simple and efficient, making it easy to grasp and implement. After receiving a query, NVIDIA Triton:\nComputes a hash of the input query, including the tensor and some metadata. This becomes the inference key.\nChecks for a previously inferred result for that tensor at that key.\nReturns any results found.\nPerforms the inference if no results are found.\nCaches the inference in memory using the key for storage.\nReturns the inference.\n\u2018Local\u2019 means that it is staying local to the process and storing the cache in the system\u2019s main memory. Figure 1 shows the implementation of this pattern.\nDiagram showing how the local cache works in NVIDIA Triton. The server checks the cache; if it has the inference, return the inference. If not, run the tensor through the model, cache the outputted inference, and return the outputted inference.\nFigure 1. NVIDIA Triton using the local cache", "Benefits of local cache\nThere are a variety of benefits that flow naturally from using this pattern. Because the queries are cached, they can be retrieved again easily without rerunning the tensor through the models. Because everything is maintained locally in the process memory, there is no need to leave the process or machine to retrieve the cached data. These two in concert can dramatically increase throughput, as well as decrease the cost of this computation.", "Drawbacks of local cache\nThis technique does have drawbacks. Because the cache is tied directly into the process memory, each time the Triton process restarts, it starts from square one (generally referred to as a cold start). You will not see the benefits from caching while the cache warms up. Also, because the cache is process-locked, other instances of Triton will not be able to share the cache, leading to duplication of caching across each node.\nThe other major drawback concerns resource contention. Since the local cache is tied to the process, it is limited to the resources of the system that Triton runs on. This means that it is impossible to horizontally scale the resources allocated to the cache (distributing the cache across multiple machines), which limits the options for expanding the local cache to vertical scaling. This makes the server running Triton bigger.", "Benefits of distributed caching with Redis\nUnlike local caching, distributed caching leverages an external service (such as Redis) to distribute the cache off the local server. This confers several advantages to the NVIDIA Triton caching API:\nRedis is not bound to the available system resources of the same machine as Triton, or for that matter, a single machine.\nRedis is decoupled from Triton\u2019s process life cycle, enabling multiple Triton instances to leverage the same cache.\nRedis is extremely fast (execution times are typically sub-milliseconds).\nRedis is a significantly more specialized, feature-rich, and tunable caching service compared to the Triton local cache.\nRedis provides immediate access to tried and tested high availability, horizontal scaling, and cache-eviction features out of the box.\nDistributed caching with Redis works much the same way as the local cache. Rather than staying within the same process, it crosses out of the Triton server process to Redis to check the cache and store inferences. After receiving a query, NVIDIA Triton:\nComputes a hash of the input query, including the tensor and some metadata. This becomes the inference key.\nChecks Redis for a previous run inference.\nReturns that inference, if it exists.\nRuns the tensor through Triton if the inference does not exist.\nStores the inference in Redis.\nReturns the inference.\nArchitecturally, this is shown in Figure 2.\nDiagram showing how the Triton Inference Server uses Redis as a cache. Similar to how it uses the local cache, but reaching out to the external service Redis for caching.\nFigure 2. NVIDIA Triton using Redis as its caching layer", "Distributed cache set up and configuration\nTo set up the distributed Redis cache requires two top-level steps:\nDeploy your Redis instance.\nConfigure NVIDIA Triton to point at the Redis instance.\nTriton will take care of the rest for you. To learn more about Redis, see redis.io, docs.redis.com, and Redis University.\nTo configure Triton to point at your Redis instance, use the ```--cache-config``` options in your start command. In the model config, enable the response cache for the model with ```{{response_cache { enable: true }}}```.\ntritonserver --cache-config redis,host=localhost --cache-config redis,port=6379 The Redis cache calls on you to minimally configure the host and port of your Redis instance. For a full enumeration of configuration options, see the Triton Redis Cache GitHub repo.", "Best practices with Redis\nRedis is lightweight, easy to use, and extremely fast. Even with its small footprint and simplicity, there is much you can configure in and around Redis to optimize it for your use case. This section highlights best practices for using and configuring Redis.", "Minimize round-trip time\nThe only real drawback of using an external service like Redis over an in-process memory cache is that the queries to Redis will, at least, have to cross process. They typically need to cross server boundaries as well.\nBecause of this, minimizing round-trip times (RTT) is of paramount importance in optimizing the use of Redis as a cache. The topic of how to minimize RTT is far too complex a topic to dive into in this post. A couple of key tips: maintain the locality of your Redis servers to your Triton servers and have them physically close to each other. If they are in a data center, try to keep them in the same rack or availability zone.", "Scaling and high availability\nRedis Cluster enables you to scale your Redis instances horizontally over multiple shards. The cluster includes the ability to replicate your Redis instance. If there is a failure in your primary shard, the replica can be promoted for high availability.", "Maximum memory and eviction\nIf Redis memory is not capped, it will use all the available memory on the system that the OS will release to it. Set the maxmemory configuration key in redis.conf. But what happens if you set maxmemory and Redis runs out of memory? The default is, as you might expect, to stop accepting new writes to Redis.\nHowever, you can also set an eviction policy. An eviction policy uses some basic intelligence to decide which keys might be good candidates to kick out of Redis. Allowing Redis to evict keys that no longer make sense to store enables it to continue accepting new writes without interruption when the memory fills.\nFor a full explanation of different Redis eviction policies, see key eviction in the Redis manual.", "Durability and persistence\nRedis is memory-first, meaning everything is stored in memory. If you do not configure persistence and the Redis process dies, it will essentially return to a cold-started state. (The cache will need to \u2018warm up\u2019 before you get the benefits from caching.)\nThere are two options for persisting Redis. Taking periodic snapshots of the state of Redis in .rdb files and keeping a log of all write commands in the append-only file. For a full explanation of these methods, see persistence in the Redis manual.", "Speed comparison\nGetting down to brass tacks, this section explores a comprehensive difference between the performance of Triton without Redis and Triton with Redis\u200c. In the interest of simplicity, we leveraged the perf_analyzer tool the Triton team built for measuring performance with Triton. We tested with two separate models, DenseNet and Simple.\nWe ran Triton Server version 23.06 on a Google Cloud Platform (GCP) n1-standard-4 VM with a single NVIDIA T4 GPU. We also ran a vanilla open-source Redis instance on a GCP n2-standard-4 VM. Finally, we ran the Triton client image in Docker on a GCP e2-medium VM.\nWe ran the ```perf_analyzer``` tool with both the DenseNet and Simple models, 10 times on each caching configuration, with no caching, with Redis as the cache, and with the local cache as the cache. We then averaged the results of these runs.\nIt is important to note that these runs assume a 100% cache-hit rate. So, the measurement is the difference between the performance of Triton when it has encountered the entry in the past and when it has not.\nWe used the following command for the DenseNet model:\nperf_analyzer -m densenet_onnx -u triton-server:8000 We used the following command for the Simple model:\nperf_analyzer -m simple -u triton-server:8000 In the case of the DenseNet model, the results showed that using either cache was dramatically better than running with no cache. Without caching, Triton was able to handle 80 inferences per second (inference/sec) with an average latency of 12,680 \u00b5s. With Redis, it was about 4x faster, processing 329 inference/sec with an average latency of 3,030 \u00b5s.\nInterestingly, while local caching was somewhat faster than Redis, as you would expect it to be, it was only marginally faster. Local caching resulted in a throughput of 355 inference/sec with a latency of 2,817 \u00b5s, only about 8% faster. In this case, it\u2019s clear that the speed tradeoff of caching locally versus in Redis is a marginal one. Given all the extra benefits that come from using a distributed versus a local cache, \u200cdistributed will almost certainly be the way to go when handling these kinds of data.\nChart showing the throughput comparison for DenseNet for No Cache, Redis, and Local. No Cache is dramatically lower while Redis and Local are close to parity.\nFigure 3. DenseNet throughput comparison, demonstrating that Redis throughput is comparable to the local cache for computationally expensive inferences\nChart showing the difference in latency for DenseNet between No Cache, Redis, and Local. Again No cache's latency is quite high while Redis and Local are near parity.\nFigure 4. DenseNet latency comparison, demonstrating that Redis latency is comparable to the local cache for computationally expensive inferences\nThe Simple model tells a slightly more complicated story. In the case of the simple model, not using any cache enabled a throughput of 1,358 inference/sec with a latency of 735 \u00b5s. Redis was somewhat faster with a throughput of 1,639 inference/sec and a latency of 608 \u00b5s. Local was faster than Redis with a throughput of 2,753 inference/sec with a latency of 363 \u00b5s.\nThis is an important case to note, as not all uses are created equal. The system of record, in this case, may be fast enough and not worth adding the extra system for the 20% boost in throughput of Redis. Even with the halving of latency in the case of the local cache, it may not be worth the resource contention, depending on other factors such as cache hit rate and available system resources.\nChart showing the difference in throughput for the simple model between No Cache, Redis, and local cache. No Cache is the slowest with both Redis and Local being somewhat faster.\nFigure 5. Simple model throughput. For computationally inexpensive inferences, there is less of a throughput advantage with Redis over the local cache Chart showing the comparison in latency on the Simple model between no cache, Redis, and local cache. Latency is somewhat higher for no cache than Redis, but not dramatically. Local has about half the latency of no cache.\nFigure 6. Simple model latency. For computationally inexpensive inferences, there is less of a latency advantage with Redis over the local cache", "Best practices for managing trade-offs\nAs shown in the experiment, the difference between models, expected inputs, and expected outputs is critically important for assessing what, if any, caching is appropriate for your Triton instance.\nWhether caching adds value is largely a function of how computationally expensive your queries are. The more computationally expensive your queries, the more each query will benefit from caching.\nThe relative performance of local versus Redis will largely be a function of how large the output tensors are from the model. The larger the output tensors, the more the transport costs will impact the throughput allowable by Redis.\nOf course, the larger the output tensors are, the fewer output tensors you\u2019ll be able to store in the local cache before you run out of room and begin contending with Triton for resources. Fundamentally, these factors need to be balanced when assessing which caching solution works best for your deployment of Triton.\nBenefits\nDrawbacks\n1. Horizontally scalable\n2. Effectively unlimited memory access\n3. Enables high availability and disaster recovery\n4. Removes resource contention\n5. Minimizes cold starts\nA distributed Redis cache requires calls\nover the network. Naturally, you can\nexpect somewhat lower throughput and\nhigher latency as compared to the local cache.\nTable 1. Benefits and drawbacks of using Redis as the caching layer rather than the local cache", "Summary\nDistributed caching is an old trick that developers use to boost system performance while enabling horizontal scalability and separation of concerns. With the introduction of the Redis Cache for Triton Inference Server, you can now leverage this technique to greatly increase the performance and efficiency of your Triton instance, while managing heavier workloads and enabling multiple Triton instances to share in the same cache. Fundamentally, by offloading caching to Redis, Triton can concentrate its resources on its fundamental role\u2014running inferences.\nGet started with Triton Redis Cache and NVIDIA Triton Inference Server. For more information about setting up and administering Redis instances, see redis.io and docs.redis.com."], "document_title": "How to Build a Distributed Inference Cache with NVIDIA Triton and Redis", "document_url": "https://developer.nvidia.com/blog/how-to-build-a-distributed-inference-cache-with-nvidia-triton-and-redis/", "document_date": "2023-08-30T19:20:39", "document_date_modified": "2023-09-07T18:39:26", "document_full_text": "How to Build a Distributed Inference Cache with NVIDIA Triton and Redis\nCaching is as fundamental to computing as arrays, symbols, or strings. Various layers of caching throughout the stack hold instructions from memory while pending on your CPU. They enable you to reload the page quickly and without re-authenticating, should you navigate away. They also dramatically decrease application workloads, and increase throughput by not re-running the same queries repeatedly.\nCaching is not new to NVIDIA Triton Inference Server, which is a system tuned to answering questions in the form of running inferences on tensors. Running inferences is a relatively computationally expensive task that often calls on the same inference to run repeatedly. This naturally lends itself to using a caching pattern.\nThe NVIDIA Triton team recently implemented the Triton response cache using the Triton local cache library. They have also built a cache API to make this caching pattern extensible within Triton. The Redis team then leveraged that API to build the Redis cache for NVIDIA Triton.\nIn this post, the Redis team explores the benefits of the new Redis implementation of the Triton Caching API. We explore how to get started and discuss some of the best practices for using Redis to supercharge your NVIDIA Triton instance.\nWhat is Redis?\nRedis is an acronym for REmote DIctionary Server. It is a NoSQL database that operates as a key-value data structure store. Redis is memory-first, meaning that the entire dataset in Redis is stored in memory, and optionally persisted to disk, based on configuration. Because it is a key-value database completely held in memory, Redis is blazingly fast. Execution times are measured in microseconds, and throughputs in tens of thousands of operations a second.\nThe remarkable speed and typical access pattern of Redis make it ideal for caching. Redis is synonymous with caching and is consequentially one of the built-in distributed caches of most major application frameworks across a variety of developer communities.\nWhat is local cache?\nThe local cache is an in-memory derivation of the most common caching pattern out there (cache-aside). It is simple and efficient, making it easy to grasp and implement. After receiving a query, NVIDIA Triton:\nComputes a hash of the input query, including the tensor and some metadata. This becomes the inference key.\nChecks for a previously inferred result for that tensor at that key.\nReturns any results found.\nPerforms the inference if no results are found.\nCaches the inference in memory using the key for storage.\nReturns the inference.\n\u2018Local\u2019 means that it is staying local to the process and storing the cache in the system\u2019s main memory. Figure 1 shows the implementation of this pattern.\nDiagram showing how the local cache works in NVIDIA Triton. The server checks the cache; if it has the inference, return the inference. If not, run the tensor through the model, cache the outputted inference, and return the outputted inference.\nFigure 1. NVIDIA Triton using the local cache\nBenefits of local cache\nThere are a variety of benefits that flow naturally from using this pattern. Because the queries are cached, they can be retrieved again easily without rerunning the tensor through the models. Because everything is maintained locally in the process memory, there is no need to leave the process or machine to retrieve the cached data. These two in concert can dramatically increase throughput, as well as decrease the cost of this computation.\nDrawbacks of local cache\nThis technique does have drawbacks. Because the cache is tied directly into the process memory, each time the Triton process restarts, it starts from square one (generally referred to as a cold start). You will not see the benefits from caching while the cache warms up. Also, because the cache is process-locked, other instances of Triton will not be able to share the cache, leading to duplication of caching across each node.\nThe other major drawback concerns resource contention. Since the local cache is tied to the process, it is limited to the resources of the system that Triton runs on. This means that it is impossible to horizontally scale the resources allocated to the cache (distributing the cache across multiple machines), which limits the options for expanding the local cache to vertical scaling. This makes the server running Triton bigger.\nBenefits of distributed caching with Redis\nUnlike local caching, distributed caching leverages an external service (such as Redis) to distribute the cache off the local server. This confers several advantages to the NVIDIA Triton caching API:\nRedis is not bound to the available system resources of the same machine as Triton, or for that matter, a single machine.\nRedis is decoupled from Triton\u2019s process life cycle, enabling multiple Triton instances to leverage the same cache.\nRedis is extremely fast (execution times are typically sub-milliseconds).\nRedis is a significantly more specialized, feature-rich, and tunable caching service compared to the Triton local cache.\nRedis provides immediate access to tried and tested high availability, horizontal scaling, and cache-eviction features out of the box.\nDistributed caching with Redis works much the same way as the local cache. Rather than staying within the same process, it crosses out of the Triton server process to Redis to check the cache and store inferences. After receiving a query, NVIDIA Triton:\nComputes a hash of the input query, including the tensor and some metadata. This becomes the inference key.\nChecks Redis for a previous run inference.\nReturns that inference, if it exists.\nRuns the tensor through Triton if the inference does not exist.\nStores the inference in Redis.\nReturns the inference.\nArchitecturally, this is shown in Figure 2.\nDiagram showing how the Triton Inference Server uses Redis as a cache. Similar to how it uses the local cache, but reaching out to the external service Redis for caching.\nFigure 2. NVIDIA Triton using Redis as its caching layer\nDistributed cache set up and configuration\nTo set up the distributed Redis cache requires two top-level steps:\nDeploy your Redis instance.\nConfigure NVIDIA Triton to point at the Redis instance.\nTriton will take care of the rest for you. To learn more about Redis, see redis.io, docs.redis.com, and Redis University.\nTo configure Triton to point at your Redis instance, use the ```--cache-config``` options in your start command. In the model config, enable the response cache for the model with ```{{response_cache { enable: true }}}```.\ntritonserver --cache-config redis,host=localhost --cache-config redis,port=6379 The Redis cache calls on you to minimally configure the host and port of your Redis instance. For a full enumeration of configuration options, see the Triton Redis Cache GitHub repo.\nBest practices with Redis\nRedis is lightweight, easy to use, and extremely fast. Even with its small footprint and simplicity, there is much you can configure in and around Redis to optimize it for your use case. This section highlights best practices for using and configuring Redis.\nMinimize round-trip time\nThe only real drawback of using an external service like Redis over an in-process memory cache is that the queries to Redis will, at least, have to cross process. They typically need to cross server boundaries as well.\nBecause of this, minimizing round-trip times (RTT) is of paramount importance in optimizing the use of Redis as a cache. The topic of how to minimize RTT is far too complex a topic to dive into in this post. A couple of key tips: maintain the locality of your Redis servers to your Triton servers and have them physically close to each other. If they are in a data center, try to keep them in the same rack or availability zone.\nScaling and high availability\nRedis Cluster enables you to scale your Redis instances horizontally over multiple shards. The cluster includes the ability to replicate your Redis instance. If there is a failure in your primary shard, the replica can be promoted for high availability.\nMaximum memory and eviction\nIf Redis memory is not capped, it will use all the available memory on the system that the OS will release to it. Set the maxmemory configuration key in redis.conf. But what happens if you set maxmemory and Redis runs out of memory? The default is, as you might expect, to stop accepting new writes to Redis.\nHowever, you can also set an eviction policy. An eviction policy uses some basic intelligence to decide which keys might be good candidates to kick out of Redis. Allowing Redis to evict keys that no longer make sense to store enables it to continue accepting new writes without interruption when the memory fills.\nFor a full explanation of different Redis eviction policies, see key eviction in the Redis manual.\nDurability and persistence\nRedis is memory-first, meaning everything is stored in memory. If you do not configure persistence and the Redis process dies, it will essentially return to a cold-started state. (The cache will need to \u2018warm up\u2019 before you get the benefits from caching.)\nThere are two options for persisting Redis. Taking periodic snapshots of the state of Redis in .rdb files and keeping a log of all write commands in the append-only file. For a full explanation of these methods, see persistence in the Redis manual.\nSpeed comparison\nGetting down to brass tacks, this section explores a comprehensive difference between the performance of Triton without Redis and Triton with Redis\u200c. In the interest of simplicity, we leveraged the perf_analyzer tool the Triton team built for measuring performance with Triton. We tested with two separate models, DenseNet and Simple.\nWe ran Triton Server version 23.06 on a Google Cloud Platform (GCP) n1-standard-4 VM with a single NVIDIA T4 GPU. We also ran a vanilla open-source Redis instance on a GCP n2-standard-4 VM. Finally, we ran the Triton client image in Docker on a GCP e2-medium VM.\nWe ran the ```perf_analyzer``` tool with both the DenseNet and Simple models, 10 times on each caching configuration, with no caching, with Redis as the cache, and with the local cache as the cache. We then averaged the results of these runs.\nIt is important to note that these runs assume a 100% cache-hit rate. So, the measurement is the difference between the performance of Triton when it has encountered the entry in the past and when it has not.\nWe used the following command for the DenseNet model:\nperf_analyzer -m densenet_onnx -u triton-server:8000 We used the following command for the Simple model:\nperf_analyzer -m simple -u triton-server:8000 In the case of the DenseNet model, the results showed that using either cache was dramatically better than running with no cache. Without caching, Triton was able to handle 80 inferences per second (inference/sec) with an average latency of 12,680 \u00b5s. With Redis, it was about 4x faster, processing 329 inference/sec with an average latency of 3,030 \u00b5s.\nInterestingly, while local caching was somewhat faster than Redis, as you would expect it to be, it was only marginally faster. Local caching resulted in a throughput of 355 inference/sec with a latency of 2,817 \u00b5s, only about 8% faster. In this case, it\u2019s clear that the speed tradeoff of caching locally versus in Redis is a marginal one. Given all the extra benefits that come from using a distributed versus a local cache, \u200cdistributed will almost certainly be the way to go when handling these kinds of data.\nChart showing the throughput comparison for DenseNet for No Cache, Redis, and Local. No Cache is dramatically lower while Redis and Local are close to parity.\nFigure 3. DenseNet throughput comparison, demonstrating that Redis throughput is comparable to the local cache for computationally expensive inferences\nChart showing the difference in latency for DenseNet between No Cache, Redis, and Local. Again No cache's latency is quite high while Redis and Local are near parity.\nFigure 4. DenseNet latency comparison, demonstrating that Redis latency is comparable to the local cache for computationally expensive inferences\nThe Simple model tells a slightly more complicated story. In the case of the simple model, not using any cache enabled a throughput of 1,358 inference/sec with a latency of 735 \u00b5s. Redis was somewhat faster with a throughput of 1,639 inference/sec and a latency of 608 \u00b5s. Local was faster than Redis with a throughput of 2,753 inference/sec with a latency of 363 \u00b5s.\nThis is an important case to note, as not all uses are created equal. The system of record, in this case, may be fast enough and not worth adding the extra system for the 20% boost in throughput of Redis. Even with the halving of latency in the case of the local cache, it may not be worth the resource contention, depending on other factors such as cache hit rate and available system resources.\nChart showing the difference in throughput for the simple model between No Cache, Redis, and local cache. No Cache is the slowest with both Redis and Local being somewhat faster.\nFigure 5. Simple model throughput. For computationally inexpensive inferences, there is less of a throughput advantage with Redis over the local cache Chart showing the comparison in latency on the Simple model between no cache, Redis, and local cache. Latency is somewhat higher for no cache than Redis, but not dramatically. Local has about half the latency of no cache.\nFigure 6. Simple model latency. For computationally inexpensive inferences, there is less of a latency advantage with Redis over the local cache\nBest practices for managing trade-offs\nAs shown in the experiment, the difference between models, expected inputs, and expected outputs is critically important for assessing what, if any, caching is appropriate for your Triton instance.\nWhether caching adds value is largely a function of how computationally expensive your queries are. The more computationally expensive your queries, the more each query will benefit from caching.\nThe relative performance of local versus Redis will largely be a function of how large the output tensors are from the model. The larger the output tensors, the more the transport costs will impact the throughput allowable by Redis.\nOf course, the larger the output tensors are, the fewer output tensors you\u2019ll be able to store in the local cache before you run out of room and begin contending with Triton for resources. Fundamentally, these factors need to be balanced when assessing which caching solution works best for your deployment of Triton.\nBenefits\nDrawbacks\n1. Horizontally scalable\n2. Effectively unlimited memory access\n3. Enables high availability and disaster recovery\n4. Removes resource contention\n5. Minimizes cold starts\nA distributed Redis cache requires calls\nover the network. Naturally, you can\nexpect somewhat lower throughput and\nhigher latency as compared to the local cache.\nTable 1. Benefits and drawbacks of using Redis as the caching layer rather than the local cache\nSummary\nDistributed caching is an old trick that developers use to boost system performance while enabling horizontal scalability and separation of concerns. With the introduction of the Redis Cache for Triton Inference Server, you can now leverage this technique to greatly increase the performance and efficiency of your Triton instance, while managing heavier workloads and enabling multiple Triton instances to share in the same cache. Fundamentally, by offloading caching to Redis, Triton can concentrate its resources on its fundamental role\u2014running inferences.\nGet started with Triton Redis Cache and NVIDIA Triton Inference Server. For more information about setting up and administering Redis instances, see redis.io and docs.redis.com."}]}