[{"id": 73964, "date": "2023-11-29T13:00:00", "date_gmt": "2023-11-29T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73964"}, "modified": "2023-12-14T11:27:34", "modified_gmt": "2023-12-14T19:27:34", "slug": "boost-meeting-productivity-with-ai-powered-note-taking-and-summarization", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/", "title": {"rendered": "Boost Meeting Productivity with AI-Powered Note-Taking and Summarization"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Meetings are the lifeblood of an organization. They foster collaboration and informed decision-making. They eliminate silos through brainstorming and problem-solving. And they further strategic goals and planning.&nbsp;&nbsp;</p>\n\n\n\n<p>Yet, leading meetings that accomplish these goals\u2014especially those involving cross-functional teams and external participants\u2014can be challenging. A unique blend of people management skills and adept documentation strategies are required to seamlessly facilitate decision-making and ensure effective post-meeting task execution.&nbsp;&nbsp;</p>\n\n\n\n<p>This post introduces the cloud-native microservice-based architecture for intelligent note-taking from <a href=\"https://adam.ai/\">adam.ai</a>. Part of the <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception</a> program, adam.ai is a comprehensive meeting management platform designed to empower organizations, teams, and professionals throughout their entire meeting lifecycle. The architecture offers high scalability, low latency, and cost-effective provisioning of automatic note-taking services in online meetings. Specifically, adam.ai leverages:</p>\n\n\n\n<ul>\n<li>Google Cloud Dataflow for automated provisioning of processing resources</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a> speech-to-text (STT) models for low-latency transcription&nbsp;</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large language models (LLMs)</a> for efficient summarization&nbsp;</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">AI-driven automatic note-taking&nbsp;</h2>\n\n\n\n<p>Manual note-taking requires real-time decisions about what information to record and what to omit. Moreover, balancing active participation with meticulous note-taking presents challenges even for those most adept. The endurance required to focus, especially during lengthy or complex discussions, remains a constant hurdle.&nbsp;&nbsp;</p>\n\n\n\n<p>Advancements in<a href=\"https://developer.nvidia.com/blog/essential-guide-to-automatic-speech-recognition-technology/\"> automatic speech recognition</a> (ASR) and LLMs pave the way for novel approaches to managing and organizing meeting information. Automatic note-taking harnesses the power of transcription to ensure accuracy and depth in capturing nuances.&nbsp;</p>\n\n\n\n<p>Transcription models transform spoken words into accurate text in real time, empowering teams, corporate executives, and professionals to create comprehensive meeting minutes, leaving no critical details overlooked. LLMs leverage a capacity for understanding, reasoning, and knowledge representation to analyze meeting data and extract invaluable insights.&nbsp;</p>\n\n\n\n<p>With the user-friendly adam.ai interface, essential agenda items become accessible, and decisions and action items are meticulously tracked (Figure 1). This intuitive approach facilitates meeting management, fosters seamless collaboration, and supports superior meeting outcomes.&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"477\" height=\"769\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface.png\" alt=\"Screenshot of adam.ai user interface of meeting AI Summary displaying insights and next steps.\n\" class=\"wp-image-73971\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface-186x300.png 186w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface-71x115.png 71w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface-56x90.png 56w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface-362x584.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-user-interface-68x110.png 68w\" sizes=\"(max-width: 477px) 100vw, 477px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. adam.ai provides insightful notes and next steps to facilitate meeting management</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Transcription and summarization architecture</h2>\n\n\n\n<p>The AI Engineering team at adam.ai developed a microservice architecture specifically designed for Google Cloud (Figure 2). This architecture, which includes the note-taking system, can seamlessly translate to other cloud platforms such as AWS and Azure.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1046\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram.png\" alt=\"The adam.ai architecture diagram for meeting transcription and automatic note-taking service. User data flows through Google Cloud for preprocessing, NVIDIA Riva state-of-the-art speech-to-text models for low-latency transcription, and LLMs for efficient summarization.\n\" class=\"wp-image-73972\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-625x327.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-768x402.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-1536x804.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-645x338.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-500x262.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-362x189.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-210x110.png 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adamai-automatic-note-taking-architecture-diagram-1024x536.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The adam.ai automatic note-taking architecture</em></figcaption></figure>\n\n\n\n<p>The architecture leverages Google Cloud components, such as Storage, Dataflow, and the Pub/Sub system for storing users\u2019 data, managing data-processing resources, and facilitating communication between the different components.&nbsp;</p>\n\n\n\n<p>Meeting transcription is powered by NVIDIA Riva models, offering unmatched accuracy and low latency while efficiently handling real-time audio processing tasks at scale. What sets Riva apart is its full customization capabilities. Riva can be fine-tuned for specialized industries such as legal and medical, providing precise transcription even in niche vocabularies and language usage. Additionally, for variable demand, deploying Riva models using Helm charts enables scalable resource management, providing a cost-effective solution.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Note-taking data flow</h2>\n\n\n\n<p>The adam.ai note-taking data flow is orchestrated through four key steps:&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 1: Initiate a note-taking job&nbsp;</h3>\n\n\n\n<p>When a new meeting recording is uploaded, an event message is generated and transmitted through the Google Cloud Pub/Sub messaging service. This event-driven, distributed mechanism establishes a loosely coupled architecture, simplifying communication between the platform and the note-taking service, especially when processing lengthy meetings that require significant analysis and summarization time.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 2: Start the data processing pipeline&nbsp;</h3>\n\n\n\n<p>Event messages, which encapsulate the location of the audio and video recordings, undergo processing through customized data processing pipelines to derive meeting insights. These pipelines are executed through Google Cloud Dataflow, enabling automated provisioning of computing resources tailored to dynamic user workload, thereby ensuring optimal performance and cost efficiency of processing tasks.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 3: Generate meeting transcriptions&nbsp;</h3>\n\n\n\n<p>The data processing pipeline begins by downloading audio and video recordings from \u200ccloud storage. Downloaded files are then meticulously transcribed by NVIDIA Riva. Producing more than a simple conversion of speech to text, Riva enhances transcription quality using contextual understanding. Punctuation and capitalization are refined to provide robust and accurate summarization and insight generation.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 4: Generate summary and actionable insights&nbsp;</h3>\n\n\n\n<p>The meticulously transcribed text is then passed to an LLM to summarize the meeting content. Through refined prompt engineering, the LLM summarizes the meeting and generates valuable, actionable insights. The meeting summary and insights are then returned to the platform for user display.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Benefits of adam.ai architecture&nbsp;</h2>\n\n\n\n<p>This architecture ensures \u200cefficient, scalable, and cost-effective meeting transcription and summarization. Specific benefits include:&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Dynamically scalable and fault-tolerant system</h3>\n\n\n\n<p>Using Google Cloud Pub/Sub, the architecture embraces a loosely coupled, event-driven microservices approach, prompting a scalable, fault-tolerant system. This not only simplifies communication but also provides independent functionality of components. Additionally, Google Cloud Dataflow automatic resource provisioning dynamically scales computing power, resulting in cost-effective data processing.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Real-time accurate meeting transcriptions</h3>\n\n\n\n<p>The Riva ASR model supports streaming audio and provides real-time accurate transcriptions. Its ability to refine punctuation and capitalization elevates transcript quality, enabling accurate summarization and extraction of valuable insights.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Intelligible and well-structured summarizations&nbsp;</h3>\n\n\n\n<p>LLM integration provides intelligible and well-structured summaries, fostering the extraction of valuable and actionable insights from the meeting transcript.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Intuitive user experience&nbsp;</h3>\n\n\n\n<p>The entire process, from transcription to summarization, is seamlessly integrated into the platform. Requests and results flow efficiently through the Pub/Sub system, providing a smooth and intuitive user experience and easy access to meeting insights. <strong>&nbsp;</strong></p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary&nbsp;</h2>\n\n\n\n<p>Transform your meetings into more productive, dynamic collaborations with adam.ai. Working together, ASR and LLMs seamlessly capture every word spoken, extract key insights, and generate detailed notes. This frees participants from the burden of note-taking so they can fully engage in the meeting.</p>\n\n\n\n<p>To ensure scalable, low-latency, and cost-effective processing of meetings&#8217; audio data, the adam.ai meeting management platform employs a cloud-native microservice-based architecture. This architecture enables real-time accurate transcriptions and enhanced punctuation and capitalization powered by <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a>, providing you with a comprehensive and polished record of your meetings.</p>\n\n\n\n<p>To explore how adam.ai can help elevate your meetings, <a href=\"https://adam.ai/?utm_source=website&amp;utm_medium=organic&amp;utm_campaign=Riva%20blogpost\">sign up for a free trial</a>. To learn more about LLM enterprise applications, see <a href=\"https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/\">Getting Started with Large Language Models for Enterprise Solutions</a>. And join the conversation on Speech AI in the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/deep-learning/riva/475\">NVIDIA Riva forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Meetings are the lifeblood of an organization. They foster collaboration and informed decision-making. They eliminate silos through brainstorming and problem-solving. And they further strategic goals and planning.&nbsp;&nbsp; Yet, leading meetings that accomplish these goals\u2014especially those involving cross-functional teams and external participants\u2014can be challenging. A unique blend of people management skills and adept documentation strategies are &hellip; <a href=\"https://developer.nvidia.com/blog/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/\">Continued</a></p>\n", "protected": false}, "author": 1926, "featured_media": 73967, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1306888", "discourse_permalink": "https://forums.developer.nvidia.com/t/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/274467", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110], "tags": [453, 2932, 1961, 2379, 3166, 106], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/trascription-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jeY", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73964"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1926"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73964"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73964/revisions"}], "predecessor-version": [{"id": 74473, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73964/revisions/74473"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73967"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73964"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73964"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73964"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74507, "date": "2023-11-29T11:11:15", "date_gmt": "2023-11-29T19:11:15", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74507"}, "modified": "2023-12-14T11:27:34", "modified_gmt": "2023-12-14T19:27:34", "slug": "train-generative-ai-models-for-drug-discovery-with-bionemo-framework", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/train-generative-ai-models-for-drug-discovery-with-bionemo-framework/", "title": {"rendered": "Train Generative AI Models for Drug Discovery with NVIDIA BioNeMo Framework"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA BioNeMo Framework has been released and is now generally available to download on NGC, enabling researchers to build and deploy generative AI, large language models (LLMs), and foundation models in drug discovery applications. </p>\n\n\n\n<p>The BioNeMo platform includes managed services, API endpoints, and training frameworks to simplify, accelerate, and scale generative AI for drug discovery. BioNeMo provides the capability to pre-train or fine-tune state-of-the-art models with end-to-end acceleration at scale. It is available as a fully managed service on NVIDIA DGX Cloud with NVIDIA Base Command platform and also as a downloadable framework for deployment with on-premises infrastructure and a variety of cloud platforms.</p>\n\n\n\n<p>This provides drug discovery researchers and developers with a fast and easy way to build and integrate state-of-the-art AI applications across the entire drug discovery pipeline, from target identification to lead optimization.</p>\n\n\n\n<h2 class=\"wp-block-heading\">BioNeMo Framework v1.0 features</h2>\n\n\n\n<ul>\n<li><strong>Easy data loading</strong> with automatic downloaders, pre-processed data, and support for common biomolecular data formats.</li>\n\n\n\n<li><strong>SOTA domain-specific models</strong>, including out-of-the-box architectures and validated checkpoints for training on protein and small molecule data.</li>\n\n\n\n<li><strong>Optimized scaling recipes</strong> for seamless accelerated training on 1,000s of GPUs, optimized to maximize throughput and reduce cost.</li>\n\n\n\n<li><strong>Flexible training workflows</strong> to enable easy large-scale pre-training from scratch, fine-tuning from reliable checkpoints, and downstream task training at speed.</li>\n\n\n\n<li><strong>Validation-in-the-loop, </strong>with periodic supervised task training<strong> </strong>to measure the quality of embeddings as the model trains. Fully automated and integrated with Weights and Biases.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Optimized training for protein and small molecule models</h2>\n\n\n\n<p>NVIDIA BioNeMo provides optimizations for generative AI models across multiple domains. BioNeMo Framework v1.0 delivers optimized model architectures and tooling for training protein and small molecule LLMs:</p>\n\n\n\n<ul>\n<li>BioNeMo ESM1 and ESM2</li>\n\n\n\n<li>BioNeMo MegaMolBART</li>\n\n\n\n<li>BioNeMo ProtT5</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">BioNeMo ESM1 and ESM2</h3>\n\n\n\n<p>The ESM model family is a collection of transformer-based, protein language models built on the BERT architecture and produced by the <a href=\"https://www.science.org/doi/abs/10.1126/science.ade2574\">Meta Fundamental AI Research Protein Team (FAIR)</a>.&nbsp;</p>\n\n\n\n<p>The general-purpose ESM-like architectures are optimized and available now in BioNeMo Framework and can be leveraged for custom training of protein LLMs. These models are trained on massive datasets of protein sequences to learn the underlying patterns and relationships between amino acids that govern protein structure and function.&nbsp;</p>\n\n\n\n<p>Importantly, trained ESM models can be harnessed for a variety of downstream tasks through transfer learning. For instance, you can use the embeddings from its encoder to train a smaller model with a supervised learning objective to infer the properties of proteins. This has been shown to produce highly accurate models for a variety of tasks such as 3D structure prediction, variant effect prediction, or designing <em>de novo</em> proteins.</p>\n\n\n\n<p>BioNeMo Framework includes validated training checkpoints for ESM-2 650 million\u2013 and 3B-parameter models, enabling a zero-shot start to create custom, domain-specific applications. A number of example downstream tasks, including secondary structure prediction, subcellular localization prediction, and thermal stability prediction are also provided.</p>\n\n\n\n<h3 class=\"wp-block-heading\">BioNeMo MegaMolBART</h3>\n\n\n\n<p>The MegaMolBART model is a generative chemistry model built using the seq2seq transformer BART architecture, and inspired by the Chemformer model developed by AstraZeneca. MegaMolBART was trained on the ZINC-15 database of small molecule SMILES strings, using 1.5B molecules for training in total.&nbsp;</p>\n\n\n\n<p>The embeddings from its encoder can be used for downstream predictive models, much in the same way as ESM or the encoder and decoder can be used together for novel molecule generation by sampling the embedding space. This means MegaMolBART can be used for a variety of cheminformatics drug discovery tasks, such as reaction prediction, molecular optimization, and <em>de novo</em> molecular generation.</p>\n\n\n\n<p>MegaMolBART was developed using BioNeMo Framework, which includes a trained and validated checkpoint for a 45M parameter model. Downstream task workflows are also provided for the prediction of retrosynthetic reactions and physicochemical properties, such as lipophilicity, aqueous solubility (ESOL), and hydration-free energy (FreeSolv).</p>\n\n\n\n<h3 class=\"wp-block-heading\">BioNeMo ProtT5</h3>\n\n\n\n<p>ProtT5 is a protein language model built on an encoder/decoder LLM, developed by the Rost Lab using the T5 architecture. Like the ESM models, ProtT5 can produce embeddings from its encoder for representation learning but it can also use the entire encoder/decoder architecture for sequence translation tasks.&nbsp;</p>\n\n\n\n<p>As with others, the base model can be extended for downstream tasks such as generating protein sequences. A recent example of this was the extension of the model by startup <a href=\"https://blogs.nvidia.com/blog/2023/01/12/generative-ai-proteins-evozyne/\">Evozyne to create two proteins</a>. The proteins have significant potential in healthcare (aiming to cure a congenital disease) and also in clean energy (designed to consume carbon dioxide to reduce global warming).&nbsp;</p>\n\n\n\n<p>Optimized as part of BioNeMo Framework, the ProtT5 model includes a trained and validated checkpoint for a 192M parameter model and a sample downstream task workflow for secondary structure prediction.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Speed and scale with BioNeMo Framework</h2>\n\n\n\n<p>BioNeMo Framework uses a variety of techniques to achieve higher throughput and improved scalability, including parallelism:</p>\n\n\n\n<ul>\n<li><strong>Model pipeline parallelism:</strong> The layers of a model are distributed for parallel training.&nbsp;</li>\n\n\n\n<li><strong>Model tensor parallelism: </strong>The layers are themselves sliced and distributed.&nbsp;</li>\n</ul>\n\n\n\n<p>Specifying optimizations like precision can also confer huge performance benefits, often with little to no effect on model accuracy.</p>\n\n\n\n<p>BioNeMo Framework includes best practices for selecting and tuning hyperparameters of the models, with the ability to easily configure many of these options for maximum performance. One example would be applying techniques such as model tensor parallelism to models over 1B parameters in size and model pipeline parallelism for models over 5B parameters.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Scaling ESM2 training across H100 GPUs with BioNeMo Framework</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1158\" height=\"837\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2.png\" alt=\"The graph shows scaling of million tokens per second as the number of GPUs increases.\" class=\"wp-image-74606\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2.png 1158w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-300x217.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-625x452.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-159x115.png 159w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-768x555.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-645x466.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-415x300.png 415w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-125x90.png 125w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-362x262.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-152x110.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-scaling-training-dgx-2-1024x740.png 1024w\" sizes=\"(max-width: 1158px) 100vw, 1158px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Example of scaling training for a 3B parameter ESM2 model on DGX H100</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows scaling from a single DGX node (eight H100 GPUs) to 32 DGX nodes (256 H100 GPUs), and the resultant increase in throughput (tokens per second).</p>\n\n\n\n<p>The full-stack optimizations afforded by BioNeMo Framework and the latest NVIDIA GPUs enable training state-of-the-art models at a much-improved speed and efficiency.&nbsp;</p>\n\n\n\n<p>As an example, ESM2 was trained as part of its original publication in 8 days for a 650M parameter model, and 30 days for a 3B parameter model, on 512 V100 GPUs. Training these same models with BioNeMo Framework and on 512 H100 GPUs (trained on 1T tokens, or 1.19B protein sequences) can now be achieved in just 1.2 days and 3.5 days respectively.&nbsp;</p>\n\n\n\n<p>This provides the opportunity to train even larger models in shorter time frames. For example, an ESM2 model of 20B parameters can be trained on 1T tokens in just 18.6 days with BioNeMo Framework and 512 H100 GPUs.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Training larger ESM2 models in less time</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"901\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2.png\" alt=\"The graph shows that training time is up to 16x faster with H100 GPUs and BioNeMo.\" class=\"wp-image-74608\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-625x282.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-768x346.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-1536x692.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-645x291.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-500x225.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-244x110.png 244w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-training-times-with-bionemo-2-1024x462.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Example training times of ESM2 for different GPUs with BioNeMo Framework</em></figcaption></figure></div>\n\n\n<p class=\"has-medium-font-size\">The <a href=\"https://www.science.org/doi/10.1126/science.ade2574\">original published model</a> training time (512 V100s) is shown for reference in gray in the first column. Models trained with BioNeMo were trained on 1T tokens, equivalent to 1.19B protein sequences.</p>\n\n\n\n<h2 class=\"wp-block-heading\">BioNeMo workflow</h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1108\" height=\"572\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources.png\" alt=\"Diagram shows the steps for access and resources: bringing your own data, getting DGX Cloud Service, selecting a BioNeMo model and training with pretrained models, data loaders, and training scripts, and a central UI for launching multi-node training.\" class=\"wp-image-74513\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources.png 1108w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-300x155.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-625x323.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-768x396.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-645x333.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-500x258.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-160x83.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-362x187.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-213x110.png 213w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-resources-1024x529.png 1024w\" sizes=\"(max-width: 1108px) 100vw, 1108px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Resources for BioNeMo Framework</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Getting started with BioNeMo Framework</h2>\n\n\n\n<p>BioNeMo Framework v1.0 is available now on NGC. For more information about access, the latest technical posts, and talks on AI for drug discovery, see the BioNeMo <a href=\"https://www.nvidia.com/en-us/clara/bionemo/\">Get Started</a> and <a href=\"https://www.nvidia.com/en-us/clara/bionemo/resources/\">Resources</a> pages.</p>\n\n\n\n<p>BioNeMo Framework is best deployed on NVIDIA DGX Cloud, which provides on-demand DGX infrastructure for optimal throughput performance. This provides a full-stack AI-training-as-a-service solution for enterprise-grade AI computing in the cloud and direct access to NVIDIA AI experts. For more information, see the <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">DGX Cloud</a> page.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA BioNeMo Framework has been released and is now generally available to download on NGC, enabling researchers to build and deploy generative AI, large language models (LLMs), and foundation models in drug discovery applications. The BioNeMo platform includes managed services, API endpoints, and training frameworks to simplify, accelerate, and scale generative AI for drug discovery. &hellip; <a href=\"https://developer.nvidia.com/blog/train-generative-ai-models-for-drug-discovery-with-bionemo-framework/\">Continued</a></p>\n", "protected": false}, "author": 1370, "featured_media": 74543, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1306855", "discourse_permalink": "https://forums.developer.nvidia.com/t/train-generative-ai-models-for-drug-discovery-with-nvidia-bionemo-framework/274464", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 3110], "tags": [3269, 2842, 453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bionemo-framework-featured.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jnJ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74507"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1370"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74507"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74507/revisions"}], "predecessor-version": [{"id": 74613, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74507/revisions/74613"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74543"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74507"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74507"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74507"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74116, "date": "2023-11-29T11:00:00", "date_gmt": "2023-11-29T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74116"}, "modified": "2023-12-14T11:27:35", "modified_gmt": "2023-12-14T19:27:35", "slug": "streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/", "title": {"rendered": "Streamline Job Initialization and CPU-Based Tasks with NVIDIA Base Command Platform"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/data-center/base-command-platform/\">NVIDIA Base Command Platform</a> software service offers increasingly streamlined workflows for accelerating AI development. This post explains several recently added features, including:</p>\n\n\n\n<ul>\n<li>One-click environment deployments with Quick Start</li>\n\n\n\n<li>CPU nodes for compute-light tasks</li>\n\n\n\n<li>Efficient data ingestion with Data Mover</li>\n\n\n\n<li>Secure data and collaborative workflows with Secrets Management Service (SMS)</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Effortless environment initialization</h2>\n\n\n\n<p>Launching an exploration environment on a supercomputer has never been easier.&nbsp;</p>\n\n\n\n<p>With the Quick Start feature, teams can programmatically define their frequent configurations\u2014from GPU and CPU instance types, memory, and storage needs to adding datasets, workspaces, swapping containers, and even setting default commands. These are defined as a template that can be launched with a single click from the Base Command Dashboard.&nbsp;</p>\n\n\n\n<p>In addition to building templates yourself, you can leverage a catalog of NVIDIA-built Quick Starts. Default Quick Starts for JupyterLab and for Dask+RAPIDS are now available. As that catalog grows, GPU-accelerated data science will become more accessible across the wide variety of toolsets data scientists use today.&nbsp;</p>\n\n\n\n<p>Deep integration of software and hardware is a core principle of Base Command Platform, and Quick Starts are no different. The NVIDIA custom scheduler ensures that these user-defined environment needs are routed appropriately in the cluster to ensure high cluster utilization across all jobs. Base Command Platform makes hard things simple.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Support for diverse workloads</h2>\n\n\n\n<p>CPU instances are now available in Base Command Platform. Adding both CPU and GPU instances to your Accelerated Compute Environment (ACE) brings flexibility, optimization, and cost-efficiency to your AI workflows.&nbsp;</p>\n\n\n\n<p>CPU instances are well suited for performing preprocessing and data wrangling tasks that don&#8217;t require the computational intensity of a GPU. GPU instances are then free for compute-heavy jobs like training.&nbsp;</p>\n\n\n\n<p>Use CPU nodes in Base Command Platform for a wide range of tasks, from downloading datasets and running notebooks to editing code, browsing files inside datasets, and even running monitoring tools like TensorBoard.&nbsp;</p>\n\n\n\n<p>You also get a seamless experience across CPU and GPU nodes in Base Command Platform. CPU instances can leverage existing constructs associated with GPU workloads, like interaction with shared resources (datasets, for example). Like the existing GPU experience, you\u2019ll receive default CPU quota, run time limits, and be able to experience CPU telemetry in the user interface.</p>\n\n\n\n<p>To request CPU nodes in Base Command Platform for your organization, contact your account team.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Streamline data import and export</h2>\n\n\n\n<p>Managing your data workflow in Base Command Platform is simpler than ever with the Data Mover feature.</p>\n\n\n\n<p>Data Mover enables the import of external object datasets directly into Base Command Platform storage using <a href=\"https://docs.ngc.nvidia.com/cli/index.html\">NVIDIA NGC CLI</a>. While Base Command Platform supports connecting to external object dataset during a training job, teams often choose to bring datasets into Base Command Platform to take advantage of our performance optimizations for the compute-adjacent storage inside your ACE.&nbsp;</p>\n\n\n\n<p>Data Mover has strong ties to two things we all care deeply about: utilization and security. Data Mover enables teams to perform their data processing jobs on CPU nodes, keeping GPU nodes free to participate in more compute-heavy workloads. A wget should never hold GPU cycles hostage. And on the security front, Data Mover requires the setup of NGC Secrets with designated keys.&nbsp;</p>\n\n\n\n<p>The following functionalities are now available: Dataset Import, Dataset Export, and Workspace Export. The ability to customize the default Dask+NVIDIA RAPIDS Quick Start and define repeated data movement jobs will be a strong combination.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Secure collaborative workflows&nbsp;</h2>\n\n\n\n<p>Handling sensitive data like API keys or tokens within an application is a critical concern. You can make security best practices the default while still delivering interesting collaborative functionality.&nbsp;</p>\n\n\n\n<p>Integrated with Base Command Platform jobs, Secrets Management Service (SMS) enables the direct injection of hidden environment variables. Information will remain private unless you decide to make it visible.</p>\n\n\n\n<p>At this time, organizations must enable Secrets Management to utilize SMS. After enablement, access to SMS is available through the Web UI Setup page and the NGC CLI from version 3.21.1.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>NVIDIA Base Command Platform enables teams to simplify high-performance workloads on NVIDIA GPUs. With recently added features, you can rapidly initialize jobs, better manage data workflows, support diverse workloads, and secure sensitive information.&nbsp;</p>\n\n\n\n<ul>\n<li>Get started with Base Command Platform and <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>.</li>\n\n\n\n<li>Read <a href=\"https://developer.nvidia.com/blog/simplifying-ai-development-with-base-command-platform/\">Simplifying AI Development with NVIDIA Base Command Platform</a> to learn more about NVIDIA Base Command Platform.</li>\n\n\n\n<li>Check out the <a href=\"https://docs.nvidia.com/base-command-platform/index.html\">NVIDIA Base Command Platform documentation</a> and <a href=\"https://docs.nvidia.com/base-command-platform/release-notes/index.html#release-20230905\">release notes</a>.</li>\n\n\n\n<li>Watch a <a href=\"https://www.youtube.com/watch?v=Mti7z-XHgb8\">demo video of NVIDIA Base Command Platform</a>.</li>\n\n\n\n<li>Join the conversation in the <a href=\"https://forums.developer.nvidia.com/\">NVIDIA Developer Forums</a>.&nbsp;</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Base Command Platform software service offers increasingly streamlined workflows for accelerating AI development. This post explains several recently added features, including: Effortless environment initialization Launching an exploration environment on a supercomputer has never been easier.&nbsp; With the Quick Start feature, teams can programmatically define their frequent configurations\u2014from GPU and CPU instance types, memory, and &hellip; <a href=\"https://developer.nvidia.com/blog/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/\">Continued</a></p>\n", "protected": false}, "author": 1904, "featured_media": 74117, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1306848", "discourse_permalink": "https://forums.developer.nvidia.com/t/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/274463", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852], "tags": [188, 1051, 453, 572, 695], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/base-command-platform-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jhq", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74116"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1904"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74116"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74116/revisions"}], "predecessor-version": [{"id": 74500, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74116/revisions/74500"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74117"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74116"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74116"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74116"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74272, "date": "2023-11-29T10:53:10", "date_gmt": "2023-11-29T18:53:10", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74272"}, "modified": "2023-12-14T11:27:35", "modified_gmt": "2023-12-14T19:27:35", "slug": "new-course-introduction-to-transformer-based-natural-language-processing", "status": "publish", "type": "post", "link": "https://courses.nvidia.com/courses/course-v1:DLI+S-FX-08+V1/?nvid=nv-int-tblg-339219.", "title": {"rendered": "New Course: Introduction to Transformer-Based Natural Language Processing"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how transformers are used as the building blocks of modern large language models in this new self-paced course.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how transformers are used as the building blocks of modern large language models in this new self-paced course.</p>\n", "protected": false}, "author": 1115, "featured_media": 74533, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1306842", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-course-introduction-to-transformer-based-natural-language-processing/274458", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://courses.nvidia.com/courses/course-v1:DLI+S-FX-08+V1/?nvid=nv-int-tblg-339219.", "_links_to_target": "_blank"}, "categories": [696, 3110], "tags": [3312, 2964, 453, 2932, 2143], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dli-social-convai-workshop-and-scaling-gpu-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jjW", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74272"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74272"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74272/revisions"}], "predecessor-version": [{"id": 74548, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74272/revisions/74548"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74533"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74272"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74272"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74272"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74316, "date": "2023-11-29T09:00:00", "date_gmt": "2023-11-29T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74316"}, "modified": "2023-12-14T11:27:35", "modified_gmt": "2023-12-14T19:27:35", "slug": "cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/", "title": {"rendered": "CUDA Quantum 0.5 Delivers New Features for Quantum-Classical Computing"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://developer.nvidia.com/cuda-quantum\">CUDA Quantum</a> is a platform for building quantum-classical computing applications. It is an open-source programming model for heterogeneous computing such as <a href=\"https://blogs.nvidia.com/blog/what-is-a-qpu/\">quantum processor units</a> (QPUs), GPUs, and CPUs.&nbsp;</p>\n\n\n\n<p>CUDA Quantum accelerates workflows such as quantum simulation, quantum machine learning, quantum chemistry, and more. It optimizes these workflows as part of its compiler toolchain and uses the power of GPUs to accelerate them. CUDA Quantum offers kernel-based programming and can be used with Python or C++.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">What\u2019s new in CUDA Quantum 0.5?&nbsp;</h2>\n\n\n\n<p>The latest release, CUDA Quantum 0.5, introduces more QPUs backends, more simulators, and other improvements. For more information, see the <a href=\"https://github.com/NVIDIA/cuda-quantum/releases/tag/0.5.0\">CUDA Quantum 0.5 release notes</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Core features</h3>\n\n\n\n<p>Quantum error correction and other forms of hybrid quantum-classical computation often require nontrivial control flow and tightly interwoven primitives. CUDA Quantum now supports running adaptive quantum kernels: a <a href=\"https://github.com/qir-alliance/qir-spec/issues\">specification </a>from the QIR alliance and a key step towards truly integrated quantum-classical programming.</p>\n\n\n\n<p>Fermionic and Givens rotation and fermionic SWAP kernels are used in quantum chemistry simulations to perform operations on fermionic systems. The Givens rotation kernel is used to perform rotations on qubits, while the fermionic SWAP kernel is used to swap the states of two qubits. The addition of these kernels to the CUDA Quantum makes it easier for researchers to perform quantum chemistry simulations and develop new quantum algorithms for chemistry applications.</p>\n\n\n\n<p>The Pauli matrices are a set of matrices that are commonly used in quantum mechanics to represent quantum states and operators. The added support of exponentials of Pauli matrices in CUDA Quantum is useful for researchers performing quantum simulations of physical systems, such as molecules, and for developing quantum algorithms for optimization problems.</p>\n\n\n\n<p>CUDA Quantum now has improved support for <code>std::vector</code> and (C style) arrays, as well as support for execution of for\u2013 and while-loops of known lengths on quantum hardware backends. These features are useful for developing quantum algorithms that require complex data structures and control flow.</p>\n\n\n\n<h3 class=\"wp-block-heading\">IQM and Oxford Quantum Circuits QPU backends&nbsp;</h3>\n\n\n\n<p>A QPU backend is a hardware computing device that acts as a quantum processing unit and can run quantum workloads. CUDA Quantum is integrated with several quantum hardware providers\u2019 QPUs. </p>\n\n\n\n<p><a href=\"https://www.meetiqm.com/\">IQM</a> and<a href=\"https://oxfordquantumcircuits.com/\"> Oxford Quantum Circuits (OQC)</a> quantum computers are now supported as QPU backends in CUDA Quantum. This is a great addition to the already supported quantum computers from<a href=\"https://www.quantinuum.com/\"> Quantinuum</a> and<a href=\"https://ionq.com/\"> IonQ</a>, which enable you to run CUDA Quantum code on a variety of different quantum technologies available today.&nbsp;&nbsp;</p>\n\n\n\n<p>For more information about how to use the backends in either Python or C++, see the <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/hardware.html#iqm\">IQM</a> or <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/hardware.html#oqc\">OQC</a> documentation.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Tensor network and matrix product state simulators&nbsp;</h3>\n\n\n\n<p>Tensor network-based simulators are suitable for large-scale simulation of certain classes of quantum circuits involving many qubits, beyond the memory limit of state vector-based simulators. Tensor network simulation is improved with this release and is accelerated with the<a href=\"https://developer.nvidia.com/cuquantum-sdk\"> cuQuantum</a> library. For more information, see <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/simulators.html#tensor-network-simulators\">Tensor Network Simulators</a>.</p>\n\n\n\n<p>A matrix product state (MPS) simulator has been added to CUDA Quantum in this release. MPS representation takes advantage of tensor network sparsity by using tensor decomposition techniques such as QR and SVD. This is an approximate simulator in nature and therefore can handle a large number of qubits and more gate depth for certain classes of quantum circuits on a relatively small memory footprint. For more information, see<a href=\"https://nvidia.github.io/cuda-quantum/latest/using/simulators.html#matrix-product-state\"> Matrix Product State Simulator</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Getting started with CUDA Quantum&nbsp;</h2>\n\n\n\n<p>The CUDA Quantum<a href=\"https://nvidia.github.io/cuda-quantum/latest/install.html\"> Getting Started</a> guide walks you through the setup steps so you can get started with <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/python.html\">Python</a> and<a href=\"https://nvidia.github.io/cuda-quantum/latest/using/cpp.html\"> C++ examples</a> that provide a quick learning path for CUDA Quantum capabilities.&nbsp;&nbsp;</p>\n\n\n\n<p>For more information about advanced use cases for quantum\u2013classical applications, see the<a href=\"https://nvidia.github.io/cuda-quantum/latest/using/tutorials.html\"> tutorials gallery</a>.&nbsp;&nbsp;Finally, explore the code in the<a href=\"https://github.com/NVIDIA/cuda-quantum\"> CUDA Quantum open-source repository</a>. This is where you can report issues and also make feature suggestions.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>CUDA Quantum is a platform for building quantum-classical computing applications. It is an open-source programming model for heterogeneous computing such as quantum processor units (QPUs), GPUs, and CPUs.&nbsp; CUDA Quantum accelerates workflows such as quantum simulation, quantum machine learning, quantum chemistry, and more. It optimizes these workflows as part of its compiler toolchain and uses &hellip; <a href=\"https://developer.nvidia.com/blog/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/\">Continued</a></p>\n", "protected": false}, "author": 1938, "featured_media": 74318, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1306795", "discourse_permalink": "https://forums.developer.nvidia.com/t/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/274450", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [453, 2735], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/newsletter-quantum-computing-600x338-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jkE", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74316"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1938"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74316"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74316/revisions"}], "predecessor-version": [{"id": 74528, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74316/revisions/74528"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74318"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74316"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74316"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74316"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74503, "date": "2023-11-28T12:29:51", "date_gmt": "2023-11-28T20:29:51", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74503"}, "modified": "2023-12-14T11:27:36", "modified_gmt": "2023-12-14T19:27:36", "slug": "take-the-ai-innovation-challenge-and-unleash-your-creativity-with-nvidia-jetson", "status": "publish", "type": "post", "link": "https://nvda.ws/46Qlbu1", "title": {"rendered": "Take the \u2018AI Innovation Challenge\u2019 and Unleash Your Creativity with NVIDIA Jetson\u00a0"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA and SparkFun invite developers to build innovative AI applications using the NVIDIA Jetson. Enter now.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA and SparkFun invite developers to build innovative AI applications using the NVIDIA Jetson. Enter now.</p>\n", "protected": false}, "author": 338, "featured_media": 74504, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/46Qlbu1", "_links_to_target": "_blank"}, "categories": [2758, 3110, 63], "tags": [453, 208, 1950, 3581, 2571, 3582], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/hackster_tw.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jnF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74503"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/338"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74503"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74503/revisions"}], "predecessor-version": [{"id": 74506, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74503/revisions/74506"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74504"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74503"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74503"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74503"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74208, "date": "2023-11-28T10:19:07", "date_gmt": "2023-11-28T18:19:07", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74208"}, "modified": "2023-12-14T11:27:37", "modified_gmt": "2023-12-14T19:27:37", "slug": "one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/", "title": {"rendered": "One Giant Superchip for LLMs, Recommenders, and GNNs: Introducing NVIDIA GH200 NVL32"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>At AWS re:Invent 2023, AWS and <a href=\"https://nvidianews.nvidia.com/news/aws-nvidia-strategic-collaboration-for-generative-ai\">NVIDIA announced</a> that AWS will be the first cloud provider to offer <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA GH200 Grace Hopper Superchips</a> interconnected with NVIDIA NVLink technology through <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a> and running on Amazon Elastic Compute Cloud (Amazon EC2). This is a game-changing technology for cloud computing.</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA GH200 NVL32</a>, a rack-scale solution within NVIDIA DGX Cloud or an Amazon instance, boasts a 32-GPU <a href=\"https://www.nvidia.com/en-us/data-center/nvlink/\">NVIDIA NVLink</a> domain and a massive 19.5 TB of unified memory. Breaking through the memory constraints of a single system, it is 1.7x faster for GPT-3 training and 2x faster for <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language model (LLM)</a> inference compared to NVIDIA HGX H100.&nbsp;</p>\n\n\n\n<p>NVIDIA GH200 Grace Hopper Superchip-powered instances in AWS will feature 4.5 TB of HBM3e memory, a 7.2x increase compared to current-generation NVIDIA H100-powered EC2 P5 instances. This enables developers to run larger models, while improving training performance.&nbsp;</p>\n\n\n\n<p>Additionally, the CPU to GPU memory interconnect is 900 GB/s, which is 7x faster than PCIe Gen 5. GPUs access CPU memory in a cache-coherent way, extending the total memory available for applications. This is the first use of the NVIDIA GH200 NVL32 scale-out design, a modular reference design for supercomputing, data centers, and cloud infrastructure. It provides a common architecture for GH200 and successor processor configurations.&nbsp;</p>\n\n\n\n<p>This post explains the reference design that makes this possible and includes some representative application performance results.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA GH200 NVL32<strong>&nbsp;</strong></h2>\n\n\n\n<p>NVIDIA GH200 NVL32 is a rack-scale reference design for <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA GH200 Grace Hopper Superchips</a> connected through NVLink targeted for hyperscale data centers. NVIDIA GH200 NVL32 supports 16 dual NVIDIA Grace Hopper server nodes compatible with the <a href=\"https://www.nvidia.com/en-us/data-center/products/mgx/\">NVIDIA MGX</a> chassis design and can be liquid cooled to maximize compute density and efficiency.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1628\" height=\"1236\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_.png\" alt=\"On the left is the front view of the GH200 NVL32 rack with three power shelves at the bottom, eight GH200 compute trays, nine NVLink switch trays, eight GH200 compute trays, and then three power shelves on top.  There are seven open shelves at the top.  On the right is a back view of the GH200 NVL32 rack showing the liquid cooling manifold and the NVLink Interconnect cable cartridges that fully interconnect the GH200 nodes with the NVLink Switches.\" class=\"wp-image-74395\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_.png 1628w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-300x228.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-625x475.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-151x115.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-768x583.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-1536x1166.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-645x490.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-395x300.png 395w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-119x90.png 119w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-362x275.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-145x110.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-diagram-front-back_-1024x777.png 1024w\" sizes=\"(max-width: 1628px) 100vw, 1628px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA GH200 NVL32 is a rack-scale solution delivering a 32-GPU NVLink domain and 19.5 TB of unified memory</em></figcaption></figure></div>\n\n\n<p>The NVIDIA GH200 Grace Hopper Superchip with a coherent <a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\">NVLink-C2C</a> creates an NVLink addressable memory address space to simplify model programming. It combines high-bandwidth and low-power system memory, LPDDR5X, and HBM3e to take full advantage of NVIDIA GPU acceleration and high-performance Arm cores in a well-balanced system.&nbsp;</p>\n\n\n\n<p>GH200 server nodes are connected with an NVLink passive copper cable cartridge to enable each Hopper GPU to access the memory of any other Grace Hopper Superchip in the network, providing 32 x 624 GB, or 19.5 TB of NVLink addressable memory (Figure 1).&nbsp;&nbsp;</p>\n\n\n\n<p>This update to the NVLink Switch System uses the NVLink copper interconnect to connect 32 GH200 GPUs together using nine NVLink switches incorporating third-generation NVSwitch chips. The NVLink Switch System implements a fully connected fat-tree network for all the GPUs in the cluster. For larger scale requirements, scaling with 400 Gb/s InfiniBand or Ethernet delivers incredible performance and an energy-efficient AI supercomputing solution.</p>\n\n\n\n<p>NVIDIA GH200 NVL32 is supported by the <a href=\"https://developer.nvidia.com/hpc-sdk\">NVIDIA HPC SDK</a> and the full suite of CUDA, <a href=\"https://www.nvidia.com/en-us/technologies/cuda-x/\">NVIDIA CUDA-X</a>, and <a href=\"https://www.nvidia.com/en-us/data-center/magnum-io/\">NVIDIA Magnum IO</a> libraries, accelerating over 3,000 GPU applications. </p>\n\n\n\n<h2 class=\"wp-block-heading\">Use cases and performance results</h2>\n\n\n\n<p>NVIDIA GH200 NVL32 is ideal for LLM training and inference, recommender systems, graph neural networks (GNNs), vector databases, and <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> models, as detailed below.</p>\n\n\n\n<h3 class=\"wp-block-heading\">AI training and inference</h3>\n\n\n\n<p>Generative AI has taken the world by storm, exemplified by the groundbreaking capabilities of services like ChatGPT. LLMs such as GPT-3 and GPT-4 are enabling the integration of AI capabilities into every product in every industry, and their adoption rate is astounding.&nbsp;</p>\n\n\n\n<p>ChatGPT became the fastest application to reach 100 million users, achieving that milestone in just 2 months. The demand for generative AI applications is immense and growing exponentially.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"866\" height=\"436\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training.png\" alt=\"Bar chart comparison shows a relative performance of 1.7x for NVIDIA GH200 NVL32 on the left and 1x for the H100 NVL8 on the right.  The comparison is for GPT-3 training performance for an Ethernet data center using a batch size of 4 million tokens.\n\" class=\"wp-image-74400\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training.png 866w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-300x151.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-625x315.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-179x90.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-768x387.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-645x325.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-500x252.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-362x182.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-training-218x110.png 218w\" sizes=\"(max-width: 866px) 100vw, 866px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. An Ethernet data center with 16K GPUs using NVIDIA GH200 NVL32 will deliver 1.7x the performance of one composed of H100 NVL8, which is an NVIDIA HGX H100 server with eight NVLink-connected H100 GPUs. (Preliminary performance estimates subject to change.)&nbsp;&nbsp;</em></figcaption></figure>\n\n\n\n<p>LLMs require large-scale, multi-GPU training. The memory requirements for GPT-175B would be 700 GB, as each parameter needs four bytes (FP32).&nbsp; A combination of model parallelism and fast communications is used to avoid running out of memory with smaller memory GPUs.&nbsp;</p>\n\n\n\n<p>NVIDIA GH200 NVL32 is built for inference and for training the next generation of LLMs. Breaking through memory, communications, and computational bottlenecks with 32 NVLink-connected GH200 Grace Hopper Superchips, the system can train a trillion-parameter model over 1.7x faster than NVIDIA HGX H100.&nbsp;&nbsp;</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"734\" height=\"434\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference.png\" alt=\"A graph with the vertical axis showing relative max throughput from 0 to 3x.  The first column represents four servers with 8-way NVIDIA HGX H100 GPUs interconnected by NVLink and Ethernet connecting the four servers as the baseline at 1x. The second column shows GH200 NVL32 with 32 GPUs interconnected by NVLink and 2x relative max throughput.\" class=\"wp-image-74403\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference.png 734w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-300x177.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-625x370.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-645x381.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-500x296.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-152x90.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-362x214.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-llm-inference-186x110.png 186w\" sizes=\"(max-width: 734px) 100vw, 734px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. NVIDIA GH200 NVL32 shows 2x faster GPT-3 530B model inference performance compared to H100 NVL8 with 80 GB GPU memory. (Preliminary performance estimates subject to change.)</em></figcaption></figure></div>\n\n\n\n<p>Figure 3 shows that the NVIDIA GH200 NVL32 system outperforms four H100 NVL8 systems by 2x on a GPT-530B inference model. The large memory space of NVIDIA GH200 NVL32 also improves operational efficiency, with the ability to store multiple models on the same node and quickly swap models in to maximize utilization.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommender systems</h3>\n\n\n\n<p>Recommender systems are the engine of the personalized internet. They\u2019re used across e-commerce and retail, media and social media, digital ads, and more to personalize content. This drives revenue and business value. Recommenders use embeddings that represent users, products, categories, and context, and can range up to tens of terabytes in size.&nbsp;</p>\n\n\n\n<p>A highly accurate recommender system will provide a more engaging user experience, but also requires a larger embedding and more precise recommender. Embeddings have unique characteristics for AI models, requiring large amounts of memory at high bandwidth and lightning-fast networking.&nbsp;</p>\n\n\n\n<p>NVIDIA GH200 NVL32 with Grace Hopper provides 7x the amount of fast-access memory compared to four HGX H100 and delivers 7x the bandwidth compared to the PCIe Gen5 connections to the GPU in conventional x86-based designs. It enables 7x more detailed embeddings compared to H100 with x86.</p>\n\n\n\n<p>NVIDIA GH200 NVL32 can also deliver up to 7.9x the training performance for models with massive embedding tables. Figure 4 shows a comparison of one GH200 NVL32 system with 144 GB HBM3e memory and 32-way NVLink interconnect compared to four HGX H100 servers with 80 GB HBM3 memory connected with 8-way NVLink interconnect using a DLRM model. The comparisons were made between GH200 and H100 systems using 10 TB embedding tables and using 2 TB embedding tables.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"910\" height=\"554\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training.png\" alt=\"Bar chart; left vertical axis scale is the geomean normalized to H100 time to train.  The comparison for GH200 NVL32 to H100 NVL8 on the left shows GH200 is 2.5x faster to train a model with 2-TB embedding tables.  The comparison for GH200 NVL32 to H100 NVL8 on the right shows GH200 is 7.9X faster to train a model with 10 TB embedding tables.\n\" class=\"wp-image-74407\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training.png 910w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-300x183.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-625x380.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-768x468.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-645x393.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-493x300.png 493w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-362x220.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-recommender-training-181x110.png 181w\" sizes=\"(max-width: 910px) 100vw, 910px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. A comparison of one NVIDIA GH200 NVL32 system to four HGX H100 servers on recommender training. (Preliminary performance estimates subject to change.)</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Graph neural networks&nbsp;</h3>\n\n\n\n<p>GNNs apply the predictive power of deep learning to rich data structures that depict objects and their relationships as points connected by lines in a graph. Many branches of science and industry already store valuable data in graph databases.&nbsp;</p>\n\n\n\n<p>Deep learning is used to train predictive models that unearth fresh insights from graphs. An expanding list of organizations are applying GNNs to improve drug discovery, fraud detection, computer graphics, cybersecurity, genomics, materials science, and recommendation systems. Today\u2019s most complex graphs processed by GNNs have billions of nodes, trillions of edges, and features spread across nodes and edges.&nbsp;</p>\n\n\n\n<p>NVIDIA GH200 NVL32 provides massive CPU-GPU memory to store these complex data structures for accelerated computing. Furthermore, graph algorithms often require random accesses over these large datasets storing vertex properties.&nbsp;</p>\n\n\n\n<p>These accesses are typically bottlenecked by internode communication bandwidth. The GPU-to-GPU NVLink connectivity of NVIDIA GH200 NVL32 provides massive speedups to such random accesses. GH200 NVL32 can increase GNN training performance by up to 5.8x compared to NVIDIA H100.&nbsp;</p>\n\n\n\n<p>Figure 5 shows a comparison of one GH200 NVL32 system with 144 GB HBM3e memory and 32-way NVLink interconnect compared to four HGX H100 servers with 80 GB HBM3 memory connected with 8-way NVLink interconnect using GraphSAGE. GraphSAGE is a general inductive framework to efficiently generate node embeddings for previously unseen data.&nbsp;</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"802\" height=\"436\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training.png\" alt=\"Bar chart with a vertical axis scale from 0 to 6x to show speed up normalized to H100.  The comparison shows GH200 NVL32 at 5.8x compared to H100 NVL8 at 1x.  \" class=\"wp-image-74409\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training.png 802w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-300x163.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-625x340.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-179x97.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-768x418.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-645x351.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-500x272.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-362x197.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-faster-graph-training-202x110.png 202w\" sizes=\"(max-width: 802px) 100vw, 802px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. A comparison of one NVIDIA GH200 NVL32 system to four HGX H100 servers on graph training. (Preliminary performance estimates subject to change.)</em></figcaption></figure></div>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Amazon and NVIDIA have announced that NVIDIA DGX Cloud is coming to AWS. AWS will be the first cloud service provider to offer NVIDIA GH200 NVL32 in DGX Cloud and as an EC2 instance. The NVIDIA GH200 NVL32 solution boasts a 32-GPU NVLink domain and a massive 19.5 TB of unified memory. This setup significantly outperforms previous models in GPT-3 training and LLM inference.&nbsp;</p>\n\n\n\n<p>The CPU-GPU memory interconnect of the NVIDIA GH200 NVL32 is remarkably fast, enhancing memory availability for applications. This technology is part of a scalable design for hyperscale data centers, supported by a comprehensive suite of NVIDIA software and libraries, accelerating thousands of GPU applications. NVIDIA GH200 NVL32 is ideal for tasks like LLM training and inference, recommender systems, GNNs, and more, offering significant performance improvements to AI and computing applications.</p>\n\n\n\n<p>To learn more, check out the <a href=\"https://reinvent.awsevents.com\">AWS re:Invent Keynote</a> and the <a href=\"https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper\">NVIDIA GH200 Grace Hopper Superchip Architecture Whitepaper</a>. You can also watch the <a href=\"https://youtu.be/6g0v3tMK2LU?si=x7H7DMgqxlj2gCkb\">NVIDIA SC23 Special Address</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>At AWS re:Invent 2023, AWS and NVIDIA announced that AWS will be the first cloud provider to offer NVIDIA GH200 Grace Hopper Superchips interconnected with NVIDIA NVLink technology through NVIDIA DGX Cloud and running on Amazon Elastic Compute Cloud (Amazon EC2). This is a game-changing technology for cloud computing. The NVIDIA GH200 NVL32, a rack-scale &hellip; <a href=\"https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/\">Continued</a></p>\n", "protected": false}, "author": 1279, "featured_media": 74425, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1305947", "discourse_permalink": "https://forums.developer.nvidia.com/t/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/274298", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1968, 1903], "tags": [296, 1051, 453, 3099, 3052, 2780, 2932, 1958, 110, 1914, 3596], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-gh200-nvl32-supercomputer-.png.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jiU", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74208"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1279"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74208"}], "version-history": [{"count": 43, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74208/revisions"}], "predecessor-version": [{"id": 74604, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74208/revisions/74604"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74425"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74208"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74208"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74208"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74346, "date": "2023-11-28T10:10:50", "date_gmt": "2023-11-28T18:10:50", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74346"}, "modified": "2024-01-22T13:25:24", "modified_gmt": "2024-01-22T21:25:24", "slug": "build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/", "title": {"rendered": "Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\" target=\"_blank\">Large language models</a> (LLMs) are transforming the AI landscape with their profound grasp of human and programming languages. Essential for next-generation enterprise productivity applications, they enhance user efficiency across tasks like programming, copy editing, brainstorming, and answering questions on a wide range of topics.</p>\n\n\n\n<p>However, these models often struggle with real-time events and specific knowledge domains, leading to inaccuracies. Fine-tuning these models can enhance their knowledge, but it&#8217;s costly and requires regular updates.&nbsp;</p>\n\n\n\n<p><a rel=\"noreferrer noopener\" href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\" target=\"_blank\">Retrieval-augmented generation</a> (RAG) offers a solution by combining information retrieval with LLMs for open-domain question-answering applications. RAG provides LLMs with vast, updatable knowledge, effectively addressing these limitations (Figure 1). NVIDIA NeMo Retriever, the latest service in the <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\" target=\"_blank\">NVIDIA NeMo framework</a>, optimizes the embedding and retrieval part of RAG to deliver higher accuracy and more efficient responses.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"527\" height=\"317\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture.png\" alt=\"High-level overview of RAG architecture.\" class=\"wp-image-74348\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture.png 527w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture-300x180.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture-179x108.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture-362x218.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-A-high-level-retrieval-augmented-generation-architecture-183x110.png 183w\" sizes=\"(max-width: 527px) 100vw, 527px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A high-level retrieval augmented generation architecture</em></figcaption></figure></div>\n\n\n<p>This post provides an overview of how RAG pipeline components work and the enterprise challenges associated with creating RAG-enabled AI applications, such as commercial viability. You will learn about NeMo Retriever, which includes production-ready components for enterprise RAG pipelines, and the model we are sharing today.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">A canonical RAG pipeline</h2>\n\n\n\n<p>RAG applications typically have multiple stages, from embedding to retrieval and response. Let\u2019s look at the canonical RAG pipeline to understand how the NeMo Retriever can help.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Encoding the knowledge base (offline)</h3>\n\n\n\n<p>In this phase, the knowledge base, typically consisting of documents in text, PDF, HTML, or other formats, is fragmented into chunks. These chunks are then fed to an embedding deep learning model, which produces a dense vector representation for each chunk. </p>\n\n\n\n<p>The resulting embeddings, along with their corresponding documents and other metadata, are stored in a vector database (Figure 2). The chunking strategy depends on the type and content of documents, the use of metadata (such as document details), and the method of generating synthetic data if applicable.  It must be carefully considered when developing a retrieval system.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"119\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database.png\" alt=\"Depiction of the offline part of a RAG pipeline chunking, embedding, and storing a knowledge base into a vector database.\" class=\"wp-image-74350\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database-300x57.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database-500x95.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database-160x31.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database-362x69.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-The-process-of-chunking-knowledge-base-documents-embedding-them-and-storing-them-in-a-vector-database-577x110.png 577w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The process of chunking knowledge-base documents, embedding them, and storing them in a vector database</em></figcaption></figure></div>\n\n\n<p>The embeddings can be used for semantic search, by calculating the similarity (for example, dot product) between embeddings from a user\u2019s query and those of the documents stored in the database. Vector databases are specialized in storing vast amounts of vectorized data and can perform fast <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/#:~:text=When%20exact%20results%20are%20not%20needed%2C%20approximate%20nearest%20neighbor%20(ANN)%20methods%20can%20often%20reduce%20the%20number%20of%20distance%20computations%20that%20must%20be%20performed%20during%20search.\" target=\"_blank\">approximate nearest-neighbor</a> searches.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Deployment (online)</h3>\n\n\n\n<p>This stage focuses on deployment when the vector database is connected to the LLM application so it can answer questions in real time. It has two phases\u2014retrieval from the vector database and generating a response.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Phase 1. Retrieval from vector database based on the user&#8217;s query\u200b</h4>\n\n\n\n<p>The user\u2019s query is first embedded as a dense vector. Typically, a special prefix is added to the query, so that the embedding model used by the retriever can understand that it is a question. This enables <em>asymmetric semantic search</em>, where a short query can be used to find a longer paragraph that answers the query.</p>\n\n\n\n<p>Next, the query embedding is used to search a vector database that retrieves a small number of the most relevant document chunks to the user\u2019s query (Figure 3). </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"120\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks.png\" alt=\"Searching with query embeddings over the vector database returns relevant chunks.\" class=\"wp-image-74351\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks-300x58.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks-500x96.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks-160x31.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks-362x70.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-The-query-embedding-is-used-to-search-the-vector-database-which-returns-the-Top-k-most-relevant-chunks-572x110.png 572w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The query embedding is used to search the vector database, which returns the Top k most relevant chunks</em></figcaption></figure></div>\n\n\n<p>The vector database achieves this by employing a similarity/distance measure, such as cosine similarity, with an approximate search algorithm. This guarantees scalability and low latency.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Phase 2. Use an LLM to generate a response leveraging the context</h4>\n\n\n\n<p>In this phase, the most relevant chunks are combined to form a context, which is then combined with the user\u2019s query as the final input for the LLM. The prompt usually contains extra instructions to guide the LLM to generate the response based on the context only.&nbsp;Figure 4 illustrates this response generation process.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"119\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response.png\" alt=\" The LLM returns a response with a prompt made of a user query, top retrieved chunks, and a prompt.\" class=\"wp-image-74352\" style=\"aspect-ratio:5.243697478991597;width:624px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response-300x57.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response-500x95.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response-160x31.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response-362x69.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-The-LLM-takes-in-the-user-query-along-with-the-top-k-retrieved-chunks-and-the-prompt-to-return-a-response-577x110.png 577w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. The LLM takes in the user query along with the top-k retrieved chunks, and the prompt to return a response</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Challenges of building a RAG pipeline for enterprise applications</h2>\n\n\n\n<p>While RAG offers significant advantages over an LLM by itself, it&#8217;s important to note that its benefits come with several challenges that must be addressed. One major issue is finding a commercially viable retriever, often constrained by licensing restrictions in training datasets like <a href=\"https://microsoft.github.io/msmarco/\">MSMARCO</a>. Real-world queries further complicate matters with their ambiguity; users tend to enter incomplete or vague queries, making retrieval difficult.</p>\n\n\n\n<p>In multi-turn conversations, this complexity increases as user queries often reference earlier parts of the conversation, necessitating contextual understanding for effective retrieval. Additionally, some queries require synthesizing information from multiple sources, demanding advanced integration capabilities.</p>\n\n\n\n<p>For LLMs, handling long-context inputs is a challenge. These models, despite continuous improvements, often struggle with forgetting details in lengthy inputs and require substantial computational resources, which become more pronounced in multi-turn scenarios.</p>\n\n\n\n<p>Deployment of these systems involves complex RAG pipelines, which include various microservices like embedding, vector databases, and LLMs. Setting up and managing these services in a secure, efficient manner is a significant task.</p>\n\n\n\n<p>For more information about how to build a production-grade RAG pipeline, refer to the <a href=\"https://nvda.ws/47OvlMU\">NVIDIA/GenerativeAIExamples</a> GitHub repo.&nbsp;</p>\n\n\n\n<p>Let\u2019s look at how NeMo Retriever\u2014which brings an optimized, commercially viable set of tools\u2014streamlines the retrieval process in complex scenarios.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA NeMo Retriever for retrieval-augmented generation</h2>\n\n\n\n<p>We <a href=\"https://nvidianews.nvidia.com/news/nemo-retriever-generative-ai-microservice\" target=\"_blank\" rel=\"noreferrer noopener\">announced</a> the latest addition to the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NeMo framework</a>, NVIDIA NeMo Retriever, an information retrieval service that can be deployed on-premises or in the cloud. It provides a secure and simplified path for enterprises to integrate enterprise-grade RAG capabilities into their customized production AI applications.</p>\n\n\n\n<p>NeMo Retriever aims to provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It also features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pretrained models available as starting points, developers can also quickly customize them for their domain-specific use cases, such as IT or HR help assistants, and R&amp;D research assistants.</p>\n\n\n\n<p>Today, we are sharing our embedding model, optimized for text question-answering retrieval. We are in the process of developing reranking and retrieval microservices, which will be available soon.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA Retrieval QA Embedding Model&nbsp;</h2>\n\n\n\n<p>An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are usually transformer encoders that process tokens of input text (for example, question, passage) to output an embedding.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"331\" height=\"317\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture.png\" alt=\"The figure depicts the Bi-encoder retrieval architecture. Query and passage texts are embedded through encoders. Cosine similarity is used to determine how close they are.\" class=\"wp-image-74353\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture.png 331w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture-300x287.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture-120x115.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture-313x300.png 313w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture-94x90.png 94w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-Bi-encoder-retrieval-architecture-115x110.png 115w\" sizes=\"(max-width: 331px) 100vw, 331px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Bi-encoder retrieval architecture</em></figcaption></figure></div>\n\n\n<p>Embedding models for text retrieval are typically trained using a bi-encoder architecture, such as the one depicted in Figure 5. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question.</p>\n\n\n\n<p>The NVIDIA Retrieval QA Embedding Model is a transformer encoder\u2014a fine-tuned version of E5-Large-Unsupervised, with 24 layers and an embedding size of 1024, trained on private and public datasets. It supports a maximum input of 512 tokens. Furthermore, we are committed to investigating cutting-edge model architectures and datasets to enable state-of-the-art (SOTA) retrieval models with NeMo Retriever.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Training dataset&nbsp;</h3>\n\n\n\n<p>The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named <a href=\"https://microsoft.github.io/msmarco/\" target=\"_blank\" rel=\"noreferrer noopener\">MSMARCO</a> restricts \u200ccommercial licensing, limiting the use of these models in commercial settings. To address this, we created our own internal open-domain QA dataset to train a commercially viable embedding model.&nbsp;</p>\n\n\n\n<p>We searched weblogs for passages related to NVIDIA proprietary data collection and chose a set of passages that were relevant to customer use cases. These passages were annotated by the NVIDIA internal data annotation team.&nbsp;</p>\n\n\n\n<p>To minimize the redundancy in our data collection process, we selected samples that maximized relevancy distance scores and increased diversity in the data. The pretrained embedding model was fine-tuned using a mixture of English language datasets, which includes our proprietary dataset, along with selected samples from public datasets that are available for commercial use.&nbsp;</p>\n\n\n\n<p>Our main objective was to refine information retrieval capabilities, specifically tailoring the embedding model for the common enterprise LLM use case of text-based question-and-answering over knowledge bases.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Evaluation results</h3>\n\n\n\n<p>The NVIDIA Retrieval QA Embedding Model is focused on question-answering applications. This is an <em>asymmetric semantic search</em> problem, as the questions and passages typically have different distributions and patterns\u2014the questions generally being shorter than paragraphs that contain the answer.</p>\n\n\n\n<p>We evaluated our embedding model with real internal customer datasets from telco, IT, consulting, and energy industries. The metric was Recall@5, to emulate a RAG scenario where we provided the top five most relevant passages as context in the prompt for the LLM model to respond to the question. We compared our model\u2019s information retrieval accuracy to a number of well-known embedding models made available by the AI community, including ones trained on non-commercial datasets, which are marked with a *. Recall@5 is a measure of how often the relevant item is present in the top five retrieved items.</p>\n\n\n\n<p>You can see the results of the benchmark in Figure 6. Notice that our retriever model achieves the best performance among those baselines.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"536\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-625x536.png\" alt=\"Bar Chart comparing Average Recall@5 scores of various community models in comparison to NVIDIA Retrieval QA Embedding Model, on internal customer datasets.\" class=\"wp-image-74450\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-625x536.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-300x258.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-134x115.png 134w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-768x659.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-645x554.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-350x300.png 350w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-105x90.png 105w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-362x311.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-128x110.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall-1024x879.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/avg-Recall.png 1200w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Average Recall@5 on customer datasets in telco, IT, consulting, and energy industries</em></figcaption></figure></div>\n\n\n<p>As shown in Figure 7, we compared the NVIDIA Retrieval QA Embedding Model to popular open source and commercial retriever models on academic benchmarks NQ, HotpotQA, <a href=\"https://huggingface.co/datasets/BeIR/fiqa\">FiQA</a> from BeIR benchmark, and the <a href=\"https://arxiv.org/pdf/1911.02984v1.pdf\">TechQA</a> dataset. In this benchmark, the metric used is <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\">Normalized Discounted Cumulative Gain</a>@10 (NDCG@10).</p>\n\n\n\n<div class=\"wp-block-image aligncenter\">\n<figure class=\"aligncenter size-full-page-width is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"386\" class=\"wp-image-74452\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-625x386.png\" alt=\"A bar chart comparing open-source and commercial retrieval models in comparison with NVIDIA retrieval QA embedding model.\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-300x185.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-768x474.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-645x398.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-486x300.png 486w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-362x223.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10-1024x632.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Average-NDCG@10.png 1368w\" sizes=\"(max-width: 625px) 100vw, 625px\" />\n<figcaption class=\"wp-element-caption\"><em>Figure 7. Average NDCG@10 comparing various open-source and commercial retrieval models on NQ, HotpotQA, FiQA, and TechQA datasets. The NVIDIA Retrieval QA model outperforms the others in terms of average NDCG@1</em>0</figcaption>\n</figure>\n<div>\u00a0</div>\n</div>\n\n\n\n<p>Note that the techQA dataset, consisting of questions and answers curated from the IBM technical forum together with 800k technotes as the knowledge base, wasn\u2019t used in a retrieval benchmark setting before. We provide a <a rel=\"noreferrer noopener\" href=\"https://gist.github.com/radekosmulski/524600c280ce3d8ee430dfdfa55abed6#file-process_data-ipynb\" target=\"_blank\">notebook</a> to convert this dataset to a BEIR-compliant format for benchmarking.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Getting started</h2>\n\n\n\n<p>The NVIDIA Retrieval QA Embedding Model will be available soon as part of a microservices container in early access (EA). Apply to be a part of the <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/nemo-microservices-early-access\" target=\"_blank\">EA program</a>.</p>\n\n\n\n<p>You can also gain free-trial access to the NVIDIA Retrieval QA embedding API in the <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-29k\" target=\"_blank\">NGC catalog</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">NVIDIA Retrieval QA Embedding Playground API</h3>\n\n\n\n<p>The NVIDIA Retriever QA Embedding Model is a fine-tuned version of<a href=\"https://huggingface.co/intfloat/e5-large-unsupervised\"> E5-Large-Unsupervised</a> and a similar input format requirement applies. When making a request, you must indicate if it is a &#8220;query&#8221; or &#8220;passage&#8221; in the payload. This is necessary for asymmetric tasks such as passage retrieval in open QA.</p>\n\n\n\n<p>The API accepts a simple payload format, with the \u201cinput\u201d being the chunk of text to produce embedding. In the following example API call, we embed two longer texts as \u201cpassages\u201d and one smaller \u201cquery\u201d text and then compute the similarity between the passages and the query using the dot product.&nbsp;</p>\n\n\n\n<p>Note that you must log in the NGC AI Playground and obtain an API key, which is used as \u201cAPI_KEY\u201d in the following snippet.</p>\n\n\n\n1. Embed two passages.\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nimport requests    \nimport numpy as np\n\n# Make sure to fill this. You will need to obtain the API_KEY from NGC playground\nAPI_KEY=&quot;&lt;YOUR_NGC_PLAYGROUND_API_KEY&gt;&quot;\n\ninvoke_url = &quot;https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/091a03bb-7364-4087-8090-bd71e9277520&quot;\nfetch_url_format = &quot;https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/&quot;\nheaders = {\n    &quot;Authorization&quot;: &quot;Bearer {}&quot;.format(API_KEY),\n    &quot;Accept&quot;: &quot;application/json&quot;,\n}\n\n# To re-use connections\nsession = requests.Session()\n\n# Note the &quot;model&quot;: &quot;passage&quot; field in the payload.\npassage_payload = {\n  &quot;input&quot;: &#91;&quot;Pablo Ruiz Picasso was a Spanish painter, sculptor, printmaker, ceramicist and theater designer who spent most of his adult life in France.&quot;,\n            &quot;Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.&quot;],\n    &quot;model&quot; : &quot;passage&quot;,\n    &quot;encoding_format&quot;: &quot;float&quot;\n}\n\npassage_response = session.post(invoke_url, headers=headers, json=passage_payload)\nwhile passage_response.status_code == 202:\n    request_id = passage_response.headers.get(&quot;NVCF-REQID&quot;)\n    fetch_url = fetch_url_format + request_id\n    passage_response = session.get(fetch_url, headers=headers)\n    \npassage_response.raise_for_status()\npassage_embeddings = np.asarray(&#91;item&#91;'embedding'] for item in passage_response.json()&#91;'data']])\n</pre></div>\n\n\n<p>2. Embed the query </p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n# Note the &quot;model&quot;: &quot;query&quot; field in the payload\nquery_payload = {\n  &quot;input&quot;: &quot;Who is a great particle physicist?&quot;,\n    &quot;model&quot; : &quot;query&quot;,\n    &quot;encoding_format&quot;: &quot;float&quot;\n}\nquery_response = session.post(invoke_url, headers=headers, json=query_payload)\nwhile query_response.status_code == 202:\n    request_id = query_response.headers.get(&quot;NVCF-REQID&quot;)\n    fetch_url = fetch_url_format + request_id\n    query_response = session.get(fetch_url, headers=headers)\n    \nquery_response.raise_for_status()\nquery_embedding = np.asarray(query_response.json()&#91;'data']&#91;0]&#91;'embedding'])\n</pre></div>\n\n\n<p>3. Calculate the similarity between the passage and query embeddings </p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n# A simple dot product\nnp.dot(passage_embeddings, query_embedding)\n</pre></div>\n\n\n<p>Output:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\narray(&#91;0.33193235, 0.52141018])\n</pre></div>\n\n\n<p>In this example, the query shares more similarities with the second paragraph, both related to the physics domain.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>NVIDIA NeMo Retriever provides \u200can embedding service tailored for question-answering applications. The embedding model and service are provided under a commercial use license. While this model has shown promising results on several public and internal benchmarks, we\u2019re working on continually improving the model\u2019s quality.</p>\n\n\n\n<p>Be sure to <a href=\"https://developer.nvidia.com/nemo-microservices-early-access\" target=\"_blank\" rel=\"noreferrer noopener\">apply for early access</a> to NeMo Retriever microservices, and expect more to come in future releases.</p>\n\n\n\n<p>To get exclusive access to over 600 SDKs and AI models, free training, and network with our community of technical experts, join the free <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/join-nvidia-developer-program?nvid=nv-int-tblg-164089-vt33\" target=\"_blank\">NVIDIA Developer Program</a>. For a limited time, new members will get a free self-paced course from the <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/training/\" target=\"_blank\">NVIDIA Deep Learning Institute</a> upon joining.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) are transforming the AI landscape with their profound grasp of human and programming languages. Essential for next-generation enterprise productivity applications, they enhance user efficiency across tasks like programming, copy editing, brainstorming, and answering questions on a wide range of topics. However, these models often struggle with real-time events and specific knowledge &hellip; <a href=\"https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/\">Continued</a></p>\n", "protected": false}, "author": 953, "featured_media": 74362, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1305946", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/274297", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1903], "tags": [453, 2932, 3613, 3270], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GenAI-Promo-AWS-DevNews-PRESS-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jl8", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74346"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/953"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74346"}], "version-history": [{"count": 42, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74346/revisions"}], "predecessor-version": [{"id": 77059, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74346/revisions/77059"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74362"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74346"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74346"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74346"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74145, "date": "2023-11-28T09:00:00", "date_gmt": "2023-11-28T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74145"}, "modified": "2023-12-14T11:27:38", "modified_gmt": "2023-12-14T19:27:38", "slug": "simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/", "title": {"rendered": "Simulating Realistic Traffic Behavior with a Bi-Level Imitation Learning AI Model"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>From last-minute cut-ins to impromptu U-turns, human drivers can be incredibly unpredictable. This unpredictability stems from the complex nature of human decision-making, which is influenced by multiple factors and varies across different operational design domains (ODD) and countries, making it difficult to emulate in simulation.</p>\n\n\n\n<p>Yet, autonomous vehicle (AV) developers need to confidently develop and deploy systems that can operate in multiple ODDs with varying traffic behaviors. In the recently published paper, <a href=\"https://arxiv.org/abs/2208.12403\">BITS: Bi-Level Imitation for Traffic Simulation</a>, the NVIDIA Research team outlines a novel approach to simulating real-world traffic behavior that enables developers to do just that.&nbsp;</p>\n\n\n\n<p>Bi-Level Imitation for Traffic Simulation (BITS) is a traffic model that captures the complexity of the real world with incredible fidelity while also outperforming previous methods. In a trial detailed in the paper, BITS improved coverage and diversity over the next best-performing model by 64% and 118%, respectively, and lowered failure rates by 36%.&nbsp;</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"177\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-planning-traffic-route.gif\" alt=\"Side-by-side views of the BITS model planning a traffic route - one showing prediction and the other showing the controller. \" class=\"wp-image-74184\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. By decoupling the traffic modeling process, BITS enables more realistic traffic simulation</em></em></figcaption></figure></div>\n\n\n\n<h2 class=\"wp-block-heading\">Traffic modeling challenges</h2>\n\n\n\n<p>Most simulators model traffic behavior by either replaying recorded data or using a predefined rule-based system to govern vehicle motion.</p>\n\n\n\n<p>While replaying data enables accurate review and testing of specific scenarios encountered in real-world driving, it is difficult to simulate behaviors outside of those already recorded. On the other hand, rule-based controllers are limited to simple behaviors, preventing accurate simulation of more complex situations.</p>\n\n\n\n<p>There are also learning-based approaches, which are trained on real-world driving logs to predict realistic future trajectories. While these models have proven effective in creating accurate and dynamic driving paths, they struggle to produce diverse trajectories that respect road boundaries and the presence of other agents.</p>\n\n\n\n<p>BITS decouples the AI model into a high-level intent prediction and a low-level controller that achieves the overarching intent. By doing so, the model can synthesize a broad spectrum of traffic patterns that closely resemble real-world behavior, while also generating specific scenarios.&nbsp;</p>\n\n\n\n<p>When BITS is run alongside other AI-powered traffic models, it consistently displays variety in traffic patterns while maintaining low failure rates (Figure 2).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"891\" height=\"164\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison.png\" alt=\"Three bar charts comparing BITS model performance with three other learning-based models in coverage, diversity, and failure. BITS shows the highest levels of coverage and diversity and the lowest in failure rates.\n\" class=\"wp-image-74153\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison.png 891w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-300x55.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-625x115.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-179x33.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-768x141.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-645x119.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-500x92.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-160x29.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-362x67.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bits-model-performance-comparison-598x110.png 598w\" sizes=\"(max-width: 891px) 100vw, 891px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. BITS shows the highest levels of coverage and diversity and the lowest in failure rates</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">The BITS approach</h2>\n\n\n\n<p>BITS achieves such high levels of fidelity and diversity due to its hierarchical structure.&nbsp;</p>\n\n\n\n<p>Both branches of the model are trained on real-world traffic logs. The high-level network is trained to identify possible goals for the vehicle, and the low-level network is trained to determine a policy that achieves the predicted goal. By splitting up these tasks, we can move the burden of modeling different trajectories to the high-level goal predictor, so the low-level goal-oriented policy can operate more efficiently.</p>\n\n\n\n<p>BITS also includes a prediction-and-planning module to help stabilize the model in new environments and over longer time horizons. It achieves this by reviewing the model\u2019s possible trajectories and selecting those that follow the rules of plausible driving behavior. This reduces the risk of diverging away from reasonable behaviors.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Evaluating BITS quality</h2>\n\n\n\n<p>Determining whether the behavior of a traffic model is realistic, as well as its ability to generate accurate and unseen scenarios, is incredibly difficult. This is because there is no ground truth for direct comparison. Thus, evaluating the BITS traffic model presents its own challenge.&nbsp;</p>\n\n\n\n<p>As detailed in <a href=\"https://arxiv.org/abs/2208.12403\">BITS: Bi-Level Imitation for Traffic Simulation</a>, we divide our evaluation into three domains: rollout metrics (coverage, diversity, and failure rates), statistical differences compared to the real world, and resemblance to human drivers.</p>\n\n\n\n<p>The first domain directly measures the low-level network in terms of its coverage area, the diversity of each run, and the frequency of collisions or off-road driving incidents. The second domain compares the speed and jerk differences of the simulated cars to real-world data. The third domain measures human-like behavior by comparing it to a prediction model that forecasts the agent&#8217;s future position at a given timestamp.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"793\" height=\"369\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1.png\" alt=\"2D sketches of car trajectories, organized by four traffic models over five trials. The TPP and TrafficSim models show little variety in repeated trials, while the BITS model shows different trajectories across all five trials.\n\" class=\"wp-image-74156\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1.png 793w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-300x140.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-625x291.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-179x83.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-768x357.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-645x300.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-500x233.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-362x168.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-trajectories-learning-based-traffic-models-1-236x110.png 236w\" sizes=\"(max-width: 793px) 100vw, 793px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Comparison of trajectories planned by various learning-based traffic models</em></em></figcaption></figure>\n\n\n\n<p>As shown in Figures 2 and 3, while other models exhibit tradeoffs between generating diverse trajectories and falling into repeated behaviors, BITS charts a new scenario each time with lower failure rates.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>The ability to model realistic traffic behavior in simulation is critical to developing robust AV technology. By optimizing fidelity and diversity, BITS brings AI-generated traffic simulation even closer to the complexity of the real world. We aim to further develop and refine BITS, and ultimately integrate it into the production <a href=\"https://developer.nvidia.com/drive/simulation\">NVIDIA DRIVE Sim</a> pipeline.</p>\n\n\n\n<p>We invite the industry to use and contribute to this developing work in simulation, which is open-sourced at <a href=\"https://github.com/NVlabs/traffic-behavior-simulation\">NVlabs/traffic-behavior-simulation</a> on GitHub.&nbsp;We are also building and open-sourcing <a href=\"https://github.com/NVlabs/trajdata\">trajdata</a>, a software tool that unifies data formats from different AV datasets and transforms scenes from existing datasets into interactive simulation environments.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>From last-minute cut-ins to impromptu U-turns, human drivers can be incredibly unpredictable. This unpredictability stems from the complex nature of human decision-making, which is influenced by multiple factors and varies across different operational design domains (ODD) and countries, making it difficult to emulate in simulation. Yet, autonomous vehicle (AV) developers need to confidently develop and &hellip; <a href=\"https://developer.nvidia.com/blog/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/\">Continued</a></p>\n", "protected": false}, "author": 1932, "featured_media": 74385, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1305875", "discourse_permalink": "https://forums.developer.nvidia.com/t/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/274289", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [2892, 3366, 453, 1962, 1409, 1877], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/traffic-scenario-bits-gif.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jhT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74145"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1932"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74145"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74145/revisions"}], "predecessor-version": [{"id": 74470, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74145/revisions/74470"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74385"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74145"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74145"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74145"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74286, "date": "2023-11-27T14:02:15", "date_gmt": "2023-11-27T22:02:15", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74286"}, "modified": "2023-12-14T11:27:39", "modified_gmt": "2023-12-14T19:27:39", "slug": "new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai/", "title": {"rendered": "New Risk Calculation Record in Financial Services with Dell Technologies and NVIDIA H100 System for HPC and AI"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>End clients are working on converged HPC quant finance and AI business solutions. Dell Technologies, along with NVIDIA, is uniquely positioned to accelerate generative AI workloads and data analytics as well as <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">high performance computing</a> (HPC) quantitative financial applications where converged HPC quantitative finance plus AI workloads are the need of the hour for clients.&nbsp;</p>\n\n\n\n<p>Dell and NVIDIA initially covered quantitative applications setting new records on NVIDIA Certified Dell PowerEdge XE9680<em> </em>servers with NVIDIA GPUs. The system was independently audited by the Strategic Technology Analysis Center (STAC) on financial quantitative HPC workloads. For more information, see <a href=\"https://developer.nvidia.com/blog/nvidia-h100-system-sets-records-for-hpc-and-generative-ai-financial-risk-calculations/\">NVIDIA H100 System for HPC and Generative AI Sets Record for Financial Risk Calculations</a>.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H00 Tensor Core GPUs</a> were featured in a stack that set several records in a recent <a href=\"https://stacresearch.com/news/NVDA231030\">STAC-A2 audit</a> with eight NVIDIA H100 SXM5 80 GiB GPUs, offering incredible speed with great efficiency and cost savings. Such systems are ideal for both HPC quantitative financial applications and AI deep learning neural net based workloads.&nbsp;</p>\n\n\n\n<p>Designed by quants and technologists from some of the world&#8217;s largest banks, STAC-A2 is a technology benchmark standard based on financial market risk analysis. The benchmark is a Monte Carlo estimation of Heston-based Greeks for path-dependent, multi-asset options with early exercise.</p>\n\n\n\n<p>STAC recently performed STAC-A2 Benchmark tests performance, scaling, quality, and resource efficiency. The stack under test (SUT) was a Dell PowerEdge XE9680 server with eight NVIDIA H100 SXM5 80 GiB GPUs. Compared to all publicly reported solutions to date, this system set numerous performance and efficiency records:</p>\n\n\n\n<ul>\n<li>The highest throughput (561 options / second) (<em>STAC-A2.\u03b22.HPORTFOLIO.SPEED</em>)</li>\n\n\n\n<li>The fastest warm time (7.40 ms) in the baseline Greeks benchmark (<em>STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD]</em>)</li>\n\n\n\n<li>The fastest warm (160 ms) and cold (598 ms) times in the large Greeks benchmarks (<em>STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD]</em>)&nbsp;&nbsp;&nbsp;</li>\n\n\n\n<li>The most correlated assets (440) and Monte Carlo paths (316,000,000) simulated in 10 minutes (<em>STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS]</em>)</li>\n\n\n\n<li>The best energy efficiency (364,945 options / kWh) (<em>STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF</em>)</li>\n</ul>\n\n\n\n<p>Compared to a liquid-cooled solution using four GPUs (<a href=\"https://stacresearch.com/news/INTC230927\">INTC230927</a>), this NVIDIA 8-GPU solution set the following records:</p>\n\n\n\n<ul>\n<li>16% more energy-efficient (<em>STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF</em>)&nbsp;</li>\n\n\n\n<li>2.5x / 1.8x the speed in the warm / cold runs of the large Greeks benchmarks (<em>STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD]</em>)</li>\n\n\n\n<li>1.2x / 7.5 x the speed in the warm / cold runs of the baseline Greeks benchmarks (<em>STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD]</em>)</li>\n\n\n\n<li>Simulated 2.4x the correlated assets and 316x the Monte Carlo paths in 10 minutes (<em>STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS]</em>)</li>\n\n\n\n<li>2.0x the throughput (<em>STAC-A2.\u03b22.HPORTFOLIO.SPEED</em>)</li>\n</ul>\n\n\n\n<p>Compared to a solution using eight NVIDIA H100 PCIe GPUs, as well as previous versions of the NVIDIA STAC Pack and CUDA, this solution using NVIDIA H100 SXM5 GPUs set the following records:</p>\n\n\n\n<ul>\n<li>1.59x the throughput (<em>STAC-A2.\u03b22.HPORTFOLIO.SPEED</em>)</li>\n\n\n\n<li>1.17x the speed in the warm runs of the baseline Greeks benchmark (<em>STAC-A2.\u03b22.GREEKS.TIME.[WARM | COLD]</em>)</li>\n\n\n\n<li>3.1x / 3.0x the speed in the warm / cold runs of the large Greeks benchmarks (<em>STAC-A2.\u03b22.GREEKS.10-100K-1260.TIME.[WARM | COLD]</em>)</li>\n\n\n\n<li>Simulated 10% more correlated assets in 10 minutes (<em>STAC-A2.\u03b22.GREEKS.[MAX_ASSETS | MAX_PATHS]</em>)</li>\n\n\n\n<li>17% more energy-efficient (<em>STAC-A2.\u03b22.HPORTFOLIO.ENERG_EFF</em>)</li>\n</ul>\n\n\n\n<p>In addition to the hardware, NVIDIA provides all the key software component layers with the NVIDIA HPC SDK. This offers multiple options to developers, including NVIDIA CUDA SDK for CUDA/C++ and enabling other languages and directive-based solutions such as OpenMP, OpenACC, accelerations with C++ 17 standard parallelism, and Fortran parallel constructs.&nbsp;</p>\n\n\n\n<p>This particular implementation was developed on CUDA 12.2 using the highly optimized libraries delivered with CUDA:&nbsp;</p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/cublas\">cuBLAS</a>: The GPU-enabled implementation of the linear algebra package BLAS.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/curand\">cuRAND</a>: A parallel and efficient GPU implementation of random-number generators.</li>\n</ul>\n\n\n\n<p>The implementation was supported by tools:</p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA Nsight Systems</a> for timeline profiling</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/nsight-compute\">NVIDIA Nsight Compute</a> for kernel profiling</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/compute-sanitizer\">NVIDIA Compute Sanitizer</a> and <a href=\"https://docs.nvidia.com/cuda/cuda-gdb/index.html\">CUDA-GDB</a> for debugging</li>\n</ul>\n\n\n\n<p>The different operations of the benchmark are implemented using building blocks from a set of quantitative finance reference implementations of state-of-the-art algorithms, including asset diffusion and Longstaff-Schwartz pricing. These are exposed in a modular and maintainable framework using object-oriented programming in CUDA/C++. &nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">HPC plus AI extensions</h2>\n\n\n\n<p>End customers perform other price-discovery, market-risk calculations in their real-world workflow calculations:</p>\n\n\n\n<ul>\n<li>Sensitivity Greeks</li>\n\n\n\n<li>Profit and loss (P&amp;L) calculations</li>\n\n\n\n<li>Value at risk (VaR)</li>\n\n\n\n<li>Margin and counterparty credit risk (CCR) calculations, such as credit valuation adjustment (CVA)</li>\n</ul>\n\n\n\n<p>Pricing/risk calculation, algorithmic trading model development, and backtesting need a robust scalable environment. In areas such as CVA, such scaled setups on dense server nodes with eight NVIDIA GPUs have been shown to reduce the number of required nodes from 100 to 4 in simulation\u2013 and compute-intensive calculations (separately from STAC benchmarking). This reduces the total cost of ownership (TCO).</p>\n\n\n\n<p>We are increasingly seeing a convergence of extended HPC quantitative finance and AI requirements from end users where end financial solutions use a combination of quantitative finance, data engineering and analytics, ML, and AI neural net algorithms.&nbsp;</p>\n\n\n\n<p>The current solutions work with traditional quantitative models, extended with data engineering and analytics with accelerated machine learning (ML), such as XGBOOST, with tools such as <a href=\"https://developer.nvidia.com/rapids\">NVIDIA RAPIDS</a>, and AI deep learning. Deep learning involves long short-term memory (LSTM), recurrent neural networks (RNNs), and other advanced areas such as large language models (LLMs).</p>\n\n\n\n<p>Also in today\u2019s AI world, we have gone on to new topic areas such as <em>generative AI</em>. Generative AI refers to AI algorithms that enable computers to use existing or past content like text, audio, and video files, images, and even code to generate new content. Examples of these AI generative models include the range of generative pretrained transformer (GPT) models to diffusion models used in image generation.&nbsp;</p>\n\n\n\n<p>More diverse HPC workloads are becoming prominent in financial areas using reinforcement learning (RL) and applied in financial areas such as <a href=\"https://developer.nvidia.com/blog/limit-order-book-dataset-generation-for-accelerated-short-term-price-prediction-with-rapids/\">limit order book price prediction</a>.&nbsp;</p>\n\n\n\n<p>Agent-based models are used to reproduce the interactions between economic agents, such as market participants in financial transactions. This is a trial and error process akin to a child learning the real world. The agent operates in an environment that has an end task with states (observable or not observed) and potential actions with positive and negative rewards to those other states with simulation. As a result, the states have a value function that is also iteratively updated based on the trail and error process. The agent must solve for the optimal policy behavior equivalent to strategy.&nbsp;</p>\n\n\n\n<p>In most real-world problems, the optimal behavior is hard to determine. With a learnable policy, the optimal policy with its associated action can be evaluated as good or bad where the agent performs an action or set of actions with rewards and RL is the technique to maximize this reward.&nbsp;</p>\n\n\n\n<p>Such RL algorithms have been extended to RL from human feedback (RLHF), which makes use of LLMs trained to optimize policy rewards, such as<a href=\"https://openai.com/blog/chatgpt\"> ChatGPT</a>. NVIDIA provides tools to train similar <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/modelguide/reinforcementlearning.html\">RLHF-incorporated, GPT-based models</a>.</p>\n\n\n\n<p>Organizations can use such foundational LLMs on unstructured sources of information such as financial news along with <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">customization</a> techniques such as training, parameter-efficient fine-tuning, and fine-tuning to improve the LLMs for understanding the financial domain better.&nbsp;&nbsp;</p>\n\n\n\n<p>End users can interface with such customized models combining it with techniques such as r<a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">etrieval augmented Generation</a> (RAG) to gain an information edge by querying and obtaining insights from unstructured data sources that are untapped for decision-making insights.&nbsp;</p>\n\n\n\n<p>For example, these techniques can be used on news, financial documents, financial 10K or 10Q filings, and federal reserve commentary beyond traditional sources of tabular market data, to generate insights and signals, referred to as alternative data (alt data). These alt data information signals can be used to support systematic algorithmic trading models as well as discretionary traders.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>The convergence of HPC and AI is happening as financial firms work on big-picture solutions. Stakeholders include both sell-side (global market banks and broker dealers) and buy-side (insurers, hedge funds, market-makers, high frequency traders, and asset managers). Solutions typically involve combining various modeling techniques, such as HPC quantitative finance, ML, RL, and NLP LLM generative AI models.&nbsp;</p>\n\n\n\n<p>Due to the ability to cater to multiple HPC plus AI workloads, end clients are able to gain the maximum ROI and lower TCO by catering to all workloads including the largest and most demanding AI, ML, DL training, HPC modeling, and simulation initiatives.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>End clients are working on converged HPC quant finance and AI business solutions. Dell Technologies, along with NVIDIA, is uniquely positioned to accelerate generative AI workloads and data analytics as well as high performance computing (HPC) quantitative financial applications where converged HPC quantitative finance plus AI workloads are the need of the hour for clients.&nbsp; &hellip; <a href=\"https://developer.nvidia.com/blog/new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai/\">Continued</a></p>\n", "protected": false}, "author": 981, "featured_media": 71202, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1305232", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-risk-calculation-record-in-financial-services-with-dell-technologies-and-nvidia-h100-system-for-hpc-and-ai/274205", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 503], "tags": [453, 105], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-gpu.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jka", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74286"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/981"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74286"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74286/revisions"}], "predecessor-version": [{"id": 74398, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74286/revisions/74398"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71202"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74286"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74286"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74286"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73937, "date": "2023-11-27T09:00:00", "date_gmt": "2023-11-27T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73937"}, "modified": "2024-01-03T15:48:02", "modified_gmt": "2024-01-03T23:48:02", "slug": "announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/", "title": {"rendered": "Announcing HelpSteer: An Open-Source Dataset for Building Helpful LLMs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA recently announced the <a href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NeMo SteerLM</a> technique as part of the <a href=\"https://developer.nvidia.com/nemo\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NeMo</a> framework. This technique enables users to control large language model (LLM) responses during inference. The developer community has shown great interest in using the approach for building custom LLMs.</p>\n\n\n\n<p>The NVIDIA NeMo team is now open-sourcing a multi-attribute dataset called <a href=\"https://huggingface.co/datasets/nvidia/HelpSteer\" target=\"_blank\" rel=\"noreferrer noopener\">Helpfulness SteerLM dataset</a> (HelpSteer). This new resource enables developers to get started with using the SteerLM technique quickly and build state-of-the-art custom models.&nbsp;</p>\n\n\n\n<p>HelpSteer is a collaborative effort between our team and Scale AI. Coupled with the SteerLM technique, it improves the factuality and coherence of responses. Developers can now guide LLM responses on additional attributes like complexity and verbosity and enhance the overall controllability of the responses for end-users.</p>\n\n\n\n<p>By using this new dataset and the SteerLM technique, NVIDIA trained a Llama 2 70B foundational model that outperforms the Llama 2-70B chat model on <a href=\"https://arxiv.org/abs/2306.05685\" target=\"_blank\" rel=\"noreferrer noopener\">MT Bench</a> and <a href=\"https://github.com/sylinrl/TruthfulQA\" target=\"_blank\" rel=\"noreferrer noopener\">TruthfulQA MC2</a> benchmarks.&nbsp;</p>\n\n\n\n<p>Let\u2019s dive deep into how we built this dataset.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Collecting high-quality human feedback</h2>\n\n\n\n<p>We collected around 10K high-quality prompts and generated four responses for each using an in-house LLM. The <a href=\"https://scale.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Scale AI</a> team then engaged their human experts to evaluate each response on a scale of 0\u20134. Scale&#8217;s Generative AI Data Engine combines automation and human intelligence to generate training data tailored to specific AI goals and data needs rapidly.&nbsp;</p>\n\n\n\n<p>All the responses in the dataset were annotated on five attributes, namely:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>Helpfulness: </strong>how well the response overall satisfies what the prompt asks for.</li>\n\n\n\n<li><strong>Correctness: </strong>whether the response contains factually accurate and relevant information without hallucinations or false information.</li>\n\n\n\n<li><strong>Coherence: </strong>how clear, logical, and self-consistent the response is.</li>\n\n\n\n<li><strong>Complexity: </strong>the sophistication of vocabulary and sentence structure from basic to expert level.</li>\n\n\n\n<li><strong>Verbosity: </strong>how concise or verbose the response is relative to what the prompt asks for.</li>\n</ul>\n\n\n\n<p>We selected these attributes to complement the attributes in OpenAssistant, the dataset used to train the original SteerLM model.&nbsp;</p>\n\n\n\n<p>A sample of the dataset has the following structure:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>### Prompt\nWhat is the spiciest chili in the world? Is it a type of chili or made from another ingredient?\n\n### Response 1\nThe spiciest chili in the world is the Carolina Reaper. It is a type of chili that is made from the chili pepper variety.\n### Label 1\nhelpfulness:1,correctness:1,coherence:3,complexity:1,verbosity:1\n\n### Response 2\nThe spiciest chili in the world is the Carolina Reaper, which is a type of chili pepper. It was created by a man named Ed Currie and is a cross between a Ghost Pepper and a Red Habanero. It is known for its extreme heat, which is measured at over one million Scoville units. The Carolina Reaper is made from a chili pepper that has been selectively bred for its high levels of capsaicin, the compound that gives chili peppers their spicy heat. While it is not made from any other ingredient, some people may add it to other ingredients to make a spicy sauce or salsa.\n### Label 2\nhelpfulness:4,correctness:4,coherence:4,complexity:2,verbosity:2</code></pre>\n\n\n\n<p>LLMs trained on this dataset are more factually correct, and coherent in their responses, making them more helpful in several industries such as customer service, education, and more. Users can also adjust the preferred level of complexity and verbosity of the language model at inference time.&nbsp;</p>\n\n\n\n<p>For example, developers can build LLMs that can explain complex subjects such as American History to a middle school student or to someone whose first language isn&#8217;t English, all with just one customization run.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Llama2-70B-SteerLM-Chat evaluation results&nbsp;</h2>\n\n\n\n<p>The Llama 2-70B foundational model trained using the HelpSteer dataset along with the SteerLM technique outperformed the Llama 2-70B chat model on several benchmarks.&nbsp;</p>\n\n\n\n<p>The following table compares the performance metrics for both the models on two popular benchmarks: <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/2306.05685\" target=\"_blank\">MT Bench</a> and <a rel=\"noreferrer noopener\" href=\"https://github.com/sylinrl/TruthfulQA\" target=\"_blank\">TruthfulQA MC2</a>. A higher score is better, meaning that the SteerLM model is both more helpful and truthful compared to the Llama 2-70B chat model.</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td><strong>Benchmark</strong></td><td><strong>Llama 2-70B chat&nbsp;</strong></td><td><strong>Llama2-70B-SteerLM-Chat</strong></td></tr><tr><td><strong>MT Bench</strong></td><td>6.86</td><td>7.54</td></tr><tr><td><strong>TruthfulQA MC2</strong></td><td>0.528&nbsp;</td><td>0.561</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Comparison of Llama 2-70B chat and Llama2-70B-SteerLM-Chat models on MT Bench and TruthfulQA MC2 benchmarks</em></figcaption></figure>\n\n\n\n<p>In Figure 1, we observe that the Llama2-70B-SteerLM-Chat model outperforms the Llama 2-70B chat model in most categories within MT Bench.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"469\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-625x469.png\" alt=\"we observe that the Llama2-70B-SteerLM-Chat model outperforms the Llama 2-70B chat model in most categories within MT Bench.\" class=\"wp-image-74064\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-768x576.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-1536x1152.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-645x484.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-362x272.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-147x110.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models-1024x768.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2models.png 1600w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Score in each category of MT Bench for Llama2-70B-SteerLM-Chat compared to Llama 2 70B chat</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Llama2-70B-SteerLM-Chat model responses</h2>\n\n\n\n<p>Let\u2019s look at some examples from this SteerLM model. For the same prompt, the model provided different responses based on different levels of complexity. Developers can use the same model and tailor it for different use cases at the time of inference.&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code><strong>Prompt</strong>\nCan we travel at the speed of light?\n\n<strong>Low-Complexity Response</strong>\n'No, it is not possible to travel at the speed of light. The speed of light is a fundamental constant of nature and is the maximum speed at which information can be transmitted in a vacuum. According to our current understanding of physics, it is not possible for any matter or energy to reach or exceed the speed of light.'\n\n<strong>High-Complexity Response</strong>\nAccording to our current understanding of physics, it is not possible to travel at the speed of light. This is because as an object approaches the speed of light, its mass would increase infinitely, making it impossible to reach the speed of light. This effect is described by Einstein's theory of relativity and is known as the \"mass-energy equivalence. Additionally, as an object approaches the speed of light, its length would contract and time would slow down, making it impossible to reach the speed of light from an observer's perspective. Therefore, it is currently believed that it is not possible for any material object to travel at the speed of light, and the speed of light is considered to be the upper limit for the speed of any matter or energy in the universe.</code></pre>\n\n\n\n<h2 class=\"wp-block-heading\">Learn more&nbsp;</h2>\n\n\n\n<p>To learn more about HelpSteer dataset and how to train custom LLMs with NeMo framework and SteerLM technique,</p>\n\n\n\n<ul>\n<li>Experience the Llama2-70B-SteerLM-Chat model directly from your browser on&nbsp;<a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/llama2-70b-steerlm\" target=\"_blank\">NVIDIA NGC Catalog</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/modelalignment/steerlm.html#train-a-steerlm-model\" target=\"_blank\" rel=\"noreferrer noopener\">Learn how to train SteerLM model with HelpSteer dataset</a></li>\n\n\n\n<li><a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/2311.09528\" data-type=\"link\" data-id=\"https://arxiv.org/abs/2311.09528\" target=\"_blank\">Read the technical paper submitted on ArXiv</a></li>\n\n\n\n<li><a href=\"https://huggingface.co/datasets/nvidia/HelpSteer\" target=\"_blank\" rel=\"noreferrer noopener\">Download the dataset from HuggingFace</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\" target=\"_blank\" rel=\"noreferrer noopener\">Read the SteerLM announcement blog post</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA recently announced the NVIDIA NeMo SteerLM technique as part of the NVIDIA NeMo framework. This technique enables users to control large language model (LLM) responses during inference. The developer community has shown great interest in using the approach for building custom LLMs. The NVIDIA NeMo team is now open-sourcing a multi-attribute dataset called Helpfulness &hellip; <a href=\"https://developer.nvidia.com/blog/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/\">Continued</a></p>\n", "protected": false}, "author": 1833, "featured_media": 73940, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1299619", "discourse_permalink": "https://forums.developer.nvidia.com/t/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/273267", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [296, 453, 2932, 3633, 3270], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Announcing-HelpSteer-An-Open-Source-Dataset-for-Building-Helpful-LLMs.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jex", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73937"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1833"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73937"}], "version-history": [{"count": 28, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73937/revisions"}], "predecessor-version": [{"id": 76149, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73937/revisions/76149"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73940"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73937"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73937"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73937"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73728, "date": "2023-11-27T01:00:00", "date_gmt": "2023-11-27T09:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73728"}, "modified": "2023-12-14T11:27:39", "modified_gmt": "2023-12-14T19:27:39", "slug": "bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security/", "title": {"rendered": "Bolstering Cybersecurity: How Large Language Models and Generative AI are Transforming Digital Security"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Identity-based attacks are on the rise, with <a href=\"https://www.ibm.com/thought-leadership/institute-business-value/en-us/c-suite-study/ceo\">phishing remaining the most common</a> and second-most expensive attack vector. Some attackers are using AI to craft more convincing phishing messages and deploying bots to get around automated defenses designed to spot suspicious behavior.</p>\n\n\n\n<p>At the same time, a continued increase in enterprise applications introduces challenges for IT teams who must support, secure, and manage these applications, often with no increase in staffing.</p>\n\n\n\n<p>The number of connected devices continues to grow, introducing security risks due to an increase in the attack surface. This is compounded by potential vulnerabilities associated with each device.</p>\n\n\n\n<p>While there are many security tools and applications available to help enterprises defend against attacks, integrating and managing a large number of tools introduces more cost, complexity, and risk.</p>\n\n\n\n<p>\u200b\u200bCybersecurity is among the <a href=\"https://www.ibm.com/thought-leadership/institute-business-value/en-us/c-suite-study/ceo\">top three challenges</a> for CEOs, second to environmental sustainability and just ahead of tech modernization. Generative AI can be transformational for cybersecurity. It can help security analysts find the information they need to do their jobs faster, generate synthetic data to train AI models to identify risks accurately, and run what-if scenarios to better prepare for potential threats.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Using AI to keep pace with an expanding threat landscape</h2>\n\n\n\n<p>Cybersecurity is a data problem, and the vast amount of data available is too large for manual screening and threat detection. This means human analysts can no longer effectively defend against the most sophisticated attacks because the speed and complexity of attacks and defenses exceed human capacity. With AI, organizations can achieve 100 percent visibility of their data and quickly discover anomalies, enabling them to detect threats faster.</p>\n\n\n\n<p>Although the exponentially increasing quantity of data poses a challenge for threat detection, AI-based approaches to cyber defense require access to training data. In some cases, this isn\u2019t readily available, because organizations don\u2019t typically share sensitive data. With generative AI, synthetic data can help \u200caddress the data gap and improve cybersecurity AI defense.</p>\n\n\n\n<p>One of the most effective ways of synthesizing and contextualizing data is through natural language. The advancements of large language models (LLMs) are expanding threat detection and data generation techniques that improve cybersecurity.&nbsp;</p>\n\n\n\n<p>This post explores three use cases showing how generative AI and LLMs improve cybersecurity and provides three examples of how AI foundation models for cybersecurity can be applied.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Copilots boost the efficiency and capabilities of security teams</h3>\n\n\n\n<p>Staffing shortages for cybersecurity professionals persist. Security copilots with <a href=\"https://blogs.nvidia.com/blog/2023/11/15/what-is-retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> enable organizations to tap into existing knowledge bases and extend the capabilities of human analysts, making them more efficient and effective.&nbsp;&nbsp;</p>\n\n\n\n<p>Copilots learn from the behaviors of security analysts, adapt to their needs, and provide relevant insights that guide them in their daily work, all in a natural interface. Organizations are quickly discovering the value of RAG chatbots.&nbsp;</p>\n\n\n\n<p>By 2025, two-thirds of businesses will leverage a combination of generative AI and RAG to power domain-specific, self-service knowledge discovery, improving decision efficacy by 50%<sup data-fn=\"985bd737-7db1-4e98-bdfd-58ae0a71965e\" class=\"fn\"><a href=\"#985bd737-7db1-4e98-bdfd-58ae0a71965e\" id=\"985bd737-7db1-4e98-bdfd-58ae0a71965e-link\">1</a></sup>.</p>\n\n\n\n<p>In addition to not having enough cybersecurity personnel, organizations are challenged in training new and existing employees. With copilots, cybersecurity professionals can get near real-time responses and guidance on complex deployment scenarios without the need for additional training or research.</p>\n\n\n\n<p>While security copilots can bring transformational benefits to an organization, they\u2019re only useful when they can provide fast, accurate, and up-to-date information. The NVIDIA <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/generative-ai-chatbots/\">AI Chatbot with Retrieval-Augmented Generation workflow</a> provides a great starting point. It demonstrates how to build agents and chatbots that can retrieve the most up-to-date information in real-time and provide accurate responses in natural language.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Generative AI can dramatically improve common vulnerability defense</h3>\n\n\n\n<p>Patching software security issues are becoming increasingly challenging as the number of reported security flaws in the <a href=\"https://www.cve.org/About/Metrics#PublishedCVERecords\">common vulnerabilities and exposures (CVEs) database</a> hit a record high in 2022. With over 200,000 cumulative vulnerabilities reported as of the third quarter of 2023, it\u2019s clear that a traditional approach to scanning and patching has become unmanageable.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.ibm.com/reports/data-breach?utm_content=SRCWW&amp;p1=Search&amp;p4=43700077724063991&amp;p5=e&amp;gclid=EAIaIQobChMI2_XLveTGggMVox6tBh0Etw_fEAAYASAAEgLpGvD_BwE&amp;gclsrc=aw.ds\">Organizations that deploy risk-based analysis experience less costly breaches compared to those that rely solely on CVE scoring to prioritize vulnerabilities</a>. Using generative AI, it\u2019s possible to improve vulnerability defense while decreasing the load on security teams.</p>\n\n\n\n<p>Using the <a href=\"https://developer.nvidia.com/morpheus-cybersecurity\">NVIDIA Morpheus</a> LLM engine integration, NVIDIA built a pipeline to address CVE risk analysis with RAG. Security analysts can determine whether a software container includes vulnerable and exploitable components using LLMs and RAG.&nbsp;</p>\n\n\n\n<p>This method enabled analysts to investigate individual CVEs 4X faster, on average, and identify vulnerabilities with high accuracy so patches could be prioritized and addressed accordingly.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"972\" height=\"640\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query.png\" alt=\"A diagram of NVIDIA Morpheus LLM engine for CVE Exploitability using retrieval augmented generation.\" class=\"wp-image-73729\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query.png 972w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-300x198.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-625x412.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-175x115.png 175w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-768x506.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-645x425.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-456x300.png 456w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-137x90.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-362x238.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CVE-Query-167x110.png 167w\" sizes=\"(max-width: 972px) 100vw, 972px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. CVE exploitability using Morpheus LLM engine supporting model-generated RAG tasks and multiple loops</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Foundation models for cybersecurity</h3>\n\n\n\n<p>While pretrained models are useful for many applications, there are times when it\u2019s beneficial to train a custom model from scratch. This is helpful when there\u2019s a specific domain with a unique vocabulary or the content has properties that do not conform to traditional language paradigms and structures.&nbsp;</p>\n\n\n\n<p>In cybersecurity, this is observed with certain types of raw logs. Think about a book and how words form sentences, sentences form paragraphs, and paragraphs form chapters. There\u2019s an inherent structure that is part of the language model. Contrast that to data contained in a format like JSON-lines or CEF. Proximity of the data keys and values doesn\u2019t have the same meaning.&nbsp;</p>\n\n\n\n<p>Using custom foundation models presents multiple opportunities.</p>\n\n\n\n<ul>\n<li><strong>Addressing the data gap</strong>: while making better use of the influx of data can lead to improved cybersecurity, the quality of the data matters. When there is a lack of available training data, the accuracy of detecting threats is compromised. Generative AI can help \u200caddress the data gap with synthetic data generation, or by using large models to generate data to train smaller models.</li>\n</ul>\n\n\n\n<ul>\n<li><strong>Performing \u201cwhat if\u201d scenarios</strong>: novel threats are challenging to defend against without data sets to build the defenses. Generative AI can be used for attack simulations and to perform \u201cwhat if\u201d scenarios\u2014to test against attack patterns that haven\u2019t yet been experienced. This dynamic model training, based on evolving threats and changing patterns in data can help to improve overall security.</li>\n</ul>\n\n\n\n<ul>\n<li><strong>Feed downstream anomaly detectors</strong>: use large models to generate data that train downstream, lightweight models used for threat detection, which can reduce infrastructure costs while keeping the same level of accuracy.</li>\n</ul>\n\n\n\n<p>NVIDIA performed many experiments and trained several cybersecurity-specific foundation models, including one based on GPT-2 style models referenced as CyberGPT. One of those is a model that is trained on identity data (including application logs like Azure AD). With this model, one can generate highly realistic synthetic data that addresses a data gap and can perform \u201cwhat if\u201d scenarios.&nbsp;</p>\n\n\n\n<p>Figure 2 shows the Rogue2 F1 scores for CyberGPT models of various sizes, with each instance achieving around 80% accuracy. This means that 8 out of 10 logs generated are virtually indistinguishable from logs generated by real network users.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"942\" height=\"674\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models.png\" alt=\"A bar chart showing 80% accuracy for Rogue2 F1 scores of CyberGPT models generated compared with authentic logs.\" class=\"wp-image-73731\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models.png 942w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-300x215.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-625x447.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-161x115.png 161w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-768x550.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-645x461.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-419x300.png 419w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-126x90.png 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-362x259.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/CyberGPT-models-154x110.png 154w\" sizes=\"(max-width: 942px) 100vw, 942px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Accuracy and realism scores of logs generated by CyberGPT models</em></figcaption></figure></div>\n\n\n<p>As for training times, a supercomputer isn\u2019t necessary to realize quality results. In testing, training times were as low as 12 GPU hours for a GPT-2-small model with character-level tokenization. This model is trained on 2.3M rows of over 100 user logs with 1,000 iterations. This model was trained on multiple types of data, including Azure, SharePoint, Confluence, and Jira.</p>\n\n\n\n<p>Experiments were also run with tokenizers\u2013primarily character-level tokenizers, off-the-shelf byte pair encoding (BPE) tokenizers, and custom-trained tokenizers. While there are benefits and drawbacks to each, the best performance comes as a result of training custom tokenizers. This not only enables more efficient use of resources due to the custom vocabulary, but it results in reduced tokenization errors and can handle log-specific syntax.</p>\n\n\n\n<p>While these results reflect experiments with language models, the same tests with LLMs achieve similar results.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Synthetic data generation provides 100% detection of spear phishing e-mails</h2>\n\n\n\n<p>Spear phishing e-mails are highly targeted, and therefore, very convincing. The only real difference between a spear phishing (and, in general, any effective phishing campaign) and a benign e-mail is the intent of the sender. This makes spear phishing challenging to defend against with AI because there is a lack of available training data.&nbsp;</p>\n\n\n\n<p>To explore the potential of synthetic data generation in enhancing spear phishing e-mail detection, a pipeline was constructed using NVIDIA Morpheus.</p>\n\n\n\n<p>With off-the-shelf models, the spear phishing detection pipeline missed 16% (about 600) of malicious e-mails. The uncaught malicious e-mails were then used to create a new synthetic dataset. A new intent model was learned from the synthetically generated e-mails, and integrated into our spear phishing detection pipeline. The addition of this new intent model feature in the detection pipeline resulted in 100% detection of spear phishing e-mails trained solely on synthetic e-mails.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/spear-phishing/\">NVIDIA spear phishing detection AI workflow</a> provides an example of how to build this solution using NVIDIA Morpheus.</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1044\" height=\"622\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline.png\" alt=\"A diagram showing NVIDIA Morpheus spear-phishing detection AI pipeline using generative AI.\" class=\"wp-image-73732\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline.png 1044w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-300x179.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-625x372.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-768x458.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-645x384.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-500x298.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-151x90.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-362x216.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-185x110.png 185w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Spear-phishing-detection-pipeline-1024x610.png 1024w\" sizes=\"(max-width: 1044px) 100vw, 1044px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Spear phishing detection pipeline built using synthetically generated spear phishing e-mails that correspond to specific behavioral intents&nbsp;</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>A comprehensive approach to enterprise security</strong></h2>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/?ncid=pa-srch-goog-679855&amp;_bt=663202418341&amp;_bk=nvidia%20ai%20platform&amp;_bm=e&amp;_bn=g&amp;_bg=153503051907&amp;gad_source=1&amp;gclid=EAIaIQobChMI4v6D99WyggMVbA2tBh1W2AxbEAAYASAAEgJDM_D_BwE\">NVIDIA AI platform</a> is uniquely positioned to help address these challenges\u2013building in security at multiple levels. At the hardware infrastructure level, and beyond the data center perimeter to the edge of every server, while also providing tools that help to secure your data with AI.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Learn more</strong></h2>\n\n\n\n<p>Watch the session from Bartley Richardson, head of cybersecurity engineering at NVIDIA, to see demonstrations of the use cases illustrated in this post. Learn about integrating language models and cybersecurity featured at <a href=\"https://www.nvidia.com/en-us/on-demand/session/llmdevday23-05/\">NVIDIA LLM Developer Day</a>.</p>\n\n\n\n<p>Check out the November 2023 release of <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/morpheus/collections/morpheus_\">NVIDIA Morpheus</a> to access the new LLM engine integration feature, and get started with accelerated AI for cybersecurity.&nbsp;</p>\n\n\n\n<p>Find out how <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a> provides an easy way to get started with building, customizing, and deploying generative AI models.&nbsp;</p>\n\n\n\n<p>NVIDIA Morpheus and NeMo are included with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, the enterprise-grade software that powers the NVIDIA AI platform.</p>\n\n\n<ol class=\"wp-block-footnotes\"><li id=\"985bd737-7db1-4e98-bdfd-58ae0a71965e\">\u00a0IDC FutureScape: Worldwide Artificial Intelligence and Automation 2024 Predictions, #AP50341323, October 2023 <a href=\"#985bd737-7db1-4e98-bdfd-58ae0a71965e-link\">\u21a9\ufe0e</a></li></ol>", "protected": false}, "excerpt": {"rendered": "<p>Identity-based attacks are on the rise, with phishing remaining the most common and second-most expensive attack vector. Some attackers are using AI to craft more convincing phishing messages and deploying bots to get around automated defenses designed to spot suspicious behavior. At the same time, a continued increase in enterprise applications introduces challenges for IT &hellip; <a href=\"https://developer.nvidia.com/blog/bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security/\">Continued</a></p>\n", "protected": false}, "author": 1264, "featured_media": 74447, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "[{\"content\":\"\u00a0IDC FutureScape: Worldwide Artificial Intelligence and Automation 2024 Predictions, #AP50341323, October 2023\",\"id\":\"985bd737-7db1-4e98-bdfd-58ae0a71965e\"}]", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696, 3110], "tags": [493, 453, 2932, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-day-cybersecurity.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jba", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73728"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1264"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73728"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73728/revisions"}], "predecessor-version": [{"id": 74605, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73728/revisions/74605"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74447"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73728"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73728"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73728"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74071, "date": "2023-11-26T06:00:00", "date_gmt": "2023-11-26T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74071"}, "modified": "2023-12-01T11:30:20", "modified_gmt": "2023-12-01T19:30:20", "slug": "accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/", "title": {"rendered": "Accelerate AI Workflows for 3D Medical Imaging with NVIDIA MONAI Cloud APIs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>AI is increasingly being used to improve medical imaging for health screenings and risk assessments. Medical image segmentation, for example, provides vital data for tumor detection and treatment planning. And yet the unique and varied nature of medical images makes achieving consistent and reliable results challenging.&nbsp;</p>\n\n\n\n<p>NVIDIA MONAI Cloud APIs help solve these challenges, simplifying the journey of AI capabilities and infrastructure setup for platform integrators. This post introduces NVIDIA MONAI Cloud APIs, VISTA-3D, and Auto3DSeg and explains how to use them together to achieve adaptable 3D medical imaging.</p>\n\n\n\n<p>NVIDIA MONAI Cloud APIs offer low-latency, interactive, and cost-effective AI-assisted annotation workflows. With the continual learning mechanism, the model adapts to new real-world data, enabling it to maintain its relevance and robustness over time.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Interactive annotation with VISTA-3D&nbsp;</h2>\n\n\n\n<p>Trained on vast datasets, VISTA-3D is a specialized interactive foundational model for 3D medical imaging. Powering NVIDIA MONAI Cloud API interactive annotation, VISTA-3D provides accurate and adaptable segmentation analysis across anatomies and modalities. It handles a variety of tasks and adapts to different conditions and anatomical areas. This versatility reduces costs and expedites AI imaging workflows without the intricate complexities of model selection and adaptation.&nbsp;</p>\n\n\n\n<p>In the realm of medical imaging annotation, VISTA-3D represents a transformative approach. By blending semantic segmentation and interactivity, it bridges the gap between AI and human experts (bioimaging scientists and radiologists, for example). VISTA-3D offers these professionals an evolving AI model that ensures both high accuracy and ease of use.</p>\n\n\n\n<p>Core to VISTA-3D are three adaptable, user-friendly workflows:</p>\n\n\n\n<ol>\n<li><strong>Segment everything</strong>: Entire image exploration useful for understanding diseases impacting multiple organs or for holistic treatment planning.</li>\n</ol>\n\n\n\n<ol start=\"2\">\n<li><strong>Segment using class</strong>: Detailed section views selectable by specific classes for targeted analysis of certain diseases or organs; valuable for mapping tumors in organs.</li>\n</ol>\n\n\n\n<ol start=\"3\">\n<li><strong>Segment point prompts</strong>: User feedback-guided image segmentation, through click-based selection of interest areas, for improved accuracy and faster creation of ground-truth data.&nbsp;</li>\n</ol>\n\n\n\n<p>VISTA-3D architecture boasts an impressive mean dice score of about 0.91, thanks to its blend of interactive and automatic systems. This foundational flexibility enables users to quickly tailor the model for their tasks.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1006\" height=\"439\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1.png\" alt=\"Diagram showing the architecture of VISTA-3D featuring the two prompt heads that help with automatic or interactive segmentation\n\" class=\"wp-image-74080\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1.png 1006w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-625x273.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-768x335.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-645x281.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vista-3d-architecture-1-252x110.png 252w\" sizes=\"(max-width: 1006px) 100vw, 1006px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. VISTA-3D architecture blends automatic and interactive systems for model tailoring</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">AI model training with Auto3DSeg</h2>\n\n\n\n<p>Built on the foundations of <a href=\"https://monai.io/\">MONAI</a> and powered by cutting-edge GPUs, Auto3DSeg provides developers with the tools for achieving top-tier 3D medical image segmentation. The computational efficiency of Auto3DSeg is optimized to ensure rapid training while extracting the most from GPU computational capabilities.</p>\n\n\n\n<p>Auto3DSeg uses MONAI components to achieve state-of-the-art segmentation performance. A <a href=\"https://docs.monai.io/en/stable/mb_specification.html\">MONAI Bundle</a> offers a more customized solution. A MONAI Bundle defines a packaged network or model that includes the information necessary for users and programs to understand how the model is used and for what purpose. Bring your own MONAI Bundle to training and take full advantage of the robust, scalable training infrastructure.</p>\n\n\n\n<p>Once you have your annotated data, use either custom model training or Auto3DSeg for an optimized model development experience.</p>\n\n\n\n<p>Training is all about flexibility\u2014bring your unique MONAI Bundle, and watch as your training scales effortlessly on the NVIDIA MONAI Cloud API platform.</p>\n\n\n\n<p>If automation is your goal, Auto3DSeg is your answer. Designed with developers in mind, Auto3DSeg stands out with its innovative features:</p>\n\n\n\n<ol>\n<li><strong>Data-driven model selection</strong>: Auto3DSeg delivers intelligent automation. By analyzing annotated imaging datasets, Auto3DSeg can automatically select and scale the most appropriate model architectures. This ensures optimized performance tailored specifically to your data.</li>\n</ol>\n\n\n\n<ol start=\"2\">\n<li><strong>Streamlined development cycle</strong>: The development cycle is significantly streamlined with Auto3DSeg. It reduces complexity by incorporating automated parallel training and hyperparameter optimization. This expedites the entire process, from dataset analysis to creating deployment-ready models, saving time and resources.</li>\n</ol>\n\n\n\n<ol start=\"3\">\n<li><strong>Proven state-of-the-art segmentation performance</strong>: Auto3DSeg has secured multiple top positions at MICCAI in various segmentation challenges, notably the BraTS 2023, KiTS 2023, SEG.A. 2023 and MVSEG 2023 competitions.</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1826\" height=\"610\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow.png\" alt=\" Diagram showing the flow of Auto3DSeg starting with labeled data on the left, the Auto3DSeg sequence in the middle, and resulting trained model(s) on the right.\" class=\"wp-image-74085\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow.png 1826w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-300x100.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-625x209.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-768x257.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-1536x513.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-645x215.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-500x167.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-362x121.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-329x110.png 329w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/auto3dseg-workflow-1024x342.png 1024w\" sizes=\"(max-width: 1826px) 100vw, 1826px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Auto3DSeg workflow</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Platform integrators striving for innovation in medical imaging can fully harness the capabilities of NVIDIA MONAI Cloud APIs, expediting the development of their AI models for medical imaging. This approach reduces operational overhead and grants immediate access to accelerated computing and AI capabilities in a streamlined manner.</p>\n\n\n\n<p>To explore VISTA-3D, custom model training, and Auto3DSeg, sign up for the <a href=\"https://developer.nvidia.com/nvidia-monai-cloud-api-early-access-program/join\">early access program</a>. To try VISTA-3D, visit <a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA AI Foundation Models</a> starting November 27.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI is increasingly being used to improve medical imaging for health screenings and risk assessments. Medical image segmentation, for example, provides vital data for tumor detection and treatment planning. And yet the unique and varied nature of medical images makes achieving consistent and reliable results challenging.&nbsp; NVIDIA MONAI Cloud APIs help solve these challenges, simplifying &hellip; <a href=\"https://developer.nvidia.com/blog/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/\">Continued</a></p>\n", "protected": false}, "author": 1003, "featured_media": 74099, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1304377", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/273998", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724], "tags": [3268, 188, 453, 1948, 1953, 90, 1288], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/monai-services-anatomy-head.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jgH", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74071"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1003"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74071"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74071/revisions"}], "predecessor-version": [{"id": 74626, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74071/revisions/74626"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74099"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74071"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74071"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74071"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74297, "date": "2023-11-24T11:30:08", "date_gmt": "2023-11-24T19:30:08", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74297"}, "modified": "2023-11-30T11:43:26", "modified_gmt": "2023-11-30T19:43:26", "slug": "explainer-what-is-retrieval-augmented-generation-aka-rag", "status": "publish", "type": "post", "link": "https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/", "title": {"rendered": "Explainer: What Is Retrieval-Augmented Generation aka RAG?"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.</p>\n", "protected": false}, "author": 1512, "featured_media": 74298, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1304191", "discourse_permalink": "https://forums.developer.nvidia.com/t/explainer-what-is-retrieval-augmented-generation-aka-rag/273947", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/", "_links_to_target": "_blank"}, "categories": [3110], "tags": [453, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Retrieval-Augmented-Generation-RAG-KV-1-1280x683-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jkl", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74297"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1512"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74297"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74297/revisions"}], "predecessor-version": [{"id": 74302, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74297/revisions/74302"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74298"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74297"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74297"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74297"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73029, "date": "2023-11-21T11:00:00", "date_gmt": "2023-11-21T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73029"}, "modified": "2023-11-30T11:43:26", "modified_gmt": "2023-11-30T19:43:26", "slug": "just-released-nvidia-modulus-23-11", "status": "publish", "type": "post", "link": "https://nvda.ws/3u9Pzlr", "title": {"rendered": "Just Released: NVIDIA Modulus 23.11"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Now available, NVIDIA Modulus 23.11 introduces a diffusion modeling framework and novel architectures.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Now available, NVIDIA Modulus 23.11 introduces a diffusion modeling framework and novel architectures.</p>\n", "protected": false}, "author": 909, "featured_media": 73036, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3u9Pzlr", "_links_to_target": "_blank"}, "categories": [852, 503], "tags": [1916, 1913, 453, 608, 2216, 3281], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/wind-turbines.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iZT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73029"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/909"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73029"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73029/revisions"}], "predecessor-version": [{"id": 73052, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73029/revisions/73052"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73036"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73029"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73029"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73029"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71300, "date": "2023-11-21T10:37:48", "date_gmt": "2023-11-21T18:37:48", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71300"}, "modified": "2023-12-29T16:44:05", "modified_gmt": "2023-12-30T00:44:05", "slug": "advanced-api-performance-intrinsics", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-api-performance-intrinsics/", "title": {"rendered": "Advanced API Performance: Intrinsics"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Intrinsics can be thought of as higher-level abstractions of specific hardware instructions.&nbsp;They offer direct access to low-level operations or hardware-specific features, enabling increased performance. In this way, operations can be performed across threads within a warp, also known as a <em>wavefront</em>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Recommended</h2>\n\n\n\n<ul>\n<li>Wave intrinsics can noticeably speed up your shaders.\n<ul>\n<li>Many sorting or reduction algorithms can use much less or no shared memory with fewer memory barriers, providing a noticeable performance boost.</li>\n\n\n\n<li>Different types of shuffles and ballots can be useful.</li>\n\n\n\n<li>Use wave instructions with <code>GroupSize</code> or <code>WorkGroup</code> values larger than the warp or subgroup size (32 threads) wave instructions. There are fewer memory barriers and shared memory accesses that are needed.</li>\n\n\n\n<li>For more information, see <a href=\"https://developer.nvidia.com/blog/reading-between-the-threads-shader-intrinsics/\">Reading Between The Threads: Shader Intrinsics</a>&nbsp;and <a href=\"https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/\">Unlocking GPU Intrinsics in HLSL</a>.</li>\n</ul>\n</li>\n\n\n\n<li>Use <code>GroupSize</code> and <code>WorkGroup</code> as a multiplier of warp size (<code>32 * N</code>), 64 is usually a sweet spot.\n<ul>\n<li>With intrinsic <code>GroupSize</code> and <code>WorkGroup</code> size equal, 32 could be a better choice to avoid shared memory usage.</li>\n</ul>\n</li>\n\n\n\n<li>Use native HLSL code when vendor-specific extensions are not applicable or are hard to implement.\n<ul>\n<li>Some instructions can be implemented with recent shader model versions.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>The following code example is an example with SM6:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nfloat(4) NvShflXor (float(4) input, uint LaneMask)\n{\nfloat(4) output = WaveReadLaneAt(input, WaveGetLaneIndex() ^ LaneMask);\nreturn output;\n}\n</pre></div>", "protected": false}, "excerpt": {"rendered": "<p>Intrinsics can be thought of as higher-level abstractions of specific hardware instructions.&nbsp;They offer direct access to low-level operations or hardware-specific features, enabling increased performance. In this way, operations can be performed across threads within a warp, also known as a wavefront. Recommended The following code example is an example with SM6:</p>\n", "protected": false}, "author": 1829, "featured_media": 66457, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1277075", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-api-performance-intrinsics/269318", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 503], "tags": [2424, 3516, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/Advanced-API-series.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iy0", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71300"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1829"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71300"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71300/revisions"}], "predecessor-version": [{"id": 76102, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71300/revisions/76102"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/66457"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71300"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71300"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71300"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72095, "date": "2023-11-21T10:08:42", "date_gmt": "2023-11-21T18:08:42", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72095"}, "modified": "2023-11-30T11:43:27", "modified_gmt": "2023-11-30T19:43:27", "slug": "unlocking-gpu-intrinsics-in-hlsl", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/", "title": {"rendered": "Unlocking GPU Intrinsics in HLSL"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>There are some useful intrinsic functions in the NVIDIA GPU instruction set that are not included in standard graphics APIs. </p>\n\n\n\n<p class=\"has-text-align-right\"><em>Updated from the original 2016 post to add information about new intrinsics and cross-vendor APIs in DirectX and Vulkan.</em></p>\n\n\n\n<p>For example, a shader can use warp shuffle instructions to exchange data between threads in a warp without going through shared memory, which is especially valuable in pixel shaders where there is no shared memory. Or a shader can perform atomic additions on half-precision floating-point numbers in global memory. </p>\n\n\n\n<p>The <a href=\"https://developer.nvidia.com/reading-between-threads-shader-intrinsics\">Reading Between The Threads: Shader Intrinsics</a> post showed you how the intrinsic instructions worked. Now, I take you into the machinery to make them work in DirectX.</p>\n\n\n\n<p>None of the intrinsics are possible in standard DirectX or OpenGL. [<em>2023: This is no longer true. More information is shared later in this post.</em>] But they have been supported and well-documented in CUDA for years. A mechanism to support them in DirectX has been available for a while but not widely documented. I happen to have an old NVAPI version 343 on my system from October 2014 and the intrinsics are supported in DirectX by that version and probably earlier versions. This post explains the mechanism for using them in DirectX.</p>\n\n\n\n<p>Unlike OpenGL or Vulkan, DirectX unfortunately doesn&#8217;t have a native mechanism for vendor-specific extensions. However, there is still a way to make all this functionality available in DirectX 11 or 12 through custom intrinsics. That mechanism is implemented in the graphics driver and accessible through the <a href=\"https://docs.nvidia.com/gameworks/content/gameworkslibrary/coresdk/nvapi/documentation.html\">NVAPI library</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Extending HLSL shaders</h2>\n\n\n\n<p>To use the intrinsics, they have to be encoded as special sequences of regular HLSL instructions that the driver can recognize and turn into the intended operations. These special sequences are provided in one of the header files that comes with the NVAPI SDK: <code>nvHLSLExtns.h</code>.</p>\n\n\n\n<p>One important thing about these instruction sequences is that they have to pass through the HLSL compiler without optimizations because the compiler does not understand their true meaning and therefore could modify them beyond recognition, change their order, or even completely remove them. </p>\n\n\n\n<p>To prevent the compiler from doing that, the sequences use atomic operations on a UAV buffer. The HLSL compiler cannot optimize away these instructions because it is unaware of possible dependencies, even though there are none. That UAV buffer is basically a fake and it is not used by the actual shader after it&#8217;s passed through the NVIDIA GPU driver. But the applications still have to allocate a UAV slot for it and tell the driver which slot that is.</p>\n\n\n\n<p>For example, the <code>NvShfl</code> function that implements warp shuffle looks like the following code example, as defined in <code>nvHLSLExtns.h</code>:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">int NvShfl(int val, uint srcLane, int width = NV_WARP_SIZE)\n{\n     uint index = g_NvidiaExt.IncrementCounter();\n     g_NvidiaExt[index].src0u.x  =  val;          // variable to be shuffled\n     g_NvidiaExt[index].src0u.y  =  srcLane;      // source lane\n     g_NvidiaExt[index].src0u.z  =  __NvGetShflMaskFromWidth(width);\n     g_NvidiaExt[index].opcode   =  NV_EXTN_OP_SHFL;\n\t    \n// result is returned as the return value of IncrementCounter on fake UAV slot\n     return g_NvidiaExt.IncrementCounter();\n}</pre>\n\n\n\n<p>A shader that uses this function would look something like the following code example:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">// Declare that the driver should use UAV 0 to encode the instruction sequences.\n// It's a pixel shader with one output, so u0 is taken by the render target - use u1.\n#define NV_SHADER_EXTN_SLOT u1\n\n// On DirectX12 and Shader Model 5.1, you can also define the register space for that UAV.\n#define NV_SHADER_EXTN_REGISTER_SPACE space0\n\n// Include the header - note that the UAV slot has to be declared before including it.\n#include \"nvHLSLExtns.h\"\n\nTexture2D tex : register(t0);\nSamplerState samp : register(s0);\n\nfloat4 main(in float2 texCoord : UV) : SV_Target\n{\n     float4 color = tex.Sample(samp, texCoord);\n\n     // Use NvShfl to distribute the color from lane 0 to all other lanes in the warp.\n     // The NvShfl function accepts and returns uint data, so use asuint/asfloat to pass float values.\n\tcolor.r = asfloat(NvShfl(asuint(color.r), 0));\n\tcolor.g = asfloat(NvShfl(asuint(color.g), 0));\n\tcolor.b = asfloat(NvShfl(asuint(color.b), 0));\n\tcolor.a = asfloat(NvShfl(asuint(color.a), 0));\n\n\treturn color;\n}</pre>\n\n\n\n<p>This example may look like it&#8217;s doing something meaningless, and it is. Realistic use cases of the intrinsics in graphics applications are usually complicated. For example, warp shuffle can be used to optimize memory access in algorithms like light culling. Floating-point atomics are used in VXGI to accumulate emittance during voxelization. However, those applications require a significant amount of shader and host code to work. This example, on the other hand, can be plugged into virtually any pixel shader, and the effect is obvious.</p>\n\n\n\n<p>When you compile this shader, each call to <code>NvShfl</code> is expanded into this sequence, give or take the register names:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">imm_atomic_alloc r1.x, u1\nmov r3.yz, l(0,0,31,0)\nmov r3.x, r2.z\nstore_structured u1.xyz, r1.x, l(76), r3.xyzx\nstore_structured u1.x, r1.x, l(0), l(1)\nimm_atomic_alloc r0.y, u1</pre>\n\n\n\n<p>And when this shader passes through the driver&#8217;s JIT compiler, each <code>NvShfl</code> function maps to just one GPU instruction:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">SHFL.IDX        PT, R3, R3, RZ, 0x1f;</pre>\n\n\n\n<h2 class=\"wp-block-heading\">Creating extended shaders in DirectX 11</h2>\n\n\n\n<p>To actually use this shader, its runtime object has to be created in a special way. A regular call to <code>ID3D11Device::CreatePixelShader</code> does not suffice because the driver must know that the shader intends to use intrinsics. It also has to know which UAV slot is used. </p>\n\n\n\n<p>If you&#8217;re working with DirectX 11, use the <code>NvAPI_D3D11_SetNvShaderExtnSlot</code> function before and after calling <code>CreatePixelShader</code>:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">// Do this one time during app initialization.\nNvAPI_Initialize();\n\nID3D11PixelShader* pShader = nullptr;\nHRESULT D3DResult = E_FAIL;\n\n// First, enable compilation of intrinsics. \n// The second parameter is the UAV slot index that is used in the shader: u1.\nNvAPI_Status NvapiStatus = NvAPI_D3D11_SetNvShaderExtnSlot(pDevice, 1);\nif(NvapiStatus == NVAPI_OK)\n{\n     // Then create the shader as usual...\n     D3DResult = pDevice-&gt;CreatePixelShader(pBytecode, BytecodeLength, nullptr, &amp;pShader);\n\n     // And disable again by telling the driver to use an invalid UAV slot.\n     NvAPI_D3D11_SetNvShaderExtnSlot(pDevice, ~0u);\n}\n\nif(FAILED(D3DResult))\n{\n     // ...Handle the error...\n}</pre>\n\n\n\n<p>This method works with any shader that can reference a UAV. So, in DirectX 11.0 it works with pixel and compute shaders. In DirectX 11.1 and later, it should work with all kinds of shaders.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Creating extended pipeline state objects in DirectX 12</h2>\n\n\n\n<p>If you&#8217;re working with DirectX 12, there are no individual shader objects. Instead, complete pipeline states (PSOs) are created. </p>\n\n\n\n<p>There are various other NVIDIA-specific pipeline state extensions that can be accessed through NVAPI, so to avoid a combinatorial explosion of functions that create PSOs with various sets of extensions, NVIDIA made just two functions, one for graphics and one for compute, that accept a list of extensions to use:</p>\n\n\n\n<ul>\n<li><code>NvAPI_D3D12_CreateGraphicsPipelineState</code> </li>\n\n\n\n<li><code>NvAPI_D3D12_CreateComputePipelineState</code></li>\n</ul>\n\n\n\n<p>The HLSL extension is described by the <code>NVAPI_D3D12_PSO_SET_SHADER_EXTENSION_SLOT_DESC</code> structure. There&#8217;s only one for the whole pipeline state though, so if two or more shaders in the pipeline use intrinsics, they must use the same UAV slot for it.</p>\n\n\n\n<pre class=\"wp-block-preformatted\">// Do this one time during app initialization.\nNvAPI_Initialize();\n\n// Fill the PSO description structure\nD3D12_GRAPHICS_PIPELINE_STATE_DESC PsoDesc;\nPsoDesc.VS = { pVSBytecode, VSBytecodeLength };\n// ...And so on, as usual...\n\n// Also fill the extension structure. \n// Use the same UAV slot index and register space that are declared in the shader.\nNVAPI_D3D12_PSO_SET_SHADER_EXTENSION_SLOT_DESC ExtensionDesc;       \nExtensionDesc.baseVersion = NV_PSO_EXTENSION_DESC_VER;\nExtensionDesc.psoExtension = NV_PSO_SET_SHADER_EXTNENSION_SLOT_AND_SPACE;\nExtensionDesc.version = NV_SET_SHADER_EXTENSION_SLOT_DESC_VER;\nExtensionDesc.uavSlot = 1;\nExtensionDesc.registerSpace = 0;\n\n// Put the pointer to the extension into an array. There can be multiple extensions enabled at one time.\n// Other supported extensions are: \n       //     - Extended rasterizer state\n       //  - Pass-through geometry shader, implicit or explicit\n       //  - Depth bound test\n       const NVAPI_D3D12_PSO_EXTENSION_DESC* pExtensions[] = { &amp;ExtensionDesc };\n\n// Now create the PSO.\nID3D12PipelineState* pPSO = nullptr;\nNvAPI_Status NvapiStatus = NvAPI_D3D12_CreateGraphicsPipelineState(pDevice, &amp;PsoDesc, ARRAYSIZE(pExtensions), pExtensions, &amp;pPSO);\n\nif(NvapiStatus != NVAPI_OK)\n     {\n        // ...Handle the error...\n     }\n}</pre>\n\n\n\n<h2 class=\"wp-block-heading\">Querying GPU feature support</h2>\n\n\n\n<p>Finally, before trying to use the intrinsics, you&#8217;ll probably want to know whether the device that the app&#8217;s working with actually supports those intrinsics. There are two NVAPI functions that can tell you just that: </p>\n\n\n\n<ul>\n<li><code>NvAPI_D3D11_IsNvShaderExtnOpCodeSupported</code></li>\n\n\n\n<li><code>NvAPI_D3D12_IsNvShaderExtnOpCodeSupported</code></li>\n</ul>\n\n\n\n<p>The <code>opCode</code> parameter identifies the specific operation that you&#8217;re interested in. Operation codes are defined in the <code>nvShaderExtnEnums.h</code> file supplied with NVAPI SDK. For example, to test whether a DirectX 11 device supports warp shuffle, use the following code example:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">#include \"nvShaderExtnEnums.h\"\n\nbool bSupported = false;\nNvAPI_Status NvapiStatus = NvAPI_D3D11_IsNvShaderExtnOpCodeSupported(pDevice, NV_EXTN_OP_SHFL, &amp;bSupported);\n\nif(NvapiStatus == NVAPI_OK &amp;&amp; bSupported)\n{\n     // Yay, the device is no older than 2012!\n}</pre>\n\n\n\n<h2 class=\"wp-block-heading\">Update 2023: New intrinsics and cross-vendor APIs</h2>\n\n\n\n<p>The intrinsics supported by NVIDIA GPUs are not limited to warp shuffle. In fact, warp shuffle and related functions are now available through cross-vendor intrinsics in both DirectX 12 and Vulkan, and there is no need to use NVAPI for them. For more information about DirectX 12 wave intrinsics, see <a href=\"https://github.com/Microsoft/DirectXShaderCompiler/wiki/Wave-Intrinsics\">Wave Intrinsics</a>. For more information about Vulkan subgroup operations, see the <a href=\"https://www.khronos.org/blog/vulkan-subgroup-tutorial\">Vulkan subgroup tutorial</a>.</p>\n\n\n\n<p>The complete list of intrinsics supported by NVIDIA GPUs can be found in the NVAPI header file called <a href=\"https://github.com/NVIDIA/nvapi/blob/main/nvHLSLExtns.h\">nvHLSLExtns.h</a>, which is now available on GitHub. The functions declared in this file can be subdivided into a few general categories:</p>\n\n\n\n<ul>\n<li>Older warp operations: shuffle, vote, ballot, lane index (<code>NvShfl*</code>, <code>NvAny</code>, <code>NvAll</code>, <code>NvBallot</code>, <code>NvGetLaneId</code>)</li>\n\n\n\n<li>Newer warp operations: wave match (<code>NvWaveMatch</code>). <code>NvWaveMatch</code> returns a mask of active lanes in the warp that passed the same parameter value as the current lane.</li>\n\n\n\n<li>Special register access (<code>NvGetSpecial</code>)</li>\n\n\n\n<li>Extended atomic operations on FP16, FP32, and Uint64 variables (<code>NvInterlocked*</code>)</li>\n\n\n\n<li>Variable rate shading (<code>NvGetShadingRate</code>, <code>NvEvaluateAttribute*</code>)</li>\n\n\n\n<li>Texture footprint evaluation (<code>NvFootprint*</code>)</li>\n\n\n\n<li>WaveMultiPrefix functions (<code>NvWaveMultiPrefix*</code>). These functions are just algorithms built on top of other intrinsics.</li>\n\n\n\n<li>Ray tracing micromap extensions (<code>NvRtMicroTriangle*</code>, <code>NvRtMicroVertex*</code>)</li>\n\n\n\n<li>Ray tracing shader execution reordering (<code>NvHitObject</code>, <code>NvReorderThread</code>)</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Update: Compiling shaders with the correct options</h2>\n\n\n\n<p>Currently, there is a known issue in the NVIDIA GPU drivers that affects HLSL intrinsics. Specifically, the intrinsics do NOT work properly if the shader is compiled with the <code>D3DCOMPILE_SKIP_OPTIMIZATION</code> flag, or the <code>/Od</code> command line option passed to FXC. If you see that the intrinsics have no effect, please make sure that this flag is not specified.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>For more information about NVAPI functions and structures, see the comments in NVAPI header files. For more use cases and examples of intrinsics, see the following resources: </p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/reading-between-threads-shader-intrinsics\">Reading Between the Threads: Shader Intrinsics</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/nvapi\">NVIDIA NVAPI SDK</a>: Now at the <a href=\"https://github.com/NVIDIA/nvapi\">/NVIDIA/nvapi</a> GitHub repo!</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/\">Faster Parallel Reductions on Kepler</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/cuda-pro-tip-kepler-shuffle/\">CUDA Pro Tip: Do The Kepler Shuffle</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/\">CUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics</a></li>\n\n\n\n<li><a href=\"http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf\">Shuffle: Tips and Tricks</a> (GTC session)</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>There are some useful intrinsic functions in the NVIDIA GPU instruction set that are not included in standard graphics APIs. Updated from the original 2016 post to add information about new intrinsics and cross-vendor APIs in DirectX and Vulkan. For example, a shader can use warp shuffle instructions to exchange data between threads in a &hellip; <a href=\"https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/\">Continued</a></p>\n", "protected": false}, "author": 804, "featured_media": 72359, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1301808", "discourse_permalink": "https://forums.developer.nvidia.com/t/unlocking-gpu-intrinsics-in-hlsl/273613", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [514, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/AdobeStock_194708302.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iKP", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72095"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/804"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72095"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72095/revisions"}], "predecessor-version": [{"id": 74132, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72095/revisions/74132"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72359"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72095"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72095"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72095"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73760, "date": "2023-11-20T09:00:00", "date_gmt": "2023-11-20T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73760"}, "modified": "2023-12-07T08:59:55", "modified_gmt": "2023-12-07T16:59:55", "slug": "transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/", "title": {"rendered": "Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Efficiency is paramount in industrial manufacturing, where even minor gains can have significant financial implications. According to the American Society of Quality, &#8220;Many organizations will have true quality-related costs as high as 15-20% of sales revenue, some going as high as 40% of total operations.&#8221; These staggering statistics reveal a stark reality: defects in industrial applications not only jeopardize product quality but also drain a significant portion of a company&#8217;s revenue.</p>\n\n\n\n<p>But what if companies could reclaim these lost profits and channel them back into innovation and expansion? This is where the potential of AI shines.</p>\n\n\n\n<p>This post explores how NVIDIA TAO can be employed to design custom AI models that pinpoint defects in industrial applications, enhancing overall quality.</p>\n\n\n\n<p>NVIDIA TAO Toolkit is a low-code AI toolkit built on TensorFlow and PyTorch. It simplifies and accelerates the model training process by abstracting away the complexity of AI models and deep learning frameworks. With the TAO Toolkit, developers can use pretrained models and fine-tune them for specific use cases.&nbsp;</p>\n\n\n\n<p>In this post, we leverage an advanced pretrained model for change detection called VisualChangeNet and fine-tune it with the TAO Toolkit to detect defects in the MV Tech Anomaly detection dataset. This comprehensive benchmarking dataset is designed for anomaly detection in machine vision, consisting of various industrial products with both normal and defective samples.&nbsp;</p>\n\n\n\n<p>Using the TAO Toolkit, we use transfer learning to train a model that achieves an overall accuracy of 99.67%, 92.3% mIoU, 95.8% mF1, 97.5 mPrecision, and 94.3% mRecall on the bottle class of the MVTec Anomaly dataset. Figure 1 shows the defect mask prediction using the trained model.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1076\" height=\"316\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks.png\" alt=\"An image showing a sample of a defective bottle, a reference golden sample of the bottle and the predicted and true defect masks.\" class=\"wp-image-73777\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks.png 1076w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-300x88.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-625x184.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-179x53.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-768x226.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-645x189.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-500x147.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-160x47.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-362x106.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-375x110.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-masks-1024x301.png 1024w\" sizes=\"(max-width: 1076px) 100vw, 1076px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Segmentation predicts a defect mask of a defective object by comparing it with a golden image</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Step 1: Setup prerequisites&nbsp;</h2>\n\n\n\n<p>To follow along with the post and recreate these steps, take the following actions.</p>\n\n\n\n<ol>\n<li>Register for an account on the NGC Catalog and generate your API key by following the steps provided in the NGC <a href=\"https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key\">User\u2019s Guide</a>.&nbsp;</li>\n\n\n\n<li>Set up the TAO Launcher by following the TAO Quickstart Guide. Download the VisualChangeNet Segmentation Jupyter Notebook for the MVTec dataset. Launch the Jupyter Notebook and run the cells to follow along with this post. <br>     *Note that the VisualChangeNet model works only from the 5.1 version.</li>\n\n\n\n<li>Download and prepare the <a href=\"https://www.mvtec.com/company/research/datasets/mvtec-ad\">MVTec anomaly detection dataset</a> by following the prompts to the download page and copying the download link for any of the 15 object classes.&nbsp;</li>\n\n\n\n<li>Paste the download link into the \u201cFIXME\u201d location in section 2.1 of the Jupyter Notebook and run the notebook cell. This post focuses on the bottle object however, all 15 objects work in the notebook. Figure 2 shows the sample defect images in the dataset.</li>\n</ol>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n#Download the data\nimport os\nMVTEC_AD_OBJECT_DOWNLOAD_URL = &quot;FIXME&quot;\nmvtec_object = MVTEC_AD_OBJECT_DOWNLOAD_URL.split(&quot;/&quot;)&#91;-1].split(&quot;.&quot;)&#91;0]\nos.environ&#91;&quot;URL_DATASET&quot;]=MVTEC_AD_OBJECT_DOWNLOAD_URL\nos.environ&#91;&quot;MVTEC_OBJECT&quot;]=mvtec_object\n!if &#91; ! -f $HOST_DATA_DIR/$MVTEC_OBJECT.tar.xz ]; then wget $URL_DATASET -O $HOST_DATA_DIR/$MVTEC_OBJECT.tar.xz; else echo &quot;image archive already downloaded&quot;; fi\n</pre></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1074\" height=\"366\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects.png\" alt=\"An image showing three sample defective objects: a cable, bottle and transistor from the MVTech dataset\u00a0\" class=\"wp-image-73843\" style=\"aspect-ratio:2.9344262295081966;width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects.png 1074w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-300x102.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-625x213.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-179x61.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-768x262.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-645x220.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-500x170.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-362x123.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-323x110.png 323w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/segment-defects-1024x349.png 1024w\" sizes=\"(max-width: 1074px) 100vw, 1074px\" /><figcaption class=\"wp-element-caption\">Figure 2. Sample defect images from the MVTech dataset of a cable, bottle, and <em>transistor (left to right)</em></figcaption></figure></div>\n\n\n<p>From MVTec-AD, we leverage the bottle class to showcase automated optical inspection for industrial inspection use cases with VisualChangeNet using the TAO Toolkit.&nbsp;</p>\n\n\n\n<p>After the Jupyter Notebook downloads the dataset, run section 2.3 of the notebook to process the dataset into the correct format for VisualChangeNet segmentation.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nimport random \nimport shutil \nfrom PIL import Image\nos.environ&#91;&quot;HOST_DATA_DIR&quot;] = os.path.join(os.environ&#91;&quot;LOCAL_PROJECT_DIR&quot;], &quot;data&quot;, &quot;changenet&quot;)\nformatted_dir = f&quot;formatted_{mvtec_object}_dataset&quot;\n\nDATA_DIR = os.environ&#91;&quot;HOST_DATA_DIR&quot;]\nos.environ&#91;&quot;FORMATTED_DATA_DIR&quot;] = formatted_dir\n\n#setup dataset folders in expected format \nformatted_path = os.path.join(DATA_DIR, formatted_dir)\na_dir = os.path.join(formatted_path, &quot;A&quot;)\nb_dir = os.path.join(formatted_path, &quot;B&quot;)\nlabel_dir = os.path.join(formatted_path, &quot;label&quot;)\nlist_dir = os.path.join(formatted_path, &quot;list&quot;)\n\n#Create the expected folders\nos.makedirs(formatted_path, exist_ok=True)\nos.makedirs(a_dir, exist_ok=True)\nos.makedirs(b_dir, exist_ok=True)\nos.makedirs(label_dir, exist_ok=True)\nos.makedirs(list_dir, exist_ok=True)\n</pre></div>\n\n\n<p>The original dataset was designed for anomaly detection. We merge the two to create a combined dataset of 283 images and then divide them into 253 training set images and 30 testing set images. Both sets include defective samples.&nbsp;</p>\n\n\n\n<p>We ensured that the test set included 30% of the defective samples from each defect class, as the &#8216;bottle&#8217; class predominantly contained &#8216;no-defect&#8217; images, with around 20 images for each of the three defect classes.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1556\" height=\"512\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input.jpg\" alt=\"An image showing sample input from the dataset consisting of a test image, a golden image, and a segmentation mask for the defect.\u00a0\" class=\"wp-image-73847\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input.jpg 1556w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-300x99.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-625x206.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-179x59.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-768x253.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-1536x505.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-645x212.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-500x165.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-160x53.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-362x119.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-334x110.jpg 334w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sample-input-1024x337.jpg 1024w\" sizes=\"(max-width: 1556px) 100vw, 1556px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. A sample input from the dataset with a test image, golden image, and segmentation mask showing the defect. The view is of a bottle from the top and the camera is mounted to look straight down</em>&nbsp;</figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Step 2: Download the VisualChangeNet model</h2>\n\n\n\n<p>VisualChangeNet model is a state-of-the-art transformer-based change detection model. Central to its design is the Siamese Network. A Siamese Network is a unique neural network architecture composed of two or more identical subnetworks. These &#8220;twin&#8221; subnetworks accept different inputs but share the same parameters and weights. In the context of VisualChangeNet, this architecture enables the model to compare features between a current image and a reference &#8220;golden&#8221; image, pinpointing variations and changes. This capability makes Siamese Networks especially adept at tasks like image comparison and anomaly detection.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/visual_changenet_segmentation.html\">model documentation</a> provides more details like architecture and training data. Instead of training a model from scratch, we leverage the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_fan_classification_nvimagenet\">pretrained FAN backbone</a>, which was trained on the NV-ImageNet dataset as a starting point. We fine-tune it with the TAO Toolkit on the MVTec-AD dataset for the bottle class.</p>\n\n\n\n<p>Run Section 3 of the notebook to install the NGC command-line tool and download the pretrained backbone from NGC.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Installing NGC CLI on the local machine.\n## Download and install\nimport os\n%env CLI=ngccli_cat_linux.zip\n!mkdir -p $HOST_RESULTS_DIR/ngccli\n\n# # Remove any previously existing CLI installations\n!rm -rf $HOST_RESULTS_DIR/ngccli/*\n!wget &quot;https://ngc.nvidia.com/downloads/$CLI&quot; -P $HOST_RESULTS_DIR/ngccli\n!unzip -u &quot;$HOST_RESULTS_DIR/ngccli/$CLI&quot; -d $HOST_RESULTS_DIR/ngccli/\n!rm $HOST_RESULTS_DIR/ngccli/*.zip\nos.environ&#91;&quot;PATH&quot;]=&quot;{}/ngccli/ngc-cli:{}&quot;.format(os.getenv(&quot;HOST_RESULTS_DIR&quot;, &quot;&quot;), os.getenv(&quot;PATH&quot;, &quot;&quot;))\n!mkdir -p $HOST_RESULTS_DIR/pretrained\n!ngc registry model list nvidia/tao/pretrained_fan_classification_nvimagenet*\n!ngc registry model download-version &quot;nvidia/tao/pretrained_fan_classification_nvimagenet:fan_base_hybrid_nvimagenet&quot; --dest $HOST_RESULTS_DIR/pretrained\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\">Step 3: Train the model using the TAO Toolkit</h2>\n\n\n\n<p>In this section, we go into the details of training the VisualChangeNet model using the TAO Toolkit. You can find the details of the Visual ChangeNet models along with the supported pretrained weights in the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/visual_changenet_segmentation_levircd\">model card</a>. You can also use the pretrained FAN backbone weights as the starting point for fine-tuning VisualChangeNet, which is what we use to fine-tune on the MVTec-AD dataset.&nbsp;</p>\n\n\n\n<p>As shown in Figure 4, the training algorithm updates the parameters across all the subnetworks in tandem. In TAO, Visual ChangeNet supports two images as input\u2014a golden sample and a test \u200csample. The goal is to detect a change between the \u201cgolden or reference&#8221; image and the &#8220;test&#8221; image. TAO supports the <a href=\"https://arxiv.org/abs/2204.12451\">FAN</a> backbone network for Visual ChangeNet architectures.&nbsp;</p>\n\n\n\n<p>TAO supports two types of Change Detection networks: Visual ChangeNet-Segmentation and Visual ChangeNet-Classification. In this post, we leverage the Visual ChangeNet-Segmentation model to demonstrate change detection by segmenting the changed pixels between the two input images from the MVTec-AD dataset.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1866\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet.png\" alt=\"An image showing the architecture of the segmentation algorithm that detects changes between a golden image and a test image of the bottle class.\" class=\"wp-image-73854\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet.png 1866w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-625x322.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-768x395.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-1536x790.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-645x332.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-500x257.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-362x186.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-214x110.png 214w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Architecture-Visual-ChangeNet-1024x527.png 1024w\" sizes=\"(max-width: 1866px) 100vw, 1866px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. An Architecture diagram of the Visual ChangeNet-Segmentation algorithm that detects changes between a golden image and a test image of the bottle class</em></figcaption></figure></div>\n\n\n<p>Fine-tuning the VisualChangeNet model is easy with the TAO Toolkit and requires zero coding experience. Simply load the data in the TAO Toolkit, set up the experiment configuration, and run the train command.&nbsp;</p>\n\n\n\n<p>The experiment config file defines the hyperparameters for the VisualChangeNet model\u2019s architecture, training, and evaluation. In the Jupyter Notebook, you can view and edit the config file before training the model.&nbsp;</p>\n\n\n\n<p>We use this config for fine-tuning the Visual ChangeNet model. In the config, let\u2019s define a Visual ChangeNet model with a pretrained FAN-Hybrid-Base backbone, which is the baseline model. Let\u2019s train the model for 30 epochs with batch size 8. The following section demonstrates a partial experiment config, showing some key parameters. The full experiment config is viewable in the Jupyter Notebook.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nencryption_key: tlt_encode\ntask: segment\ntrain:\n  resume_training_checkpoint_path: null\n  pretrained_model_path: null\n  segment:\n    loss: &quot;ce&quot;\n    weights: &#91;0.5, 0.5, 0.5, 0.8, 1.0]\n  num_epochs: 30\n  num_nodes: 1\n  val_interval: 1\n  checkpoint_interval: 1\n  optim:\n    lr: 0.0002\n    optim: &quot;adamw&quot;\n    policy: &quot;linear&quot; \n    momentum: 0.9\n    weight_decay: 0.01\nresults_dir: &quot;/results&quot;\nmodel:\n  backbone:\n    type: &quot;fan_base_16_p4_hybrid&quot;\n    pretrained_backbone_path: /results/pretrained/pretrained_fan_classification_nvimagenet_vfan_base_hybrid_nvimagenet/fan_base_hybrid_nvimagenet.pth\n</pre></div>\n\n\n<p>Some common values that can be modified to tune the performance of the model are the number of training epochs, the learning rate (lr), the optimizer, and the pretrained backbone. To train from scratch, the pretrained_backbone_path can be set to null, however, this will likely increase the number of epochs and amount of data needed to achieve high accuracy. For more information about the parameters in the experiment config file, see the <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/visual_changenet/index.html\">VisualChangeNet User\u2019s Guide</a>.</p>\n\n\n\n<p>Now that the dataset and experiment config is ready, let\u2019s start the training in the TAO Toolkit. Run the code block in section 5.1 to launch a Visual ChangeNet training with a single GPU.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nprint(&quot;Train model&quot;)\n!tao model visual_changenet train \\\n                  -e $SPECS_DIR/experiment.yaml \\\n                    train.num_epochs=$NUM_EPOCHS \\\n                    dataset.segment.root_dir=$DATA_DIR \\\n                    model.backbone.pretrained_backbone_path=$BACKBONE_PATH\n</pre></div>\n\n\n<p>This cell will begin training the Visual ChangeNet Segmentation model on the MVTec dataset. During training, the model will learn how to identify defective objects and output a segmentation mask showing the defective region. The training log, which includes accuracy on the validation dataset, training loss, learning rate, and trained model, is saved in the results directory set in the experiment config.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Step 4: Evaluate the model</h2>\n\n\n\n<p>After training is complete, we can use TAO to evaluate the model on a validation dataset. For Visual ChangeNet Segmentation, the output is a segmentation change map for the 2 given input images denoting the pixel-level defects. Section 6 of the notebook will run the command to evaluate the model\u2019s performance.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n!tao model visual_changenet evaluate \\\n                   -e $SPECS_DIR/experiment.yaml \\\n                    evaluate.checkpoint=$RESULTS_DIR/train/changenet.pth \\\n                    dataset.segment.root_dir=$DATA_DIR\n</pre></div>\n\n\n<p>The evaluate command in TAO will return several KPIs on the validation set such as accuracy, precision, recall, F1 score, and IoU for the defect class (defect pixels).&nbsp;</p>\n\n\n\n<p>OA = overall accuracy of change/no change pixels (input dimension &#8211; 256&#215;256)&nbsp;</p>\n\n\n\n<p><strong>MVTec-AD Binary CD (Bottle Class)</strong></p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Model</strong></td><td><strong>Backbone</strong></td><td><strong>mPrecision</strong></td><td><strong>mRecall</strong></td><td><strong>mF1</strong></td><td><strong>mIOU</strong></td><td><strong>OA</strong></td></tr><tr><td>VisualChangeNet</td><td>FAN-Hybrid-B (pretrained)</td><td>97.5&nbsp;</td><td>94.3</td><td>95.8</td><td>92.3</td><td><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-vivid-green-cyan-color\">99.67</mark></strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Evaluation metrics of the VisualChangeNet model for MVTec-AD Binary CD (Bottle Class)</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Step 5: Deploy the model&nbsp;</h2>\n\n\n\n<p>You can use this fine-tuned model and deploy it using <a href=\"https://developer.nvidia.com/deepstream-sdk\">NVIDIA DeepStream</a> or <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton</a>. Let&#8217;s export it to the .onnx format. Section 8 of the notebook will run the TAO export command.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n!tao model visual_changenet export \\\n                    -e $SPECS_DIR/experiment.yaml \\\n                        export.checkpoint=$RESULTS_DIR/train/changenet.pth \\\n                        export.onnx_file=$RESULTS_DIR/export/changenet.onnx\n</pre></div>\n\n\n<p>The output .onnx model is saved in the same directory as the trained .pth model. To deploy to Triton, check out the <a href=\"https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps\">tao-toolkit-triton</a> repository on GitHub. This project provides reference implementations to deploy many TAO models, including Visual ChangeNet Segmentation, to a Triton inference server.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Real-time inference performance</h3>\n\n\n\n<p>The inference is run on the provided unpruned model at FP16 precision. The inference performance is run using trtexec on embedded Jetson Orin GPUs and data center GPUs. The Jetson devices are running at Max-N configuration for maximum GPU frequency. </p>\n\n\n\n<p>Run the following command to run trtexec:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>/usr/src/tensorrt/bin/trtexec --onnx=&lt;ONNX path&gt; --minShapes=input0:1x3x512x512,input1:1x3x512x512 --maxShapes=input0:8x3x512x512,input1:8x3x512x512 --optShapes=input0:4x3x512x512,input1:4x3x512x512 \n--saveEngine=&lt;engine path&gt;\n</code></pre>\n\n\n\n<p>The performance shown here is the inference-only performance. The end-to-end performance with streaming video data might vary depending on other bottlenecks in the hardware and software.</p>\n\n\n\n<figure class=\"wp-block-table\"><table><thead><tr><th>Platform</th><th><strong>Batch Size</strong></th><th><strong>FPS</strong></th></tr></thead><tbody><tr><td>NVIDIA Jetson Orin Nano 8 GB</td><td>16</td><td>15.19</td></tr><tr><td>NVIDIA Jetson Orin NX 16 GB</td><td>16</td><td>21.92</td></tr><tr><td>NVIDIA Jetson AGX Orin 64 GB</td><td>16</td><td>55.07</td></tr><tr><td>NVIDIA A2 Tensor Core GPU</td><td>16</td><td>36.02</td></tr><tr><td>NVIDIA T4 Tensor Core GPU</td><td>16</td><td>59.7</td></tr><tr><td>NVIDIA L4 Tensor Core GPU</td><td>8</td><td>131.48</td></tr><tr><td>NVIDIA A30 Tensor Core GPU</td><td>16</td><td>204.12</td></tr><tr><td>NVIDIA L40 GPU</td><td>8</td><td>364</td></tr><tr><td>NVIDIA A100 Tensor Core GPU</td><td>32</td><td>435.18</td></tr><tr><td>NVIDIA H100 Tensor Core GPU</td><td>32</td><td>841.68</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Performance metrics for inference of unpruned model at FP16 precision on different platforms</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Summary&nbsp;</h2>\n\n\n\n<p>In this post, we learned how to use the TAO Toolkit to fine-tune the VisualChangeNet model and use it for segmenting defects in the MVTech dataset, achieving an overall accuracy of 99.67%.&nbsp;</p>\n\n\n\n<p>You can also now leverage NVIDIA TAO to detect defects in your manufacturing workflows. </p>\n\n\n\n<p>To get started:&nbsp;</p>\n\n\n\n<ul>\n<li>Download the VisualChangeNet model from the NVIDIA NGC catalog.&nbsp;</li>\n\n\n\n<li>Follow the <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html\">TAO Quickstart Guide to set up the TAO </a>launcher.</li>\n\n\n\n<li>Download the Visual ChangeNet Segmentation Notebook from <a href=\"https://github.com/NVIDIA/tao_tutorials/tree/main/notebooks/tao_launcher_starter_kit/visual_changenet\">GitHub</a></li>\n</ul>\n\n\n\n<p>Learn more about the NVIDIA TAO Toolkit from <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/overview.html#:~:text=NVIDIA%20TAO%20Toolkit%20is%20a,and%20the%20deep%20learning%20framework.\">NVIDIA Docs</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Efficiency is paramount in industrial manufacturing, where even minor gains can have significant financial implications. According to the American Society of Quality, &#8220;Many organizations will have true quality-related costs as high as 15-20% of sales revenue, some going as high as 40% of total operations.&#8221; These staggering statistics reveal a stark reality: defects in industrial &hellip; <a href=\"https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/\">Continued</a></p>\n", "protected": false}, "author": 1921, "featured_media": 73882, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1300772", "discourse_permalink": "https://forums.developer.nvidia.com/t/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/273477", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758], "tags": [453, 1646, 369, 375], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/defect-detection-TAO-bottles.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jbG", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73760"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73760"}], "version-history": [{"count": 36, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73760/revisions"}], "predecessor-version": [{"id": 75002, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73760/revisions/75002"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73882"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73760"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73760"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73760"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73447, "date": "2023-11-20T08:00:00", "date_gmt": "2023-11-20T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73447"}, "modified": "2023-11-30T12:00:53", "modified_gmt": "2023-11-30T20:00:53", "slug": "early-bird-pricing-now-open-for-hands-on-training-at-gtc", "status": "publish", "type": "post", "link": "https://www.nvidia.com/gtc/training/?nvid=nv-int-tblg-348123", "title": {"rendered": "Early Bird Pricing Now Open for Hands-on Training at GTC"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Register for expert-led technical workshops at NVIDIA GTC and save with early bird pricing through February 7, 2024.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Register for expert-led technical workshops at NVIDIA GTC and save with early bird pricing through February 7, 2024.</p>\n", "protected": false}, "author": 338, "featured_media": 73448, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://www.nvidia.com/gtc/training/?nvid=nv-int-tblg-348123", "_links_to_target": "_blank"}, "categories": [2724, 1050, 3110, 503, 1903], "tags": [2964, 1935, 453, 36, 3257, 2932, 3270], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/gtc24-spring-dli-early-email-thumbnail-600x338-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-j6D", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73447"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/338"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73447"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73447/revisions"}], "predecessor-version": [{"id": 73450, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73447/revisions/73450"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73448"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73447"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73447"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73447"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74008, "date": "2023-11-17T13:11:29", "date_gmt": "2023-11-17T21:11:29", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74008"}, "modified": "2023-11-30T11:43:29", "modified_gmt": "2023-11-30T19:43:29", "slug": "boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/", "title": {"rendered": "Boosting Custom ROS Graphs Using NVIDIA Isaac Transport for ROS"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA Isaac Transport for ROS (<a rel=\"noreferrer noopener\" href=\"https://nvidia-isaac-ros.github.io/concepts/nitros/index.html\" target=\"_blank\">NITROS</a>) is the implementation of two hardware-acceleration features introduced with ROS 2 Humble-type adaptation and type negotiation.&nbsp;</p>\n\n\n\n<p><strong>Type adaptation </strong>enables ROS nodes to work in a data format optimized for specific hardware accelerators. The adapted type is used by processing graphs to eliminate memory copies between the CPU and the memory accelerator.&nbsp;</p>\n\n\n\n<p>Through <strong>type negotiation</strong>, different ROS nodes in a processing graph can advertise their supported types and the ROS framework can choose data formats, resulting in ideal performance.</p>\n\n\n\n<center><figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"800\" height=\"450\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-nvidia.gif\" alt=\"GIF showing a comparison between inefficient hardware acceleration and NITROS-enabled efficient hardware acceleration. In the first case, image data is copied multiple times between CPU and GPU resulting in slow data transfer. In the second case, image data is transferred once from CPU to GPU, and is then accessible directly from GPU memory to NITROS-compatible nodes.\" class=\"wp-image-74011\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. NITROS enables efficient acceleration by reducing memory copies between CPU and GPU</em></figcaption></figure></center>\n\n\n\n<p>When two NITROS-capable ROS nodes are next to each other in a graph, they can discover each other through type negotiation and then use type adaptation for sharing data. Together, type adaptation and type negotiation significantly improve the performance of AI and computer vision tasks in ROS-based applications, by removing unnecessary memory copies.&nbsp;</p>\n\n\n\n<p>This reduces CPU overhead and optimizes performance on the underlying hardware. Figure 1 shows efficient hardware acceleration using NITROS. Data is accessible from GPU memory instead of frequent CPU copies.</p>\n\n\n\n<p>You can use a combination of NITROS-based Isaac ROS nodes and other ROS nodes in your processing graphs, as the ROS framework maintains compatibility with legacy nodes that don&#8217;t support negotiation. A NITROS-capable node functions like a typical ROS 2 node while communicating with a non-NITROS node. Most <a href=\"https://nvidia-isaac-ros.github.io/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">Isaac ROS GEMs</a> are NITROS-accelerated.&nbsp;</p>\n\n\n\n<p>Learn more about NITROS and system assumptions from <a href=\"https://nvidia-isaac-ros.github.io/concepts/nitros/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NITROS docs</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA CUDA with NITROS</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/cuda-zone#:~:text=CUDA%C2%AE%20is%20a%20parallel,harnessing%20the%20power%20of%20GPUs.\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA CUDA</a> is a parallel computing programming model that can drastically speed up functions in robotic systems with GPUs. Your custom ROS 2 nodes can use CUDA with NITROS through the Managed NITROS Publisher and Managed NITROS Subscriber.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"218\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-625x218.png\" alt=\"Block diagram showing CUDA with NITROS. Your ROS 2 node can use a Managed NITROS Subscriber \u200cor Publisher to communicate with other NITROS-capable nodes.\" class=\"wp-image-74016\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-625x218.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-300x104.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-768x267.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-645x225.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-500x174.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-160x56.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-362x126.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-316x110.png 316w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS-1024x356.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Overview-of-CUDA-with-NITROS.png 1040w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Overview of CUDA with NITROS</em></figcaption></figure></div>\n\n\n<p>CUDA code in a ROS node can share its output buffers in GPU memory with NITROS-capable Isaac ROS nodes using the <strong>Managed</strong> <strong>NITROS Publisher</strong>. This removes expensive CPU memory copies, improving performance as a result. NITROS also maintains compatibility with non-NITROS nodes, by publishing the same data as a normal ROS 2 message.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"397\" height=\"222\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node.png\" alt=\"Block diagram showing the use of Managed NITROS Publisher in your ROS 2 node. Your node can then communicate with both NITROS-capable and non-NITROS nodes (like RViz) through type adaptation.\" class=\"wp-image-74017\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node.png 397w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node-300x168.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node-362x202.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NITROS-Publisher-in-a-ROS-2-node-197x110.png 197w\" sizes=\"(max-width: 397px) 100vw, 397px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. NITROS Publisher in a ROS 2 node</em></figcaption></figure></div>\n\n\n<p>On the Subscriber side, CUDA code in a ROS node can receive input in GPU memory using the <strong>Managed</strong> <strong>NITROS Subscriber</strong>. Input can come from either a NITROS-capable Isaac ROS node or another CUDA-enabled ROS node using a NITROS Publisher. Just like the Managed NITROS Publisher, this gives better performance by increasing the parallel compute between the GPU and CPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"379\" height=\"114\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node.png\" alt=\"lock diagram showing the use of Managed NITROS Subscriber in your ROS 2 node. Your node can then subscribe to NITROS-typed messages from a NITROS-capable node through type adaptation.\" class=\"wp-image-74018\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node.png 379w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node-300x90.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node-179x54.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node-160x48.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node-362x109.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-4.-NITROS-Subscriber-in-a-ROS-2-node-366x110.png 366w\" sizes=\"(max-width: 379px) 100vw, 379px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. NITROS Subscriber in a ROS 2 node</em></figcaption></figure></div>\n\n\n<p>To understand this better, let\u2019s consider an example graph performing DNN-based point cloud segmentation. At a high level, these are the three main components using CUDA with NITROS:</p>\n\n\n\n<ol>\n<li>Encoder node with Managed NITROS Publisher to convert a <em>sensor_msgs/PointCloud2</em> message into a <em>NitrosTensorList</em>&nbsp;</li>\n\n\n\n<li>Isaac ROS TensorRT node to perform DNN inference, taking in an input <em>NitrosTensorList</em> and producing an output <em>NitrosTensorList</em></li>\n\n\n\n<li>Decoder node with Managed NITROS Subscriber to convert the output <em>NitrosTensorList</em> into a segmented <em>sensor_msgs/PointCloud2</em> message</li>\n</ol>\n\n\n\n<p>The Managed NITROS Publisher and Subscriber offer a familiar interface, comparable to the standard <em>rclcpp::Publisher</em> and <em>rclcpp::Subscriber</em> APIs, making integration with existing ROS 2 nodes intuitive. CUDA with NITROS also enables a more modular software design. With Managed NITROS Publishers and Subscribers, CUDA nodes can be used anywhere in a graph with Isaac ROS nodes and other CUDA nodes to get the advantages of accelerated computing in each node.</p>\n\n\n\n<p>Digging just a little deeper, NITROS is based on the NVIDIA <a rel=\"noreferrer noopener\" href=\"https://docs.nvidia.com/clara-holoscan/archive/clara-holoscan-0.2.0/gxf/index.html\" target=\"_blank\">Graph Execution Framework</a> (GXF), an extensible framework for building high-performance compute graphs. NITROS leverages GXF to achieve efficient ROS application graphs. CUDA with NITROS removes the need for developers to understand the underlying workings of GXF as a prerequisite to making their nodes NITROS-capable. The GXF layer is abstracted away\u2014making it easy and speedy for users to write ROS 2 nodes like they usually do, with straightforward tweaks to enable NITROS.&nbsp;&nbsp;</p>\n\n\n\n<p>Learn more about the <a href=\"https://nvidia-isaac-ros.github.io/concepts/nitros/cuda_with_nitros.html#core-concepts\" target=\"_blank\" rel=\"noreferrer noopener\">core concepts</a> of CUDA with NITROS.&nbsp;</p>\n\n\n\n<p>Currently, the Managed NITROS Publisher and Subscriber are only compatible with the Isaac ROS <em>NitrosTensorList</em> message type. Visit <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_nitros/tree/main/isaac_ros_nitros_type\" target=\"_blank\" rel=\"noreferrer noopener\">isaac_ros_nitros_type</a> for a complete list of NITROS data types.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Object detection using CUDA with NITROS and YOLOv8</h2>\n\n\n\n<p>Isaac ROS provides a <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_object_detection/tree/main/isaac_ros_yolov8\" target=\"_blank\" rel=\"noreferrer noopener\">YOLOv8 sample</a> showing how to use Managed NITROS utilities with your custom ROS decoders to take advantage of NITROS. This sample uses packages from <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_dnn_inference/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">Isaac ROS DNN Inference</a> to perform TensorRT accelerated object detection using YOLOv8. The Managed NITROS Publisher and Subscriber use NITROS-typed messages and currently are only compatible with the Isaac ROS <em>NitrosTensorList</em> message type. This message type is used to share tensors between your nodes and Isaac ROS DNN Inference nodes.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1005\" height=\"877\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference.png\" alt=\"An image of a group of people on bicycles with bounding boxes drawn around detected objects in the image. These are the results of object detection using YOLOv8 and Isaac ROS DNN Inference.\" class=\"wp-image-74019\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference.png 1005w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-300x262.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-625x545.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-132x115.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-768x670.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-645x563.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-344x300.png 344w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-103x90.png 103w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-362x316.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-YOLOv8-object-detection-using-Isaac-ROS-DNN-Inference-126x110.png 126w\" sizes=\"(max-width: 1005px) 100vw, 1005px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. YOLOv8 object detection using Isaac ROS DNN Inference</em></figcaption></figure></div>\n\n\n<p>Let\u2019s say you want to use a custom object detection model with Isaac ROS DNN Inference and CUDA NITROS acceleration. There are three main steps involved in the detection pipeline: input image encoding, DNN inference, and output decoding. Isaac ROS DNN Inference has implementations for the first two steps.</p>\n\n\n\n<p>In the decoding step, relevant information must be extracted from inferred results, which are tensors. For a task like 2D object detection, relevant information includes bounding boxes and class scores for each detected output in the image.&nbsp;</p>\n\n\n\n<p>Let\u2019s look into each step in some more detail.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 1: Encoding</h3>\n\n\n\n<p>On the input side, Isaac ROS provides a NITROS-accelerated <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_dnn_image_encoder/index.html\">DNN image encoder</a>. This preprocesses input images and converts them into tensors, which are communicated through the <em>isaac_ros_tensor_list</em> type to the TensorRT or Triton nodes for inference.&nbsp;</p>\n\n\n\n<p>You can specify parameters like image size and the input size your network expects, for various preprocessing functions like resizing. Note that you&#8217;ll need different encoders based on the task. For instance, you can\u2019t use this image encoder with language models because the networks expect different input encodings.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"830\" height=\"227\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node.png\" alt=\"Diagram showing an overview of the Isaac ROS DNN Image Encoder node. The node takes in a ROS 2 image message as input, encodes it into a list of tensors and outputs an Isaac ROS TensorList message. This message is passed onto the Isaac ROS inference node.   \" class=\"wp-image-74020\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node.png 830w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-300x82.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-625x171.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-179x49.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-768x210.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-645x176.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-500x137.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-160x44.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-362x99.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-Overview-of-the-Isaac-ROS-DNN-image-encoder-node-402x110.png 402w\" sizes=\"(max-width: 830px) 100vw, 830px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Overview of the Isaac ROS DNN image encoder node</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Step 2: Inference</h3>\n\n\n\n<p>Isaac ROS provides two ROS nodes for DNN inference\u2014the <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_tensor_rt/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">TensorRT node</a> and <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_triton/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">Triton node</a>. Of these, the YOLOv8 sample currently uses the TensorRT node. You provide your trained model to the TensorRT node, which performs inference and outputs a tensor containing detection results.&nbsp;</p>\n\n\n\n<p>This output tensor list is passed onto the decoder node. You can specify parameters like dimensions and tensor names expected by the network\u2014information that can be found easily from the ONNX model using tools like <a href=\"https://netron.app/\" target=\"_blank\" rel=\"noreferrer noopener\">Netron</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"807\" height=\"165\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node.png\" alt=\"Block diagram showing an overview of the Isaac ROS TensorRT inference node. It takes a trained model and an input tensor list from the image encoder node as input, performs inference, and outputs a tensor list containing inference results to the decoder node.\" class=\"wp-image-74022\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node.png 807w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-300x61.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-625x128.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-179x37.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-768x157.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-645x132.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-500x102.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-160x33.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-362x74.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-7.-Overview-of-the-Isaac-ROS-TensorRT-inference-node-538x110.png 538w\" sizes=\"(max-width: 807px) 100vw, 807px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Overview of the Isaac ROS TensorRT inference node</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Step 3: Decoding</h3>\n\n\n\n<p>The inferred output tensor from the TensorRT or Triton node must be parsed into the desired bounding box and class information. Let\u2019s say you&#8217;ve written your model\u2019s decoder as a ROS 2 node (not NITROS-capable yet). The decoder node doesn\u2019t support NITROS-typed messages and expects a typical ROS 2 message from the inference node. This still works because NITROS maintains compatibility with non-NITROS nodes.&nbsp;</p>\n\n\n\n<p>However, in this case, the output NITROS-typed message from the inference node (in GPU memory) is converted to a ROS 2 message and brought over to the CPU memory for the decoder to consume. This introduces some overhead as the data now lives in CPU memory, resulting in CPU memory copies while working with downstream ROS nodes.&nbsp;&nbsp;</p>\n\n\n\n<p>Now let\u2019s say you want to upgrade your decoder to communicate with the inference node (and other NITROS-accelerated nodes) through NITROS, instead of incurring the CPU memory copying cost. All the data stays in GPU memory in this case.&nbsp;</p>\n\n\n\n<p>This is made easy by using Managed NITROS Subscriber in your decoder node. It subscribes to the NITROS-typed output message from the inference node and uses <a href=\"https://nvidia-isaac-ros.github.io/concepts/nitros/cuda_with_nitros.html#nitros-views\" target=\"_blank\" rel=\"noreferrer noopener\">NITROS Views</a> to obtain the CUDA buffer containing the detection output. You can then implement your decoding logic to this data and publish the results through an appropriate ROS message type.</p>\n\n\n\n<p>The YOLOv8 decoder can be configured with parameters such as NMS threshold and confidence threshold to filter candidate detections. A simple visualization node can be used to subscribe to the resultant ROS message and draw bounding boxes on the input image. Note that Managed NITROS can only be integrated with CPP ROS 2 nodes.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"953\" height=\"215\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node.png\" alt=\"Diagram showing an overview of the YOLOv8 Decoder node. This node takes in an encoded tensor list from the inference node, extracts required information from the detection results and output results as a Detection2DArray ROS 2 message.\" class=\"wp-image-74023\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node.png 953w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-300x68.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-625x141.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-179x40.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-768x173.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-645x146.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-500x113.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-160x36.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-362x82.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-8.-Overview-of-the-YOLOv8-Decoder-node-488x110.png 488w\" sizes=\"(max-width: 953px) 100vw, 953px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. Overview of the YOLOv8 Decoder node</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Isaac ROS NITROS bridge</h2>\n\n\n\n<p>If your robotics applications are currently based on ROS 1, you can still get the benefits of accelerated computing using the newly released <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_nitros_bridge\" target=\"_blank\" rel=\"noreferrer noopener\">Isaac ROS NITROS bridge</a>. This is also helpful for developers using ROS 2 versions where type adaptation and negotiation aren\u2019t available (pre-Humble versions).&nbsp;</p>\n\n\n\n<p>To highlight the speedups achievable, the NITROS bridge moves 1080p images between ROS 1 Noetic and NITROS packages up to <strong>2.5x faster</strong> than the <a href=\"https://github.com/ros2/ros1_bridge\" target=\"_blank\" rel=\"noreferrer noopener\">ROS 1 bridge.</a></p>\n\n\n\n<p>The ROS bridge includes a CPU-based memory copy cost, which the Isaac ROS NITROS bridge eliminates by moving data from CPU to GPU. This data can be used in place in GPU memory.&nbsp;</p>\n\n\n\n<p>NITROS bridge consists of two converter nodes. One is used on the ROS (for example Noetic) side and the other on the ROS 2 (for example Humble) side. Using the ROS bridge without NITROS converters results in images being sent from Noetic to Humble and back through copies across ROS processes in CPU memory, increasing latency. This problem is especially apparent between nodes sending huge amounts of data like segmented point clouds.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"754\" height=\"147\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters.png\" alt=\"Block diagram of ROS bridge without NITROS converters.\" class=\"wp-image-74024\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters.png 754w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-300x58.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-625x122.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-179x35.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-645x126.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-500x97.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-160x31.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-750x147.png 750w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-362x71.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-9.-ROS-bridge-without-NITROS-converters-564x110.png 564w\" sizes=\"(max-width: 754px) 100vw, 754px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. ROS bridge without NITROS converters</em></figcaption></figure></div>\n\n\n<p>The NITROS bridge is designed with the goal of reducing end-to-end latency across ROS versions. Consider the same example, this time using NITROS converters. The converter on the Noetic side (Figure 10) moves the image to GPU memory, avoiding CPU memory copies over the bridge. The converter on the Humble side (Figure 10) converts the image in GPU memory to a NITROS image type that is compatible to be used with other NITROS-accelerated nodes.&nbsp;</p>\n\n\n\n<p>Things work similarly in the reverse direction\u2014with the image data being sent as a NITROS image from Humble through the converter on either side to an image in CPU-accessible memory in Noetic.</p>\n\n\n\n<p>For more information about performance gains, visit <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_benchmark/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">Isaac ROS Benchmark</a> for <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_benchmark/blob/main/scripts/isaac_ros_nitros_bridge.py\" target=\"_blank\" rel=\"noreferrer noopener\">NITROS bridge</a> and <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_benchmark/blob/main/scripts/isaac_ros_nitros_bridge_reference.py\" target=\"_blank\" rel=\"noreferrer noopener\">ros1_bridge</a>. Note that the Isaac ROS NITROS bridge doesn\u2019t support NVIDIA Jetson platforms yet.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"756\" height=\"213\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge.png\" alt=\"Block diagram showing an overview of NITROSb. On the ROS 1 side, an image in CPU memory is copied over to the GPU through the NITROS Converter ROS node. This image can be used on the ROS 2 side through the NITROS Converter ROS 2 node without any CPU copies since it is available in GPU memory. In this way, the image is also accessible to other NITROS nodes. \" class=\"wp-image-74025\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge.png 756w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-300x85.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-625x176.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-179x50.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-645x182.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-500x141.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-160x45.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-362x102.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-10.-Overview-of-NITROS-bridge-390x110.png 390w\" sizes=\"(max-width: 756px) 100vw, 756px\" /><figcaption class=\"wp-element-caption\"><em>Figure 10. Overview of NITROS bridge</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Benefits of integrating ROS 2 nodes with NITROS</h2>\n\n\n\n<p>The following summarizes the many benefits of integrating your ROS 2 nodes with NITROS:</p>\n\n\n\n<ul>\n<li>Improved performance by reducing CPU memory copies.</li>\n\n\n\n<li>Compatibility with other non-NITROS ROS nodes such as RViz.</li>\n\n\n\n<li>Easy integration of custom ROS 2 nodes with hardware-accelerated Isaac ROS nodes through Managed NITROS Publisher and Subscriber.</li>\n\n\n\n<li>Modular software design using CUDA with NITROS.</li>\n\n\n\n<li>Improved performance of applications based on earlier ROS versions using NITROS bridge.</li>\n</ul>\n\n\n\n<p>Try accelerating your own ROS nodes using <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_nitros/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">Isaac ROS NITROS</a> and our <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_object_detection/tree/main\" target=\"_blank\" rel=\"noreferrer noopener\">YOLOv8 object detection</a> sample!&nbsp;&nbsp;<br>Visit the <a href=\"https://nvidia-isaac-ros.github.io/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Isaac ROS documentation</a> page to learn more about our hardware-accelerated packages. Check out the <a href=\"https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/isaac-ros/600\" target=\"_blank\" rel=\"noreferrer noopener\">Developer </a><a href=\"https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/isaac-ros/600\">Forum</a> for the latest information on Isaac ROS.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Isaac Transport for ROS (NITROS) is the implementation of two hardware-acceleration features introduced with ROS 2 Humble-type adaptation and type negotiation.&nbsp; Type adaptation enables ROS nodes to work in a data format optimized for specific hardware accelerators. The adapted type is used by processing graphs to eliminate memory copies between the CPU and the &hellip; <a href=\"https://developer.nvidia.com/blog/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/\">Continued</a></p>\n", "protected": false}, "author": 890, "featured_media": 74130, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1299600", "discourse_permalink": "https://forums.developer.nvidia.com/t/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/273260", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63], "tags": [453, 2571], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Boosting-Custom-ROS-Graphs-Using-NVIDIA-Isaac-Transpot.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jfG", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74008"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/890"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74008"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74008/revisions"}], "predecessor-version": [{"id": 74131, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74008/revisions/74131"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74130"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74008"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74008"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74008"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73739, "date": "2023-11-17T07:00:00", "date_gmt": "2023-11-17T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73739"}, "modified": "2024-01-25T10:57:32", "modified_gmt": "2024-01-25T18:57:32", "slug": "mastering-llm-techniques-inference-optimization", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/", "title": {"rendered": "Mastering LLM Techniques: Inference Optimization"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Stacking transformer layers to create large models results in better accuracies, few-shot learning capabilities, and even near-human emergent abilities on a wide range of language tasks. These foundation models are expensive to train, and they can be memory- and compute-intensive during inference (a recurring cost). The most popular <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language models (LLMs)</a> today can reach tens to hundreds of billions of parameters in size and, depending on the use case, may require ingesting long inputs (or contexts), which can also add expense.&nbsp;For example, <a href=\"https://developer.nvidia.com/blog/tag/retrieval-augmented-generation-rag/\">retrieval-augmented generation</a> (RAG) pipelines require putting large amounts of information into the input of the model, greatly increasing the amount of processing work the LLM has to do.</p>\n\n\n\n<p>This post discusses the most pressing challenges in LLM inference, along with some practical solutions. Readers should have a basic understanding of <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">transformer architecture</a> and the attention mechanism in general. It is essential to have a grasp of the intricacies of LLM inference, which we will address in the next section.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Understanding LLM inference</h2>\n\n\n\n<p>Most of the popular decoder-only LLMs (GPT-3, for example) are pretrained on the causal modeling objective, essentially as next-word predictors. These LLMs take a series of tokens as inputs, and generate subsequent tokens autoregressively until they meet a stopping criteria (a limit on the number of tokens to generate or a list of stop words, for example) or until it generates a special <code>&lt;end&gt;</code> token marking the end of generation. This process involves two phases: the prefill phase and the decode phase.</p>\n\n\n\n<p>Note that <em>tokens</em> are the atomic parts of language that a model processes. One token is approximately four English characters. All inputs in natural language are converted to tokens before inputting into the model.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Prefill phase or processing the input</h3>\n\n\n\n<p>In the prefill phase, the LLM processes the input tokens to compute the intermediate states (keys and values), which are used to generate the \u201cfirst\u201d new token. Each new token depends on all the previous tokens, but because the full extent of the input is known, at a high level this is a matrix-matrix operation that&#8217;s highly parallelized. It effectively saturates GPU utilization.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Decode phase or generating the output&nbsp;</h3>\n\n\n\n<p>In the decode phase, the LLM generates output tokens autoregressively one at a time, until a stopping criteria is met. Each sequential output token needs to know all the previous iterations\u2019 output states (keys and values). This is like a matrix-vector operation that underutilizes the GPU compute ability compared to the prefill phase. The speed at which the data (weights, keys, values, activations) is transferred to the GPU from memory dominates the latency, not how fast the computation actually happens. In other words, this is a memory-bound operation.</p>\n\n\n\n<p>Many of the inference challenges and corresponding solutions featured in this post concern the optimization of this decode phase: efficient attention modules, managing the keys and values effectively, and others.</p>\n\n\n\n<p>Different LLMs may use different tokenizers, and thus, comparing output tokens between them may not be straightforward. When comparing inference throughput, even if two LLMs have similar tokens per second output, they may not be equivalent if they use different tokenizers. This is because corresponding tokens may represent a different number of characters.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Batching</h3>\n\n\n\n<p>The simplest way to improve GPU utilization, and effectively throughput, is through batching. Since multiple requests use the same model, the memory cost of the weights is spread out. Larger batches getting transferred to the GPU to be processed all at once will leverage more of the compute available.&nbsp;</p>\n\n\n\n<p>Batch sizes, however, can only be increased up to a certain limit, at which point they may lead to a memory overflow. To better understand why this happens requires looking at key-value (KV) caching and LLM memory requirements.</p>\n\n\n\n<p>Traditional batching (also called static batching) is suboptimal. This is because for each request in a batch, the LLM may generate a different number of completion tokens, and subsequently they have different execution times. As a result, all requests in the batch must wait until the longest request is finished, which can be exacerbated by a large variance in the generation lengths. There are methods to mitigate this, such as in-flight batching, which will be discussed later.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key-value caching</h3>\n\n\n\n<p>One common optimization for the decode phase is KV caching. The decode phase generates a single token at each time step, but each token depends on the key and value tensors of all previous tokens (including the input tokens\u2019 KV tensors computed at prefill, and any new KV tensors computed until the current time step).&nbsp;</p>\n\n\n\n<p>To avoid recomputing all these tensors for all tokens at each time step, it&#8217;s possible to cache them in GPU memory. Every iteration, when new elements are computed, they are simply added to the running cache to be used in the next iteration. In some implementations, there is one KV cache for each layer of the model.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"619\" height=\"424\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_.png\" alt=\"An illustration of KV caching depicted in Prefill and Decode phases. Prefill is a highly parallelized operation where the KV tensors of all input tokens can be computed simultaneously. During decode, new KV tensors and subsequently the output token at each step is computed autoregressively.\n\" class=\"wp-image-73750\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_.png 619w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_-300x205.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_-168x115.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_-438x300.png 438w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_-131x90.png 131w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_-362x248.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/key-value-caching_-161x110.png 161w\" sizes=\"(max-width: 619px) 100vw, 619px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An illustration of the key-value caching mechanism</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">LLM memory requirement&nbsp;</h3>\n\n\n\n<p>In effect, the two main contributors to the GPU LLM memory requirement are model weights and the KV cache.</p>\n\n\n\n<ul>\n<li><strong>Model weights:</strong> Memory is occupied by the model parameters. As an example, a model with 7 billion parameters (such as <a href=\"https://huggingface.co/meta-llama/Llama-2-7b/blob/main/params.json\">Llama 2 7B</a>), loaded in 16-bit precision (FP16 or BF16) would take roughly 7B * sizeof(FP16) ~= 14 GB in memory.</li>\n\n\n\n<li><strong>KV caching</strong>: Memory is occupied by the caching of self-attention tensors to avoid redundant computation.</li>\n</ul>\n\n\n\n<p>With batching, the KV cache of each of the requests in the batch must still be allocated separately, and can have a large memory footprint. The formula below delineates the size of the KV cache, applicable to most common LLM architectures today.</p>\n\n\n\n<p class=\"has-text-align-center\"><strong>Size of KV cache per token in bytes = 2 * (num_layers) * (num_heads * dim_head) *&nbsp; precision_in_bytes</strong></p>\n\n\n\n<p>The first factor of 2 accounts for the K and V matrices. Commonly, the value of (num_heads * dim_head) is the same as the hidden_size (or dimension of the model, d_model) of the transformer. These model attributes are commonly found in model cards or associated config files.</p>\n\n\n\n<p>This memory size is required for each token in the input sequence, across the batch of inputs. Assuming half-precision, the total size of KV cache is given by the formula below.</p>\n\n\n\n<p class=\"has-text-align-center\"><strong>Total size of KV cache in bytes = (batch_size) * (sequence_length) * 2 * (num_layers) * (hidden_size) *&nbsp; sizeof(FP16)</strong></p>\n\n\n\n<p>For example, with a Llama 2 7B model in 16-bit precision and a batch size of 1, the size of the KV cache will be 1 * 4096 * 2 * 32 * 4096 * 2 bytes, which is ~2 GB.<br></p>\n\n\n\n<p>Managing this KV cache efficiently is a challenging endeavor. Growing linearly with batch size and sequence length, the memory requirement can quickly scale. Consequently, it limits the throughput that can be served, and poses challenges for long-context inputs. This is the motivation behind several optimizations featured in this post.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Scaling up LLMs with model parallelization</h2>\n\n\n\n<p>One way to reduce the per-device memory footprint of the model weights is to distribute the model over several GPUs. Spreading the memory and compute footprint enables running larger models, or larger batches of inputs. Model parallelization is a necessity to train or infer on a model requiring more memory than available on a single device, and to make training times and inference measures (latency or throughput) suitable for certain use cases. There are several ways of parallelizing the model based on how the model weights are split.&nbsp;</p>\n\n\n\n<p>Note that data parallelism is also a technique often mentioned in the same context as the others listed below. In this, weights of the model are copied over multiple devices, and the (global) batch size of inputs is sharded across each of the devices into microbatches. It reduces the overall execution time by processing larger batches. However, it is a training time optimization that is less relevant during inference.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Pipeline parallelism</h3>\n\n\n\n<p>Pipeline parallelism involves sharding the model (vertically) into chunks, where each chunk comprises a subset of layers that is executed on a separate device. Figure 2a is an illustration of four-way pipeline parallelism, where the model is sequentially partitioned and a quarter subset of all layers are executed on each device. The outputs of a group of operations on one device are passed to the next, which continues executing the subsequent chunk. <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=F_n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"F_n\" class=\"latex\" /> and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=B_n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"B_n\" class=\"latex\" /> <sub>&nbsp;</sub>indicate forward and backward passes respectively on device n. The memory requirement for storing model weights on each device is effectively quartered.</p>\n\n\n\n<p>The main limitation of this method is that, due to the sequential nature of the processing, some devices or layers may remain idle while waiting for the output (activations, gradients) of previous layers. This results in inefficiencies or &#8220;pipeline bubbles&#8221; in both the forward and backward passes. In Figure 2b, the white empty areas are the large pipeline bubbles with naive pipeline parallelism where devices are idle and underutilized.<br><br>Microbatching can mitigate this to some extent, as shown in Figure 2c. The global batch size of inputs is split into sub-batches, which are processed one by one, with gradients being accumulated at the end. Note that <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=F_%7Bn%2Cm%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"F_{n,m}\" class=\"latex\" /> and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=B_%7Bn%2Cm%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"B_{n,m}\" class=\"latex\" /> indicate forward and backward passes respectively on device <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"n\" class=\"latex\" /> with microbatch <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=m&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"m\" class=\"latex\" />. This approach shrinks the size of pipeline bubbles, but it does not completely eliminate them.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1244\" height=\"585\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism.png\" alt=\"Depiction of four-way pipeline parallelism. (a) Model is partitioned across layers in 4 parts, each subset executed on a separate device. (b) Naive pipeline parallelism results in large pipeline bubbles and GPU under-utilization. (c) Micro-batching reduces the size of pipeline bubbles, and improves GPU utilization.\n\" class=\"wp-image-73769\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism.png 1244w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-625x294.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-768x361.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-645x303.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-500x235.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-234x110.png 234w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/four-way-pipeline-parallelism-1024x482.png 1024w\" sizes=\"(max-width: 1244px) 100vw, 1244px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. An illustration of four-way pipeline parallelism. Credit: <a href=\"https://arxiv.org/pdf/1811.06965.pdf\">GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a></em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Tensor parallelism</h3>\n\n\n\n<p>Tensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of tensor parallelism. In multi-head attention blocks, each head or group of heads can be assigned to a different device so they can be computed independently and in parallel.&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"563\" height=\"566\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_.png\" alt=\"Illustration of Tensor Parallelism in MLPs and Self-Attention Layers. In MLPs, the weight matrix is partitioned across multiple devices, enabling simultaneous computation on a batch of inputs using the split weights. In self-attention layers, the multiple attention heads are naturally parallel and can be distributed across devices.\n\" class=\"wp-image-73771\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_.png 563w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-298x300.png 298w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-114x115.png 114w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-362x364.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tensor-parallelsim-mlp-self-attention-layers_-109x110.png 109w\" sizes=\"(max-width: 563px) 100vw, 563px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Illustration of tensor parallelism in multi-layer perceptron (MLP) and self-attention layers. Credit: <a href=\"https://arxiv.org/pdf/1909.08053.pdf\">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></em></figcaption></figure>\n\n\n\n<p>Figure 3a shows an example of two-way tensor parallelism on a two-layer MLP, with each layer represented by a rounded box. Within the first layer, the weight matrix <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=A&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"A\" class=\"latex\" /> is split into <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=A_1&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"A_1\" class=\"latex\" /><sub> </sub>and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=A_2&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"A_2\" class=\"latex\" />. The computations <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=XA_1&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"XA_1\" class=\"latex\" /> and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=XA_2&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"XA_2\" class=\"latex\" /> can be independently executed on the same batch (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=f&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"f\" class=\"latex\" /> is an identity operation) of inputs <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> on two different devices. This effectively halves the memory requirement of storing weights on each device. A reduction operation <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=g&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"g\" class=\"latex\" /> combines the outputs in the second layer.</p>\n\n\n\n<p>Figure 3b is an example of two-way tensor parallelism in the self-attention layer. The multiple attention heads are parallel by nature and can be split across devices.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Sequence parallelism</h3>\n\n\n\n<p>Tensor parallelism has limitations, as it requires layers to be divided into independent, manageable blocks. It&#8217;s not applicable to operations like LayerNorm and Dropout, which are instead replicated across the tensor-parallel group. While LayerNorm and Dropout are computationally inexpensive, they do require a considerable amount of memory to store (redundant) activations.<br><br>As shown in <a href=\"https://arxiv.org/pdf/2205.05198.pdf\">Reducing Activation Recomputation in Large Transformer Models</a>, these operations are independent across the input sequence, and these ops can be partitioned along that \u201csequence-dimension,\u201d making them more memory efficient. This is called sequence parallelism.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1026\" height=\"389\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism.png\" alt=\" Illustration of a transformer layer with both Tensor parallelism and Sequence parallelism. Sequence parallelism is applicable for operations like LayerNorm and Dropout, which are not well-suited for tensor parallelism.\n\" class=\"wp-image-73783\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism.png 1026w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-300x114.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-625x237.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-179x68.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-768x291.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-645x245.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-500x190.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-160x61.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-362x137.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-290x110.png 290w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/transformer-layer-tensor-and-sequence-parallelism-1024x388.png 1024w\" sizes=\"(max-width: 1026px) 100vw, 1026px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. An illustration of a transformer layer with both tensor and sequence parallelism. Credit: <a href=\"https://arxiv.org/pdf/2205.05198.pdf\">Reducing Activation Recomputation in Large Transformer Models</a></em></figcaption></figure>\n\n\n\n<p>Techniques for model parallelism are not exclusive and can be used in conjunction. They can help scale and reduce the per-GPU memory footprint of LLMs, but there are also optimization techniques specifically for the attention module.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Optimizing the attention mechanism</h2>\n\n\n\n<p>The scaled dot-product attention (SDPA) operation maps query and key-value pairs to an output, as described in <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Attention Is All You Need</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Multi-head attention</h3>\n\n\n\n<p>As an enhancement to the SDPA, executing the attention layer multiple times in parallel with different, learned projections of the Q, K, and V matrices, enables the model to jointly attend to information from different representational subspaces at different positions. These subspaces are learned independently, providing the model with a richer understanding of different positions in the input.</p>\n\n\n\n<p>As depicted in Figure 5, the outputs from the multiple parallel attention operations are concatenated and linearly projected to combine them. Each parallel attention layer is called a \u2018head,\u2019 and this approach is called multi-head attention (MHA).</p>\n\n\n\n<p>In the original work, each attention head operates on a reduced dimension of the model (such as <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=d_%7Bmodel%7D%2F8&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"d_{model}/8\" class=\"latex\" />) when using eight parallel attention heads. This keeps the computational cost similar to single-head attention.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"686\" height=\"374\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention.png\" alt=\"An illustration of the scaled dot-product attention and multi-head attention.\n\" class=\"wp-image-73792\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention.png 686w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-300x164.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-625x341.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-645x352.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-500x273.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-362x197.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scaled-dot-product-attention-and-multi-head-attention-202x110.png 202w\" sizes=\"(max-width: 686px) 100vw, 686px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. An illustration of the scaled dot-product attention (left) and multi-head attention (right), which is simply multiple SDPA heads in parallel. Credit: <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Attention Is All You Need</a></em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Multi-query attention</h3>\n\n\n\n<p>One of the inference optimizations to MHA, called multi-query attention (MQA), as proposed in <a href=\"https://arxiv.org/abs/1911.02150\">Fast Transformer Decoding</a>, shares the keys and values among the multiple attention heads. The query vector is still projected multiple times, as before.&nbsp;</p>\n\n\n\n<p>While the amount of computation done in MQA is identical to MHA, the amount of data (keys, values) read from memory is a fraction of before. When bound by memory-bandwidth, this enables better compute utilization. It also reduces the size of the KV-cache in memory, allowing space for larger batch sizes.</p>\n\n\n\n<p>The reduction in key-value heads comes with a potential accuracy drop. Additionally, models that need to leverage this optimization at inference need to train (or <a href=\"https://arxiv.org/pdf/2305.13245.pdf\">at least fine-tuned</a> with ~5% of training volume) with MQA enabled.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Grouped-query attention</h3>\n\n\n\n<p><a href=\"https://arxiv.org/pdf/2305.13245v2.pdf\">Grouped-query attention</a> (GQA) strikes a balance between MHA and MQA by projecting key and values to a few groups of query heads (Figure 6). Within each of the groups, it behaves like multi-query attention.&nbsp;</p>\n\n\n\n<p>Figure 6 shows that multi-head attention has multiple key-value heads (left). Grouped-query attention (center) has more key-value heads than one, but fewer than the number of query heads, which is a balance between memory requirement and model quality. Multi-query attention (right) has a single key-value head to help save memory.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1326\" height=\"445\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms.png\" alt=\"Different attention mechanisms compared. Left: Multi-head attention has multiple key-value heads. Right: Multi-query attention has a single key-value head, which reduces memory requirements. Center: Grouped-query attention has a few key-value heads, balancing memory and model quality.\n\" class=\"wp-image-73799\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms.png 1326w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-300x101.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-625x210.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-768x258.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-645x216.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-500x168.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-160x54.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-362x121.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-328x110.png 328w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-attention-mechanisms-1024x344.png 1024w\" sizes=\"(max-width: 1326px) 100vw, 1326px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. A comparison of different attention mechanisms. Credit: <a href=\"https://arxiv.org/pdf/2305.13245v2.pdf\">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></em></figcaption></figure>\n\n\n\n<p>Models originally trained with MHA, can be \u201cuptrained\u201d with GQA using a fraction of the original training compute. They attain quality close to MHA while maintaining a computational efficiency closer to MQA. <a href=\"https://ai.meta.com/llama/\">Llama 2 70B</a> is an example of a model that leverages GQA.&nbsp;</p>\n\n\n\n<p>Optimizations like MQA and GQA help reduce the memory required by KV caches by reducing the number of key and value heads that are stored. There may still be inefficiencies in how this KV cache is managed. Of a different flavor than optimizing the attention module itself, the next section presents a technique for more efficient KV cache management.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Flash attention</h3>\n\n\n\n<p>Another way of optimizing the attention mechanism is to modify the ordering of certain computations to take better advantage of the memory hierarchy of GPUs. Neural networks are generally described in terms of layers, and most implementations are laid out that way as well, with one kind of computation done on the input data at a time in sequence. This doesn\u2019t always lead to optimal performance, since it can be beneficial to do more calculations on values that have already been brought into the higher, more performant levels of the memory hierarchy.&nbsp;</p>\n\n\n\n<p>Fusing multiple layers together during the actual computation can enable minimizing the number of times the GPU needs to read from and write to its memory and to group together calculations that require the same data, even if they are parts of different layers in the neural network.&nbsp;</p>\n\n\n\n<p>One very popular fusion is FlashAttention, an I/O aware exact attention algorithm, as detailed in <a href=\"https://arxiv.org/abs/2205.14135\">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>. <em>Exact attention</em> means that it is mathematically identical to the standard multi-head attention (with variants available for multi-query and grouped-query attention), and so can be swapped into an existing model architecture or even an already-trained model with no modifications.&nbsp;</p>\n\n\n\n<p><em>I/O aware</em> means it takes into account some of the memory movement costs previously discussed when fusing operations together. In particular, FlashAttention uses \u201ctiling\u201d to fully compute and write out a small part of the final matrix at once, rather than doing part of the computation on the whole matrix in steps, writing out the intermediate values in between.</p>\n\n\n\n<p>Figure 7 shows the tiled FlashAttention computation pattern and the memory hierarchy on a 40 GB GPU. The chart on the right shows the relative speedup that comes from fusing and reordering the different components of the Attention mechanism.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"979\" height=\"347\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu.png\" alt=\" Diagram depicting the memory hierarchy and the FlashAttention computation.\" class=\"wp-image-73803\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu.png 979w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-300x106.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-625x222.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-179x63.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-768x272.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-645x229.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-500x177.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-160x57.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-362x128.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flash-attention-computation-pattern-memory-hierarchy-gpu-310x110.png 310w\" sizes=\"(max-width: 979px) 100vw, 979px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. The tiled FlashAttention computation pattern and the memory hierarchy on a 40 GB GPU. Credit: <a href=\"https://arxiv.org/abs/2205.14135\">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Efficient management of KV cache with paging</h2>\n\n\n\n<p>At times, KV caches are statically \u201cover-provisioned\u201d to account for the largest possible input (the supported sequence length) because the size of inputs is unpredictable. For example, if the supported maximum sequence length of a model is 2,048, then regardless of the size of input and the generated output in a request, a reservation of size 2,048 would be made in memory. This space may be contiguously allocated, and often, much of it remains unused, leading to memory waste or fragmentation. This reserved space is tied up for the lifetime of the request.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1050\" height=\"202\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache.png\" alt=\"An illustration of memory wastage and fragmentation due to over-provisioning and inefficient management of KV cache. 1) \u201creserved\u201d indicates memory set aside for future use that is reserved for the entirety of the request duration. 2) \u201cinternal fragmentation\u201d happens because it\u2019s hard to predict how long the generation will be and thus memory is overprovisioned to account for the maximum sequence length. 3) \u201cexternal fragmentation\u201d indicates inefficiencies due to requests in a batch requiring different pre-allocated sizes.  \n\" class=\"wp-image-73807\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache.png 1050w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-300x58.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-625x120.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-768x148.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-645x124.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-500x96.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-160x31.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-362x70.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-572x110.png 572w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/memory-wastage-fragmentation-inefficient-kv-cache-1024x197.png 1024w\" sizes=\"(max-width: 1050px) 100vw, 1050px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. An illustration of memory wastage and fragmentation due to over-provisioning and inefficient KV cache management. Credit: <a href=\"https://arxiv.org/pdf/2309.06180.pdf\">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></em></figcaption></figure>\n\n\n\n<p>Inspired by paging in operating systems, the <a href=\"https://vllm.ai/\">PagedAttention</a> algorithm enables storing continuous keys and values in noncontiguous space in memory. It partitions the KV cache of each request into blocks representing a fixed number of tokens, which can be stored non-contiguously.&nbsp;</p>\n\n\n\n<p>These blocks are fetched as required during attention computation using a block table that keeps account. As new tokens are generated, new block allocations are made. The size of these blocks is fixed, eliminating inefficiencies arising from challenges like different requests requiring different allocations. This significantly limits memory wastage, enabling larger batch sizes (and, consequently, throughput).</p>\n\n\n\n<h2 class=\"wp-block-heading\">Model optimization techniques</h2>\n\n\n\n<p>So far, we\u2019ve discussed the different ways LLMs consume memory, some of the ways memory can be distributed across several different GPUs, and optimizing the attention mechanism and KV cache. There are also several model optimization techniques to reduce the memory use on each GPU by making modifications to the model weights themselves. GPUs also have dedicated hardware for accelerating operations on these modified values, providing even more speedups for models.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Quantization</h3>\n\n\n\n<p><em>Quantization</em> is the process of reducing the precision of a model\u2019s weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory\u2014a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.&nbsp;&nbsp;</p>\n\n\n\n<p>Figure 9 shows the distribution of values before and after one possible method of quantization. In this case, some precision is lost to rounding, and some dynamic range is lost to clipping, allowing the values to be represented in a much smaller format.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"788\" height=\"492\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution.png\" alt=\" Two distribution plots, one showing the full range of values at high precision and another showing the compressed and rounded range at low precision. \" class=\"wp-image-73808\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution.png 788w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-300x187.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-625x390.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/quantization-value-distribution-176x110.png 176w\" sizes=\"(max-width: 788px) 100vw, 788px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. The distribution of values before and after one possible method of quantization</em></figcaption></figure></div>\n\n\n<p>Reducing the precision of a model can yield several benefits. If the model takes up less space in memory, you can fit larger models on the same amount of hardware. Quantization also means you can transfer more parameters over the same amount of bandwidth, which can help to accelerate models that are bandwidth-limited.&nbsp;</p>\n\n\n\n<p>There are many different quantization techniques for LLMs involving reduced precision on either the activations, the weights, or both. It\u2019s much more straightforward to quantize the weights because they are fixed after training. However, this can leave some performance on the table because the activations remain at higher precisions. GPUs don\u2019t have dedicated hardware for multiplying INT8 and FP16 numbers, so the weights must be converted back into a higher precision for the actual operations.&nbsp;</p>\n\n\n\n<p>It\u2019s also possible to quantize the activations, the inputs of transformer blocks and network layers, but this comes with its own challenges. Activation vectors often contain outliers, effectively increasing their dynamic range and making it more challenging to represent these values at a lower precision than with the weights.&nbsp;</p>\n\n\n\n<p>One option is to find out where those outliers are likely to show up by passing a representative dataset through the model, and choosing to represent certain activations at a higher precision than others (LLM.int8()). Another option is to borrow the dynamic range of the weights, which are easy to quantize, and reuse that range in the activations.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Sparsity</h3>\n\n\n\n<p>Similar to quantization, it\u2019s been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. <em>Sparse matrices</em> are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.</p>\n\n\n\n<div class=\"wp-block-image aligncenter\"><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"701\" height=\"305\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_.png\" alt=\"A sparse matrix represented in a compressed format.\" class=\"wp-image-73815\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_.png 701w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-625x272.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-645x281.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/sparse-matrix-compressed-format_-253x110.png 253w\" sizes=\"(max-width: 701px) 100vw, 701px\" /><figcaption class=\"wp-element-caption\"><em>Figure 10. A sparse matrix represented in a compressed format consisting of non-zero data values and their corresponding two-bit indices</em></figcaption></figure></div>\n\n\n\n<p>GPUs in particular have hardware acceleration for a certain kind of <em>structured sparsity</em>, where two out of every four values are represented by zeros. Sparse representations can also be combined with quantization to achieve even greater speedups in execution. Finding the best way to represent large language models in a sparse format is still an active area of research, and offers a promising direction for future improvements to inference speeds.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Distillation</h3>\n\n\n\n<p>Another approach to shrinking the size of a model is to transfer its knowledge to a smaller model through a process called <em>distillation</em>. This process involves training a smaller model (called a student) to mimic the behavior of a larger model (a teacher).</p>\n\n\n\n<p>Successful examples of distilled models include <a href=\"https://arxiv.org/abs/1910.01108\">DistilBERT</a>, which compresses a BERT model by 40% while retaining 97% of its language understanding capabilities at a speed 60% faster.<br><br>While distillation in LLMs is an active field of research, the general approach was first described for neural networks in <a href=\"https://arxiv.org/abs/1503.02531\">Distilling the Knowledge in a Neural Network</a>:</p>\n\n\n\n<ul>\n<li>The student network is trained to mirror the performance of a larger teacher network, using a loss function that measures the discrepancy between their outputs. This objective is in addition to potentially including the original loss function of matching the student\u2019s outputs with the ground-truth labels.</li>\n\n\n\n<li>The teacher\u2019s outputs that are matched can be the very last layer (called <em>logits</em>) or intermediate layer activations.</li>\n</ul>\n\n\n\n<p>Figure 11 shows a general framework for knowledge distillation. The logits of the teacher are soft targets that the student optimizes for using a distillation loss. Other distillation methods may use other measures of loss to \u201cdistill\u201d knowledge from the teacher.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1225\" height=\"289\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework.png\" alt=\"Figure depicting a general framework for knowledge distillation using a distillation loss between the logits of the teacher and student.\n\" class=\"wp-image-73819\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework.png 1225w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-300x71.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-625x147.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-179x42.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-768x181.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-645x152.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-500x118.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-160x38.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-362x85.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-466x110.png 466w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/knowledge-distillation-general-framework-1024x242.png 1024w\" sizes=\"(max-width: 1225px) 100vw, 1225px\" /><figcaption class=\"wp-element-caption\"><em>Figure 11. A general framework for knowledge distillation. Credit: <a href=\"https://arxiv.org/pdf/2006.05525.pdf\">Knowledge Distillation: A Survey</a></em></figcaption></figure>\n\n\n\n<p>An alternative approach to distillation is to use data synthesized by the teacher for supervised training of a student LLM, which is especially useful when human annotations are scarce or not available. <a href=\"https://arxiv.org/abs/2305.02301\">Distilling Step by Step!</a> goes one step further by extracting rationales from a teacher LLM in addition to the labels that serve as ground truth. These rationales serve as intermediate reasoning steps to train smaller student LLMs in a data-efficient way.</p>\n\n\n\n<p>It\u2019s important to note that many state-of-the-art LLMs today have restrictive licenses that prohibit using their outputs to train other LLMs, making it challenging to find a suitable teacher model.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Model serving techniques</h2>\n\n\n\n<p>Model execution is frequently memory-bandwidth bound\u2014in particular, bandwidth-bound in the weights. Even after applying all the model optimizations previously described, it\u2019s still very likely to be memory bound. So you want to do as much as possible with your model weights when they are loaded. In other words, try doing things in parallel. Two approaches can be taken:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>In-flight batching</strong> involves executing multiple different requests at the same time.&nbsp;</li>\n\n\n\n<li><strong>Speculative inference</strong> involves executing multiple different steps of the sequence in parallel to try to save time.&nbsp;</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">In-flight batching</h3>\n\n\n\n<p>LLMs have some unique execution characteristics that can make it difficult to effectively batch requests in practice. A single model can be used simultaneously for a variety of tasks that look very different from one another. From a simple question-and-answer response in a chatbot to the summarization of a document or the generation of a long chunk of code, workloads are highly dynamic, with outputs varying in size by several orders of magnitude.&nbsp;</p>\n\n\n\n<p>This versatility can make it challenging to batch requests and execute them in parallel effectively\u2014a common optimization for serving neural networks. This could result in some requests finishing much earlier than others.</p>\n\n\n\n<p>To manage these dynamic loads, many LLM serving solutions include an optimized scheduling technique called continuous or in-flight batching. This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model.&nbsp;</p>\n\n\n\n<p>With in-flight batching, rather than waiting for the whole batch to finish before moving on to the next set of requests, the server runtime immediately evicts finished sequences from the batch. It then begins executing new requests while other requests are still in flight. In-flight batching can therefore greatly increase the overall GPU utilization in real-world use cases.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Speculative inference</h3>\n\n\n\n<p>Also known as speculative sampling, assisted generation, or blockwise parallel decoding, speculative inference is a different way of parallelizing the execution of LLMs. Normally, GPT-style large language models are autoregressive models that generate text token by token.&nbsp;</p>\n\n\n\n<p>Every token that is generated relies on all of the tokens that come before it to provide context. This means that in regular execution, it\u2019s impossible to generate multiple tokens from the same sequence in parallel\u2014you have to wait for the nth token to be generated before you can generate n+1.&nbsp;</p>\n\n\n\n<p>Figure 12 shows an example of speculative inference in which a draft model temporarily predicts multiple future steps that are verified or rejected in parallel. In this case, the first two predicted tokens in the draft are accepted, while the last is rejected and removed before continuing with the generation.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"863\" height=\"337\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_.png\" alt=\"From the prompt \u201cI saw a dog ride\u201d, the draft model predicts \u201cin the bus\u201d. The verification model predicts \u201cin the car\u201d in parallel, so we reject the \u201ccar\u201d token. \n\" class=\"wp-image-73827\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_.png 863w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-300x117.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-625x244.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-768x300.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-645x252.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-500x195.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-362x141.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/speculative-inference-example_-282x110.png 282w\" sizes=\"(max-width: 863px) 100vw, 863px\" /><figcaption class=\"wp-element-caption\"><em>Figure 12. An example of speculative inference. Credit: <a href=\"https://arxiv.org/abs/1811.03115\">Blockwise Parallel Decoding for Deep Autoregressive Models</a></em></figcaption></figure>\n\n\n\n<p>Speculative sampling offers a workaround. The basic idea of this approach is to use some \u201ccheaper\u201d process to generate a draft continuation that is several tokens long. Then,&nbsp; execute the main \u201cverification\u201d model at multiple steps in parallel, using the cheap draft as \u201cspeculative\u201d context for the execution steps where it is needed.&nbsp;</p>\n\n\n\n<p>If the verification model generates the same tokens as the draft, then you know to accept those tokens for the output. Otherwise, you can throw out everything after the first non-matching token, and repeat the process with a new draft.&nbsp;</p>\n\n\n\n<p>There are many different options for how to generate draft tokens, and each comes with different tradeoffs. You can train multiple models, or fine-tune multiple heads on a single pretrained model, that predict tokens that are multiple steps in the future. Or, you can use a small model as the draft model, and a larger, more capable model as the verifier.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>This post outlines many of the most popular solutions to help optimize and serve LLMs efficiently, be it in the data center or at the edge on a PC. Many of these techniques are optimized and available through <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0\">NVIDIA TensorRT-LLM</a>, an open-source library consisting of the TensorRT deep learning compiler alongside optimized kernels, preprocessing and postprocessing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs. To learn more, see <a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available</a>.</p>\n\n\n\n<p>NVIDIA TensorRT-LLM is now supported by <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a>, enabling enterprises to serve multiple AI models concurrently across different AI frameworks, hardware accelerators, and deployment models with peak throughput and minimum latency.&nbsp;&nbsp;</p>\n\n\n\n<p>TensorRT-LLM also powers <a href=\"https://developer.nvidia.com/nemo\">NVIDIA NeMo</a>, which provides an end-to-end cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">Get started with NeMo</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Stacking transformer layers to create large models results in better accuracies, few-shot learning capabilities, and even near-human emergent abilities on a wide range of language tasks. These foundation models are expensive to train, and they can be memory- and compute-intensive during inference (a recurring cost). The most popular large language models (LLMs) today can reach &hellip; <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">Continued</a></p>\n", "protected": false}, "author": 953, "featured_media": 73742, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1299507", "discourse_permalink": "https://forums.developer.nvidia.com/t/mastering-llm-techniques-inference-optimization/273239", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [296, 453, 3650, 2932, 1133, 2143, 1177], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-optimize-deploy-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jbl", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73739"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/953"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73739"}], "version-history": [{"count": 93, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73739/revisions"}], "predecessor-version": [{"id": 77199, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73739/revisions/77199"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73742"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73739"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73739"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73739"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72977, "date": "2023-11-16T11:07:51", "date_gmt": "2023-11-16T19:07:51", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72977"}, "modified": "2023-11-30T11:43:30", "modified_gmt": "2023-11-30T19:43:30", "slug": "unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/", "title": {"rendered": "Unlock the Power of NVIDIA Grace and NVIDIA Hopper Architectures with Foundational HPC Software"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">High-performance computing</a> (HPC) powers applications in simulation and modeling, healthcare and life sciences, industry and engineering, and more. In the modern data center, HPC synergizes with AI, harnessing data in transformative new ways.</p>\n\n\n\n<p>The performance and throughput demands of next-generation HPC applications call for an accelerated computing platform that can handle diverse workloads and has a tight coupling between the CPU and GPU. The <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\">NVIDIA Grace CPU</a> and <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper GPU</a> are industry-leading hardware ecosystems for HPC development.</p>\n\n\n\n<p>NVIDIA provides tools, libraries, and compilers to help developers take advantage of the NVIDIA Grace and NVIDIA Grace Hopper architectures. These tools support innovation and help applications make full use of accelerated computing. This foundational software stack provides the means for GPU acceleration, and porting and optimizing your applications on NVIDIA Grace-based systems. For more information about NVIDIA Grace compilers, tools, and libraries, see the <a href=\"https://developer.nvidia.com/grace-cpu\">NVIDIA Grace</a> product page.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA HPC SDK 23.11</h2>\n\n\n\n<p>The new hardware developments in the <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA Grace Hopper</a> systems enable dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that you can develop your application for both processors while using a single, unified address space.\u00a0</p>\n\n\n\n<p>Each processor retains its own physical memory that is designed with the bandwidth, latency, and capacity characteristics matched to the workloads most suited for each processor. Code written for existing discrete-memory GPU systems continues to run performantly without modification for the new NVIDIA Grace Hopper architecture.</p>\n\n\n\n<p>All application threads (GPU or CPU) can directly access the application\u2019s system-allocated memory, removing the need to copy data between processors. This new ability to read or write directly to the full application memory address space significantly improves programmer productivity for all programming models built on top of <a href=\"https://developer.nvidia.com/cuda-toolkit\">NVIDIA CUDA</a>: </p>\n\n\n\n<ul>\n<li>CUDA C++</li>\n\n\n\n<li>CUDA Fortran</li>\n\n\n\n<li>Standard parallelism in ISO C++, ISO Fortran, OpenACC, OpenMP</li>\n\n\n\n<li>&#8230;and many others</li>\n</ul>\n\n\n\n<p><a href=\"https://developer.nvidia.com/hpc-sdk\">NVIDIA HPC SDK 23.11</a> introduces new unified memory programming support, enabling workloads bottlenecked by host-to-device or device-to-host transfers to achieve up to a 7x speedup due to the <a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\">chip-to-chip (C2C) interconnect </a>in NVIDIA Grace Hopper systems. Application development can also be dramatically simplified because considerations for data location and movement are handled automatically by the system.</p>\n\n\n\n<p>For more information about how <a href=\"https://developer.nvidia.com/hpc-sdk\">HPC compilers</a> use these new hardware capabilities to simplify GPU programming with ISO C++, ISO Fortran, OpenACC, and CUDA Fortran, see <a href=\"https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/\">Simplifying GPU Programming for HPC with the NVIDIA Grace Hopper Superchip</a>.</p>\n\n\n\n<p>Get started with the NVIDIA HPC SDK for free and download <a href=\"https://developer.nvidia.com/nvidia-hpc-sdk-2311-downloads\">version 23.11</a> now.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA Performance Libraries\u00a0</h2>\n\n\n\n<p>NVIDIA has grown to become a full-stack, enterprise platform provider, now offering CPUs as well as GPUs and <a href=\"https://blogs.nvidia.com/blog/2020/05/20/whats-a-dpu-data-processing-unit/\">DPUs</a>. NVIDIA math software offerings now support CPU-only workloads in addition to existing GPU-centric solutions.&nbsp;</p>\n\n\n\n<p>NVIDIA Performance Libraries (NVPL) are a collection of essential math libraries optimized for Arm 64-bit architectures. Many HPC applications rely on mathematical APIs like BLAS and LAPACK, which are crucial to their performance. NVPL math libraries are drop-in replacements for these standardized math APIs.&nbsp;</p>\n\n\n\n<p>They are optimized for the NVIDIA Grace CPU. Applications being ported to or built on NVIDIA Grace-based platforms can fully use the high-performance and high-efficiency architecture. A primary goal of NVPL is to provide developers and system administrators with the smoothest experience porting and deploying existing HPC applications to the NVIDIA Grace platform with no source code changes required to achieve maximal performance when using CPU-based, standardized math libraries.</p>\n\n\n\n<p>The beta release of NVPL, available now, includes BLAS, LAPACK, FFT, RAND, and SPARSE to accelerate your applications on the NVIDIA Grace CPU.&nbsp;</p>\n\n\n\n<p>Learn more and download the <a href=\"http://developer.nvidia.com/nvpl\">NVPL beta</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA CUDA Direct Sparse Solvers</h2>\n\n\n\n<p>A new standard math library is being introduced to the suite of <a href=\"https://developer.nvidia.com/gpu-accelerated-libraries\">NVIDIA GPU-accelerated libraries</a>. The NVIDIA CUDA Direct Sparse Solvers library, NVIDIA cuDSS, is optimized for solving linear systems with very sparse matrices. While the first version of cuDSS supports execution on a single-GPU, multi-GPU, and multi-node support will be added in an upcoming release.&nbsp;</p>\n\n\n\n<p>Honeywell is one of the early adopters of cuDSS and is in the final phase of performance benchmarking in its UniSim Design process simulation product.</p>\n\n\n\n<p>The <a href=\"http://developer.nvidia.com/cudss\" data-type=\"link\" data-id=\"http://developer.nvidia.com/cudss\">cuDSS preview</a> is available to download now. For more information about supported features, see the <a href=\"https://docs.nvidia.com/cuda/cudss/index.html\">NVIDIA cuDSS documentation</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA cuTENSOR 2.0</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/cutensor\">NVIDIA cuTENSOR 2.0</a> is a performant and flexible library for accelerating your applications at the intersection of HPC and AI. </p>\n\n\n\n<p>In this major release, cuTENSOR 2.0 adds new features and performance improvements, including for arbitrarily high dimensional tensors. To make the new optimizations easily extensible across all tensor operations uniformly, while delivering high performance, the cuTENSOR 2.0 APIs have been completely revised with a focus on flexibility and extensibility.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1480\" height=\"287\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart.png\" alt=\"cuTENSOR 2.0 API flowchart.\" class=\"wp-image-73042\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart.png 1480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-300x58.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-625x121.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-179x35.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-768x149.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-645x125.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-500x97.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-160x31.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-362x70.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-567x110.png 567w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-API-flowchart-1024x199.png 1024w\" sizes=\"(max-width: 1480px) 100vw, 1480px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. cuTENSOR APIs are now shared across different tensor operations</em><br></figcaption></figure></div>\n\n\n<p>The plan-based multi-stage API is extended to all operations through a set of shared APIs. The new APIs can take opaque heap-allocated data structures as input for passing any operation-specific problem descriptors defined for that execution.&nbsp;</p>\n\n\n\n<p>cuTENSOR 2.0 also adds support for just-in-time (JIT) kernels.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1681\" height=\"978\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance.png\" alt=\"cuTENSOR 2.0 benchmark performance.\" class=\"wp-image-73047\" style=\"width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance.png 1681w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-300x175.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-625x364.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-768x447.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-1536x894.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-645x375.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-500x291.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-155x90.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-362x211.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-189x110.png 189w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-benchmark-performance-1024x596.png 1024w\" sizes=\"(max-width: 1681px) 100vw, 1681px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Average incremental performance improvements from using JIT for various input tensor types compared for two benchmarks: QC-like and </em><a href=\"https://tensornetwork.org/benchmarks/\"><em>Rand1000</em></a><em>. Performance improvements from JIT are significant for QC-like test cases with high dimensional tensors</em></figcaption></figure></div>\n\n\n<p>Using JIT kernels helps realize unparalleled performance by tuning the right configuration and optimization knobs for the target configuration at runtime, supporting a myriad of high-dimensional tensors not achievable through generic pre-compiled kernels that the library can ship.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1934\" height=\"1129\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph.png\" alt=\"cuTENSOR 2.0 speedup graph.\" class=\"wp-image-73049\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph.png 1934w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-300x175.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-625x365.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-768x448.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-1536x897.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-645x377.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-500x292.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-154x90.png 154w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-362x211.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-188x110.png 188w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuTENSOR-speedup-graph-1024x598.png 1024w\" sizes=\"(max-width: 1934px) 100vw, 1934px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. cuTENSOR 2.0.0 performance gains over the previous 1.7.0 version when tuned with JIT and other capabilities</em></figcaption></figure></div>\n\n\n<p>Learn about migration and <a href=\"http://developer.nvidia.com/cutensor\">download cuTENSOR 2.0</a> now.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA Grace CPU performance tuning with NVIDIA Nsight Systems 2023.4\u00a0</h2>\n\n\n\n<p>Applications on NVIDIA Grace-based platforms benefit from tuning instruction execution on the CPU cores, and from optimizing the CPU\u2019s interaction with other hardware units in the system. When porting applications to NVIDIA Grace CPUs, insight into functions at the hardware level helps you configure your software for the new platform.\u00a0</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA Nsight Systems</a> is a system-wide performance analysis tool that collects hardware and API metrics and correlates them on a unified timeline. For NVIDIA Grace CPU performance tuning, Nsight Systems samples instruction pointers and backtraces to visualize where CPU code is busiest, and how the CPU is using resources across the system. Nsight Systems also captures context switching to build a utilization graph for all the NVIDIA Grace CPU cores.</p>\n\n\n\n<p>NVIDIA Grace CPU core event rates, like CPU cycles and instructions retired, show how the NVIDIA Grace cores are handling work. The summary view for backtrace samples also helps you quickly identify which instruction pointers are causing hotspots.\u00a0</p>\n\n\n\n<p>Now available in Nsight Systems 2023.4, NVIDIA Grace CPU uncore event rates monitor activity outside of the cores\u2014like <a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\">NVLink-C2C</a> and PCIe activity. Uncore metrics show how activity between sockets supports the work of the cores, helping you find ways to improve the NVIDIA Grace CPU\u2019s integration with the rest of the system.</p>\n\n\n\n<p>NVIDIA Grace CPU uncore and core event sampling in Nsight Systems 2023.4 help you find the best optimizations for code running on NVIDIA Grace. For more information about performance tuning, as well as tips on optimizing your CUDA code in conjunction, see the following video.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/5Gxx59Q0g6o?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. NVIDIA Grace CPU Performance Tuning with NVIDIA Nsight Tools</em></figcaption></figure>\n\n\n\n<p>Learn more and get started with <a href=\"http://developer.nvidia.com/nsight-systems\">Nsight Systems 2023.4</a>. Nsight Systems is also available in the <a href=\"http://developer.nvidia.com/hpc-sdk\">HPC SDK</a> and <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA Toolkit</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Accelerated computing for HPC</h2>\n\n\n\n<p>NVIDIA provides an ecosystem of tools, libraries, and compilers for accelerated computing on the NVIDIA Grace and Hopper architectures. The HPC software stack is foundational for research and science on NVIDIA data center silicon.&nbsp;</p>\n\n\n\n<p>Dive deeper into accelerated computing topics in the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/5\">Developer Forums</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>High-performance computing (HPC) powers applications in simulation and modeling, healthcare and life sciences, industry and engineering, and more. In the modern data center, HPC synergizes with AI, harnessing data in transformative new ways. The performance and throughput demands of next-generation HPC applications call for an accelerated computing platform that can handle diverse workloads and has &hellip; <a href=\"https://developer.nvidia.com/blog/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/\">Continued</a></p>\n", "protected": false}, "author": 759, "featured_media": 73058, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298851", "discourse_permalink": "https://forums.developer.nvidia.com/t/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/273118", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 503, 1903], "tags": [453, 1914], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Foundational.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iZ3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72977"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/759"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72977"}], "version-history": [{"count": 18, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72977/revisions"}], "predecessor-version": [{"id": 74279, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72977/revisions/74279"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73058"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72977"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72977"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72977"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73464, "date": "2023-11-16T06:00:00", "date_gmt": "2023-11-16T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73464"}, "modified": "2024-01-22T14:05:25", "modified_gmt": "2024-01-22T22:05:25", "slug": "mastering-llm-techniques-training", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/mastering-llm-techniques-training/", "title": {"rendered": "Mastering LLM Techniques: Training\u00a0"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\" target=\"_blank\" rel=\"noreferrer noopener\">Large language models (LLMs)</a> are a class of generative AI models built using transformer networks that can recognize, summarize, translate, predict, and generate language using very large datasets. LLMs have the promise of transforming society as we know it, yet training these foundation models is incredibly challenging.&nbsp;</p>\n\n\n\n<p>This blog articulates the basic principles behind LLMs, built using transformer networks, spanning model architectures, \u200cattention mechanisms, \u200cembedding techniques, and foundation model training strategies.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Model architectures</h2>\n\n\n\n<p>Model architectures define the backbone of transformer networks, broadly dictating the capabilities and limitations of the model. The architecture of an LLM is often called an <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\" target=\"_blank\" rel=\"noreferrer noopener\">encoder, decoder, or encoder-decoder model</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"736\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-625x736.png\" alt=\"Image depicts the canonical architecture of transformer networks, including encoder-decoder, encoder-only, and decoder-only architectures. \" class=\"wp-image-73570\" style=\"aspect-ratio:0.8491847826086957;width:415px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-625x736.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-255x300.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-98x115.png 98w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-768x905.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-645x760.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-76x90.png 76w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-362x427.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder-93x110.png 93w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-encoder.png 819w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The canonical structure of large language models</em></figcaption></figure></div>\n\n\n<p>Some popular architectures include:</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td><strong>Architecture&nbsp;</strong></td><td><strong>Description</strong></td><td><strong>Suitable for</strong></td></tr><tr><td><strong>Bi-directional Encoder Representation from Transformers (BERT)&nbsp;</strong></td><td>Encoder-only architecture, best suited for tasks that can understand language.</td><td>Classification and sentiment analysis&nbsp;</td></tr><tr><td><strong>Generative Pre-trained Transformer (GPT)&nbsp;</strong></td><td>Decoder-only architecture suited for generative tasks and fine-tuned with labeled data on discriminative tasks.&nbsp;<br>Given the unidirectional architecture, context only flows forward. The GPT framework helps achieve strong natural language understanding using a single-task-agnostic model through generative pre-training and discriminative fine-tuning.</td><td>Textual entailment, sentence similarity, question answering.&nbsp;</td></tr><tr><td><strong>Text-To-Text Transformer (Sequence-to-Sequence models)&nbsp;</strong></td><td>Encoder-decoder architecture. It leverages the transfer learning approach to convert every text-based language problem into a text-to-text format, that is taking text as input and producing the next text as output. With a bidirectional architecture, context flows in both directions.&nbsp;</td><td>Translation, Question &amp; Answering, Summarization.</td></tr><tr><td><strong>Mixture of Experts (MoE)&nbsp;</strong></td><td>Model architecture decisions that can be applied to any of the\u200c architectures. Designed to scale up model capacity substantially while adding minimal computation overhead, converting dense models into sparse models. The MoE layer consists of many expert models and a sparse gating function. The gates route each input to the top-K (K&gt;=2 or K=1) best experts during inference.</td><td>Generalize well across tasks for computational efficiency during inference, with low latency</td></tr></tbody></table></figure>\n\n\n\n<p>Another popular architecture decision is to expand to multimodal models that combine information from multiple modalities or forms of data such as text, images, audio, and video. Although challenging to train, multimodal models offer key benefits of complementary information from different modalities, much as humans understand by analyzing data from multiple senses.</p>\n\n\n\n<p>These models contain separate encoders for each modality, like a CNN for images, and transformers for text to extract high-level feature representations from the respective input data. The combination of features extracted from multiple modalities can be a challenge. It can be addressed by fusing features extracted from each modality, or by using attention mechanisms to weigh the contribution of each modality relative to the task.&nbsp;</p>\n\n\n\n<p>The joint representation captures interactions between modalities. The model architecture may contain additional decoders for generating task-specific outputs like classifications, caption generation, translation, image generation given prompt text, image editing given prompt text, and the like.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Delving into transformer networks</h2>\n\n\n\n<p>Within the realm of transformer networks, the process of tokenization assumes a pivotal role in fragmenting text into smaller units known as tokens.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Tokenizers</h3>\n\n\n\n<p>Tokenization is the first step to building a model, which involves splitting text into smaller units called tokens that become the basic building blocks for LLMs. These extracted tokens are used to build a vocabulary index mapping tokens to numeric IDs, to numerically represent text suitable for deep learning computations. During the encoding process, these numeric tokens are encoded into vectors representing each token&#8217;s meaning. During the decoding process, when LLMs perform generation, tokenizers decode the numeric vectors back into readable text sequences.&nbsp;&nbsp;</p>\n\n\n\n<p>The process begins with normalization to process lowercase, pruning punctuation and whitespaces, stemming, lemmatization, handling contractions, and \u200cremoving accents. Once the text is cleaned up, the next step is to segment the text by recognizing word and sentence boundaries. Depending on the boundary, tokenizers can be at word, sub-word, or character-level granularity.&nbsp;</p>\n\n\n\n<p>Although word and character-based tokenizers are prevalent, there are challenges with these. Word-based tokenizers lead to a large vocabulary size and words not seen during the tokenizer training process cause many out-of-vocabulary tokens. Character-based tokenizers lead to long sequences and less meaningful individual tokens.&nbsp;</p>\n\n\n\n<p>Due to these shortcomings, subword-based tokenizers have gained popularity. The focus of subword tokenization algorithms is to split rare words into smaller, meaningful subwords, based on common character n-grams and patterns. For This technique enables the representation of rare and unseen words via known subwords, resulting in a reduced vocabulary size. During inference, it also handles out-of-vocabulary words effectively reducing vocabulary size, while handling out-of-vocabulary words gracefully during inference.&nbsp;</p>\n\n\n\n<p>Popular subword tokenization algorithms include Byte Pair Encoding (BPE), WordPiece, Unigram, and SentencePiece.&nbsp;</p>\n\n\n\n<ul>\n<li>BPE starts with character vocabulary and iteratively merges frequent adjacent character pairs into new vocabulary terms, achieving text compression with faster inference at decoding time by replacing most common words with single tokens.&nbsp;</li>\n\n\n\n<li>WordPiece is similar to BPE in doing merge operations, however, this leverages the probabilistic nature of the language to merge characters to maximize training data likelihood.&nbsp;</li>\n\n\n\n<li>Unigram starts with a large vocabulary, calculates the probability of tokens, and removes tokens based on a loss function until it reaches the desired vocabulary size.&nbsp;</li>\n\n\n\n<li>SentencePiece learns subword units from raw text based on language modeling objectives and uses Unigram or BPE tokenization algorithms to construct the vocabulary.&nbsp;</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Attention Mechanisms</h3>\n\n\n\n<p>As \u200ctraditional seq-2-seq encoder-decoder language models like Recurrent Neural Networks (RNNs) don\u2019t scale well with the length of the input sequence, the concept of attention was introduced and has proved to be seminal. The attention mechanism enables the decoder to use the most relevant parts of the input sequence weighted by the encoded input sequence, with the most relevant tokens being assigned the highest weight. This concept improves the scaling of input sequence lengths by carefully selecting \u200ctokens by importance.&nbsp;</p>\n\n\n\n<p>This idea was furthered with self-attention and introduced in 2017 with the transformer model architecture, removing the need for RNNs. Self-attention mechanisms create representations of the input sequence relying on the relationship between different words in the same sequence. By enhancing the information content of an input embedding through the inclusion of input context, self-attention mechanisms play a crucial role in \u200ctransformer architectures.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"319\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture.png\" alt=\"Computational steps in self-attention.\" class=\"wp-image-73467\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture-300x153.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture-500x255.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Self-attention-architecture-216x110.png 216w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Self-attention architecture</em> (<em>source: <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</a></em>)<em> </em></figcaption></figure></div>\n\n\n<p>Self-attention is called scaled-dot product attention because of how it achieves context-aware input representation. Each token in the input sequence is used to project itself into Query (Q), Key (K), and Value (V) sequences using their respective weight matrices. The goal is to compute an attention-weighted version of each input token given all the other input tokens as its context. By computing a scaled dotduct of Q and K matrices with relevant pairs determined by the V matrix getting higher weights, the self-attention mechanism finds a suitable vector for each input token (Q) given all key-value pairs that are other tokens in the sequence.&nbsp;</p>\n\n\n\n<p>Self-attention further evolved into multi-head attention. The three matrices (Q,K,V) described preceding can be considered as single-head. Multi-head self-attention is when multiple such heads are used. These heads function like multiple kernels in CNNs, attending to different parts of the sequence, focusing on longer-term compared to shorter-term dependencies.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"379\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention.png\" alt=\"Multiple heads focusing on different parts of the sequence.\" class=\"wp-image-73468\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention-300x182.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention-494x300.png 494w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention-362x220.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Multi-head-self-attention-181x110.png 181w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Multi-head self-attention</em> (<em>source: <a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\">Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</a></em>)<em> </em></figcaption></figure></div>\n\n\n<p>And finally, the concept of cross-attention came about, where instead of a single input sequence as in the case of self-attention, this involves two different input sequences. In the transformer model architecture, that\u2019s one input sequence from the encoder and another processed by the decoder.&nbsp;&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">FlashAttention</h4>\n\n\n\n<p>Transformers of a larger size are limited by the memory requirements of the attention layer, which increases in proportion to the length of the sequence. This growth is quadratic. To speed up \u200cattention layer computations and reduce its memory footprint, FlashAttention optimizes the naive implementation bottlenecked by repeated reads and writes from slower GPU high bandwidth memory (HBM).&nbsp;</p>\n\n\n\n<p>FlashAttention uses classical tiling to load blocks of query, key, and value from GPU HBM (its main memory) to SRAM (its fast cache) for attention computation, and then writes back the output to HBM. It also improves upon memory usage, by not storing large attention matrices from the forward pass; instead relies on recomputing the attention matrix during backprop in SRAM. With these optimizations, FlashAttention brings significant speedup (2-4x) for longer sequences.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"257\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness.png\" alt=\"Speedup and memory savings from using FlashAttention.\" class=\"wp-image-73469\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness-300x124.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness-500x206.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness-362x149.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FlashAttention-fast-and-memory-efficient-exact-attention-with-IO-awareness-267x110.png 267w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. FlashAttention fast and memory-efficient exact attention with IO-awareness (s</em><em>ource: <a rel=\"noreferrer noopener\" href=\"https://github.com/Dao-AILab/flash-attention\" target=\"_blank\">https://github.com/Dao-AILab/flash-attention</a></em>)</figcaption></figure></div>\n\n\n<p>Further improved FlashAttention-2 is 2x faster than FlashAttention by adding further optimizations with sequence parallelism, better work partitioning, and reducing non-matmul FLOPs. This newer version also supports multi-query attention as well as grouped-query attention that we describe next.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Multi-Query Attention (MQA)</h4>\n\n\n\n<p>A variant of attention where multiple heads of query attend to the same head of key and value projections. This reduces the KV cache size and hence the memory bandwidth requirements of incremental decoding. The resulting models support faster autoregressive decoding during inference with minor quality degradation than the baseline multi-head attention architecture.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Group Query Attention (GQA)</h4>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"227\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture.png\" alt=\"Grouped-query attention shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.Group-query attention (GQA) is an improvement over MQA to overcome quality degradation issues while retaining the speed-up at inference time. Moreover, models trained using multi-head attention don't have to be retrained from scratch and can employ GQA during inference by up-training existing model checkpoints using only 5% of original training compute. Also, this is a generalization of MQA using an intermediate (more than one, less than number of query heads) number of key-value heads. GQA achieves quality close to baseline multi-head attention with comparable speed to MQA.   \n\nEmbedding techniques\nThe order in which words appear in a sentence is important. This Information is encoded in LLMs using positional encoding by assigning the order of occurrence of each input token to a 2D positional encoding matrix.  Each row of the matrix represents an encoded token of the sequence summed with its positional information. This allows the model to differentiate between words with similar meanings but different positions in the sentence and enables encoding of the relative position of words.\n\nThe original transformer architecture combines absolute positional encoding with word embeddings using sinusoidal functions. However, this approach doesn\u2019t allow extrapolation to longer sequences at inference time than those seen during training. Relative position encoding solved this challenge. In this, the content representations for query and key vectors are combined with positional representations that are trainable, relative to the distance between a query and a key that is clipped beyond a certain distance. \n\nRoPE\n\" class=\"wp-image-73470\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture-500x182.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture-362x132.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Grouped-query-attention-Architecture-302x110.png 302w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Grouped-query attention architecture (<a rel=\"noreferrer noopener\" href=\"https://github.com/fkodom/grouped-query-attention-pytorch\" target=\"_blank\">source: </a></em><em><a rel=\"noreferrer noopener\" href=\"https://github.com/fkodom/grouped-query-attention-pytorch\" target=\"_blank\">https://github.com/fkodom/grouped-query-attention-pytorch</a></em>)</figcaption></figure>\n\n\n\n<p>Group-query attention (GQA) is an improvement over MQA to overcome quality degradation issues while retaining the speed-up at inference time. Moreover, models trained using multi-head attention don&#8217;t have to be retrained from scratch. They can employ GQA during inference by up-training existing model checkpoints using only 5% of the original training compute. Also, this is a generalization of MQA using an intermediate (more than one, less than number of query heads) number of key-value heads. GQA achieves quality close to baseline multi-head attention with comparable speed to MQA.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Embedding techniques</h3>\n\n\n\n<p>The order in which words appear in a sentence is important. This information is encoded in LLMs using positional encoding by assigning the order of occurrence of each input token to a 2D positional encoding matrix.&nbsp; Each row of the matrix represents an encoded token of the sequence summed with its positional information. This allows the model to differentiate between words with similar meanings but different positions in the sentence and enables encoding of the relative position of words.</p>\n\n\n\n<p>The original transformer architecture combines absolute positional encoding with word embeddings using sinusoidal functions. However, this approach doesn\u2019t allow extrapolation to longer sequences at inference time than those seen during training. Relative position encoding solved this challenge. In this, the content representations for query and key vectors are combined with positional representations that are trainable, relative to the distance between a query and a key that is clipped beyond a certain distance.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">RoPE</h4>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"355\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding.png\" alt=\"Enhancing linear self-attention with relative position encoding. Absolute position is encoded using a rotation matrix.\" class=\"wp-image-73471\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding-300x171.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding-500x284.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding-158x90.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding-362x206.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Implementation-of-Rotary-Position-Embedding-193x110.png 193w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Implementation of Rotary Position Embedding</em> (source: <a href=\"https://arxiv.org/pdf/2104.09864v4.pdf\">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>)<br><em>Source: </em></figcaption></figure></div>\n\n\n<p>Rotary Position Embeddings (RoPE) combines the concepts of absolute and relative position embeddings. The absolute position is encoded using a rotation matrix. The relative position dependency is incorporated in self-attention formulation and added to the contextual representation in a multiplicative manner. This technique retains the benefit of sequence length flexibility introduced in the transformer&#8217;s sinusoidal position embedding while equipping linear self-attention with relative position encoding. It also introduces decaying inter-token dependency with increasing relative distances, enabling extrapolation to longer sequences at inference time.</p>\n\n\n\n<h4 class=\"wp-block-heading\">AliBi</h4>\n\n\n\n<p>Transformer-based LLMs don\u2019t scale well to longer sequences due to the quadratic cost of self-attention, which limits the number of tokens of context. Additionally, the sinusoidal position method introduced in the original transformer architecture doesn\u2019t extrapolate to sequences that are longer than it saw during training. This limits the set of real-world use cases where LLMs can be applied. To overcome this, Attention with Linear Biases (ALiBi) was introduced. This technique does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance.</p>\n\n\n\n<p>To facilitate efficient extrapolation for much longer sequences than seen at training time, ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Compared to sinusoidal models, this method requires no additional runtime or parameters and incurs a negligible (0\u20130.7%) memory increase. ALiBi\u2019s edge over sinusoidal embeddings is largely explained by its improved avoidance of the early token curse. This method can also achieve further gains by more efficiently exploiting longer context histories.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Training transformer networks</h2>\n\n\n\n<p>While training LLMs, there are several techniques to improve efficiency and optimize resource usage of underlying hardware configurations. Scaling these massively large AI models with billions of parameters and trillions of tokens comes with huge memory capacity requirements.&nbsp;</p>\n\n\n\n<p>To alleviate this requirement, a few methods such as model parallelism and activation recomputation are popular. Model parallelism partitions the model parameters and optimizer states across multiple GPUs so that each GPU stores a subset of the model parameters. It is further classified into tensor and pipeline parallelism.&nbsp;</p>\n\n\n\n<ul>\n<li>Tensor parallelism splits operations across GPUs, often known as intra-layer parallelism focused on parallelizing computation within an operation such as matrix-matrix multiplication. This technique requires additional communication to make sure that the result is correct.&nbsp;</li>\n\n\n\n<li>Pipeline parallelism splits model layers across GPUs, also known as inter-layer parallelization, focused on splitting the model by layers into chunks. Each device computes for its chunk and passes intermediate activations to the next stage. This could lead to bubble time where some devices are engaged in computation and others waiting, leading to a waste of computational resources.&nbsp;</li>\n\n\n\n<li>Sequence parallelism expands upon tensor-level model parallelism by noticing that the regions of a transformer layer that haven\u2019t previously been parallelized and are independent along the sequence dimension. Splitting these layers along the sequence dimension enables distribution of the compute as well as the activation memory for these regions across the tensor parallel devices. Since activations are distributed and have a smaller memory footprint, more activations can be saved for the backward pass.&nbsp;</li>\n\n\n\n<li>Selective activation recomputation goes hand-in-hand with sequence parallelism. It improves cases where memory constraints force the recomputation of some, but not all, of the activations, by noticing that different activations require different numbers of operations to recompute. Instead of checkpointing and recomputing full transformer layers, it\u2019s possible to checkpoint and recompute only parts of each transformer layer that take up a lot of memory but aren\u2019t computationally expensive to recompute.&nbsp;</li>\n</ul>\n\n\n\n<p>All techniques add communication or computation overhead. Therefore, finding the configuration that achieves maximum performance and then scaling training with data parallelism is essential for efficient LLM training.&nbsp;</p>\n\n\n\n<p>In data parallel training, the dataset is split into several shards, where each shard is allocated to a device. This is equivalent to parallelizing the training process along the batch dimension. Each device will hold a full copy of the model replica and train on the dataset shard allocated. After back-propagation, the gradients of the model will be all-reduced so that the model parameters on different devices can stay synchronized.&nbsp;</p>\n\n\n\n<p>A variant of this is called the fully sharded data parallelism (FSDP) technique. It shards model parameters and training data uniformly across data parallel workers, where the computation for each micro-batch of data is local to each GPU worker.</p>\n\n\n\n<p>FSDP offers configurable sharding strategies that can be customized to match the physical interconnect topology of the cluster to handle hardware heterogeneity. It can minimize bubbles to overlap communication with computation aggressively through operation reordering and parameter prefetching. And lastly, FSDP optimizes memory usage by restricting the number of blocks allocated for inflight unsharded parameters. Due to these optimizations, FSDP provides support for significantly larger models with near-linear scalability in terms of TFLOPS.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Quantization Aware Training</h3>\n\n\n\n<p>Quantization is the process in which deep learning models perform all or part of the computation in reduced precision as compared to full precision (floating point) values. This technique enables inference speedups, memory savings, and cost reduction of using deep learning models with minimal accuracy loss.&nbsp;</p>\n\n\n\n<p>Quantization Aware Training (QAT) is a method that takes into account the impact of quantization during the training process. The model is trained with quantization-aware operations that mimic the quantization process during training. Models learn how to perform well in \u200cquantized representations, leading to improved accuracy compared to post-training quantization. The forward pass quantizes weights and activations to low-precision representations. The backward pass computes gradients using full-precision weights and activations. This enables the model to learn parameters that are robust to quantization errors introduced in the forward pass. The result is a trained model that can be quantized post-training with minimal impact on accuracy.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Train LLMs today</h3>\n\n\n\n<p>This post covered various model training techniques and when to use them. Check out the post on <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\" target=\"_blank\">Mastering LLM Techniques: Customization</a>, to continue your learning journey on the LLM workflow.&nbsp;</p>\n\n\n\n<p>Many of the training methods are supported on <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/nemo\" target=\"_blank\">NVIDIA NeMo</a>, which provides an accelerated workflow for training with 3D parallelism techniques. It also offers a choice of several customization techniques. It is optimized for at-scale inference of large-scale models for language and image workloads, with multi-GPU and multi-node configurations. <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/nemo\" target=\"_blank\">Download the NeMo framework</a> today and train LLMs on your preferred on-premises and cloud platforms.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) are a class of generative AI models built using transformer networks that can recognize, summarize, translate, predict, and generate language using very large datasets. LLMs have the promise of transforming society as we know it, yet training these foundation models is incredibly challenging.&nbsp; This blog articulates the basic principles behind LLMs, &hellip; <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-training/\">Continued</a></p>\n", "protected": false}, "author": 1837, "featured_media": 73602, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298742", "discourse_permalink": "https://forums.developer.nvidia.com/t/mastering-llm-techniques-training/273091", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [453, 3650, 2932, 3270, 3596, 3267], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-mastering-large-language-model-training.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-j6U", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73464"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1837"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73464"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73464/revisions"}], "predecessor-version": [{"id": 77063, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73464/revisions/77063"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73602"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73464"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73464"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73464"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73609, "date": "2023-11-15T10:00:00", "date_gmt": "2023-11-15T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73609"}, "modified": "2023-11-30T11:43:31", "modified_gmt": "2023-11-30T19:43:31", "slug": "best-practices-for-securing-llm-enabled-applications", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/", "title": {"rendered": "Best Practices for Securing LLM-Enabled Applications"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large language models (LLMs)</a> provide a wide range of powerful enhancements to nearly any application that processes text. And yet they also introduce new risks, including:</p>\n\n\n\n<ul>\n<li><strong>Prompt injection</strong>, which may enable attackers to control the output of the LLM or LLM-enabled application.</li>\n\n\n\n<li><strong>Information leaks</strong>, which<strong> </strong>occur<strong> </strong>when private data used to train the LLM or used at runtime can be inferred or extracted by an attacker.\u00a0</li>\n\n\n\n<li><strong>LLM reliability</strong>, which<strong> </strong>is a threat when LLMs occasionally produce incorrect information simply by chance.\u00a0</li>\n</ul>\n\n\n\n<p>This post walks through these security vulnerabilities in detail and outlines best practices for designing or evaluating a secure LLM-enabled application.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Prompt injection</h2>\n\n\n\n<p>Prompt injection is the most common and well-known LLM attack. It enables attackers to control the output of the LLM, potentially affecting the behavior of downstream queries and plugins connected to the LLM. This can have additional downstream consequences or responses for future users. Prompt injection attacks can be either direct or indirect.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Direct prompt injection</h3>\n\n\n\n<p>In the case of direct prompt injection attacks, the attacker interacts with the LLM directly, attempting to make the LLM produce a specific response. An example of a direct prompt injection leading to remote code execution is shown in Figure 1. For more details about direct prompt injection, see <a href=\"https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/\">Securing LLM Systems Against Prompt Injection</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"870\" height=\"613\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example.png\" alt=\"A text listing, showing a request to an LLM that says \u201cplease solve the following problem\u201d and then lists some python code invoking an os.system call; the result shows a listing of the /etc/password file on the system hosting the LLM, indicating a successful code execution.\n\" class=\"wp-image-73615\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example.png 870w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-300x211.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-625x440.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-163x115.png 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-768x541.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-645x454.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-426x300.png 426w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-362x255.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/direct-prompt-injection-attack-example-156x110.png 156w\" sizes=\"(max-width: 870px) 100vw, 870px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An example of a direct prompt injection attack in which an LLM-powered application is made to execute attacker code</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Indirect prompt injection</h3>\n\n\n\n<p>Indirect prompt injection relies on the LLM having access to an external data source that it used when constructing queries to the system. An attacker can insert malicious content into these external data sources, which is ingested by the LLM and inserted into the prompt to produce the response desired by the attacker. For more information about indirect prompt injection, see <a href=\"https://developer.nvidia.com/blog/mitigating-stored-prompt-injection-attacks-against-llm-applications/\">Mitigating Stored Prompt Injection Attacks Against LLM Applications</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Trust boundaries</h3>\n\n\n\n<p>With both direct and indirect prompt injection, once the attacker is able to successfully introduce their input into the LLM context, they have significant influence (if not outright control) over the output of the LLM. Because the external sources that LLMs may use can be so difficult to control, and LLM users themselves may be malicious, it&#8217;s important to treat any LLM responses as potentially untrustworthy.&nbsp;</p>\n\n\n\n<p>A trust boundary must be established between those responses and any responses that process them. Some practical steps to enforce this separation are listed below.</p>\n\n\n\n<p><strong>Parameterize plug-ins.</strong> Strictly limit the number of actions that a given plug-in can perform. For example, a plug-in to operate on a user\u2019s email might require a message ID, a specific operation such as \u2018reply\u2019 or \u2018forward\u2019, or only accept free-form text to be inserted into the body of an email.</p>\n\n\n\n<p><strong>Sanitize inputs to the plug-in</strong> <strong>before use</strong>. Email body text might have any HTML elements forcibly removed before inserting, for example. Or a forward email operation might require that the recipient be present in the user\u2019s address book.</p>\n\n\n\n<p><strong>Request explicit user authorization from the user</strong> <strong>when a plug-in operates on a sensitive system.</strong> Any such operation should result in an immediate re-request for explicit authorization from the user to perform the action, as well as provide a summary of the action that is about to be performed.</p>\n\n\n\n<p><strong>Require specific authorization from the user when multiple plug-ins are called in sequence</strong>. This pattern\u2014allowing the output of one plug-in to be fed to another plug-in\u2014can quite rapidly lead to unexpected and even dangerous behavior. Allowing the user to check and verify which plug-ins are being called and what action they will take can help mitigate the issue.</p>\n\n\n\n<p><strong>Manage plug-in authorization carefully.</strong> Separate any service account from the LLM service account. If user authorization is required for a plug-in\u2019s action, then that authorization should be delegated to the plug-in using a secure method such as OAuth2.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Information leaks</h2>\n\n\n\n<p>Information leaks from the LLM and LLM-enabled applications create confidentiality risk. If an LLM is either trained or customized on private data, a skilled attacker can perform model inversion or training data extraction attacks to access data that application developers considered private.&nbsp;&nbsp;</p>\n\n\n\n<p>Logging of both prompts and completions can accidentally leak data across permission boundaries by violating service-side role-based access controls for data at rest. If the LLM itself is provided access rights to information, or stores logs, it can often be induced into revealing this data.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Leaks from the LLM itself</h3>\n\n\n\n<p>The LLM itself can leak information to an attacker in several ways. With prompt extraction attacks, an attacker can use prompt injection techniques to induce the LLM to reveal information contained in its prompt template, such as model instructions, model persona information, or even secrets such as passwords.&nbsp;</p>\n\n\n\n<p>With model inversion attacks, an attacker can recover some of the data used to train the model. Depending on the details of the attack, these records might be recovered at random, or the attacker may be able to bias the search to particular records they suspect might be present. For instance, they might be able to extract examples of Personal Identifiable Information (PII) used in training the LLM. To learn more, see <a href=\"https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0083\">Algorithms that Remember: Model Inversion Attacks and Data Protection Law</a>.</p>\n\n\n\n<p>Finally, training data membership inference attacks enable an attacker to determine whether a particular bit of information already known to them was likely contained within the training data of the model. For instance, they might be able to determine whether their PII in particular was used to train the LLM.&nbsp;&nbsp;</p>\n\n\n\n<p>Fortunately, mitigation for these attacks is relatively straightforward.&nbsp;</p>\n\n\n\n<p>To avoid the risk of prompt extraction attacks, do not share any information that the current LLM user is not authorized to see in the system prompt template. This may include information retrieved from a retrieval augmented generation (RAG) architecture. Assume that anything included in a prompt template is visible to a sufficiently motivated attacker. In particular, passwords, access tokens, or API keys should never be placed in the prompt, or anywhere else directly accessible to the LLM. Strict isolation of information is the best defense.</p>\n\n\n\n<p>To reduce the risk of sensitive training data being extracted from the model, the best approach is simply to not train on it. Given enough queries, it is inevitable that the LLM will eventually incorporate some element of that sensitive data into its response. If the model must be able to use or answer questions about sensitive information, a RAG architecture may be a more secure approach.&nbsp;</p>\n\n\n\n<p>In such an architecture, the LLM is not trained on sensitive documents, but is given access to a document store that is capable of 1) identifying and returning relevant sensitive documents to the LLM to assist in generation, and 2) verifying the authorization of the current user to access those documents.&nbsp;</p>\n\n\n\n<p>While this avoids the need to train the LLM on sensitive data to produce acceptable results, it does introduce additional complexity to the application with respect to conveying authorization and tracking document permissions. This must be carefully handled to prevent other confidentiality violations.</p>\n\n\n\n<p>If the sensitive data has already been trained into the model, then the risk can still be somewhat mitigated by rate-limiting queries, not providing detailed information about probabilities of the LLM completions back to the user, and adding logging and alerting to the application.&nbsp;&nbsp;</p>\n\n\n\n<p>Restricting the query budget to the minimum that is consistent with the functionality of the LLM-enabled application, and not providing any detailed probability information to the final user, make both the inversion and inference attacks extremely difficult and time-consuming to execute.&nbsp;</p>\n\n\n\n<p>Working with an <a href=\"https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/\">AI Red Team</a> to evaluate data leakage may be helpful in quantifying risk, setting appropriate rate limits for a particular application, and identifying queries or patterns of queries within user sessions that might indicate an attempt to extract training data that should be alerted on.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Application-related leaks</h3>\n\n\n\n<p>In addition to LLM-specific attacks, the novelty of LLMs can lead to more basic errors in constructing an LLM-enabled application. Logging of prompts and responses can often lead to service-side information leaks. Either a user who has not been properly educated introduces proprietary or sensitive information into the application, or the LLM provides a response based on sensitive information which is logged without the appropriate access controls.&nbsp;&nbsp;</p>\n\n\n\n<p>In Figure 2, a user makes a request to a RAG system, which requests documents the user alone is authorized to see on the user\u2019s behalf in order to fulfill the request. Unfortunately, the request and response\u2014which contain information related to the privileged document\u2014are logged in a system with a different access level, thus leaking information.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"771\" height=\"384\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging.png\" alt=\"A system diagram showing a \u2018user device\u2019 and \u2018document store\u2019 within a green confidentiality boundary; they both have a bidirectional connection to an \u2018inference service\u2019 that is not within a confidentiality boundary (as it is stateless). There is an arrow from the inference service to a \u2018logging and monitoring\u2019 service that is within a red confidentiality boundary.\n\" class=\"wp-image-73616\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging.png 771w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-300x149.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-625x311.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-179x89.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-768x383.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-645x321.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-500x249.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-160x80.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-362x180.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-leakage-logging-221x110.png 221w\" sizes=\"(max-width: 771px) 100vw, 771px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. An example of data leakage through logging</em></figcaption></figure>\n\n\n\n<p>If RAG is being used to improve LLM responses, it&#8217;s important to track user authorization with respect to the documents being retrieved, and where the responses are being logged. The LLM should only be able to access documents that the current user is authorized to access. The completions (which by design incorporate some of the information contained in those access-controlled documents) should be logged in a manner such that unauthorized users cannot see the summaries of the sensitive documents.&nbsp;</p>\n\n\n\n<p>It is therefore extremely important that authentication and authorization mechanisms are executed outside the context of the LLM. If relying on transmitting user context as part of the prompt, a sufficiently skilled attacker can use prompt injection to impersonate other users.</p>\n\n\n\n<p>Finally, the behavior of any plug-ins should be scrutinized to ensure that they do not maintain any state that could lead to cross-user information leakage. For instance, if a search plug-in happens to cache queries, then the speed with which it returns information might allow an attacker to infer what topics other users of the application query most often.</p>\n\n\n\n<h2 class=\"wp-block-heading\">LLM reliability</h2>\n\n\n\n<p>Despite significant improvements in the reliability and accuracy of LLM generations, they are still subject to some amount of random error. How words are sampled randomly from the set of possible next words increases the \u201ccreativity\u201d of LLMs, but also increases the chance that they will produce incorrect results.&nbsp;&nbsp;</p>\n\n\n\n<p>This has the potential to impact both users, who may act on inaccurate information, and downstream processes, plug-ins, or other computations that may fail or produce additional inaccurate results based on the inaccurate input (Figure 3).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"730\" height=\"576\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail.png\" alt=\"An image displaying two turns of interaction with an LLM. The LLM user requested that the LLM tell a joke without the letter \u2018e\u2019. The LLM provides a joke that contains two instances of the letter \u2018e\u2019 and then asserts that the letter \u2018e\u2019 appears zero times in the joke. The user then asks the LLM to count again, and the LLM correctly identifies that the letter \u2018e\u2019 appears twice, but incorrectly states that one of those instances is in the word \u2018salad\u2019.\n\" class=\"wp-image-73617\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail.png 730w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-300x237.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-625x493.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-146x115.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-645x509.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-380x300.png 380w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-114x90.png 114w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-362x286.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-task-fail-139x110.png 139w\" sizes=\"(max-width: 730px) 100vw, 730px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. An example of an LLM failing to complete a task and correctly answer a related question</em></figcaption></figure>\n\n\n\n<p>Downstream processes and plug-ins must be designed with the potential for LLM errors in mind. As with prompt injection, good security design up front, including parameterization of plug-ins, sanitization of inputs, robust error handling, and ensuring that the user authorization is explicitly requested when performing a sensitive operation. All of these approaches help mitigate risks associated with LLMs.&nbsp;</p>\n\n\n\n<p>In addition, ensure that any LLM orchestration layer can terminate early and inform the user in the event of an invalid request or LLM generation. This helps avoid compounding errors if a sequence of plugins is called. Compounding errors across LLM and plug-in calls is the most common way exploitation vectors are built for these systems. The standard practice of failing closed when bad data is identified should be used here.</p>\n\n\n\n<p>User education around the scope, reliability, and applicability of the LLM powering the application is important. Users should be reminded that the LLM-enabled application is intended to supplement\u2014not replace\u2014their skills, knowledge, and creativity. The final responsibility for the use of any result, LLM-derived or not, rests with the user.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>LLMs can provide significant value to both users and the organizations that deploy them. However, as with any new technology, new security risks come along with them. Prompt injection techniques are \u200cthe best known, and any application, including an LLM, should be designed with that risk in mind.&nbsp;</p>\n\n\n\n<p>Less familiar security risks include the various forms of information leaks that LLMs can create, which require careful tracing of data flows and management of authorization. The occasionally unreliable nature of LLMs must also be considered, both from a user reliability standpoint and from an application standpoint.&nbsp;</p>\n\n\n\n<p>Making your application robust to both natural and malicious errors can increase its security. By considering the risks outlined in this post, and applying the mitigation strategies and best practices described, you can reduce your exposure to these risks and help ensure a successful deployment.</p>\n\n\n\n<p>To learn more about attacking and defending machine learning models, check out the NVIDIA training at <a href=\"https://www.blackhat.com/eu-23/training/schedule/index.html#blackhat-machine-learning-33946\">Black Hat Europe 2023</a>.</p>\n\n\n\n<p>Register for <a href=\"https://www.nvidia.com/en-us/events/llm-developer-day/\">LLM Developer Day</a>, a free virtual event on November 17, and join us for the session, Reinventing the Complete Cybersecurity Stack with AI Language Models.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) provide a wide range of powerful enhancements to nearly any application that processes text. And yet they also introduce new risks, including: This post walks through these security vulnerabilities in detail and outlines best practices for designing or evaluating a secure LLM-enabled application.&nbsp;&nbsp; Prompt injection Prompt injection is the most common &hellip; <a href=\"https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/\">Continued</a></p>\n", "protected": false}, "author": 1588, "featured_media": 73612, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298071", "discourse_permalink": "https://forums.developer.nvidia.com/t/best-practices-for-securing-llm-enabled-applications/272987", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 1464, 3110], "tags": [1511, 453, 3550, 2932, 1953], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/security-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-j9f", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73609"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1588"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73609"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73609/revisions"}], "predecessor-version": [{"id": 73642, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73609/revisions/73642"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73612"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73609"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73609"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73609"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73575, "date": "2023-11-15T10:00:00", "date_gmt": "2023-11-15T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73575"}, "modified": "2023-12-08T10:53:36", "modified_gmt": "2023-12-08T18:53:36", "slug": "mastering-llm-techniques-llmops", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/", "title": {"rendered": "Mastering LLM Techniques: LLMOps"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Businesses rely more than ever on data and AI to innovate, offer value to customers, and stay competitive. The adoption of machine learning (ML), created a need for tools, processes, and organizational principles to manage code, data, and models that work reliably, cost-effectively, and at scale. This is broadly known as <a href=\"https://developer.nvidia.com/blog/demystifying-enterprise-mlops/\">machine learning operations</a> (<a href=\"https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/\">MLOps</a>).</p>\n\n\n\n<p>The world is venturing rapidly into a new generative AI era powered by foundation models and large language models (LLMs) in particular. The release of ChatGPT further accelerated this transition. </p>\n\n\n\n<p>New and specialized areas of generative AI operations (GenAIOps) and large language model operations (LLMOps) emerged as an evolution of MLOps for addressing the challenges of developing and managing generative AI and LLM-powered apps in production.</p>\n\n\n\n<p>In this post, we outline the generative AI app development journey, define the concepts of GenAIOps and LLMOps, and compare them with MLOps. We also explain why mastering operations becomes paramount for business leaders executing an enterprise-wide AI transformation.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Building modern generative AI apps for enterprises&nbsp;</h2>\n\n\n\n<p>The journey towards a modern generative AI app starts from a foundation model, which goes through a pretraining stage to learn the foundational knowledge about the world and gain emergent capabilities. The next step is aligning the model with human preferences, behavior, and values using a curated dataset of human-generated prompts and responses. This gives the model precise instruction-following capabilities. Users can choose to train their own foundation model or use a pretrained model.</p>\n\n\n\n<p>For example, various foundation models such as <a href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\">NVIDIA Nemotron-3</a> and community models like <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/playground/models/llama2\">Llama</a> are available through <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/#nemo-foundation\">NVIDIA AI Foundations</a>. These are all enhanced with NVIDIA proprietary algorithmic and system optimizations, security, and enterprise-grade support covered by <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1104\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application.png\" alt=\"Diagram of lifecycle of model customization techniques and retrieval augmented generation.\" class=\"wp-image-73587\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-300x166.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-625x345.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-768x424.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-1536x848.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-645x356.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-500x276.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-362x200.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-199x110.png 199w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1.-Lifecycle-generative-AI-application-1024x566.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A lifecycle of a generative AI application powered by a customized foundation model and retrieval augmented generation</em></figcaption></figure></div>\n\n\n<p>Next, comes the customization stage. A foundation model is combined with a task-specific prompt or fine-tuned on a curated enterprise dataset. The knowledge of a foundation model is limited to the pretraining and fine-tuning data, becoming outdated over time unless the model is continuously retrained, which can be costly.&nbsp;</p>\n\n\n\n<p>A <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51916/\">retrieval augmented generation</a> (<a href=\"https://docs.nvidia.com/ai-enterprise/workflows-generative-ai/0.1.0/technical-brief.html\">RAG</a>) workflow is used to maintain freshness and keep the model grounded with external knowledge during query time. This is one of the most critical steps in the generative AI app development lifecycle and when a model learns unique relationships hidden in enterprise data.&nbsp;</p>\n\n\n\n<p>After customization, the model is ready for real-world use either independently or as a part of a chain, which combines multiple foundation models and APIs to deliver the end-to-end application logic. At this point, it is crucial to test the complete AI system for accuracy, speed, and vulnerabilities, and add guardrails to ensure the model outputs are accurate, safe, and secure.</p>\n\n\n\n<p>Finally, the feedback loop is closed. Users interact with an app through the user interface or collect data automatically using system instrumentation. This information can be used to update the model and the A/B test continuously,&nbsp;increasing its value to the customers.</p>\n\n\n\n<p>An enterprise typically has many generative AI apps tailored to different use cases, business functions, and workflows. This AI portfolio requires continuous oversight and risk management to ensure smooth operation, ethical use, and prompt alerts for addressing incidents, biases, or regressions.</p>\n\n\n\n<p>GenAIOps accelerates this journey from research to production through automation. It optimizes development and operational costs, improves the quality of models, adds robustness to the model evaluation process, and guarantees sustained operations at scale.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Understanding GenAIOps, LLMOps, and RAGOps</h2>\n\n\n\n<p>There are several terms associated with <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a>. We outline the definitions in the following section.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"807\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy.png\" alt=\"An illustration showing the nested relationship from MLOps, GenAIOps, LLMOps, and RAGOps.\" class=\"wp-image-73588\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-300x121.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-625x252.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-179x72.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-768x310.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-1536x620.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-645x260.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-500x202.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-160x65.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-362x146.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-272x110.png 272w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2.-AI-ops-hierarchy-1024x413.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. A hierarchy of AI types and associated Ops organized by the level of specialization</em></figcaption></figure></div>\n\n\n<p>Think of AI as a series of nested layers. At the outermost layer, ML covers intelligent automation, where the logic of the program is not explicitly defined but learned from data. As we dive deeper, we encounter specialized AI types, like those built on LLMs or RAGs. Similarly, there are nested concepts enabling reproducibility, reuse, scalability, reliability, and efficiency.&nbsp;</p>\n\n\n\n<p>Each one builds on the previous, adding, or refining capabilities\u2013from foundational MLOps to the newly developed RAGOps lifecycle:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>MLOps</strong> is the overarching concept covering the core tools, processes, and best practices for end-to-end machine learning system development and operations in production.</li>\n\n\n\n<li><strong>GenAIOps</strong> extends MLOps to develop and operationalize generative AI solutions. The distinct characteristic of GenAIOps is the management of and interaction with a foundation model.</li>\n\n\n\n<li><strong>LLMOps</strong> is a distinct type of GenAIOps focused specifically on developing and productionizing LLM-based solutions.&nbsp;</li>\n\n\n\n<li><strong>RAGOps</strong> is a subclass of LLMOps focusing on the delivery and operation of RAGs, which can also be considered the ultimate reference architecture for generative AI and LLMs, driving massive adoption.</li>\n</ul>\n\n\n\n<p>GenAIOps and LLMOps span the entire AI lifecycle. This includes foundation model pretraining, model alignment through supervised fine-tuning, and reinforcement learning from human feedback (RLHF), customization to a specific use case coupled with pre/post-processing logic, chaining with other foundation models, APIs, and guardrails. RAGOps scope doesn\u2019t include pretraining and assumes that a foundation model is provided as an input into the RAG lifecycle.</p>\n\n\n\n<p>GenAIOps, LLMOps, and RAGOps are not only about tools or platform capabilities to enable AI development. They also cover methodologies for setting goals and KPIs, organizing teams, measuring progress, and continuously improving operational processes.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Extending MLOps for generative AI and LLMs</h2>\n\n\n\n<p>With the key concepts defined, we can focus on the nuances differentiating one from the other.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"927\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle-.png\" alt=\"New GenAIOps-specific capabilities, including Synthetic Data Management, Embedding Management, Agent / Chain Management, Guardrails, and Prompt Management.\" class=\"wp-image-73589\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle-.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--300x139.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--625x290.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--179x83.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--768x356.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--1536x712.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--645x299.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--500x232.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--362x168.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--237x110.png 237w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/3.-end-to-end-ml-lifecycle--1024x475.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. An end-2-end machine learning lifecycle showcasing core MLOps (gray) and GenAIOps capabilities (green)</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">MLOps</h3>\n\n\n\n<p>MLOps lays the foundation for a structured approach to the development, training, evaluation, optimization, deployment, inference, and monitoring of machine learning models in production.&nbsp;</p>\n\n\n\n<p>The key <a href=\"https://developer.nvidia.com/blog/demystifying-enterprise-mlops/\">MLOps</a> ideas and capabilities are relevant for generative AI, including the following.</p>\n\n\n\n<ul>\n<li><strong>Infrastructure management</strong>: request, provision, and configure compute, storage, and networking resources to access the underlying hardware programmatically.&nbsp;</li>\n\n\n\n<li><strong>Data management</strong>: collect, ingest, store, process, and label data for training and evaluation. Configure role-based access control; dataset search, browsing, and exploration; data provenance tracking, data logging, dataset versioning, metadata indexing, data quality validation, dataset cards, and dashboards for data visualization.</li>\n\n\n\n<li><strong>Workflow and pipeline management: </strong>work with cloud resources or a local workstation; connect data preparation, model training, model evaluation, model optimization, and model deployment steps into an end-to-end automated and scalable workflow combining data and compute.&nbsp;</li>\n\n\n\n<li><strong>Model management: </strong>train, evaluate, and optimize models for production; store and version models along with their model cards in a centralized model registry; assess model risks, and ensure compliance with standards.</li>\n\n\n\n<li><strong>Experiment management and observability:</strong> track and compare different machine learning model experiments, including changes in training data, models, and hyperparameters. Automatically search the space of possible model architectures and hyperparameters for a given model architecture; analyze model performance during inference, monitor model inputs and outputs for concept drift.</li>\n\n\n\n<li><strong>Interactive development:</strong> manage development environments and integrate with external version control systems, desktop IDEs, and other standalone developer tools, making it easier for teams to prototype, launch jobs, and <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">collaborate on projects</a>.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">GenAIOps&nbsp;</h3>\n\n\n\n<p>GenAIOps encompasses MLOps, code development operations (DevOps), data operations (DataOps), and model operations (ModelOps), for all generative AI workloads from language, to image, to multimodal. Data curation and model training, customization, evaluation, optimization, deployment, and risk management must be rethought for generative AI.&nbsp;</p>\n\n\n\n<p>New emerging GenAIOps capabilities include:</p>\n\n\n\n<ul>\n<li>Synthetic data management: extend data management with a new native generative AI capability. Generate synthetic training data through domain randomization to increase transfer learning capabilities. Declaratively define and generate edge cases to evaluate, validate, and certify model accuracy and robustness. </li>\n\n\n\n<li><strong>Embedding management</strong>: represent data samples of any modality as dense multi-dimensional embedding vectors; generate, store, and version embeddings in a vector database. Visualize embeddings for improvised exploration. Find relevant contextual information through vector similarity search for RAGs, data labeling, or data curation as a part of the active learning loop. For GenAIOps, using embeddings and vector databases replaces feature management and feature stores relevant to MLOps.</li>\n\n\n\n<li>Agent/chain management: define complex multi-step application logic. Combine multiple foundation models and APIs together, and augment a foundation model with external memory and knowledge, following the RAG pattern. Debug, test, and trace chains with non-deterministic outputs or complex planning strategies, visualize, and inspect the execution flow of a multi-step chain in real-time and offline. Agent/chain management is valuable throughout the entire generative AI lifecycle as a key part of the inference pipeline. It serves as an extension of the workflow/pipeline management for MLOps.</li>\n\n\n\n<li>Guardrails: intercept adversarial or unsupported inputs before sending them to a foundation model. Make sure that model outputs are accurate, relevant, safe, and secure. Maintain and check the state of the conversation and active context, detect intents, and decide actions while enforcing content policies. Guardrails build upon rule-based pre/post-processing of AI inputs/outputs covered under model management.</li>\n\n\n\n<li><strong>Prompt management:</strong> create, store, compare, optimize, and version prompts. Analyze inputs and outputs and manage test cases during <a href=\"https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/\">prompt </a>engineering. Create parameterized prompt templates, select optimal inference-time hyperparameters and system prompts serving as the starting point during the interaction of a user with an app; and adjust prompts for each foundation model. Prompt management, with its distinct capabilities, is a logical extension of experiment management for generative AI.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">LLMOps</h3>\n\n\n\n<p>LLMOps is a subset of the broader GenAIOps paradigm, focused on operationalizing transformer-based networks for language use cases in production applications. Language is a foundational modality that can be combined with other modalities to guide AI system behavior, for example, <a href=\"https://www.nvidia.com/en-us/gpu-cloud/picasso/\">NVIDIA Picasso</a> is a multimodal system combining text and image modalities for visual content production. </p>\n\n\n\n<p>In this case, text drives the control loop of an AI system with other data modalities and foundation models being used as plug-ins for specific tasks. The natural language interface expands the user and developer bases and decreases the AI adoption barrier. The set of operations encompassed under LLMOps includes prompt management, agent management, and RAGOps.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Driving generative AI adoption with RAGOps</h2>\n\n\n\n<p>RAG is a workflow designed to enhance the capabilities of general-purpose LLMs. Incorporating information from proprietary datasets during query time and grounding generated answers on facts guarantees factual correctness. While traditional models can be fine-tuned for tasks like sentiment analysis without needing external knowledge, RAG is tailored for tasks that benefit from accessing external knowledge sources, like question answering.</p>\n\n\n\n<p>RAG integrates an information retrieval component with a text generator. This process consists of two steps:</p>\n\n\n\n<ol>\n<li><strong>Document retrieval and ingestion</strong>\u2014the process of ingesting documents and chunking the text with an embedding model to convert them into vectors and store them in a vector database.</li>\n\n\n\n<li><strong>User query and response generation</strong>\u2014a user query is converted to the embedding space at a query time along with the embedding model, which in turn is used to search against the vector database for the closest matching chunks and documents. The original user query and the top documents are fed into a customized generator LLM, which generates a final response and renders it back to the user.</li>\n</ol>\n\n\n\n<p>It also offers the advantage of updating its knowledge without the need for comprehensive retraining. This approach ensures reliability in generated responses and addresses the issue of &#8220;hallucination&#8221; in outputs.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"746\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence.png\" alt=\"Diagram: User query gets transformed into an embedding vector and then it is matched to document chunks, also represented as embeddings, with the help of the vector database.\" class=\"wp-image-73590\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-300x112.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-625x233.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-179x67.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-768x287.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-1536x573.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-645x241.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-500x187.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-160x60.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-362x135.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-295x110.png 295w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/4.-RAG-sequence-1024x382.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Retrieval Augmented Generation (RAG) sequence diagram</em></figcaption></figure></div>\n\n\n<p>RAGOps is an extension of LLMOps. This involves managing documents and databases, both in the traditional sense, as well as in the vectorized formats, alongside embedding and retrieval models. RAGOps distills the complexities of generative AI app development into one pattern. Thus, it enables more developers to build new powerful applications and decreases the AI adoption barrier.</p>\n\n\n\n<h2 class=\"wp-block-heading\">GenAIOps offers many business benefits</h2>\n\n\n\n<p>As researchers and developers master GenAIOps to expand beyond DevOps, DataOps, and ModelOps, there are many business benefits. These include the following.</p>\n\n\n\n<ul>\n<li><strong>Faster time-to-market</strong>: Automation and acceleration of end-to-end generative AI workflows lead to shorter AI product iteration cycles, making the organization more dynamic and adaptable to new challenges.</li>\n\n\n\n<li><strong>Higher yield and innovation:</strong> Simplifying the AI system development process and increasing the level of abstraction, enables GenAIOps more experiments, and higher enterprise application developer engagement, optimizing AI product releases.</li>\n\n\n\n<li><strong>Risk mitigation: </strong>Foundation models hold the potential to revolutionize industries but also run the risk of amplifying inherent biases or inaccuracies from their training data. The defects of one foundation model propagate to all downstream models and chains. GenAIOps ensures that there is a proactive stance on minimizing these defects and addressing ethical challenges head-on.&nbsp;</li>\n\n\n\n<li><strong>Streamlined collaboration:</strong> GenAIOps enables smooth handoffs across teams, from data engineering to research to product engineering inside one project, and facilitates artifacts and knowledge sharing across projects. It requires stringent operational rigor, standardization, and collaborative tooling to keep multiple teams in sync.</li>\n\n\n\n<li><strong>Lean operations: </strong>GenAIOps helps reduce waste through workload optimization, automation of routine tasks, and availability of specialized tools for every stage in the AI lifecycle. This leads to higher productivity and lower TCO (TCO).</li>\n\n\n\n<li><strong>Reproducibility: </strong>GenAIOps helps maintain a record of code, data, models, and configurations ensuring that a successful experiment run can be reproduced on-demand. This becomes especially critical for regulated industries, where reproducibility is no longer a feature but a hard requirement to be in business.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">The transformational potential of generative AI</h2>\n\n\n\n<p>Incorporating GenAIOps into the organizational fabric is not just a technical upgrade. It is a strategic move with long-term positive effects for both customers and end users across the enterprise.</p>\n\n\n\n<ul>\n<li><strong>Enhancing user experiences: </strong>GenAIOps provides optimal performance of AI apps in production. Businesses can offer enhanced user experiences. Be it through chatbots, autonomous agents, content generators, or data analysis tools.</li>\n\n\n\n<li><strong>Unlocking new revenue streams: </strong>With tailored applications of generative AI, facilitated by GenAIOps, businesses can venture into previously uncharted territories, unlocking new revenue streams and diversifying their offerings.</li>\n\n\n\n<li><strong>Leading ethical standards: </strong>In a world where brand image is closely tied to ethical considerations, businesses that proactively address AI\u2019s potential pitfalls, guided by GenAIOps, can emerge as industry leaders, setting benchmarks for others to follow.&nbsp;</li>\n</ul>\n\n\n\n<p>The world of AI is dynamic, rapidly evolving, and brimming with potential. Foundation models, with their unparalleled capabilities in understanding and generating text, images, molecules, and music, are at the forefront of this revolution. </p>\n\n\n\n<p>When examining the evolution of AI operations, from MLOps to GenAIOps, LLMOps, and RAGOps, businesses must be flexible, advance, and prioritize precision in operations. With a comprehensive understanding and strategic application of GenAIOps, organizations stand poised to shape the trajectory of the generative AI revolution.</p>\n\n\n\n<h2 class=\"wp-block-heading\">How to get started</h2>\n\n\n\n<p>Try state-of-the-art generative AI models running on an optimized NVIDIA accelerated hardware/software stack from your browser using&nbsp;<a href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\">NVIDIA AI Foundation Models</a>.</p>\n\n\n\n<p>Get started with LLM development on <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a>, an end-to-end, cloud-native framework for building, customizing, and deploying generative AI models anywhere.&nbsp;</p>\n\n\n\n<p>Or, begin your learning journey with <a href=\"https://www.nvidia.com/en-us/learn/organizations/\">NVIDIA training</a>. Our expert-led courses and workshops provide learners with the knowledge and hands-on experience necessary to unlock the full potential of NVIDIA solutions. For generative AI and LLMs, check out our focused <a href=\"https://nvdam.widen.net/s/rvsgdxpfkz/dli-generative-ai-llm-learning-path-2740963\">Gen AI/LLM learning path</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Businesses rely more than ever on data and AI to innovate, offer value to customers, and stay competitive. The adoption of machine learning (ML), created a need for tools, processes, and organizational principles to manage code, data, and models that work reliably, cost-effectively, and at scale. This is broadly known as machine learning operations (MLOps). &hellip; <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/\">Continued</a></p>\n", "protected": false}, "author": 1922, "featured_media": 73591, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298070", "discourse_permalink": "https://forums.developer.nvidia.com/t/mastering-llm-techniques-llmops/272986", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110], "tags": [2584, 453, 3650, 1595, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLMOps.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-j8H", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73575"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1922"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73575"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73575/revisions"}], "predecessor-version": [{"id": 75079, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73575/revisions/75079"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73591"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73575"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73575"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73575"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]