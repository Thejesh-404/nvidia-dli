[{"id": 77164, "date": "2024-01-31T09:00:00", "date_gmt": "2024-01-31T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=77164"}, "modified": "2024-01-31T10:16:39", "modified_gmt": "2024-01-31T18:16:39", "slug": "new-self-paced-course-synthetic-tabular-data-generation-using-transformers", "status": "publish", "type": "post", "link": "https://nvda.ws/3vUDGRd", "title": {"rendered": "New Self-Paced Course: Synthetic Tabular Data Generation Using Transformers"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Synthetic data generation is a data augmentation technique necessary for increasing the robustness of models by supplying training data. Explore the use of Transformers for synthetic tabular data generation in the new self-paced course.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Synthetic data generation is a data augmentation technique necessary for increasing the robustness of models by supplying training data. Explore the use of Transformers for synthetic tabular data generation in the new self-paced course.</p>\n", "protected": false}, "author": 1115, "featured_media": 77302, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3vUDGRd", "_links_to_target": "_blank"}, "categories": [3110], "tags": [2964, 1935, 3650, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/dli-tech-blog-synthetic-data-generation-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-k4A", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77164"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=77164"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77164/revisions"}], "predecessor-version": [{"id": 77301, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77164/revisions/77301"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77302"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=77164"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=77164"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=77164"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76698, "date": "2024-01-30T12:02:55", "date_gmt": "2024-01-30T20:02:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76698"}, "modified": "2024-01-30T17:06:02", "modified_gmt": "2024-01-31T01:06:02", "slug": "create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/", "title": {"rendered": "Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">NVIDIA AI Workbench</a> is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning (ML) projects. Announced at SIGGRAPH 2023, NVIDIA AI Workbench enables developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. To learn more, see <a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a>.</p>\n\n\n\n<p>This post explains how NVIDIA AI Workbench helps streamline the AI workflow and details new features of the beta release. It also walks through a coding copilot reference example, which enables you to use AI Workbench to create, test, and customize a pretrained generative AI model on your platform of choice.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What is NVIDIA AI Workbench?</h2>\n\n\n\n<p>With AI Workbench, developers and data scientists have the flexibility to start an AI or ML project locally on a PC or workstation and then migrate it anywhere. Projects can be pushed out to a data center, public cloud, or <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>, or moved to a local RTX PC or workstation for inference and lightweight customization, depending on project requirements.</p>\n\n\n\n<p>AI Workbench helps developers simplify and shorten setup, development, and migration for AI workflows by providing the ability to work on their choice of heterogeneous compute resources. Benefits include:</p>\n\n\n\n<ul>\n<li>Free and quick install on the system of choice with an intuitive UX or CLI for project creation and management.</li>\n\n\n\n<li>Streamlined configuration for compute resources and runtimes, providing reproducibility and flexibility to work on different GPU resources.</li>\n\n\n\n<li>Simplified version control and management for containers and Git repositories and integrations with GitHub, GitLab, and the <a href=\"https://catalog.ngc.nvidia.com/containers?filters=&amp;orderBy=scoreDESC&amp;query=workbench\">NVIDIA NGC catalog</a>.</li>\n\n\n\n<li>Automation and streamlining to handle Git and container-based developer environments, enabling users to work on their choice of system, laptop, workstation, server, or the cloud.</li>\n\n\n\n<li>Reproducibility across users and systems with transparent handling for idiosyncrasies like credentials, secrets, and file system changes without the overhead.</li>\n\n\n\n<li>Scalable creation and distribution of complex workflows and applications for generative AI, GPU-enabled ML, and data science.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">What\u2019s new in the beta release</h2>\n\n\n\n<p>The AI Workbench beta release includes the following exciting new features, with updates to the user interface and expanded support for container runtimes and Git servers.</p>\n\n\n\n<p>Simplified setup and installation<strong> </strong>on Windows 11, Ubuntu, 22.04, and macOS 11 or higher.&nbsp;</p>\n\n\n\n<ul>\n<li>Install AI Workbench quickly in two ways: click-through install using the desktop app on local systems or command-line install on remote systems.</li>\n\n\n\n<li>Work from anywhere with support for the three major operating systems for a uniform experience. AI Workbench runs on Windows distributions that support WSL2, Ubuntu 22.04, and macOS version 11 and higher.</li>\n</ul>\n\n\n\n<p>Simplified version control and streamlined development with containerized environments.</p>\n\n\n\n<ul>\n<li>Access simple and comprehensive Git-compliant version control with both the Desktop App and CLI. Push, pull, and fetch features are now included.&nbsp;</li>\n\n\n\n<li>Create a containerized JupyterLab environment with isolation and reproducibility without having to handle details.</li>\n\n\n\n<li>Choose from two container runtime options: Docker or Podman.</li>\n</ul>\n\n\n\n<p>Expanded feature parity between the user interface and the CLI.</p>\n\n\n\n<ul>\n<li>See commit history and summaries directly in the Desktop App.</li>\n\n\n\n<li>View improved container state and application status notifications in the Desktop App.</li>\n</ul>\n\n\n\n<p>Expanded default base images.</p>\n\n\n\n<ul>\n<li>Access three new base images for project creation, in addition to the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-workbench/containers/python-basic\">Python Basic</a> and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-workbench/containers/pytorch\">PyTorch Basic</a> images already in the NGC catalog. New base images for CUDA 11.0, CUDA 12.0, and CUDA 12.2 provide the foundation for further customization.</li>\n</ul>\n\n\n\n<p>Three new example projects for reference.</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral</a>: Fine-tune a Mistral 7B large language model (LLM) on a custom code instructions dataset using QLoRA PEFT.</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/workbench-example-local-rag\">RAG</a>: Converse with your data using a local, user-friendly developer workflow for <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG).</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/workbench-example-nemotron-finetune\">NeMotron-3</a>: Fine-tune a Nemotron-3 8B LLM on a custom QA dataset using <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a>.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Create your own coding copilot</h2>\n\n\n\n<p>This section walks through an example of how AI Workbench can significantly simplify the process of using and fine-tuning a generative AI model on a GPU system of the user\u2019s choice.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key concepts</h3>\n\n\n\n<p>A few key concepts used in this example are outlined below.</p>\n\n\n\n<h4 class=\"wp-block-heading\">AI Workbench Project</h4>\n\n\n\n<p>An AI Workbench Project is a Git repository that contains a set of configuration files that can be read by AI Workbench to automate the creation and management of a containerized development environment. A project references everything needed for a configured, containerized development environment and includes:&nbsp;</p>\n\n\n\n<ul>\n<li>Code, data, and models</li>\n\n\n\n<li>Simple configuration files that drive AI Workbench automation for container customization and package installation</li>\n\n\n\n<li>A project specification metadata file to wrap the repository in a way that&#8217;s compatible with AI Workbench&nbsp;</li>\n</ul>\n\n\n\n<p>Visit <a href=\"https://github.com/nvidia?q=workbench&amp;type=all&amp;language=&amp;sort=\">NVIDIA on GitHub</a> to reference NVIDIA projects that provide starting points for adapting your own data and use cases. Additionally, <a href=\"https://developer.nvidia.com/ai-workbench-early-access/join\">AI Workbench early access</a> members can contribute and use third-party <a href=\"https://developer.nvidia.com/ai-workbench-early-access/members\">community project examples</a>.</p>\n\n\n\n<p>The <a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral 7B fine-tuning reference project</a> showcased in this post highlights how to leverage the power of AI Workbench to build a basic coding copilot on a system of your choice.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Fine-tuning</h4>\n\n\n\n<p>While Mistral 7B is a strong baseline for multiple downstream tasks, it can lack domain-specific knowledge based on proprietary or otherwise sensitive information. Fine-tuning is used to improve the model\u2019s responses in these cases.&nbsp;</p>\n\n\n\n<p>There are two versions of fine-tuning. The first, <em>full fine-tuning</em>, uses the new data to update all of the model weights. This can improve domain-specific results but often requires more time and larger, more expensive GPUs. The second, <em>parameter efficient fine-tuning (PEFT)</em>, is a family of techniques that update a subset of the model weights. PEFT is often preferable to full fine-tuning because it produces comparable results in far less time and with smaller, less expensive GPUs.</p>\n\n\n\n<p>This example focuses primarily on the Quantized Low Rank Adaptation (QLoRA) method of PEFT. Low Rank Adaptation (LoRA) is a method of PEFT that uses smaller weight matrices in the retraining as approximations instead of updating the full weight matrix. This rank decomposition optimization technique enables greater memory efficiency and can reduce the GPU size required for successful fine-tuning.&nbsp;</p>\n\n\n\n<p>QLoRA<strong> </strong>is a further optimization that reduces the precision of model weights to provide even greater advances in memory and space efficiency. The most common quantization used for this LoRA fine-tuning workflow is 4-bit quantization, which provides a decent balance between model performance and fine-tuning feasibility.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Walkthrough of Mistral 7B fine-tuning project in NVIDIA AI Workbench</h3>\n\n\n\n<p>This walkthrough includes high-level code and details. For more information, see the full <a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral 7B fine-tuning reference project</a> on GitHub. The project fine-tunes the Mistral 7B base model on the <a href=\"https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style\">TokenBender code instructions dataset</a>, consisting of 122K Alpaca-style code instructions and code solutions.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1480\" height=\"955\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench.png\" alt=\"Screenshot of the Mistral 7B fine-tuning project in the NVIDIA AI Workbench user interface.\" class=\"wp-image-76708\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench.png 1480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-768x496.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-465x300.png 465w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-1024x661.png 1024w\" sizes=\"(max-width: 1480px) 100vw, 1480px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Building the Mistral 7B fine-tuning project in NVIDIA AI Workbench</em></figcaption></figure></div>\n\n\n<p>First, download the data and split it into 80% training, 10% validation, and 10% testing datasets. One entry of an instruction in the dataset is shown below as an example:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction: Output the maximum element in an array. ### Input: &#91;1, 5, 10, 8, 15] ### Output: 15\n</pre></div>\n\n\n<p>Next, download the Mistral 7B model weights to this project:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nmodel_id = &quot;mistralai/Mistral-7B-v0.1&quot;\nbb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bb_config)\n</pre></div>\n\n\n<p>Notice that the 4-bit quantization configuration is specified for the base model.&nbsp;</p>\n\n\n\n<p>Next, evaluate the performance of the base model on a specific sample programming question. This establishes a baseline for comparison between the base model and the final, fine-tuned model.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nbase_prompt = &quot;&quot;&quot;Write a function to output the prime factorization of 2023 in python, C, and C++&quot;&quot;&quot;\n\nbase_tokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n    add_bos_token=True,\n)\n\nmodel_input = base_tokenizer(base_prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)\n\nmodel.eval()\nwith torch.no_grad():\n    print(base_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)&#91;0], skip_special_tokens=True))\n\n***** Output ***** \n\n## Prime Factorization of 2023\n\nThe prime factorization of 2023 is 13 x 157.\n\n## Prime Factorization of 2023 in Python\n\nThe prime factorization of 2023 in python is given below.\n\ndef prime_factorization(n):\n    factors = &#91;]\n    for i in range(2, n + 1):\n        if n % i == 0:\n            factors.append(i)\n    return factors\n\nprint(prime_factorization(2023))\n\n...\n</pre></div>\n\n\n<p>Notice that the base model doesn&#8217;t perform well out of the box. First, the base model seems to think the prime factorization of 2,023 is 13 x 157. This amounts to 2041. The actual answer is 7 x 17 x 17.</p>\n\n\n\n<p>Second, the Python function the model outputs is incorrect as well. Running the suggested code gives an answer of [7, 17, 119, 289, 2,023] when in fact 119, 289, and 2,023 are not prime factors.</p>\n\n\n\n<p>Fine-tuning is necessary to improve model performance. Begin with preprocessing the dataset by reformatting the dataset entries to better fit the instruction prompt [INST] for fine-tuning. Then tokenize each of these prompts.&nbsp;</p>\n\n\n\n<p>Next, specify the configuration for QLoRA fine-tuning and perform the fine-tuning. By default, the fine-tuning takes 1,000 iterations, with checkpointing and evaluation every 50 steps. These hyperparameters can be adjusted depending on hardware resource constraints. On an <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100</a> 80 GB GPU system, this configuration can take about 6.5 hours.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=&#91;\n        &quot;q_proj&quot;,\n        &quot;k_proj&quot;,\n        &quot;v_proj&quot;,\n        &quot;o_proj&quot;,\n        &quot;gate_proj&quot;,\n        &quot;up_proj&quot;,\n        &quot;down_proj&quot;,\n        &quot;lm_head&quot;,\n    ],\n    bias=&quot;none&quot;,\n    lora_dropout=0.05,\n    task_type=&quot;CAUSAL_LM&quot;,\n)\n\nmodel = get_peft_model(model, config)\n\n# Training configs\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_val_ds,\n    args=transformers.TrainingArguments(\n        output_dir=&quot;./mistral-code-instruct&quot;,\n        warmup_steps=5,\n        per_device_train_batch_size=2,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=1000,\n        learning_rate=2.5e-5,\n        logging_steps=50,\n        bf16=True,\n        optim=&quot;paged_adamw_8bit&quot;,\n        logging_dir=&quot;./logs&quot;,\n        save_strategy=&quot;steps&quot;,\n        save_steps=50,\n        evaluation_strategy=&quot;steps&quot;, \n        eval_steps=50,\n        report_to=&quot;none&quot;,\n        do_eval=True,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# Train! \ntrainer.train()\n</pre></div>\n\n\n<p>Using the final fine-tuning checkpoint, define the updated Mistral 7B model:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nft_model = PeftModel.from_pretrained(base_model, &quot;mistral-code-instruct/checkpoint-1000&quot;)\n</pre></div>\n\n\n<p>To evaluate the fine-tuned model\u2019s performance, ask a coding question similar to the initial one and request the generation of a code snippet:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\neval_prompt = f&quot;&quot;&quot;\nFor a given integer n, print out all its prime factors one on each line. \nn = 30\n&quot;&quot;&quot;\n\ninput_ids = tokenizer(eval_prompt, return_tensors=&quot;pt&quot;, truncation=True).input_ids.cuda()\noutputs = ft_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True, top_p=0.9,temperature=0.5)\n\n***** Output ***** \n\nGenerated response:\n2\n3\n5 \n\n#include &lt;stdio.h&gt;\n\nint main() {\n    int n = 30;\n    int i;\n    for (i = 2; i &lt;= n; i++) {\n        while (n % i == 0) {\n            printf(&quot;%d\\n&quot;, i);\n            n /= i;\n        }\n    }\n    return 0;\n}\n\n...\n</pre></div>\n\n\n<p>The generated code snippet response from the fine-tuned model looks much better. Use a sandbox environment to try the code for yourself.</p>\n\n\n\n<p>That\u2019s all there is to fine-tuning the Mistral 7B LLM. This project provides a reference workflow for your development needs. You can always choose to customize the project to better suit your enterprise data or use case. Switch out the dataset with one of your own, or fine-tune the model to another use case, such as text summarization or question-answering.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started with AI Workbench</h2>\n\n\n\n<p>NVIDIA AI Workbench helps you create, share, and scale enterprise AI and ML workflows between different GPU-enabled environments. <a href=\"https://developer.nvidia.com/ai-workbench-early-access\">Sign up for beta access to NVIDIA AI Workbench</a>. To learn more about AI Workbench, check out these resources:</p>\n\n\n\n<ul>\n<li>Watch a <a href=\"https://youtu.be/ntMRzPzSvM4?feature=shared\">video demo</a> of AI Workbench that walks through a custom image generation example project with Stable Diffusion XL.&nbsp;</li>\n\n\n\n<li>Read <a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a> to get a sneak peek of more example projects.</li>\n\n\n\n<li>Reference the <a href=\"https://docs.nvidia.com/ai-workbench/\">NVIDIA AI Workbench User Guide</a> to get your AI and ML projects up and running with NVIDIA AI Workbench.</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA AI Workbench is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning (ML) projects. Announced at SIGGRAPH 2023, NVIDIA AI Workbench enables developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. To learn more, see Develop &hellip; <a href=\"https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/\">Continued</a></p>\n", "protected": false}, "author": 1677, "featured_media": 76767, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1342644", "discourse_permalink": "https://forums.developer.nvidia.com/t/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/280710", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [3284, 1051, 2932, 1953, 1133, 559, 61, 3420, 492], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-workbench-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jX4", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76698"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1677"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76698"}], "version-history": [{"count": 69, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76698/revisions"}], "predecessor-version": [{"id": 77319, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76698/revisions/77319"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76767"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76698"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76698"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76698"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 77260, "date": "2024-01-30T12:00:00", "date_gmt": "2024-01-30T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=77260"}, "modified": "2024-01-30T17:05:31", "modified_gmt": "2024-01-31T01:05:31", "slug": "modernizing-the-data-center-with-accelerated-networking", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/", "title": {"rendered": "Modernizing the Data Center with Accelerated Networking"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/networking/\">Accelerated networking</a> combines CPUs, GPUs, <a href=\"https://blogs.nvidia.com/blog/2020/05/20/whats-a-dpu-data-processing-unit/\">DPUs</a> (data processing units), or <a href=\"https://blogs.nvidia.com/blog/what-is-a-supernic/\">SuperNICs</a> into an <a href=\"https://blogs.nvidia.com/blog/what-is-accelerated-computing/\">accelerated computing</a> fabric specifically designed to optimize networking workloads. It uses specialized hardware to offload demanding tasks to enhance server capabilities. As AI and other new workloads continue to grow in complexity and scale, the need for accelerated networking becomes paramount.</p>\n\n\n\n<p><a href=\"https://www.youtube.com/watch?v=rzdBHBx3eJk\">Data centers are the new unit of computing</a>, and modern workloads are starting to challenge network infrastructure as networking services place further strains on the CPU. The network infrastructure, with an agile, automated, and programmable framework with accelerators and offloads, is key to unlocking the full potential of AI technologies and driving innovation.&nbsp;</p>\n\n\n\n<p>This post explores the benefits and implementation tactics of accelerated networking technologies in data centers, highlighting their role in enhancing performance, scalability, and efficiency.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Accelerating your network</h2>\n\n\n\n<p>Network acceleration requires optimizing every aspect of the network, including processors, network interface cards (NICs), switches, cables, optics, and networking acceleration software. Leveraging lossless networking, remote direct memory access (RDMA), adaptive routing, congestion control, performance isolation, and in-network computing will help organizations unlock the full potential of modern applications, including AI.</p>\n\n\n\n<p>Maximum efficiency across shared networks can be obtained by properly controlling \u200cdata injection rates. When dealing with large data flows, <a href=\"https://www.nvidia.com/en-us/networking/ethernet-switching/\">Ethernet switches</a> that implement adaptive routing algorithms can dynamically load-balance the data across the network, prevent congestion, and reduce latency. Switch multipathing and packet spraying techniques can further enhance network efficiency, ensuring timely data arrival and minimizing bottlenecks. This prevents data collisions between the switch and NICs or DPUs, while traffic flow isolation techniques ensure timely delivery by preventing one flow from negatively impacting others.</p>\n\n\n\n<p>Another optimization technique is to deploy SuperNICs and DPUs. A SuperNIC is a type of network accelerator for AI cloud data centers that delivers robust and seamless connectivity between GPU servers. A DPU is a rapidly emerging class of processor that enables enhanced, accelerated networking. With the help of SuperNICs and DPUs, workloads can be offloaded from the host processor to accelerate communications, enabling data centers to cope with the ever-increasing need to move data.</p>\n\n\n\n<p>To implement accelerated networking, consider the following techniques.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Accelerated services&nbsp;</h3>\n\n\n\n<p>Workloads have undergone a significant paradigm shift, transitioning to decentralization, splitting workloads through containers and micro-segmentation. This has caused a dramatic increase in in-network bandwidth between servers (east-west traffic).&nbsp;</p>\n\n\n\n<p>AI workloads are a distributed computing problem, requiring the utilization of multiple interconnected servers or nodes. This places a tremendous strain on the network and CPU. Workload decentralization requires re-examining network infrastructure to add accelerators to relieve the CPU and GPUs from processing networking, storage, and security services. This frees the CPU to focus on application workloads. Acceleration ensures high-speed, low-latency data transfers between these nodes, and enables efficient workload distribution and faster model training.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Network abstraction&nbsp;</h3>\n\n\n\n<p>The move to highly virtualized data centers and cloud models is straining legacy networks. Traditional data center networks were not designed to support the dynamic nature of today&#8217;s virtualized workloads. Network abstraction, including network overlays, can run multiple separate, discrete virtualized network layers on top of the physical network. These are crucial in providing flexibility, scale, and acceleration. However, if not implemented properly, they can impede network flows.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Network optimization</h3>\n\n\n\n<p>A vast amount of collected and processed data has moved workloads into a data-centric era. The availability of large datasets combined with technological advances such as machine learning and generative AI increase the need for more data to feed learning algorithms. A ramification of this data explosion is the need to move, process, retrieve, and store large datasets.&nbsp;</p>\n\n\n\n<p>Lossless networking can guarantee accurate data transmission without any loss or corruption and is vital for moving, processing, retrieving, and storing these large datasets. RDMA technology enhances networking performance by enabling direct data transfers between memory locations without involving CPUs. The combination of lossless networking and RDMA can optimize data transfer efficiency and reduce CPU and GPU idle time, enabling the efficient movement of data to feed modern applications.</p>\n\n\n\n<h3 class=\"wp-block-heading\">End-to-end stack optimization</h3>\n\n\n\n<p>Modern workloads have unique network traffic patterns. Traditional workloads generate traffic patterns with many flows, small packets, and low variance. Traffic for modern applications involves large packets, fewer flows, and high variance, including elephant flows and frequent changes in traffic patterns.&nbsp;</p>\n\n\n\n<p>Adaptive routing algorithms are used to dynamically load-balance data across the network, preventing congestion and high latency for these new traffic patterns. Congestion control mechanisms, such as explicit congestion notification (ECN), also ensure efficient data flow and minimize performance degradation. To account for this, networks must be architected with an optimized end-to-end stack to accelerate new traffic patterns.</p>\n\n\n\n<h3 class=\"wp-block-heading\">In-network computing</h3>\n\n\n\n<p>The large datasets of modern workloads require ultra-fast processing of highly parallelized algorithms and therefore are more complex. As computing requirements grow, <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41552/\">in-network computing</a> offers hardware-based acceleration of collective communication operations, effectively offloading collective operations from the CPU to the network. This feature significantly improves the performance of distributed AI model training, reduces communication overhead, and accelerates model convergence to eliminate the need to send data multiple times between endpoints and accelerates network performance.</p>\n\n\n\n<p>Network acceleration reduces CPU utilization, leaving more capacity for CPUs to process application workloads. It also reduces jitter to improve data streams, and offers higher overall throughput, which enables more data to be processed faster.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Techniques for network acceleration continue to evolve and are becoming more specialized. The newest evolution will address AI workloads, which require consistent, predictable performance and compute and power efficiencies capable of running multi-tenant environments. </p>\n\n\n\n<p>To learn more about building the most efficient, high-performance networks with acceleration, see the two whitepapers, <a href=\"https://nvdam.widen.net/s/h6klwtqv5z/nvidia-spectrum-x-whitepaper-2959968\">NVIDIA Spectrum-X Network Platform Architecture</a> and <a href=\"https://nvdam.widen.net/s/bvpmlkbgzt/networking-overall-whitepaper-networking-for-ai-2911204\">Networking for the Era of AI: The Network Defines the Data Center</a>, and the ebook, <a href=\"https://www.nvidia.com/en-us/networking/data-center-transformation-with-bluefield-dpus/\">Modernize Your Data Center with Accelerated Networking</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Accelerated networking combines CPUs, GPUs, DPUs (data processing units), or SuperNICs into an accelerated computing fabric specifically designed to optimize networking workloads. It uses specialized hardware to offload demanding tasks to enhance server capabilities. As AI and other new workloads continue to grow in complexity and scale, the need for accelerated networking becomes paramount. Data &hellip; <a href=\"https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/\">Continued</a></p>\n", "protected": false}, "author": 1151, "featured_media": 77264, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1342648", "discourse_permalink": "https://forums.developer.nvidia.com/t/modernizing-the-data-center-with-accelerated-networking/280711", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1205], "tags": [3564, 2753, 1460, 1634, 608], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/highway-lights.png", "jetpack_shortlink": "https://wp.me/pcCQAL-k68", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77260"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1151"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=77260"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77260/revisions"}], "predecessor-version": [{"id": 77289, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77260/revisions/77289"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77264"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=77260"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=77260"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=77260"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75844, "date": "2024-01-29T09:00:00", "date_gmt": "2024-01-29T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75844"}, "modified": "2024-01-30T10:10:43", "modified_gmt": "2024-01-30T18:10:43", "slug": "emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/", "title": {"rendered": "Emulating the Attention Mechanism in Transformer Models with a Fully Convolutional Network"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The past decade has seen a remarkable surge in the adoption of deep learning techniques for computer vision (CV) tasks. Convolutional neural networks (CNNs) have been the cornerstone of this revolution, exhibiting exceptional performance and enabling significant advancements in visual perception.&nbsp;</p>\n\n\n\n<p>By employing localized filters and hierarchical architectures, CNNs have proven adept at capturing spatial hierarchies, detecting patterns, and extracting informative features from images, as explained in <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noreferrer noopener\">Deep Residual Learning for Image Recognition</a>. Convolutional layers exhibit translation equivariance, enabling them to generalize to translations and spatial transformations. However, despite their success, CNNs exhibit limitations in capturing long-range dependencies and global contextual understanding, which become increasingly crucial in complex scenes or tasks requiring fine-grained understanding.&nbsp;&nbsp;</p>\n\n\n\n<p>In contrast, transformers have emerged as a compelling alternative architecture in computer vision, driven by their success in natural language processing (NLP), as explained in <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\">Attention Is All You Need</a>. By eschewing local convolutions, transformers offer a self-attention mechanism that supports global relationships among visual features. The attention mechanism enables transformers to capture long-range interactions between image elements, facilitating a more holistic understanding of the visual scene that leads to better accuracy. Figure 1 shows an example of self-attention for vision applications.&nbsp;For more details, see <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/2010.11929\" target=\"_blank\">An Image is Worth 16&#215;16 Words: Transformers for Image Recognition at Scale</a> and <a rel=\"noreferrer noopener\" href=\"https://www.computer.org/csdl/proceedings-article/iccv/2021/281200j992/1BmGKZoEzug\" target=\"_blank\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>.</p>\n\n\n\n<p>Self-attention, nonetheless, encounters challenges in effectively capturing local contextual information within images, accentuating the significance of broader global receptive fields. Additionally, the computational complexity associated with self-attention, characterized by quadratic interactions between visual feature elements, poses significant challenges for handling large images in computer vision.&nbsp;</p>\n\n\n\n<p>At the frontier of innovation, the automotive industry is increasingly recognizing the need for the widespread incorporation of transformer-like networks. However, the integration of these networks presents unique challenges. Specifically, within the <a href=\"https://developer.nvidia.com/tensorrt\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA TensorRT</a> framework for certain operating systems on <a href=\"https://developer.nvidia.com/drive\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA DRIVE</a> products, limited specialized functionality is supported as compared to standard use cases.&nbsp;</p>\n\n\n\n<p>These specialized APIs still include highly optimized convolution operations, among others, reflecting the industry&#8217;s long-standing commitment to refining convolutional networks. We aim to harness these optimized convolution operations strategically to drive more efficient and effective implementation of transformer networks. Our goal is to empower the automotive industry to meet the dynamic demands of modern applications while working harmoniously within the confines of existing software frameworks and hardware platforms.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"5469\" height=\"2309\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block.png\" alt=\"Illustration of a transformer-based model.\" class=\"wp-image-77216\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block.png 5469w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-625x264.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-768x324.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-1536x648.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-2048x865.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-645x272.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-500x211.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-362x153.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-261x110.png 261w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/self-attention-typical-transformer-block-1024x432.png 1024w\" sizes=\"(max-width: 5469px) 100vw, 5469px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An illustration of self-attention in a typical transformer block</em>&nbsp;<em>for vision applications</em></figcaption></figure></div>\n\n\n<p>While recognizing the value of \u200cself-attention, it is imperative to place greater emphasis on the impact of convolutions, especially in CV tasks for. This is true for the following reasons:&nbsp;</p>\n\n\n\n<ol>\n<li>The inherent characteristic differences between images and texts, as previously mentioned, highlight the challenges of applying self-attention directly to CV tasks, calling for hybrid approaches or alternative architectures that combine the strengths of self-attention and convolutional layers.&nbsp;&nbsp;</li>\n\n\n\n<li>In autonomous vehicle (AV) applications, high-resolution images are often used in real-time applications. The optimization of self-attention computation on hardware platforms has fallen behind the rapid emergence of new transformers by AV industry and chip makers, failing to meet user demands. Current implementations of transformer-based models inadequately leverage the computational capabilities of GPUs.&nbsp;</li>\n\n\n\n<li>In many cases of production in autonomous driving, inference performed in the restricted mode of deep learning runtime libraries may not yet have full support of state-of-the-art transformer networks. For example, current operations in transformers are not fully covered in TensorRT restricted mode.&nbsp;</li>\n</ol>\n\n\n\n<p>This post presents our recent work on emulating the attention mechanism in transformer models using a fully convolutional network. Our method combines the strengths of conventional convolution kernels optimized for current GPU hardware platforms with self-attention modules, resulting in superior performance compared to contemporary transformer-like models. Our work addresses the increasing user demand for transformer usability in various industries with computer vision problems. Our method not only provides the fastest latency performance with comparable accuracy when running on TensorRT, but is also fully compatible in TensorRT restricted mode.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Fusing convolutions and self-attention&nbsp;&nbsp;</h2>\n\n\n\n<p>Recent research shows a growing interest in merging the strengths of CNNs and transformers. By combining convolution operations for local feature information with self-attention modules for global feature relations, researchers aim to enhance the capabilities of both architectures.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.computer.org/csdl/proceedings-article/iccv/2021/281200j992/1BmGKZoEzug\" target=\"_blank\" rel=\"noreferrer noopener\">Swin Transformer</a> is one notable example. This recent vision-transformer \u200cintroduces the concept of shifted windows to enable transformers to effectively learn local features. By incorporating local self-attention within smaller regions, Swin captures local relationships and dependencies, thereby improving performance in tasks requiring fine-grained information. However, a challenge arises with the quadratic increase in computational complexity of self-attention as input sizes grow, which can quickly impose latency burdens.&nbsp;&nbsp;</p>\n\n\n\n<p>To address this issue, researchers have explored merging convolution operations and self-attentions. Convolution-based approaches mimic transformer training configurations or selectively use convolutions and self-attentions in various parts of networks.</p>\n\n\n\n<p>For example, <a href=\"https://arxiv.org/abs/2103.15808\" target=\"_blank\" rel=\"noreferrer noopener\">Convolutional Vision Transformer (CvT)</a> intuitively incorporated convolutional features into self-attention modules. <a href=\"https://arxiv.org/abs/2201.03545\" target=\"_blank\" rel=\"noreferrer noopener\">Conv-Next</a>, on the other hand, resembles vision transformers with conventional CNNs. Nonetheless, the approach fails to explicitly address the limited receptive fields commonly encountered in traditional convolutional network models. Unlike self-attention, convolutional operations possess a fixed receptive field size and a shared set of parameters. This characteristic enables convolutions to process input data in a locally focused and parameter-efficient manner.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Convolutional Self-Attention&nbsp;</h2>\n\n\n\n<p>We present Convolutional Self-Attention (CSA), which completely replaces conventional attention mechanisms with convolution operations for vision tasks, enabling the modeling of both local and global feature relations. By relying solely on convolutions, our overall model achieves remarkable efficiency on highly optimized GPUs and deep learning accelerators. Experimental results convincingly demonstrate its competitive accuracy in comparison to contemporary transformer networks, while displaying improved hardware utilization and significantly reduced deployment latency. &nbsp;</p>\n\n\n\n<p>The overall proposed model consists of repetitive uses of down-sampling convolution layers and our proposed CSA blocks along its feed-forwarding flow, as depicted in Figure 2. Each CSA block emulates a transformer block employing convolution operations.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"3309\" height=\"494\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption.png\" alt=\"Illustration of the overall proposed model and our proposed CSA blocks along its feed-forwarding flow.\" class=\"wp-image-75849\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption.png 3309w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-300x45.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-625x93.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-179x27.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-768x115.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-1536x229.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-2048x306.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-645x96.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-500x75.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-160x24.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-362x54.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-737x110.png 737w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure2_noCaption-1024x153.png 1024w\" sizes=\"(max-width: 3309px) 100vw, 3309px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Our network inference flow with convolutional self-attention blocks</em></figcaption></figure></div>\n\n\n<p>Figure 3 shows the structure and flow of the CSA module. The CSA blocks can differ in implementation but are designed to emulate the relational encoding process of self-attention. To achieve relational encoding, we rotate the tensor along the channel-axis, converting channel features into spatial format (height and width).&nbsp;</p>\n\n\n\n<p>This rotated feature tensor is elementwise multiplied with the original tensor before rotation, followed by convolutions. This replicates the first inner product of self-attention, but with a difference in concept, as our method allows for one-to-many relational embedding through elementwise multiplication and convolution. The resulting relational feature tensor is then normalized, activated, and multiplied with another visual feature from the input tensor, <em>value </em>(V).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"4768\" height=\"2643\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption.png\" alt=\"Illustration of the structure and flow of the CSA module.\" class=\"wp-image-75850\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption.png 4768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-300x166.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-625x346.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-768x426.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-1536x851.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-2048x1135.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-645x358.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-500x277.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-362x201.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-198x110.png 198w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure3_noCaption-1024x568.png 1024w\" sizes=\"(max-width: 4768px) 100vw, 4768px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. An illustration of a convolutional self-attention module</em></figcaption></figure></div>\n\n\n<p>Our method achieves a global receptive field by strategically rearranging feature tensors and utilizing local convolution kernel windows. This explicit relational encoding enables each feature pixel to be projected to all others, resulting in comprehensive inter-pixel interactions. This is because the structural rearrangements of tensors in our approach enable convolution windows to capture global relationships among visual features, leveraging the strengths of convolutional operations for one-to-many visual feature relational reasoning.&nbsp;</p>\n\n\n\n<p>In comparison, CSA modules encode relations among all feature pixels through inner-product operations, which can impose significant computational burdens on hardware. By achieving one-to-all relational encoding, our approach reduces the computational load while preserving the ability to capture long-range dependencies and structural information across the entire feature map.&nbsp;</p>\n\n\n\n<p>To manage the quadratic computational increments resulting from increasing input sizes, our design can incorporate spatial reduction layers to reduce the tensor size, as illustrated in Figure 4. This not only helps decrease computational overhead but also enables the network to focus on regional relationships among visual features, which carries more semantics, rather than pixel-level relationships.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"5206\" height=\"2642\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption.png\" alt=\"Illustration of our design can incorporate spatial reduction layers to reduce the tensor size.\" class=\"wp-image-75851\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption.png 5206w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-300x152.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-625x317.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-768x390.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-1536x780.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-2048x1039.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-645x327.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-500x254.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-362x184.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-217x110.png 217w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure4_noCaption-1024x520.png 1024w\" sizes=\"(max-width: 5206px) 100vw, 5206px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Use of spatial reduction for cases with large input size</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Performance in accuracy and latency&nbsp;</h2>\n\n\n\n<p>The CSA module is compared against relevant contemporary CV classification models with the ImageNet-1K dataset in terms of accuracy against the validation data and latency measured with TensorRT-8.6.11.4. We target for AV application of CSA with restricted mode in mind, so the models are compared on the NVIDIA DRIVE Orin platform. NVIDIA DRIVE Orin is a high-performance and energy-efficient system-on-a-chip (SoC) and is part of the NVIDIA DRIVE platform for use in autonomous vehicles.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Benchmark entries&nbsp;</h3>\n\n\n\n<ul>\n<li>The Swin Transformer network is an innovative deep learning architecture that combines self-attention mechanisms, originally popularized by models like the vision transformer (ViT), with a hierarchical and parallelizable design.&nbsp;</li>\n\n\n\n<li>The ConvNext model is developed through a progressive transformation of a standard ResNet to resemble a vision Transformer, competing favorably with Swin Transformers in specific tasks while maintaining the simplicity and efficiency of conventional convolutional networks.&nbsp;</li>\n\n\n\n<li>The Convolutional Vision Transformer<strong> </strong>(CvT) enhances performance and efficiency by incorporating convolutions into ViT. CvT performs very competitively on ImageNet-1k with fewer parameters and lower GMACs.&nbsp;&nbsp;</li>\n</ul>\n\n\n\n<p>The rationale for the benchmark entries lies in their relevance to CSA and contemporary significance. In our experiments, we specifically compared CSA with the benchmarks of Swin-tiny, ConvNext-tiny, and CvT-13 that share similar model sizes, which is succinctly shown in Table 1.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"645\" height=\"292\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1.png\" alt=\"A table showing model size and safety compliance of all model entries.\" class=\"wp-image-76412\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-300x136.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-625x283.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-500x226.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-362x164.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/model-size-safety-compliance-table-1-243x110.png 243w\" sizes=\"(max-width: 645px) 100vw, 645px\" /><figcaption class=\"wp-element-caption\"><em>Table 1. Model size and safety compliance of all model entries</em></figcaption></figure></div>\n\n\n<p>We are presenting our results using two kinds of precision modes, FP16 and mixed precision (that is, with FP32, FP16, and INT8 all enabled) in TensorRT. This approach allows us to provide a balanced assessment of our models&#8217; performance. The model quantization for all methods was achieved using Post-Training Quantization (PTQ) and 500 images were used during the calibration process.&nbsp;</p>\n\n\n\n<p>We measure accuracy using the Top-1 accuracy on the ImageNet dataset and report inference latency in milliseconds with various batch sizes of 1, 4, 8, and 16. This ensures that our comparison is conducted in a fair and unbiased manner. One can further optimize the TensorRT inference of CSA to strike a balance between accuracy and latency, with the aim of exploiting latency within acceptable accuracy constraints. &nbsp;</p>\n\n\n\n<p>In the pursuit of optimizing precision modes to enhance latency at the potential expense of accuracy, quantization strategies emerge as a compelling solution. TensorRT offers a diverse array of quantization methods, including percentile, mean squared error (MSE), and entropy quantization, all of which demonstrate effectiveness in mitigating precision loss. In our study, which centers on mixed-precision inferences across a range of benchmark methods, we selected the entropy quantization methodology. This approach, grounded in information theory, allocates codes to minimize the average code word length, resulting in minimal accuracy degradation, with the noteworthy exception of the CvT benchmark.&nbsp;</p>\n\n\n\n<p>While all benchmarks, including CSA, maintain an indistinguishable level of accuracy even after precision reduction, ConvNext outperforms the other benchmarks to a slight extent despite the reduced precisions. Conversely, CSA exhibits the smallest drop in accuracy.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"3505\" height=\"1863\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison.png\" alt=\"TensorRT accuracy benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\" class=\"wp-image-76406\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison.png 3505w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-300x159.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-625x332.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-768x408.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-1536x816.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-2048x1089.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-645x343.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-500x266.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-362x192.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-207x110.png 207w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-accuracy-comparison-1024x544.png 1024w\" sizes=\"(max-width: 3505px) 100vw, 3505px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. TensorRT accuracy comparison for ImageNet-1K dataset</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"6129\" height=\"3251\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts.png\" alt=\"TensorRT latency benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\" class=\"wp-image-76408\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts.png 6129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-300x159.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-625x332.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-768x407.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-1536x815.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-2048x1086.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-645x342.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-500x265.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-362x192.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-207x110.png 207w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tensorrt-latency-benchmark-charts-1024x543.png 1024w\" sizes=\"(max-width: 6129px) 100vw, 6129px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. TensorRT-8.6.11.4 latency comparison for 224 x 224 inputs with the batch sizes of 1, 4, 8, and 16</em></figcaption></figure></div>\n\n\n<p>ConvNext stands out for its accuracy in both FP16 and mixed-precision modes but comes with the trade-off of comparably slow latency. In this context, CSA emerges as a highly competitive option, offering commendable accuracy, while achieving the fastest latency.&nbsp;</p>\n\n\n\n<p>Compared to ConvNext-tiny, CSA delivers a remarkable 49% improvement in latency for the case of batch size of one while maintaining its strong accuracy performance. This underscores CSA&#8217;s impressive capabilities and positions it as a strong choice within this context.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"3573\" height=\"1636\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart.png\" alt=\"Memory bandwidth benchmark comparing CSA against Swin-tiny, ConvNext-tiny, and CvT-13.\" class=\"wp-image-76410\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart.png 3573w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-300x137.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-625x286.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-768x352.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-1536x703.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-2048x938.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-645x295.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-500x229.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-362x166.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-240x110.png 240w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/average-memory-bw-per-frame-chart-1024x469.png 1024w\" sizes=\"(max-width: 3573px) 100vw, 3573px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Average memory bandwidth per frame for batch sizes of 1, 4, 8, and 16</em></figcaption></figure></div>\n\n\n<p>CSA outperforms the benchmarks for efficient memory traffic during inference, resulting in the least bandwidth of average memory per frame. It should be noted that CSA\u2019s memory traffic is persistent even as the batch size grows, while the other methods in the benchmark are growing gradually.&nbsp;&nbsp;</p>\n\n\n\n<p>Thanks to the strategic design of CSA, our model can leverage efficient convolution kernel implementation in TensorRT, resulting in highly efficient computations that strike a harmonious balance between accuracy and latency, while being fully compatible with TensorRT restricted mode. Other methods, either with higher accuracy or lower latency, are not compatible. In practice, this makes it currently difficult to deploy those models in production where the TensorRT restricted model is required.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion&nbsp;</h2>\n\n\n\n<p>Unlike other convolutional models that try to ingest the attention module from a transformer model, Convolutional Self-Attention (CSA) explicitly finds relationships among features one-to-many with only convolutions in conjunction with simple tensor shape manipulations. The differences between our method and relevant methods listed below:&nbsp;</p>\n\n\n\n<ol>\n<li>By strategically rearranging feature tensors, explicit relational encoding ensures that each feature pixel is projected to all others, achieving a global receptive field while utilizing local convolution kernel windows.&nbsp;</li>\n\n\n\n<li>In contrast to conventional self-attention modules that encode relations among all input features with increase computational cost with respect to the input size, our method succinctly achieves all-to-all relational encoding with convolution operations in a hierarchical manner at each stage with reduced input size, which lower the computational load on hardware.&nbsp;</li>\n\n\n\n<li>These advantages enable faster inference speed for models of comparable size and match or exceed the performance of other methods.&nbsp;</li>\n</ol>\n\n\n\n<p>More importantly, CSA operates without bells and hassles in TensorRT restricted model, making it suitable for AV production for safety-critical applications. We expect CSA to serve as a reference model design for our customers who are using the NVIDIA DRIVE platform and beyond.&nbsp;&nbsp;For more information, visit the NVIDIA Developer <a href=\"https://forums.developer.nvidia.com/c/autonomous-vehicles/517\" target=\"_blank\" rel=\"noreferrer noopener\">AV Forum</a> and <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/deep-learning/tensorrt/92\" target=\"_blank\" rel=\"noreferrer noopener\">TensorRT Forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The past decade has seen a remarkable surge in the adoption of deep learning techniques for computer vision (CV) tasks. Convolutional neural networks (CNNs) have been the cornerstone of this revolution, exhibiting exceptional performance and enabling significant advancements in visual perception.&nbsp; By employing localized filters and hierarchical architectures, CNNs have proven adept at capturing spatial &hellip; <a href=\"https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/\">Continued</a></p>\n", "protected": false}, "author": 1977, "featured_media": 75872, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1341315", "discourse_permalink": "https://forums.developer.nvidia.com/t/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/280505", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758], "tags": [296, 501, 2892, 2850, 3545, 367, 2143], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/NVIDIA-Convolution-Self-Attention-Blocks.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jJi", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75844"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1977"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75844"}], "version-history": [{"count": 26, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75844/revisions"}], "predecessor-version": [{"id": 77296, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75844/revisions/77296"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75872"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75844"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75844"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75844"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 77171, "date": "2024-01-26T10:00:00", "date_gmt": "2024-01-26T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=77171"}, "modified": "2024-01-30T17:05:12", "modified_gmt": "2024-01-31T01:05:12", "slug": "new-self-paced-course-augment-your-llm-using-retrieval-augmented-generation", "status": "publish", "type": "post", "link": "https://nvda.ws/3u4liEM", "title": {"rendered": "New Self-Paced Course: Augment Your LLM Using Retrieval-Augmented Generation"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn the basics of retrieval-augmented generation (RAG), an end-to-end architecture used to optimize the output of an LLM.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn the basics of retrieval-augmented generation (RAG), an end-to-end architecture used to optimize the output of an LLM.</p>\n", "protected": false}, "author": 1466, "featured_media": 77176, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3u4liEM", "_links_to_target": "_blank"}, "categories": [3110], "tags": [2964, 1935, 2932, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/retrieval-augmented-generation-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-k4H", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77171"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=77171"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77171/revisions"}], "predecessor-version": [{"id": 77188, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77171/revisions/77188"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77176"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=77171"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=77171"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=77171"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76670, "date": "2024-01-25T10:30:00", "date_gmt": "2024-01-25T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76670"}, "modified": "2024-01-25T10:41:47", "modified_gmt": "2024-01-25T18:41:47", "slug": "announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/", "title": {"rendered": "Announcing NVIDIA Metropolis Microservices for Jetson for Rapid Edge AI Development"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Building vision AI applications for the edge often comes with notoriously long and costly development cycles. At the same time, quickly developing edge AI applications that are cloud-native, flexible, and secure has never been more important. Now, a powerful yet simple API-driven edge AI development workflow is available with the new NVIDIA Metropolis microservices.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/metropolis-microservices\">NVIDIA Metropolis microservices</a> is a suite of customizable, cloud-native building blocks for developing vision AI applications and solutions. This release introduces an expanded set of APIs and microservices on the <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">NVIDIA Jetson</a> platform to further accelerate the development and deployment of vision AI applications at the edge.&nbsp;</p>\n\n\n\n<p>These new Jetson microservices empower developers to modernize their AI application stack, streamline processes, and safeguard applications for the future. You can easily incorporate the latest in <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> advancements through APIs and microservices such as video storage and management, prebuilt AI perception pipelines, tracking algorithms, system monitoring, IoT services for secure edge-to-cloud connectivity, and more.&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/metropolis-microservices/jetson-application-form\">Download NVIDIA Metropolis microservices for Jetson</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"622\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic.png\" alt=\"A graphic that compares using a do-it-yourself approach, where developers have to create all the pieces from scratch, and using prebuilt NVIDIA Metropolis microservices, which reduce development time from years to months. \" class=\"wp-image-76862\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-300x93.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-625x194.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-179x56.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-768x239.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-1536x478.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-645x201.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-500x156.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-160x50.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-362x113.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-354x110.png 354w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-diy-comparison-graphic-1024x319.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Develop edge AI applications faster with NVIDIA Metropolis microservices</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Release highlights</h2>\n\n\n\n<p>Production-ready edge AI applications require numerous components, including AI models, optimized processing and inference pipelines, glue logic, security measures, cloud connectivity, and more. NVIDIA Metropolis microservices for Jetson streamline the application development process by offering pre-built microservices for the most ubiquitous components using a cloud-native, modular, and extensible architecture.&nbsp;&nbsp;</p>\n\n\n\n<p>The flexibility of the platform enhances development efficiency, with standard APIs seamlessly integrating with other applications and services. The platform also provides essential services such as IoT, security, and monitoring, offering ready-to-use core components for production applications.&nbsp;</p>\n\n\n\n<p>With access to more than 15 microservices across Application, Platform services, and Cloud services, developers are freed to concentrate on building intellectual property and achieving differentiation in the market.</p>\n\n\n\n<p>NVIDIA partners are incorporating NVIDIA Metropolis microservices into their offerings, including <a href=\"https://www.aaeon.ai/en/product/aaeon-nvidia-ai\">AAEON</a>, <a href=\"https://www.aetina.com/products-features.php?t=337\">Aetina</a>, <a href=\"https://www.advantech.com/en/campaign/industrial-ai-iot-solution-nvidia\">Advantech</a>, <a href=\"https://www.allxon.com/jetson\">Allxon</a>, <a href=\"https://crg.co.il/catalog_category/autonomous-machines/\">CRG</a>, <a href=\"https://www.cvedia.com/\">CVEDIA</a>, <a href=\"https://namla.cloud/\">Namla</a>, <a href=\"https://rebotnix.com/\">Rebotnix</a>, <a href=\"https://www.ridgerun.com/nvidia\">RidgeRun</a>, <a href=\"https://www.seeedstudio.com/tag/nvidia.html\">Seeed Studio</a>, and <a href=\"https://siliconhighway.com/\">Silicon Highway</a>. More are added daily.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Reference workflows and applications</h2>\n\n\n\n<p>Two reference applications are included to help get you started with NVIDIA Metropolis microservices for Jetson: the AI-enabled network video recorder (AI-NVR) and a generative AI application with zero-shot detection. These workflows show how the microservices and APIs come together to build complete applications from video ingestion, AI inference, analytics, and monitoring to securely connecting to the cloud.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">AI-enabled network video recorder&nbsp;</h3>\n\n\n\n<p>The AI-NVR reference workflow brings nearly all the microservices together in one comprehensive app. It includes:</p>\n\n\n\n<ul>\n<li>&nbsp;Video ingestion and storage using the Video Storage Toolkit (VST) microservice</li>\n\n\n\n<li>People detection and tracking with the AI Perception service with <a href=\"https://developer.nvidia.com/deepstream-sdk\">NVIDIA DeepStream</a></li>\n\n\n\n<li>Line crossing and Region of Interest (ROI) insights and alerts using the Analytics service&nbsp;</li>\n</ul>\n\n\n\n<p>An Android reference mobile application is provided to demonstrate the use of APIs to build client applications. To learn more, check out the NVIDIA On-Demand playlist, <a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-0c90eddb-9fdf-43aa-9c5e-aac8418659d4/aa-9c5e-aac8418659d4/\">AI-NVR Using Metropolis Microservices for Jetson</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"612\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture.png\" alt=\"Graphic showing the complete cloud-native architecture of AI-NVR application showing VST, AI perception service, analytics service, and all the other platform services. \n\" class=\"wp-image-76865\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-300x92.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-625x191.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-179x55.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-768x235.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-1536x470.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-645x197.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-500x153.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-160x49.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-362x111.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-359x110.png 359w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ai-enabled-network-video-recorder-architecture-1024x314.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The AI-enabled network video recorder (AI-NVR) application architecture</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Zero-shot detection using generative AI</h3>\n\n\n\n<p>Metropolis microservices for Jetson enables developers to prototype and productize generative AI applications for the edge. The generative AI reference application enables zero-shot detection of live streaming data. Models can detect any objects specified with a prompt.&nbsp;</p>\n\n\n\n<p>Prompts can be made remotely over REST APIs to the AI Perception service to dynamically change classes to detect. Generative AI enables a new breed of AI-powered applications for the edge.&nbsp;To learn more about generative AI with Metropolis microservices, see <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/\">Bringing&nbsp;Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1384\" height=\"652\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection.png\" alt=\"Graphic showing the cloud-native architecture of the generative AI reference application for zero-shot object detection using a visual prompting agent.\n\" class=\"wp-image-76868\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection.png 1384w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-625x294.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-768x362.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-645x304.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-500x236.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-233x110.png 233w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generative-ai-reference-application-zero-shot-object-detection-1024x482.png 1024w\" sizes=\"(max-width: 1384px) 100vw, 1384px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Generative AI reference application for zero-shot object detection</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Powerful microservices and APIs</h2>\n\n\n\n<p>Metropolis microservices for Jetson is a collection of feature-rich microservices and APIs, including application services, platform and Board Support Package (BSP) services, and cloud services. The modular and extensible microservices make it easy to build modern cloud-native applications for the edge.&nbsp;</p>\n\n\n\n<p>As a developer, you have the flexibility to choose one, several, or all of the services depending on the maturity of your product.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"5737\" height=\"3695\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1.png\" alt=\"Graphic showing the complete software stack from the reference AI workflow, application microservice, platform, and BSP services to cloud services. \" class=\"wp-image-77040\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1.png 5737w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-300x193.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-768x495.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-1536x989.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-2048x1319.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-645x415.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-466x300.png 466w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-362x233.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-171x110.png 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-jetson-software-stack-1-1024x660.png 1024w\" sizes=\"(max-width: 5737px) 100vw, 5737px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Metropolis microservices for Jetson software stack</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Application services</h3>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-238cd4a8-7f1d-42b4-9b0a-a949ade92845/\">Video Storage Toolkit</a>: Service for video ingestion and storage</li>\n\n\n\n<li>AI Perception service using NVIDIA DeepStream: For AI inference, object tracking and metadata creation&nbsp;</li>\n\n\n\n<li>AI Perception service for zero-shot detection: For generative AI inference with the NanoOWL model and visual prompting</li>\n\n\n\n<li>Analytics service: Object counting analytics such as line crossing, Region of Interest, and Field of View</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Platform services</h3>\n\n\n\n<ul>\n<li>Redis: Global message bus for inter-process communication</li>\n\n\n\n<li>API Gateway: Maps incoming API requests to the subsequent services&nbsp;</li>\n\n\n\n<li>Monitoring: Monitor and visualize edge device status such as utilization and app KPIs</li>\n\n\n\n<li>IoT Gateway: Secure bidirectional communication between edge and cloud</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Cloud services</h3>\n\n\n\n<ul>\n<li>IoT Cloud: Create secure connection from cloud to edge, including authentication and authorization</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>NVIDIA Metropolis microservices fast-tracks vision AI development for the edge, providing ready-to-use applications, over 15 microservices for platform services, pixel perception, video storage, analytics, and more. <a href=\"https://developer.nvidia.com/metropolis-microservices/jetson-application-form\">Download NVIDIA Metropolis microservices for Jetson</a>.&nbsp;</p>\n\n\n\n<p>To learn about the technical details of Metropolis microservices for Jetson, read the <a href=\"https://resources.nvidia.com/en-us-metropolis-microservices-for-jetson/jetson-whitepaper\">NVIDIA Metropolis Microservices for Jetson whitepaper</a>.&nbsp;</p>\n\n\n\n<p>For a tutorial on building applications using Metropolis APIs, see <a href=\"https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/\">Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs</a>. To learn even more, register to join us for the two-part webinar, <a href=\"https://info.nvidia.com/metropolis-on-jetson-webinar.html\">Accelerate Edge AI Development With Metropolis APIs and Microservices for Jetson</a> (Part 1) and <a href=\"https://info.nvidia.com/build-with-metropolis-microservices-for-jetson.html\">How to Build With Metropolis Microservices for Jetson</a> (Part 2).&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Building vision AI applications for the edge often comes with notoriously long and costly development cycles. At the same time, quickly developing edge AI applications that are cloud-native, flexible, and secure has never been more important. Now, a powerful yet simple API-driven edge AI development workflow is available with the new NVIDIA Metropolis microservices.&nbsp;&nbsp;&nbsp; NVIDIA &hellip; <a href=\"https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/\">Continued</a></p>\n", "protected": false}, "author": 676, "featured_media": 76961, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338054", "discourse_permalink": "https://forums.developer.nvidia.com/t/announcing-nvidia-metropolis-microservices-for-jetson-for-rapid-edge-ai-development/279874", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 852, 2758, 3110, 63, 1903], "tags": [422, 453, 347, 2792, 1472], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-iva-microservices-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jWC", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76670"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/676"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76670"}], "version-history": [{"count": 49, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76670/revisions"}], "predecessor-version": [{"id": 77159, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76670/revisions/77159"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76961"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76670"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76670"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76670"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76666, "date": "2024-01-25T10:00:00", "date_gmt": "2024-01-25T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76666"}, "modified": "2024-01-29T14:41:19", "modified_gmt": "2024-01-29T22:41:19", "slug": "advancing-production-ai-with-nvidia-ai-enterprise", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/", "title": {"rendered": "Advancing Production AI with NVIDIA AI Enterprise"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>While harnessing the potential of AI is a priority for many of today\u2019s enterprises, developing and deploying an AI model involves time and effort. Often, challenges must be overcome to move a model into production, especially for mission-critical business operations. According to <a href=\"https://resources.nvidia.com/en-us-idc-infobrief\">IDC research</a>, only 18% of enterprises surveyed could put an AI model into production in under a month.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>This post explores the challenges that slow down AI deployments and introduces the benefits of using a consistent, secure, and reliable platform to accelerate the journey of taking AI into production.</p>\n\n\n\n<h2 class=\"wp-block-heading\">The ever-growing complexity of the AI software stack</h2>\n\n\n\n<p>Open-source software (OSS) plays a critical role in advancing AI adoption. According to <a href=\"https://github.blog/2023-11-08-the-state-of-open-source-and-ai/\">The State of the Octoverse 2023</a>, there were 65K public generative AI-related GitHub projects in 2023 with 248% year-over-year growth. While the open-source community has helped fuel the AI era, the diverse range of OSS used in building AI applications makes maintaining a reliable, enterprise-grade AI software stack a complex and resource-intensive endeavor that is similar to maintaining an open-source OS.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>For example, <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, used to standardize and scale AI deployments, relies on countless software dependencies. In Figure 1, green dots represent CUDA libraries, white dots represent OSS packages, and the lines in between represent dependencies. Any single change, such as a regular software update or security patch, can introduce an API change and result in an application failure or downtime.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"607\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-625x607.png\" alt=\"A graphic representation of NVIDIA Triton Inference Server software dependencies. Green dots represent CUDA libraries, white dots represent OSS packages, and the lines in between represent dependencies.\n\" class=\"wp-image-76900\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-625x607.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-300x291.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-118x115.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-768x745.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-1536x1491.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-645x626.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-309x300.png 309w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-93x90.png 93w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-362x351.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-113x110.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies-1024x994.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-inference-server-software-dependencies.png 1638w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1.&nbsp; Software dependencies of NVIDIA Triton Inference Server</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Continuous security monitoring&nbsp;</h2>\n\n\n\n<p>The inevitable increase in security vulnerabilities makes maintaining the AI software stack even more challenging. A <a href=\"https://www.synopsys.com/software-integrity/engage/ossra/rep-ossra-2023-pdf#rep-ossra-2023.indd%3A.122237%3A3001\">recent open-source security and risk analysis report</a> by Synopsys, indicates a 236% surge in high-risk attack patterns in OSS vulnerabilities for big data, AI, Business Intelligent, and machine learning over a 5-year period.&nbsp;</p>\n\n\n\n<p>New vulnerabilities are constantly being discovered. For example, Figure 2 shows a comparison of security scanning results for the NVIDIA Triton container. In just over 3 weeks, one critical vulnerability was identified. In addition, the number of high vulnerabilities grew from four to 11. Continuous monitoring and rapid response times to fix vulnerabilities are critical for maintaining business continuity.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1947\" height=\"672\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1.png\" alt=\"Two screenshots showing that the vulnerabilities of NVIDIA Triton increased in 3 weeks.\" class=\"wp-image-76904\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1.png 1947w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-300x104.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-625x216.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-768x265.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-1536x530.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-645x223.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-500x173.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-362x125.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-319x110.png 319w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-triton-security-scan-results-1-1024x353.png 1024w\" sizes=\"(max-width: 1947px) 100vw, 1947px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Security scan results comparison for NVIDIA Triton&nbsp;</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">NVIDIA AI Enterprise for production AI&nbsp;</h2>\n\n\n\n<p>To help address these challenges, NVIDIA introduced&nbsp; <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, &nbsp;an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade AI.&nbsp; Built on open source and curated, optimized, and supported by NVIDIA, the NVIDIA AI Enterprise software platform enables developers to focus on building and deploying new AI services.&nbsp;</p>\n\n\n\n<p>NVIDIA AI Enterprise includes three supported branches: production branches, feature branches, and long-term support branches. Customers have access to all three branches and can use any mix of the three.</p>\n\n\n\n<p><strong>Production branches </strong>ensure API stability and regular security updates; ideal for deploying AI in production when stability is required. Released every 6 months with a 9-month lifecycle.</p>\n\n\n\n<p><strong>Feature branches</strong> include the top-of-tree software updates; ideal for AI developers who want the faster-moving, latest development environment. Released monthly.</p>\n\n\n\n<p><strong>Long-term support branches</strong> are ideal for highly regulated industries. Released every 2.5 years with a lifecycle of up to 3 years.</p>\n\n\n\n<h3 class=\"wp-block-heading\">API stability and security</h3>\n\n\n\n<p>Throughout the 9-month lifecycle of each NVIDIA AI Enterprise production branch, NVIDIA continuously monitors critical and high common vulnerabilities and exposures (CVEs) and releases monthly security patches (Figure 3). By doing so, the AI frameworks, libraries, models, and tools included in NVIDIA AI Enterprise can be updated for security fixes while eliminating the risk of breaking an API.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1601\" height=\"407\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline.png\" alt=\"Graphic of NVIDIA AI Enterprise production branch lifecycle timeline.\" class=\"wp-image-76906\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline.png 1601w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-300x76.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-625x159.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-179x46.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-768x195.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-1536x390.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-645x164.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-500x127.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-160x41.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-362x92.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-433x110.png 433w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-lifecycle-timeline-1024x260.png 1024w\" sizes=\"(max-width: 1601px) 100vw, 1601px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. NVIDIA AI Enterprise production branch lifecycle timeline</em></figcaption></figure></div>\n\n\n<p>Figure 4 compares the version of Triton available through the production branch release of NVIDIA AI Enterprise to the open-source version of Triton. The commercial version available with the production branch of NVIDIA AI Enterprise has zero critical and high vulnerabilities, while the open-source version has nine high vulnerabilities.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"847\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison.png\" alt=\"Two screenshots of vulnerability scanning results of two PyTorch images. One from NGC, and one from NVIDIA AI Enterprise.\n\" class=\"wp-image-76909\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-625x265.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-768x325.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-1536x651.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-645x273.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-500x212.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-362x153.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-260x110.png 260w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/triton-security-scan-results-comparison-1024x434.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Triton security scan results comparison</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Security through transparency&nbsp;</h3>\n\n\n\n<p>In addition to product branches with monthly CVE patches and bug fixes, NVIDIA AI Enterprise customers can also receive security advisories and exploitability information from NVIDIA, including Vulnerability Exploitability eXchange (VEX) and Software Bill of Materials (SBOM), vulnerability context, and remediation guidance.&nbsp;</p>\n\n\n\n<p>A VEX document is a relatively recent addition to the field of cybersecurity. Unlike a CVE entry, which provides general information about a vulnerability, a VEX document programmatically provides important context-specific details. It indicates whether a vulnerability is relevant (or exploitable) to particular components within the AI stack. It is also used to communicate false positives flagged by vulnerability scanning tools. VEX documents at NVIDIA are delivered in the <a href=\"https://cyclonedx.org/docs/1.4/json/#vulnerabilities_items_analysis\">CyclonDX</a> format, which provides a standardized machine-readable way to share the information with customers.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Software optimization over time for better performance and lower TCO</h3>\n\n\n\n<p>As NVIDIA continues to evolve AI software and optimize performance over time, advances in NVIDIA AI software deliver up to 54% performance gains without a hardware upgrade. Figure 5 shows NVIDIA MLPerf Inference v3.0 compared to v2.1 submission results with <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100</a> GPUs. This not only improves efficiency and performance, but also reduces energy consumption, footprint, and investment in the data center or cloud.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1898\" height=\"1158\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart.png\" alt=\"Chart of NVIDIA MLPerf Inference v3.0 compared to v2.1 submission results on NVIDIA H100.\n\" class=\"wp-image-76914\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart.png 1898w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-300x183.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-625x381.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-768x469.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-1536x937.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-645x394.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-492x300.png 492w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-362x221.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-180x110.png 180w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mlperf-nvidia-inference-speedup-chart-1024x625.png 1024w\" sizes=\"(max-width: 1898px) 100vw, 1898px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. NVIDIA inference software delivers up to 54% performance gains without a hardware upgrade</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Enterprise support&nbsp;</h3>\n\n\n\n<p>Enterprise support is included with every NVIDIA AI Enterprise subscription, enabling organizations to benefit from the transparency of open source with the assurance of full software stack support provided by NVIDIA. Business-standard support includes:&nbsp;</p>\n\n\n\n<ul>\n<li>Unlimited technical support cases accepted through the customer portal and phone 24 hours a day, 7 days a week&nbsp;</li>\n\n\n\n<li>Escalation support during local business hours</li>\n\n\n\n<li>Timely resolution from NVIDIA experts and engineers</li>\n\n\n\n<li>Up to 3 years of long-term support</li>\n</ul>\n\n\n\n<p>Whether you need to connect with AI experts, access knowledge base resources, or troubleshoot performance issues, NVIDIA is here to help you and provide the support you need to keep your AI stable and secure.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started with NVIDIA AI Enterprise</h2>\n\n\n\n<p>NVIDIA AI Enterprise reduces the costs and burden of maintaining and securing the complex software platform for production AI, freeing organizations to focus on building AI and harnessing its game-changing insights.&nbsp;</p>\n\n\n\n<p>To experience the enterprise platform, request a free <a href=\"https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;ProductFamily=NVAIEnterprise\">90-day evaluation license</a> that grants access to all software branches and enterprise support.&nbsp;</p>\n\n\n\n<p>Already an NVIDIA AI Enterprise user? <a href=\"https://ngc.nvidia.com/signin\">Access the latest version of the production branch</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>While harnessing the potential of AI is a priority for many of today\u2019s enterprises, developing and deploying an AI model involves time and effort. Often, challenges must be overcome to move a model into production, especially for mission-critical business operations. According to IDC research, only 18% of enterprises surveyed could put an AI model into &hellip; <a href=\"https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/\">Continued</a></p>\n", "protected": false}, "author": 1395, "featured_media": 77038, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1339598", "discourse_permalink": "https://forums.developer.nvidia.com/t/advancing-production-ai-with-nvidia-ai-enterprise/280200", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [2584, 453, 1177, 3535], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jWy", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76666"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1395"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76666"}], "version-history": [{"count": 26, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76666/revisions"}], "predecessor-version": [{"id": 77294, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76666/revisions/77294"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77038"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76666"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76666"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76666"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76978, "date": "2024-01-24T12:30:00", "date_gmt": "2024-01-24T20:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76978"}, "modified": "2024-01-25T10:41:13", "modified_gmt": "2024-01-25T18:41:13", "slug": "build-enterprise-grade-ai-with-nvidia-ai-software", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/", "title": {"rendered": "Build Enterprise-Grade AI with NVIDIA AI Software"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Following the introduction of ChatGPT, enterprises around the globe are realizing the benefits and capabilities of AI, and are racing to adopt it into their workflows.&nbsp;</p>\n\n\n\n<p>As this adoption accelerates, it becomes imperative for enterprises not only to keep pace with the rapid advancements in AI, but also address related challenges such as optimization, scalability, and security.</p>\n\n\n\n<p>The enterprise AI development journey typically begins with the data ETL (extract, transform, load) phase, during which data is prepared for training. This is followed by training the AI models. Once the models are trained, the next steps involve deployment and running inference. Enterprises need to use optimized and secure software for each of these stages to build production-ready AI applications.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Accelerate AI development with the NGC catalog</h2>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/?filters=&amp;orderBy=weightPopularDESC&amp;query=\">NVIDIA NGC catalog</a> enables enterprises to accelerate their AI development by providing a suite of GPU-optimized software and SDKs in the form of containers.&nbsp;</p>\n\n\n\n<p>Some of the popular containers in the catalog include <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/rapidsai/containers/notebooks\">RAPIDS</a> for the data ETL phase, <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow\">TensorFlow</a> and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch\">PyTorch</a> for the model development phase, and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorrt\">NVIDIA TensorRT</a> and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\">NVIDIA Triton Inference Server</a> for the model deployment phase.&nbsp;</p>\n\n\n\n<p>Figure 1 shows the different containers available for various use cases, such as NLP, object detection, recommendation, and more. <a href=\"https://catalog.ngc.nvidia.com/?filters=&amp;orderBy=weightPopularDESC&amp;query=\">Explore the NGC catalog</a> to find the right software for your use case.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1215\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers.png\" alt=\"Screenshot of the available containers for various use cases on the NGC catalog.\" class=\"wp-image-76984\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-300x182.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-625x380.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-768x467.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-1536x934.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-645x392.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-494x300.png 494w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-362x220.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-181x110.png 181w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ngc-catalog-containers-1024x622.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The NGC catalog offers containers for a wide variety of use cases&nbsp;</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Benefits of NGC containers</h3>\n\n\n\n<p>All the containers in the catalog are optimized to run on the latest NVIDIA accelerated computing stack and are updated regularly to provide enhanced performance on the same hardware. The containers are tested to ensure compatibility and performance across various NVIDIA GPUs.</p>\n\n\n\n<p>Enterprises can deploy these containers anywhere, including on-premises and both multi-cloud and hybrid-cloud environments. The containers can be deployed with various container orchestration systems, including the widely used open-source platform Kubernetes. Additionally, the catalog offers a range of Helm charts to facilitate the deployment and management of containers on Kubernetes clusters.&nbsp;</p>\n\n\n\n<p>Security and trust are \u200calso an integral part of the containers and models available in the catalog. All \u200ccontainers are scanned for CVEs and are assigned a security rating so that enterprises can be confident about the software they are downloading. Figure 2 shows the different security ratings for the containers.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1708\" height=\"1014\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings.png\" alt=\"Screenshot of the different security ratings available for the containers in the NGC catalog.\" class=\"wp-image-76987\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings.png 1708w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-300x178.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-625x371.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-768x456.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-1536x912.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-645x383.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-500x297.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-152x90.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-362x215.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-185x110.png 185w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-catalog-security-ratings-1024x608.png 1024w\" sizes=\"(max-width: 1708px) 100vw, 1708px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Different security ratings of the containers in the NGC catalog&nbsp;</em></figcaption></figure></div>\n\n\n<p>Containers in general also provide several other benefits, such as:</p>\n\n\n\n<ul>\n<li>\u200b\u200b<strong>Encapsulation of dependencies: </strong>Containers encapsulate all the dependencies, such as libraries and other software components, to ensure that the application can run reliably in any environment without the need for additional configuration.</li>\n\n\n\n<li><strong>Reproducibility: </strong>Containers contain everything needed to run an application, so they will behave the same way regardless of where they are deployed. This reduces the chances of issues related to environment-specific configurations.</li>\n\n\n\n<li><strong>Time and resource savings: </strong>By simplifying the deployment process and ensuring consistency across environments, containers save considerable time and resources that would otherwise be spent on manual setup and troubleshooting.</li>\n</ul>\n\n\n\n<p>Using the catalog software, enterprise developers can start building POCs and testing their solutions. When enterprises are ready to move into production, they need to ensure that the requirements for integrating AI into their business applications are met.&nbsp;</p>\n\n\n\n<p>These requirements include enterprise-grade security, software stack stability, enterprise support, and manageability. Having a clear path to assist in transitioning from development to production is part of the success factor of time to value.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Seamless transition from development to production</h2>\n\n\n\n<p>The diverse range of software components and associated interdependencies make maintaining a secure, reliable AI software stack a massive undertaking, especially when AI needs to be deployed and integrated into enterprise applications. NGC catalog enables enterprise developers to seamlessly transition by offering both community-based software for development and enterprise-grade software branches for production.&nbsp;</p>\n\n\n\n<p>For example, the catalog offers multiple containers for <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\">NVIDIA Triton Inference Server</a> (Figure 3). NVIDIA Triton Inference Server Feature Branch is the latest version, released in a monthly cadence, to provide developers access to the latest features and performance optimizations.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"850\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1.png\" alt=\"Image showing the search results for NVIDIA Triton Inference Server query.\" class=\"wp-image-76991\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-300x128.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-625x266.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-768x327.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-1536x653.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-645x274.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-500x213.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-259x110.png 259w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-containers-nvidia-triton-inference-server-1-1024x435.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Different NGC containers are available for NVIDIA Triton Inference Server</em></figcaption></figure></div>\n\n\n<p>NVIDIA Triton Inference Server Production Branch, available exclusively with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, is the production-grade version purposely built to provide stability and a secure environment for building mission-critical AI applications. The production branch releases every 6 months with a 9-month lifetime to ensure API stability. Within the 9-month lifecycle, NVIDIA continuously tracks critical vulnerabilities and releases CVE patches and bug fixes monthly without breaking the software stack.</p>\n\n\n\n<p>With different optimized AI software options, enterprise developers can leverage the catalog to choose the proper software packages that support a given AI pipeline while maintaining security.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Uninterrupted AI excellence with NVIDIA AI Enterprise</h2>\n\n\n\n<p>Security, reliability, and manageability are critical for enterprise-grade AI. To address these challenges, NVIDIA has introduced <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, an enterprise-grade software platform that accelerates the data science pipeline and streamlines the development and deployment of production-grade AI applications.&nbsp;</p>\n\n\n\n<p>Built on open source and curated, optimized, and supported by NVIDIA, NVIDIA AI Enterprise offers multiple supported branches (production, feature, and long-term support). It also includes enterprise-grade security, stability, manageability, and support throughout your AI journey with NGC.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>The NGC catalog and NVIDIA AI Enterprise offer enterprises the necessary tools to keep pace with the rapid advancements in AI while addressing related challenges such as optimization, scalability, and security.</p>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/?filters=&amp;orderBy=weightPopularDESC&amp;query=\">Visit the NGC catalog</a> to browse hundreds of pretrained models, containers, Helm charts, and Jupyter Notebooks. To get started with NVIDIA AI Enterprise, <a href=\"https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;ProductFamily=NVAIEnterprise\">apply for a free 90-day trial</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Following the introduction of ChatGPT, enterprises around the globe are realizing the benefits and capabilities of AI, and are racing to adopt it into their workflows.&nbsp; As this adoption accelerates, it becomes imperative for enterprises not only to keep pace with the rapid advancements in AI, but also address related challenges such as optimization, scalability, &hellip; <a href=\"https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/\">Continued</a></p>\n", "protected": false}, "author": 1921, "featured_media": 77027, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338897", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-enterprise-grade-ai-with-nvidia-ai-software/280064", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [2584, 453, 1953, 559, 1177, 3535], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-production-ai-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-k1A", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76978"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76978"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76978/revisions"}], "predecessor-version": [{"id": 77198, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76978/revisions/77198"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77027"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76978"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76978"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76978"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76643, "date": "2024-01-24T11:31:39", "date_gmt": "2024-01-24T19:31:39", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76643"}, "modified": "2024-01-25T10:17:24", "modified_gmt": "2024-01-25T18:17:24", "slug": "delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/", "title": {"rendered": "Delivering Efficient, High-Performance AI Clouds with NVIDIA DOCA 2.5"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>As a comprehensive software framework for data center infrastructure developers, NVIDIA DOCA has been adopted by leading AI, cloud, enterprise, and ISV innovators. The release of DOCA 2.5 marks its third anniversary. And, due to the stability and robustness of the code base combined with several networking and platform upgrades, DOCA 2.5 is the first NVIDIA BlueField-3 long-term support (LTS) version for AI cloud deployments.</p>\n\n\n\n<p>Alongside NVIDIA switches, BlueField DPUs, and SuperNICs, DOCA 2.5 is an essential element of a co-designed platform created to support the most demanding AI workloads. Forming part of the NVIDIA full stack architecture, networking components from NVIDIA deliver optimum application performance and security and data center efficiency. When deployed alongside the NVIDIA computing platform and software tools, they offer additional benefits and synergies.</p>\n\n\n\n<p>Here are some of the newest network offerings from NVIDIA and how DOCA 2.5 is an integral part of AI infrastructure.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Backbone of AI infrastructure</h2>\n\n\n\n<p>It\u2019s now widely understood that a high-performance network is the backbone of efficient AI infrastructure. To achieve optimal AI performance, significant consideration must be given to its capabilities, implementation, and deployment for both generative AI and foundational models.</p>\n\n\n\n<p>Due to their distinct properties and significant computational demands, modern AI workloads require specialized network infrastructure to operate at peak efficiency. Leading the way in AI and accelerated computing, we created the NVIDIA Spectrum-X Ethernet networking platform to meet this requirement and improve the effectiveness and performance of AI clouds.</p>\n\n\n\n<p>The Spectrum-4 Ethernet switch and BlueField-3 SuperNIC from NVIDIA form the basis of the Spectrum-X platform and the foundation of our accelerated computing fabric for artificial intelligence. The BlueField-3 SuperNIC offers numerous technology benefits for a wide range of industries. When deployed in our flagship AI systems, BlueField-3 SuperNICs not only enhance performance but also provide deterministic and isolated performance for tenant jobs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1545\" height=\"360\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware.png\" alt=\"Pictures of an NVIDIA Spectrum-4 Switch and NVIDIA BlueField-3 DPU.\" class=\"wp-image-76647\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware.png 1545w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-300x70.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-625x146.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-179x42.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-768x179.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-1536x358.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-645x150.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-500x117.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-160x37.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-362x84.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-472x110.png 472w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/spectrum-x-bluefield-hardware-1024x239.png 1024w\" sizes=\"(max-width: 1545px) 100vw, 1545px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA Spectrum-X and BlueField-3 hardware</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">NVIDIA synergy</h2>\n\n\n\n<p>The Spectrum-X platform combines co-designed, best-in-class hardware to deliver unparalleled performance synergies and an unmatched customer experience.<strong> </strong>Integral to the design, BlueField-3 SuperNICs take Ethernet networking to new heights for AI systems running on a cluster of GPU-based servers.&nbsp;</p>\n\n\n\n<p>In contrast, conventional network interface cards lack the required features for AI workloads. BlueField SuperNICs ensure that the required processes for effectively executed cloud-based AI workloads are delivered with efficiency and speed.</p>\n\n\n\n<p>When combined with an NVIDIA GPU, this marriage of technologies (available for most enterprise-class servers) creates an optimized solution for AI cloud computing, delivering matchless levels of efficiency, performance, and flexibility.</p>\n\n\n\n<p>Validated across the full stack of NVIDIA hardware and software, Spectrum-X and NVIDIA GPUs create a truly peerless Ethernet solution for AI clouds. With such broad levels of integration available, the opportunity for fine-tuning provides custom-like levels of modification for truly unique solutions, dedicated to the delivery of precision workloads.</p>\n\n\n\n<p>As a component of the full stack, DOCA is a critical piece of the AI puzzle and ties together compute, networking, storage, and security.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1518\" height=\"505\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack.png\" alt=\"Diagram includes SONic, Cumulus, NetQ, DOCA Services, NVIDIA Air, SAI/SPSDK, DOCA, and Magnum IO.\" class=\"wp-image-76649\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack.png 1518w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-300x100.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-625x208.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-768x255.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-645x215.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-500x166.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-362x120.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-331x110.png 331w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-hardware-software-stack-1024x341.png 1024w\" sizes=\"(max-width: 1518px) 100vw, 1518px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA hardware and software stack</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">New features for AI clouds and data center infrastructure</h2>\n\n\n\n<p>DOCA helps to enable the most advanced, GPU-accelerated, AI workloads today. For systems that include a GPU and NVIDIA BlueField-3 DPUs or BlueField-3 SuperNICs, there are further advantages for developers.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\">\n<table style=\"width: 99.7619%;\">\n<tbody>\n<tr>\n<td style=\"width: 18.0191%;\">\u00a0</td>\n<td style=\"width: 43.7947%;\"><strong>BlueField-3 DPU</strong></td>\n<td style=\"width: 33.6516%;\"><strong>BlueField-3 SuperNIC</strong></td>\n<td style=\"width: 4.29594%;\">\u00a0</td>\n</tr>\n<tr>\n<td style=\"width: 18.0191%;\"><strong>Mission</strong></td>\n<td style=\"width: 43.7947%;\"><strong>&gt;</strong> Cloud infrastructure processor<br /><strong>&gt;</strong> Offload, accelerate, and isolate data center infrastructure<br /><strong>&gt;</strong> Optimized for N-S in GPU-class systems</td>\n<td style=\"width: 33.6516%;\"><strong>&gt;</strong> Accelerated networking for AI computing<br /><strong>&gt;</strong> Best-in-class RoCE networking<br /><strong>&gt;</strong> Optimized for E-W in GPU-class systems</td>\n<td style=\"width: 4.29594%;\">\u00a0</td>\n</tr>\n<tr>\n<td style=\"width: 18.0191%;\"><strong>Shared\u00a0</strong><strong>Capabilities</strong></td>\n<td style=\"width: 77.4463%; text-align: center;\" colspan=\"2\"><strong>&gt;</strong> VPC network acceleration<br /><strong>&gt;</strong> Network encryption acceleration<br /><strong>&gt;</strong> Programmable network pipeline<br /><strong>&gt;</strong> Precision timing<br /><strong>&gt;</strong> Platform security<strong><br /></strong></td>\n<td style=\"width: 4.29594%;\">\u00a0</td>\n</tr>\n<tr>\n<td style=\"width: 18.0191%;\"><strong>Unique\u00a0</strong><strong>Capabilities</strong></td>\n<td style=\"width: 43.7947%;\"><strong>&gt; </strong>Powerful computing<br /><strong>&gt; </strong>Secure, zero-trust management\u00a0<br /><strong>&gt; </strong>Data storage acceleration<br /><strong>&gt;</strong> Elastic infrastructure provisioning<br /><strong>&gt;</strong> 1-2 DPUs per system</td>\n<td style=\"width: 33.6516%;\"><strong>&gt;</strong> Powerful networking<br /><strong>&gt;</strong> AI networking feature set<br /><strong>&gt;</strong> Full-stack NVIDIA AI optimization<br /><strong>&gt;</strong> Power-efficient, low-profile design<br /><strong>&gt;</strong> Up to 8 SuperNICs per system</td>\n<td style=\"width: 4.29594%;\">\u00a0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"wp-element-caption\"><em>Table 1. NVIDIA BlueField-3 DPU and SuperNIC comparison</em></figcaption>\n</figure>\n\n\n\n<p>Specifically, DOCA capitalizes on the numerous NVIDIA-led development, integration, and testing programs that enable and optimize the entire range of AI application frameworks. The convergence of NVIDIA technologies fuels data center innovation and rapid AI application deployment.</p>\n\n\n\n<p>Released in December 2023, DOCA 2.5 offers several enhancements that boost performance within the data center. There\u2019s a continuing increase in both the number of virtual functions and the volume of \u2018east-west\u2019 network traffic. In response, the use of DOCA and BlueField-3 SuperNICs is imperative to optimize the network and establish its function as the backbone of a modern AI infrastructure.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1672\" height=\"1066\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture.png\" alt=\"Diagram shows an application layer (including networking, security, and storage), DOCA services (including Orchestration, Telemetry, and Firefly), libraries (including Crypto, App Shield, and Rivermax), and drivers (including UCX, UCC, and RDMA).\" class=\"wp-image-76650\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture.png 1672w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-300x191.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-625x398.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-768x490.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-1536x979.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-645x411.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-471x300.png 471w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-141x90.png 141w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-362x231.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-173x110.png 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/doca-2-5-architecture-1024x653.png 1024w\" sizes=\"(max-width: 1672px) 100vw, 1672px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. DOCA 2.5 architecture</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">DOCA-PCC now available</h3>\n\n\n\n<p>Within multi-tenant AI cloud environments where multiple AI jobs run simultaneously, there is a potential for network congestion to arise.<strong>&nbsp;</strong></p>\n\n\n\n<p>The DOCA PCC library, now GA, provides a high-level programming interface that enables partners to implement customized congestion control (CC) algorithms. This library uses the NVIDIA BlueField-3 SuperNIC acceleration for CC management and provides an API that abstracts hardware complexity to simplify programming. Partners can focus on the functionality of your CC algorithm and implement it quickly with BlueField hardware acceleration.</p>\n\n\n\n<p>DOCA PCC also gives you the flexibility to develop an optimal solution to handle congestion in your clusters. Customized congestion control is critical for AI workflows, enabling performance isolation, improving fairness, and preventing packet drop on lossy networks.&nbsp;</p>\n\n\n\n<p>NVIDIA Spectrum-X is a breakthrough Ethernet networking solution for building multi-tenant, hyperscale AI clouds. It uses DOCA PCC to implement congestion control.</p>\n\n\n\n<h3 class=\"wp-block-heading\">DOCA Flow: New and enhanced features for cloud deployments</h3>\n\n\n\n<p>DOCA Flow is an essential programming tool used to develop DOCA services. DOCA 2.5 adds additional support for the development of NVIDIA OVS-DOCA, an innovative and performant virtual switch that is native to NVIDIA NICs and DPUs, and NVIDIA DOCA HBN services.</p>\n\n\n\n<p>With NVIDIA DOCA Flow, you can define and control the flow of network traffic, implement network policies, and manage network resources programmatically. It offers network virtualization, telemetry, load balancing, security enforcement, and traffic monitoring.&nbsp;</p>\n\n\n\n<p>These capabilities are beneficial for processing high-packet workloads with low latency, conserving CPU resources, and reducing power usage. Fundamentally, DOCA Flow is a key enabler for multiple use cases in cloud networking. Used for the development of custom software-defined networking (SDN), this is a key building block for CSPs designing the networks of tomorrow.</p>\n\n\n\n<h2 class=\"wp-block-heading\">DOCA services</h2>\n\n\n\n<p>The following are some examples of DOCA services that have been upgraded in the DOCA 2.5 release:</p>\n\n\n\n<ul>\n<li>Host-base networking</li>\n\n\n\n<li>DOCA Firefly</li>\n\n\n\n<li>Storage SNAPv4</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Host-based networking</h3>\n\n\n\n<p>Upgraded in DOCA 2.5, host-based networking (HBN) is a DOCA service that enables network architects to design networks based purely on L3 protocols, enabling routing to run on the servers of the network. In the case of BlueField, the HBN solution packages a set of network functions inside a container that is packaged as a service pod running on the DPU.</p>\n\n\n\n<p>DOCA HBN gives network architects the ability to create controller-less virtual private clouds (VPCs). This is ideal for CSPs, telcos, and enterprise customers deploying bare-metal as a service (BMaaS) infrastructures.</p>\n\n\n\n<p>Compared to conventional networking solutions, using DOCA HBN presents you with a number of benefits. In addition to improving the scalability and efficiency of deployment, DOCA HBN offers enhanced security options, a simplified underlay network fabric, and reduced OPEX. If used in conjunction with a third-party switch manufacturer, DOCA HBN shifts several top-of-rack (ToR) switch functions to the BlueField-3 DPU or SuperNIC, leading to a reduction in third-party license costs.</p>\n\n\n\n<p>For more information about the new HBN functions, including support of RoCE, Routing, and ACL enhancements, see the <a href=\"https://docs.nvidia.com/doca/sdk/nvidia+doca+release+notes/index.html\" data-type=\"link\" data-id=\"https://docs.nvidia.com/doca/sdk/nvidia+doca+release+notes/index.html\">DOCA 2.5 release notes</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">DOCA Firefly</h3>\n\n\n\n<p>This feature provides Precision Time Protocol (PTP)\u2013based time synchronization services that use the hardware acceleration of NVIDIA DPUs and SuperNICs.</p>\n\n\n\n<p>Industry-specific PTP use cases include the following:</p>\n\n\n\n<ul>\n<li><strong>Telco: </strong>Network-based time synchronization essential for 5G mobile deployment</li>\n\n\n\n<li><strong>Media and entertainment:</strong>\n<ul>\n<li>QoS for video, audio, and metadata transmission</li>\n\n\n\n<li>Meeting stringent broadcast quality requirements</li>\n</ul>\n</li>\n\n\n\n<li><strong>Data center:</strong> Time distribution</li>\n\n\n\n<li><strong>Financial services industry:</strong>\n<ul>\n<li>High-frequency trading (HFT)&nbsp;</li>\n\n\n\n<li>MiFID II compliance (required)</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>New to DOCA 2.5, DOCA Firefly now includes industry-specific profiles to improve the user experience and simplify deployment. Profiles currently include Media and Telco, which are configured to include industry-specific functions and performance parameters.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Storage SNAPv4</h3>\n\n\n\n<p>The DOCA SNAPv4 service on BlueField-3 adds inline AES-XTS, the default cryptographic algorithm for protecting the confidentiality of data-at-rest on storage devices. SNAP now accelerates AES-XTS encryption in hardware, which optimizes and improves the encryption process while benefiting from a reduced CPU overhead.</p>\n\n\n\n<p>The SNAPv4 service for virtio-blk now offers Recovery/Hot-Upgrade/LM without force-in-order. This new feature improves support for Recovery, Hot-Upgrade, and Live-Migration functions and means that it\u2019s no longer necessary to operate using force-in-order traffic. This equates to a more practical tool for customers in real-world settings whereby typical customers, such as CSPs, can now offer improved uptime and uninterrupted performance for end users undertaking vital storage tasks.</p>\n\n\n\n<h2 class=\"wp-block-heading\">More updates</h2>\n\n\n\n<p>For more information about the following list of updates and features, see the <a href=\"https://docs.nvidia.com/doca/sdk/nvidia+doca+release+notes/index.html\">DOCA 2.5 release notes</a>:</p>\n\n\n\n<ul>\n<li>Device Attestation</li>\n\n\n\n<li>DPA User Application Signing and Authentication [Beta]&nbsp;</li>\n\n\n\n<li>DPU Firmware TPM [Beta]</li>\n\n\n\n<li>DPU Upgrade tool</li>\n\n\n\n<li>New qualification, certification, and management features</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>Modern AI workloads require sophisticated network solutions to operate effectively at peak efficiency. Today, organizations across the globe are facing a similar significant challenge when trying to embed AI into their existing operational and technical infrastructure.</p>\n\n\n\n<p>To meet this requirement, NVIDIA, as the leader in AI and accelerated computing, has created an optimized networking platform to drive the performance of AI cloud computing. Central to the effectiveness of this platform are the synergies gained from complementary technologies employed by the various NVIDIA-branded hardware and software solutions.&nbsp;</p>\n\n\n\n<p>In their full-stack architecture, NVIDIA implemented several design considerations to ensure increased operational effectiveness between the various platforms. When combined with NVIDIA GPUs, Spectrum-X, a solution comprised of NVIDIA Ethernet Switches and BlueField SuperNICs, creates a truly peerless Ethernet platform for AI clouds. With the latest release of NVIDIA DOCA SDK, NVIDIA has made additional strides to further enable the most advanced, GPU-accelerated, AI workloads today.</p>\n\n\n\n<p>To begin your development journey with all the benefits DOCA has to offer, <a href=\"https://developer.nvidia.com/doca-sdk-early-access\">download NVIDIA DOCA</a> today. For more information, see the following resources:</p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/blog/demystifying-doca/\">Demystifying NVIDIA DOCA</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/understanding-when-to-use-doca-drivers-and-doca-libraries/\">Understanding When to Use DOCA Drivers and DOCA Libraries</a></li>\n\n\n\n<li><a href=\"https://courses.nvidia.com/courses/course-v1:DLI+S-NP-01+V1/\">Introduction to DOCA for DPUs</a> free course</li>\n\n\n\n<li><a href=\"https://courses.nvidia.com/courses/course-v1:DLI+S-NP-02+V1/\">Getting Started with DOCA Flow</a> self-paced course&nbsp;</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41144/\">Delivering an AI-Ready Infrastructure Today for Powering the AI Factories of Tomorrow</a> GTC session</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>As a comprehensive software framework for data center infrastructure developers, NVIDIA DOCA has been adopted by leading AI, cloud, enterprise, and ISV innovators. The release of DOCA 2.5 marks its third anniversary. And, due to the stability and robustness of the code base combined with several networking and platform upgrades, DOCA 2.5 is the first &hellip; <a href=\"https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/\">Continued</a></p>\n", "protected": false}, "author": 1294, "featured_media": 77017, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338867", "discourse_permalink": "https://forums.developer.nvidia.com/t/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/280061", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 852, 2758, 1205], "tags": [1460, 453, 1958, 1226], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tech-blog-doca-2.5-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jWb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76643"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1294"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76643"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76643/revisions"}], "predecessor-version": [{"id": 77191, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76643/revisions/77191"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77017"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76643"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76643"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76643"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 77126, "date": "2024-01-24T10:00:00", "date_gmt": "2024-01-24T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=77126"}, "modified": "2024-01-25T10:17:25", "modified_gmt": "2024-01-25T18:17:25", "slug": "webinar-improve-spear-phishing-detection-with-ai", "status": "publish", "type": "post", "link": "https://nvda.ws/48FJ4pR", "title": {"rendered": "Webinar: Improve Spear Phishing Detection with AI"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how generative AI can help defend against spear phishing in this January 30 webinar.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how generative AI can help defend against spear phishing in this January 30 webinar.</p>\n", "protected": false}, "author": 1264, "featured_media": 77137, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/48FJ4pR", "_links_to_target": "_blank"}, "categories": [1464, 2758, 3110], "tags": [1511, 453, 2127, 2877, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/security-spear-phish-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-k3Y", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77126"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1264"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=77126"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77126/revisions"}], "predecessor-version": [{"id": 77143, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77126/revisions/77143"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77137"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=77126"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=77126"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=77126"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 77101, "date": "2024-01-24T09:00:00", "date_gmt": "2024-01-24T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=77101"}, "modified": "2024-01-25T10:17:26", "modified_gmt": "2024-01-25T18:17:26", "slug": "using-the-power-of-ai-to-make-factories-safer", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/", "title": {"rendered": "Using the Power of AI to Make Factories Safer"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>As industrial automation increases, safety becomes a greater challenge and top priority for enterprises.&nbsp;</p>\n\n\n\n<p>Safety encompasses multiple aspects:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>System safety:</strong> A rational pursuit of acceptable mishap risk within a systems perspective.</li>\n\n\n\n<li><strong>Safety of the intended functionality:</strong> The absence of unreasonable risk due to hazards resulting from functional insufficiencies of the intended functionality or by reasonably foreseeable misuse by persons.</li>\n\n\n\n<li><strong>Functional safety:</strong> Part of the overall safety that depends on the correct functioning of the safety-related systems and external risk reduction facilities.&nbsp;</li>\n\n\n\n<li><strong>Worker safety:</strong> Providing a safe working environment for employees by incorporating safe equipment and safe procedures at the workplace.</li>\n\n\n\n<li>And other, more general types of safety.</li>\n</ul>\n\n\n\n<p>The same technological solution that&#8217;s driving automation can be used to also address safety: artificial intelligence.&nbsp;</p>\n\n\n\n<p>AI-powered stationary <em>outside-in</em> safety platforms, which monitor activity across many distributed machines or robots, can predictively and proactively orchestrate consistent safety policies. Machines and robots equipped with <em>inside-out</em> reactive safety can detect any specific interaction within their workspace and take appropriate measures.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1177\" height=\"782\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety.png\" alt=\"Diagram shows orchestrating distributed mobile machines with stationary safety platforms.\" class=\"wp-image-77108\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety.png 1177w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-300x199.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-625x415.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-173x115.png 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-768x510.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-645x429.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-452x300.png 452w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-135x90.png 135w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-362x241.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-166x110.png 166w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/proactive-reactive-safety-1024x680.png 1024w\" sizes=\"(max-width: 1177px) 100vw, 1177px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Inside-out and outside-in safety</em></figcaption></figure></div>\n\n\n<p><a href=\"https://www.iec.ch/functional-safety/faq\">IEC 61508</a> is an international standard published by the International Electrotechnical Commission (IEC). It consists of methods to apply, design, deploy, and maintain safety-related systems.</p>\n\n\n\n<p>Using IEC 61508 general architecture, AI can either be used to implement the safety function or to assist functional safety to provide additional risk reduction measures.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"506\" height=\"255\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety.png\" alt=\"Workflow diagram shows an EUC control system assisting reactive functional safety with proactive safety functions.\u00a0\u00a0\" class=\"wp-image-77109\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety.png 506w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety-300x151.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety-179x90.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety-500x252.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety-362x182.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/euc-control-safety-218x110.png 218w\" sizes=\"(max-width: 506px) 100vw, 506px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Reactive and proactive safety</em></figcaption></figure></div>\n\n\n<p>The use of AI in safety-related systems is being analyzed in upcoming international standardization activities such as <a href=\"https://www.iec.ch/blog/new-standard-increase-safety-ai\">ISO/IEC TR 5469</a> and <a href=\"https://www.iso.org/standard/87118.html?browse=tc\">ISO/IEC AWI TS 22440</a> being developed by <a href=\"https://www.iec.ch/blog/iec-and-iso-launch-working-group-advance-functional-safety-ai-systems\">ISO/IEC joint working group</a> JWG 4.</p>\n\n\n\n<p>The examples in Table 1 describe some typical safety use cases that can benefit from AI.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Use case</strong></td><td><strong>Surveilled machines</strong></td></tr><tr><td>Detecting position and movement of humans toward a machine safety zone.</td><td>conveyor belt, high-speed door, collaborative robot, autonomous mobile robot, forklift</td></tr><tr><td>Detecting machines moving in the direction of a forbidden area or in the direction of other machines; machines moving at excessive speed or excessive strength.</td><td>autonomous mobile robot, forklift&nbsp;</td></tr><tr><td>Detecting defects or material that could cause safety hazards based on norms such as <a href=\"https://webstore.ansi.org/standards/ria/ansiriar15082020?gad_source=1&amp;gclid=CjwKCAiA5L2tBhBTEiwAdSxJX3gsXXCCBw7UfHpK-u8IcRxg3Uf39V0OEltz-nSNLAj5nvjiJ09bWxoC-dsQAvD_BwE\" data-type=\"link\" data-id=\"https://webstore.ansi.org/standards/ria/ansiriar15082020?gad_source=1&amp;gclid=CjwKCAiA5L2tBhBTEiwAdSxJX3gsXXCCBw7UfHpK-u8IcRxg3Uf39V0OEltz-nSNLAj5nvjiJ09bWxoC-dsQAvD_BwE\">ANSI/RIA R15.08</a>, and so on.</td><td>autonomous mobile robot, train, airplane&nbsp;</td></tr><tr><td>Detecting and classifying worker or object poses and movement.</td><td>Factory, warehouses&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Use cases and machines that can benefit from AI proactive safety functions</em></figcaption></figure>\n\n\n\n<p>The following examples of responses to those proactive safety functions are ordered by increasing level of intervention:</p>\n\n\n\n<ul>\n<li>Updating statistics to later optimize worker-machine or machine-machine interactions, change the factory floor plan, or improve worker training.</li>\n\n\n\n<li>Informing a safety supervisor.</li>\n\n\n\n<li>Generating visible and audible alerts to workers, through machine local alerts or personal devices.</li>\n\n\n\n<li>Slowing down or stopping a machine (acting as a speed or force limiter).</li>\n\n\n\n<li>Changing the position or trajectory of the machine.</li>\n\n\n\n<li>Preventing the start of the equipment mission.</li>\n\n\n\n<li>Activating hazard protection devices.</li>\n</ul>\n\n\n\n<p>NVIDIA IGX Orin combines industrial-grade hardware with enterprise-level software and support to implement both inside-out reactive and outside-in proactive safety in the same platform:</p>\n\n\n\n<ul>\n<li><strong>AI safety framework</strong>\n<ul>\n<li>Framework to provide proactive safety through AI.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Safety Extension Package (SEP)</strong>\n<ul>\n<li>Safety services for fault avoidance, detection, and control leveraging hardware safety extensions</li>\n\n\n\n<li>Edge Safety Link, a safety communication protocol.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Hardware safety extensions:</strong>\n<ul>\n<li>Hundreds of diagnostic mechanisms implemented in NVIDIA Orin SoC elements (CPU, GPU, accelerators, interfaces, ECC, and so on), In-System-Test, clock, temperature and voltage monitors, and so on.&nbsp;</li>\n\n\n\n<li>Functional Safety Island (FSI): An independent, redundant set of processors with onboard memory that is used as a monitoring and safety processor working beside other SoC engines.</li>\n\n\n\n<li>Safety MCU: A third-party, safety-certified microcontroller, collaborating with FSI and Orin SoC, and controlling board-level voltage and temperature monitors.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Safety documentation</strong>\n<ul>\n<li>Set of documents (safety application note, safety manual, and FMEDA) to support customer safety cases.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Safety in action example</h2>\n\n\n\n<p>Here\u2019s an example of proactive and reactive safety in action, implemented in collaboration with <a href=\"https://www.fortrobotics.com/\">FORT Robotics</a> and <a href=\"https://www.protex.ai/\">Protex.AI</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/protex-ai-fort-safety-example.png\" alt=\"Workflow diagram shows warn zone, stop zone, and communication channels to safety protocol elements.\" class=\"wp-image-77110\"/><figcaption class=\"wp-element-caption\"><em>Figure 3. Proactive and reactive safety in action</em></figcaption></figure></div>\n\n\n<p>In the example, an HD IP camera connected with Ethernet to NVIDIA IGX running Protex AI stack detects a worker moving toward an industrial mobile machine.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"391\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-625x391.png\" alt=\"Photo of FORT robotics warn and stop zones labeled.\" class=\"wp-image-77113\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-1536x960.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones-1024x640.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/warn-stop-zones.png 1920w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Overall setting with camera and safety zones</em></figcaption></figure></div>\n\n\n<ul>\n<li>If a worker crosses the warning safety zone, IGX commands switch on a visible alarm (proactive safety).&nbsp;</li>\n\n\n\n<li>If the worker continues or crosses the stop safety zone, the FORT safety stack on IGX commands (WiFi) the FORT endpoint (embedded in the machine) to activate the emergency stop (reactive safety).&nbsp;</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"391\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-625x391.png\" alt=\"Still of the camera feed in the safety stack.\u00a0\" class=\"wp-image-77111\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-1536x960.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera-1024x640.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fort-robotics-camera.png 1920w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Setting safety zones in Protex.AI user interface</em></figcaption></figure></div>\n\n\n<p>The Protex edge computer vision app uses NVIDIA DeepStream 6.2 and the Protex model has been trained with NVIDIA TAO Toolkit and quantized with NVIDIA TensorRT.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"391\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-625x391.png\" alt=\"Screenshot of Protex.AI user interface showing how easily the safety zone is defined and the camera-person-zone-event chain built up.\u00a0\" class=\"wp-image-77112\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-1536x960.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain-1024x640.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-person-zone-event-chain.png 1920w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Proactive safety stack and the camera-person-zone-event chain</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Get started</h2>\n\n\n\n<p>For more information, see <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\">NVIDIA IGX Orin</a> and be sure to sign up for the <a href=\"https://www.nvidia.com/gtc/session-catalog/?tab.allsessions=1700692987788001F1cG&amp;search=safety#/session/1695925853682001yGBt\">Functional Safety for Industry 4.0: Keeping Supply Chains Safe, Secure, and Efficient using AI at the Edge</a> GTC session.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As industrial automation increases, safety becomes a greater challenge and top priority for enterprises.&nbsp; Safety encompasses multiple aspects:&nbsp; The same technological solution that&#8217;s driving automation can be used to also address safety: artificial intelligence.&nbsp; AI-powered stationary outside-in safety platforms, which monitor activity across many distributed machines or robots, can predictively and proactively orchestrate consistent safety &hellip; <a href=\"https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/\">Continued</a></p>\n", "protected": false}, "author": 1990, "featured_media": 77105, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338754", "discourse_permalink": "https://forums.developer.nvidia.com/t/using-the-power-of-ai-to-make-factories-safer/280032", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63], "tags": [3553, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/power-ai-factory-safety.png", "jetpack_shortlink": "https://wp.me/pcCQAL-k3z", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77101"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1990"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=77101"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77101/revisions"}], "predecessor-version": [{"id": 77146, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77101/revisions/77146"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77105"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=77101"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=77101"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=77101"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76977, "date": "2024-01-23T10:00:00", "date_gmt": "2024-01-23T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76977"}, "modified": "2024-01-25T10:17:27", "modified_gmt": "2024-01-25T18:17:27", "slug": "simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/", "title": {"rendered": "Simplifying Network Operations for AI with NVIDIA Quantum InfiniBand"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>A common technological misconception is that performance and complexity are directly linked. That is, the highest-performance implementation is also the most challenging to implement and manage. When considering data center networking, however, this is not the case.&nbsp;</p>\n\n\n\n<p>InfiniBand is a protocol that sounds daunting and exotic in comparison to Ethernet, but because it is built from the ground up for the highest performance, it is actually simpler to deploy and maintain. When you are considering connectivity for AI infrastructure, the <a href=\"https://docs.nvidia.com/networking/display/ibclusteropmainguide\">InfiniBand Cluster Operation and Maintenance Guide</a> helps make setting up and operating a full-stack InfiniBand network as simple as possible.</p>\n\n\n\n<p>This comprehensive guide covers the essential steps needed to streamline Day 0, Day 1, and Day 2 network operations. In particular, the guide details how to use <a href=\"https://www.nvidia.com/en-us/networking/infiniband/ufm/\">NVIDIA Unified Fabric Manager</a> (UFM) to assist in initial provisioning as well as ongoing maintenance plans.&nbsp;</p>\n\n\n\n<p>UFM is a powerful toolset with wide-ranging telemetry and analytics capabilities. However, getting started with UFM for the basics of cluster monitoring and management does not require any advanced prerequisites or specialized knowledge.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1100\" height=\"536\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard.png\" alt=\"Image of UFM Enterprise page with bar graphs\" class=\"wp-image-76994\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard.png 1100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-625x305.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-768x374.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-645x314.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-362x176.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-226x110.png 226w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-fabric-dashboard-1024x499.png 1024w\" sizes=\"(max-width: 1100px) 100vw, 1100px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. UFM Fabric dashboard</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Cluster build and operation</h2>\n\n\n\n<p>The guide walks you through bring-up:</p>\n\n\n\n<ul>\n<li>Verifying UFM operational status</li>\n\n\n\n<li>Generating the fabric health report and topology validation</li>\n\n\n\n<li>Verifying cluster performance</li>\n</ul>\n\n\n\n<p>The guide also provides an introduction to congestion analysis with UFM Telemetry. UFM telemetry and monitoring capabilities are powerful. Third-party plugins to tools such as Grafana, Fluentd, Slurm, and Zabbix enable you to capture vital networking metrics and use them with your platform of choice.</p>\n\n\n\n<p>When the administrator knows that the cluster is in a healthy initial state, the guide suggests a cluster maintenance regime, with a list of checks for periodic maintenance.</p>\n\n\n\n<p><strong>Minutely/Ongoing Maintenance:</strong></p>\n\n\n\n<ul>\n<li>Check for scenarios on the troubleshooting list and follow the instructions for resolution.</li>\n</ul>\n\n\n\n<p><strong>Weekly Maintenance:</strong></p>\n\n\n\n<ul>\n<li>Monitor trends in link monitoring key indicators (available in the UFM user interface).</li>\n\n\n\n<li>Run cluster topology validation checks and fabric health validation tests.</li>\n\n\n\n<li>Verify performance KPIs with ClusterKit (included as part of the HPC-X<sup> </sup>software package).</li>\n\n\n\n<li>Review temperature differentials captured within UFM to ensure that your cooling system is working properly.</li>\n</ul>\n\n\n\n<p><strong>Quarterly/Annual Maintenance:</strong></p>\n\n\n\n<ul>\n<li>Examine the most recent firmware and software release notes and <a href=\"https://www.nvidia.com/en-us/networking/products/infiniband-bundle/\">validated configurations</a> and upgrade if possible.</li>\n\n\n\n<li>Conduct an annual review of NVIDIA network health by contacting NVIDIA Networking support or your designated NVIDIA point of contact</li>\n</ul>\n\n\n\n<p>Many of these checks may be automated and are configurable through the API. The guide provides links to the appropriate <a href=\"https://docs.nvidia.com/networking/display/ufmenterpriserestapiv6151/events+rest+api\">UFM API documentation</a> to make this setup easy and seamless.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Issue resolution</h2>\n\n\n\n<p>Of course, no system is perfect. Even a well-oiled machine like an InfiniBand cluster encounters unexpected issues every so often.&nbsp;</p>\n\n\n\n<p>However, as an admin, the cluster maintenance guide is your one-stop shop. It includes a chapter describing the most frequently encountered scenarios, and how to solve them. This section includes the scenario and how to detect it (with corresponding UFM Alert Event IDs), and then a set of steps to take to reach a resolution. It covers simple and common errors such as bad ports, flapping links, and cable connection issues, as well as more complex challenges such as performance degradation or low bandwidth.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"975\" height=\"489\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard.png\" alt=\" Image of UFM Enterprise page with events and alarms listed.\" class=\"wp-image-76988\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-300x150.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-625x313.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-179x90.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-768x385.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-645x323.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-500x251.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-160x80.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-362x182.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ufm-events-alarms-dashboard-219x110.png 219w\" sizes=\"(max-width: 975px) 100vw, 975px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. UFM <strong>Events &amp; Alarms</strong> dashboard</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>When building a network, performance is a key consideration, but performance and ease of use don\u2019t have to be thought of as a tradeoff.&nbsp;</p>\n\n\n\n<p>InfiniBand is easy to adopt, deploy, and operationalize for AI. Leveraging the power of UFM, the <a href=\"https://docs.nvidia.com/networking/display/ibclusteropmainguide\">cluster operations and maintenance guide</a> contains everything a network administrator needs to know. It\u2019s a lot simpler than cracking open your networking certification textbooks as the cluster guide is less than 40 pages.&nbsp;</p>\n\n\n\n<p>Consider choosing the simplicity of NVIDIA Quantum InfiniBand for your AI infrastructure.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>A common technological misconception is that performance and complexity are directly linked. That is, the highest-performance implementation is also the most challenging to implement and manage. When considering data center networking, however, this is not the case.&nbsp; InfiniBand is a protocol that sounds daunting and exotic in comparison to Ethernet, but because it is built &hellip; <a href=\"https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/\">Continued</a></p>\n", "protected": false}, "author": 1127, "featured_media": 76983, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338074", "discourse_permalink": "https://forums.developer.nvidia.com/t/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband-and-nvidia-ufm/279878", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205], "tags": [453, 1631], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/quantum-infiniband-guide-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-k1z", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76977"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1127"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76977"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76977/revisions"}], "predecessor-version": [{"id": 77163, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76977/revisions/77163"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76983"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76977"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76977"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76977"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76684, "date": "2024-01-23T09:00:00", "date_gmt": "2024-01-23T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76684"}, "modified": "2024-01-25T10:17:27", "modified_gmt": "2024-01-25T18:17:27", "slug": "how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/", "title": {"rendered": "Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://developer.nvidia.com/metropolis-microservices\">NVIDIA Metropolis microservices</a> provide powerful, customizable, cloud-native APIs and microservices to develop vision AI applications and solutions. The framework now includes <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">NVIDIA Jetson</a>, enabling developers to quickly build and productize performant and mature vision AI applications at the edge.</p>\n\n\n\n<p>APIs enhance flexibility, interoperability, and efficiency in software development by enabling seamless communication and integration between different applications and services. Two of the common functionalities for building video analytics applications are video streaming and AI-based generation of insights and analytics.&nbsp;</p>\n\n\n\n<p>This post features the API workflow for building vision AI applications and integrating them into any client application. We explain three key steps for building your applications:&nbsp;</p>\n\n\n\n<ul>\n<li>Stream video from the edge to any device through WebRTC using APIs.</li>\n\n\n\n<li>Generate insights and alerts for people/object movement using tripwire functionality, accessed through APIs.</li>\n\n\n\n<li>Use the reference cloud for secure, remote device API access.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Modular architecture&nbsp;</h2>\n\n\n\n<p>NVIDIA Metropolis microservices for Jetson provide a modular architecture with a large collection of software, including customizable, reusable microservices for building vision AI applications. The suite also offers platform services for infrastructural capabilities and a reference cloud. Various microservices include Video Storage Toolkit (VST), an AI perception service based on <a href=\"https://developer.nvidia.com/deepstream-sdk\">NVIDIA DeepStream</a>, and an Analytics service. Each provides APIs to configure and access the functionality of the microservices.&nbsp;</p>\n\n\n\n<p>These APIs are presented externally to the system using the Ingress platform service, based on a standard pattern used in cloud-native architectures to expose APIs within a system using a single gateway. Client applications exercise microservice functionality by invoking the respective APIs through the Ingress service. Further, NVIDIA Metropolis microservices provide an IoT cloud module that enables clients to be authenticated and authorized when accessing these APIs remotely.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"780\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson.png\" alt=\"Diagram showing the cloud-native workflow for a complete application using NVIDIA Metropolis microservice for NVIDIA Jetson.\" class=\"wp-image-76743\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-300x117.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-625x244.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-768x300.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-1536x599.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-645x252.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-500x195.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-362x141.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-282x110.png 282w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/diagram-cloud-native-nvidia-metropolis-microservices-jetson-1024x400.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Cloud-native NVIDIA Metropolis microservices for Jetson</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Video streaming through WebRTC</h2>\n\n\n\n<p>Viewing video streams from cameras connected to a system using a client app such as a mobile app or browser is a common requirement for video analytics systems. This functionality is supported through a standardized call flow based on the VST APIs. The VST microservice supports remote streaming using the WebRTC (Web Real-Time Communication) protocol, which is designed for reliable, peer-to-peer delivery of video and other data over the Internet.&nbsp;</p>\n\n\n\n<p>This section presents an overview of the salient concepts underlying the WebRTC protocol, and using VST APIs to enable WebRTC-based streaming. WebRTC is a powerful open-source project that enables real-time communication directly between two peers, such as a web browser and a Jetson device running VST.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Entities for WebRTC streaming</h3>\n\n\n\n<p>A typical WebRTC session involves a few different entities, which are listed below:</p>\n\n\n\n<p><strong>User agent:</strong> Represents the mobile, browser, or web application initiating the communication using the VST APIs.</p>\n\n\n\n<p><strong>Signaling server:</strong> A web server implemented within the VST involved in the establishment of communication channels for the WebRTC session.</p>\n\n\n\n<p><strong>ICE (Interactive Connectivity Establishment) server:</strong> A logical module implemented within the VST-WebRTC stack to determine the best connection path between peers. This is necessary for traversing firewalls and NATs (Network Address Translators).</p>\n\n\n\n<p><strong>STUN (Session Traversal Utilities for NAT) server:</strong> A type of ICE server that helps in discovering public IP addresses and Ports. It&#8217;s necessary when peers are using private (NAT-based) IP addresses. This is a third-party entity hosted on a public cloud network.</p>\n\n\n\n<p><strong>TURN (Traversal Using Relays around NAT) server:</strong> Acts as a relay if direct peer-to-peer communication fails and is only needed when peers are on different networks. This is supported through third-party services such as Twilio\u200c.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"4901\" height=\"2141\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1.png\" alt=\"Graphic showing the different entities involved with peer-to-peer WebRTC streaming, including signaling server, WebRTC, STUN, TURN.\n\" class=\"wp-image-77044\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1.png 4901w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-625x273.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-768x336.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-1536x671.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-2048x895.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-645x282.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-252x110.png 252w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-streaming-entities-1-1024x447.png 1024w\" sizes=\"(max-width: 4901px) 100vw, 4901px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Entities for WebRTC streaming</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">WebRTC session stages</h3>\n\n\n\n<p>WebRTC sessions use <em>control paths</em> and <em>data paths</em> to enable session creation and streaming.</p>\n\n\n\n<p>A control path enables setting up and managing sessions between peers whose stages include initialization, signaling, ICE candidate exchange, and connection establishment. VST enables user agents to perform these operations remotely through its APIs.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>A data path enables real-time media data transfer along with adaptation and quality control, and finally closing the connection.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Enabling WebRTC streaming through VST APIs</h3>\n\n\n\n<p>Figure 3 shows the call flow between clients and VST capturing the control and data paths for enabling WebRTC sessions.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"622\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-625x622.png\" alt=\"Diagram showing the call flow between clients and VST, capturing the control and data paths for enabling WebRTC sessions.\n\" class=\"wp-image-77046\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-625x622.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-768x765.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-1536x1530.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-2048x2040.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-645x642.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-301x300.png 301w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-362x361.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-110x110.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/webrtc-call-flow-with-vst-1-1024x1020.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. WebRTC call flow with VST</em></figcaption></figure>\n\n\n\n<p>The call flow starts with a client discovering various video streams using the <code>api/v1/sensor/list</code>&nbsp;API.&nbsp;</p>\n\n\n\n<p>The control and data paths are implemented based on the following call flow:&nbsp;</p>\n\n\n\n<ul>\n<li>Client calls <code>GET api/v1/live/iceServers or api/v1/replay/iceServers</code> to get the list of ICE servers from VST.</li>\n\n\n\n<li>Client creates a local offer and sends the offer to VST using <code>POST api/v1/live/stream/start or api/v1/replay/stream/start</code>.</li>\n\n\n\n<li>VST creates an answer for the client and returns it as a response.&nbsp;</li>\n\n\n\n<li>Client completes the ICE exchange using <code>GET</code> and <code>POST</code> requests for <code>api/v1/live/iceCandidate</code> or <code>api/v1/replay/iceCandidate</code> with the <code>peerid</code> as query parameter.</li>\n\n\n\n<li>Video data starts flowing when the peer connection is complete.</li>\n</ul>\n\n\n\n<p>When streaming is underway, the client can control streaming using the following stream APIs:</p>\n\n\n\n<ul>\n<li>Pause the video pipeline: <code>api/v1/replay/stream/pause</code></li>\n</ul>\n\n\n\n<ul>\n<li>Resume the video pipeline: <code>api/v1/replay/stream/resume</code></li>\n</ul>\n\n\n\n<ul>\n<li>Find a specific time in the video: <code>api/v1/replay/stream/seek</code></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Build client applications</h2>\n\n\n\n<p>These concepts can be applied to add video streaming capability to browser-based web applications by invoking VST APIs over HTTP in JavaScript, while leveraging WebRTC support in JavaScript supported by most browsers. Similar concepts can be applied to building native WebRTC client applications as well.&nbsp;</p>\n\n\n\n<p>To set up WebRTC streaming with JavaScript, follow these steps:</p>\n\n\n\n<h3 class=\"wp-block-heading\">Initialize a peer connection</h3>\n\n\n\n<p>Create a new <code>RTCPeerConnection</code> object with appropriate configuration settings.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Handle the track addition</h3>\n\n\n\n<ul>\n<li>Set up an event listener for the <code>ontrack</code> event.</li>\n\n\n\n<li>When a new track is added, update the remote video element to display the incoming video stream.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Generate an offer</h3>\n\n\n\n<ul>\n<li>Use the <code>createOffer</code> method to generate an offer for the peer connection.</li>\n\n\n\n<li>Set the local description of the peer connection to the generated offer.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Send the offer to VST</h3>\n\n\n\n<ul>\n<li>Obtain the local description (offer) using <code>peerConnection.localDescription</code>.</li>\n\n\n\n<li>Send the offer to the VST using the appropriate start API; for example, <code>api/v1/live/stream/start</code>.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Receive an answer from VST</h3>\n\n\n\n<p>When an answer SDP is received from VST as the start API response, set it as the remote description using <code>peerConnection.setRemoteDescription</code>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Handle ICE candidates</h3>\n\n\n\n<ul>\n<li>Exchange ICE candidates using <code>GET</code> and <code>POST</code> requests of the <code>api/v1/live/iceCandidate</code> API.</li>\n\n\n\n<li>Add the received ICE candidates to the peer connection using <code>peerConnection.addIceCandidate</code>.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Generate spatial insights and alerts for object movement</h2>\n\n\n\n<p>The Analytics microservice supports three people or objects analytics modules:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>Field of View (FOV)</strong>: Count people or objects in the camera&#8217;s field of view.</li>\n\n\n\n<li><strong>Tripwire</strong>: Detect people or objects crossing across user-defined tripwire line segments.</li>\n\n\n\n<li><strong>Region of Interest (ROI)</strong>: Count people or objects in the defined region of interest.</li>\n</ul>\n\n\n\n<p>In combination, these modules provide a powerful set of tools to understand the movement of people or objects across physical spaces, with use cases ranging across retail warehouses, security, and safety. Client applications use APIs to identify sensor lists, create tripwires, and retrieve counts and alerts for each of these functionalities.&nbsp;</p>\n\n\n\n<p>This section walks through an end-to-end example of these operations for tripwires. A similar methodology can be used to enable FOV and ROI. For each case, invoke the HTTP APIs in the programming language or HTTP client of your choice.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Retrieve the sensor list</h3>\n\n\n\n<p>As a first step, retrieve the name of the sensor for which the tripwire is to be configured.&nbsp;</p>\n\n\n\n<p>Invoke the VST API for listing all the sensors. Identify the sensor of interest from the returned list. The <code>name</code> attribute of the sensor object is to be used as the sensor ID in subsequent steps to configure and retrieve tripwire counts and alerts. Replace &lt;device-ip&gt; with the IP address of your device.</p>\n\n\n\n<p><code>http://&lt;device-ip&gt;:30080/vst/api/v1/sensor/list</code></p>\n\n\n\n<h3 class=\"wp-block-heading\">Create a tripwire configuration</h3>\n\n\n\n<p>In this step, configure a tripwire to specify which lines you wish to count the people crossing.</p>\n\n\n\n<p>When configuring a tripwire, specify the following properties:</p>\n\n\n\n<ul>\n<li><strong>Sensor ID</strong>: Identification of the sensor for which the tripwire is to be configured.</li>\n\n\n\n<li><strong>Tripwire ID</strong>: Identification of the tripwire. A sensor may have multiple definitions. Each tripwire needs to have a unique identifier.</li>\n\n\n\n<li><strong>Wire</strong>: A sequence of points representing the line segments comprising the tripwire.</li>\n\n\n\n<li><strong>Direction</strong>: A vector (two points) depicting the directionality of crossing (entry/exit).</li>\n</ul>\n\n\n\n<p>Note that the coordinates for points are in the camera coordinates (image plane). The top left corner is (0,0).</p>\n\n\n\n<p>Client applications like the reference mobile app provided with NVIDIA Metropolis microservices offer a visual aid to select a point without needing to determine the (x, y) locations manually. Figure 4 shows an example of a tripwire created and rendered through the mobile app. The user selects the tripwire anchor points using the touch interface in the app to draw the tripwire (green lines), along with directionality (red arrow).</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"436\" height=\"589\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1.png\" alt=\"Visual rendering in the reference mobile app, provided with Metropolis microservices. Users can draw tripwires, specify directions, and view the real-time analytics. \" class=\"wp-image-76812\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1.png 436w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1-222x300.png 222w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1-85x115.png 85w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1-67x90.png 67w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1-362x489.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-metropolis-microservices-mobile-app-tripwire-1-81x110.png 81w\" sizes=\"(max-width: 436px) 100vw, 436px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Tripwire visual rendering in the reference mobile app provided with NVIDIA Metropolis microservices</em></figcaption></figure></div>\n\n\n\n<p>To configure a tripwire with <code>Id = main_door</code> for a sensor with <code>Id = Amcrest_3</code>, invoke the following HTTP API call in the programming language of your choice:</p>\n\n\n\n<p><code>http://&lt;device-ip&gt;:30080/emdx/api/config/tripwire?sensorId=Amcrest_3</code></p>\n\n\n\n<pre class=\"wp-block-code\"><code>{\n  \"deleteIfPresent\": false,\n  \"tripwires\": &#091;\n    {\n        \"direction\": {\n          \"entry\": {\n            \"name\": \"Inside the room\"\n          },\n          \"exit\": {\n            \"name\": \"Outside of the room\"\n          },\n          \"p1\": { \"x\": 753, \"y\": 744},\n          \"p2\": { \"x\": 448, \"y\": 856}\n        },\n        \"id\": \"main_door\",\n        \"name\": \"Main door\",\n        \"wire\": &#091;\n          { \"x\": 321, \"y\": 664 },\n          { \"x\": 544, \"y\": 648 },\n          { \"x\": 656, \"y\": 953 },\n          { \"x\": 323, \"y\": 1067}\n        ]\n      }\n  ],\n  \"sensorId\": \"Amcrest_3\"\n}\n</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Configure tripwire alert rule (optional)</h3>\n\n\n\n<p>It is optional to configure an alert rule for a given tripwire. Alert rules are specific conditions which when met result in an alert event being generated.</p>\n\n\n\n<p>To configure an alert rule that will raise an alert whenever one person crosses the tripwire (main door) in the direction of entry, invoke the following API request:</p>\n\n\n\n<p><code>http://&lt;device-ip&gt;:30080/emdx/api/config/rule/alerts/tripwire</code></p>\n\n\n\n<pre class=\"wp-block-code\"><code>{\n  \"sensorId\": \"Amcrest_3\",\n  \"rules\": &#091;\n    {\n      \"rule_id\": \"cd2218f6-e4d2-4ad4-9b15-3396e4336064\",\n      \"id\": \"main_door\",\n      \"type\": \"tripwire\",\n      \"rule_type\": \"increment\",\n      \"time_interval\": 1,\n      \"count_threshold\": 1,\n      \"direction\": \"entry\"\n    }\n  ]\n}\n</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Retrieve tripwire counts and alerts</h3>\n\n\n\n<p>This step explains how to retrieve the counts of the people crossing the tripwire defined previously. Optionally, you can also retrieve the alerts generated based on configured alert rules for that tripwire.</p>\n\n\n\n<p>The counts can be queried for a particular tripwire (<code>sensorId</code>, <code>tripwireId</code>), for a time range (<code>fromTimestamp</code>, <code>toTimestamp</code>) and aggregated to a specified time window (<code>fixedInterval</code>). Optionally, you can retrieve alerts and counts by setting the <code>alerts</code><em> </em>query parameter to <code>true</code>:</p>\n\n\n\n<p><code>http://&lt;device-ip&gt;:30080/emdx/api/metrics/tripwire/histogram?sensorId=Amcrest_3&amp;tripwireId=main_door&amp;fromTimestamp=2020-10-30T20:00:00.000Z&amp;toTimestamp=2020-10-30T20:01:00.000Z&amp;fixedInterval=1000&amp;alerts=true</code></p>\n\n\n\n<pre class=\"wp-block-code\"><code>{\n    \"alerts\": &#091;\n     {\n            \"count\": 1,\n            \"description\": \"1 people entered tripwire\",\n            \"duration\": 1.000,\n            \"startTimestamp\": \"2020-10-30T20:00:59.000Z\",\n            \"endTimestamp\": \"2020-10-30T20:01:00.000Z\",\n            \"id\": \"unique-alert-id\",\n            \"rule_type\": \"increment\",\n            \"rule_id\": \"cd2218f6-e4d2-4ad4-9b15-3396e4336064\",\n            \"sensorId\": \"Amcrest_3\",\n            \"type\": \"tripwire\",\n            \"direction\": \"entry\",\n            \"directionName\": \"Inside the room\", \n            \"attributes\": &#091;..],\n        }\n     ],\n    \"counts\": &#091;\n      {\n        \"agg_window\": \"1 sec\",\n        \"histogram\": &#091;\n          {\n            \"end\": \"2020-10-30T20:00:01.000Z\",\n            \"start\": \"2020-10-30T20:00:00.000Z\",\n            \"sum_count\": 1\n          }\n        ],\n        \"attributes\": &#091;...],\n        \"sensorId\": \"Amcrest_3\",\n        \"type\": \"exit\"\n      },\n      {\n        \"agg_window\": \"1 sec\",\n        \"histogram\": &#091;\n          {\n            \"end\": \"2020-10-30T20:00:01.000Z\",\n            \"start\": \"2020-10-30T20:00:00.000Z\",\n            \"sum_count\": 0\n          },\n          \u2026..\n        ],\n        \"attributes\": &#091;.. ],\n        \"sensorId\": \"Amcrest_3\",\n        \"type\": \"entry\"\n      }\n    ]\n  }\n</code></pre>\n\n\n\n<p>The histograms are returned for each direction separately. The entire time range is divided into time windows of <code>fixedInterval</code>. Crossings for each time window <code>start,end</code> are reported as <code>sum_count</code>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Retrieve tripwire alerts</h3>\n\n\n\n<p>To retrieve all the alerts for a given sensor, invoke the following API:</p>\n\n\n\n<p><code>http://&lt;device-ip&gt;:30080/emdx/api/alerts?sensorId=Amcrest_3&amp;fromTimestamp=2020-10-30T20:00:00.000Z&amp;toTimestamp=2020-10-31T01:00:00.000Z</code></p>\n\n\n\n<h2 class=\"wp-block-heading\">Secure, remote, cloud-based API access&nbsp;</h2>\n\n\n\n<p>APIs enable clients to access device configuration and functionality remotely using the HTTP protocol. While in the development stage, API invocation is recommended by directing HTTP requests to the device IP address. However, in production scenarios, the IP addresses of devices would typically be unknown to clients.&nbsp;</p>\n\n\n\n<p>In addition, Jetson devices might be located behind firewalls, rendering them unreachable, or they may use NAT-based IP addresses that may not be valid externally. The IoT cloud facilitates product-grade remote API invocation by providing a mechanism to forward requests from network-separated clients to devices in a secure manner.&nbsp;</p>\n\n\n\n<p>This section describes the mechanism by which clients can obtain security tokens and use them to make HTTP through the cloud, to be forwarded to the appropriate device.&nbsp;</p>\n\n\n\n<p>While the focus of this section is to showcase how clients can invoke device APIs through the cloud,\u200c note that cloud architecture provides a secure \u2018device claim\u2019 mechanism for authorized access to specific devices through the cloud. All user device access through the cloud goes through authentication and authorization, and users can only access devices that they have previously claimed.&nbsp;</p>\n\n\n\n<p>This functionality is designed with high customizability, enabling seamless integration with Original Design Manufacturers (ODM) and Original Equipment Manufacturers (OEM) operators&#8217; existing security frameworks and cloud backend infrastructures.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Workflow for device API invocation through the IoT Cloud</h2>\n\n\n\n<p>The reference IoT cloud implementation uses <a href=\"https://aws.amazon.com/cognito/\">Amazon Cognito</a> as the identity provider (IdP), but users are welcome to use any third-party identity provider. To access the device APIs through cloud endpoints, use the authentication and authorization call flow outlined below.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Authenticate with the Amazon Cognito</h3>\n\n\n\n<p>Sign in to the login URI page using a web console to be authenticated with Amazon Cognito. With successful authentication, Amazon Cognito returns a unique authorization code. Using authorization code, makes a request to Amazon Cognito to issue a time-bound ID token. Present this ID token while invoking IoT cloud security APIs.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"553\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-625x553.png\" alt=\"The call flow diagram initiated by a user to an IDP service such as Amazon Cognito to authenticate the user. \" class=\"wp-image-77070\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-625x553.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-300x266.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-130x115.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-768x680.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-1536x1360.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-2048x1813.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-645x571.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-339x300.png 339w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-102x90.png 102w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-362x321.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-124x110.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-with-idp-1-1024x907.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Call flow to authorize with IDP</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Generate a JWT token and invoke device APIs</h3>\n\n\n\n<p>To access the IoT device APIs, first request an authorization token from the IoT cloud security. Upon a valid request, the IoT cloud security issues an ephemeral signed authorization JWT token. Use this token to then invoke device APIs through the IoT cloud transport, which validates it and forwards the request to the device.&nbsp;</p>\n\n\n\n<p>Note that an unauthorized HTTP error code is returned if the user does not have rights to perform the operation based on the device claim.&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"6744\" height=\"4127\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1.png\" alt=\"Call flow diagram showing the user initiating the authorization by requesting an authorization token from IoT cloud and using it to invoke the device APIs. \n\" class=\"wp-image-77079\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1.png 6744w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-300x184.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-625x382.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-768x470.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-1536x940.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-2048x1253.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-645x395.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-490x300.png 490w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-147x90.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-362x222.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-180x110.png 180w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/call-flow-diagram-authorize-iot-cloud-service-1-1024x627.png 1024w\" sizes=\"(max-width: 6744px) 100vw, 6744px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Call flow to authorize a user with IoT cloud </em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Summary&nbsp;</h2>\n\n\n\n<p>Build powerful, market-ready vision AI applications at the edge with NVIDIA Metropolis APIs and microservices. APIs provide a standardized, secure, and distributed means of exercising the capabilities of various NVIDIA Metropolis microservices. The reference mobile application included as part of this release showcases a mature end-user application built using these APIs with a user-friendly interface capturing configuration, video streaming, analytics, alerts, cloud integration, and device claim. This application includes source code, with a walkthrough of the various modules in the mobile app section of the release documentation. </p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/metropolis-microservices/jetson-application-form\">Download NVIDIA Metropolis microservices for Jetson</a>.&nbsp;And register to join us for the two-part webinar, <a href=\"https://info.nvidia.com/metropolis-on-jetson-webinar.html\">Accelerate Edge AI Development With Metropolis APIs and Microservices for Jetson</a> (Part 1) and <a href=\"https://info.nvidia.com/build-with-metropolis-microservices-for-jetson.html\">How to Build With Metropolis Microservices for Jetson</a> (Part 2).</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Metropolis microservices provide powerful, customizable, cloud-native APIs and microservices to develop vision AI applications and solutions. The framework now includes NVIDIA Jetson, enabling developers to quickly build and productize performant and mature vision AI applications at the edge. APIs enhance flexibility, interoperability, and efficiency in software development by enabling seamless communication and integration between &hellip; <a href=\"https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/\">Continued</a></p>\n", "protected": false}, "author": 430, "featured_media": 76954, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338053", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/279873", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63], "tags": [2424, 422, 453, 347, 2792, 1472, 1394], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-icon-shuffle-gif.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jWQ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76684"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/430"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76684"}], "version-history": [{"count": 87, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76684/revisions"}], "predecessor-version": [{"id": 77161, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76684/revisions/77161"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76954"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76684"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76684"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76684"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76663, "date": "2024-01-23T09:00:00", "date_gmt": "2024-01-23T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76663"}, "modified": "2024-01-25T10:17:29", "modified_gmt": "2024-01-25T18:17:29", "slug": "bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/", "title": {"rendered": "Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://developer.nvidia.com/metropolis-microservices\">NVIDIA Metropolis Microservices for Jetson</a> provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches.</p>\n\n\n\n<p>This post explains how to develop and deploy <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a>\u2013powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that can be used as a general recipe for nearly any model.&nbsp;</p>\n\n\n\n<p>The reference example uses a stand-alone zero-shot detection <a href=\"https://github.com/NVIDIA-AI-IOT/nanoowl\">NanoOwl application</a> and integrates it with <a href=\"https://developer.nvidia.com/metropolis-microservices\">Metropolis Microservices for Jetson</a>, so that you can quickly prototype and deploy it in production.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Transform your applications with generative AI</h2>\n\n\n\n<p>Generative AI is a new class of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> that enables models to understand the world in a more open way than previous methods.&nbsp;</p>\n\n\n\n<p>At the heart of most generative AI is a transformer-based model that has been trained on internet-scale data. These models have a much broader understanding across domains, enabling them to be used as a backbone for a variety of tasks. This flexibility enables models like CLIP, Owl, Llama, GPT, and Stable Diffusion to comprehend natural language inputs. They are capable of zero or few-shot learning.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"566\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/zero-shot-detection-nanoowl.gif\" alt=\"GIF shows the NanoOwl model detecting a person, face, hands, and shoes on request.\u00a0\" class=\"wp-image-76965\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Zero-shot detection using NanoOwl</em></figcaption></figure></div>\n\n\n<p>For more information about generative AI models for Jetson, see the <a href=\"http://www.jetson-ai-lab.com/\">NVIDIA Jetson Generative AI Lab</a> and <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/\">Bringing Generative AI to Life with NVIDIA Jetson</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Metropolis Microservices for Jetson</h3>\n\n\n\n<p>Metropolis Microservices can be used to rapidly build production-ready AI applications on Jetson. Metropolis Microservices are a set of modular and easily deployable Docker containers for camera management, system monitoring, IoT device integration, networking, storage, and more. These can be brought together to create powerful applications. Figure 2 shows the available microservices.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"403\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-625x403.png\" alt=\"The diagram shows reference AI workflows, app microservices, an AI stack, and platform software.\" class=\"wp-image-77116\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-300x193.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-768x495.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-1536x990.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-466x300.png 466w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-362x233.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-171x110.png 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-1024x660.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Metropolis Microservices for Jetson stack</em></figcaption></figure></div>\n\n\n<p>For more information, see the <a href=\"https://resources.nvidia.com/en-us-metropolis-microservices-for-jetson/jetson-whitepaper\" data-type=\"link\" data-id=\"https://resources.nvidia.com/en-us-metropolis-microservices-for-jetson/jetson-whitepaper\">Metropolis Microservices for Jetson</a> whitepaper.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Integrating generative AI apps with Metropolis Microservices</h3>\n\n\n\n<p>Metropolis Microservices and generative AI can be combined to take advantage of models that require little to no training. Figure 3 shows a diagram of the NanoOwl reference example that can be used as a general recipe to build generative AI\u2013powered applications with Metropolis Microservices on Jetson.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1384\" height=\"652\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson.png\" alt=\"The system diagram shows the user interfaces, live camera streams, and microservices on Jetson, including the generative AI application, Video Storage Toolkit, ingress, Redis, and monitoring.\" class=\"wp-image-76967\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson.png 1384w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-625x294.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-768x362.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-645x304.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-500x236.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-233x110.png 233w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-1024x482.png 1024w\" sizes=\"(max-width: 1384px) 100vw, 1384px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Generative AI reference application using Metropolis Microservices for Jetson</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Application customization with Metropolis Microservices</h3>\n\n\n\n<p>There are many open-source generative AI models available on GitHub and some have been optimized to run specifically on Jetson. You can find several of these models in the <a href=\"https://www.jetson-ai-lab.com/\">Jetson Generative AI Lab</a>.&nbsp;</p>\n\n\n\n<p>Most of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output.&nbsp;</p>\n\n\n\n<p>In the Python reference example, we used NanoOwl as the generative AI model. However, the general recipe of the reference example can be applied to nearly any generative AI model.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1734\" height=\"824\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1.png\" alt=\"The diagram shows how a generative AI application can take in an RTSP stream and output detection information to RTSP and Redis.\" class=\"wp-image-77119\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1.png 1734w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-300x143.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-625x297.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-768x365.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-1536x730.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-645x307.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-500x238.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-362x172.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-231x110.png 231w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-1024x487.png 1024w\" sizes=\"(max-width: 1734px) 100vw, 1734px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Overview of a generative AI application using Metropolis Microservices</em></figcaption></figure></div>\n\n\n<p>To run any generative AI model with Metropolis Microservices, you must first align the input and output from other microservices (Figure 4).&nbsp;</p>\n\n\n\n<p>For streaming video, the input and output uses the RTSP protocol. RTSP is streamed from Video Storage Toolkit (VST), a video ingestion and management microservice. The output is streamed over RTSP with the overlaid inference output. The output metadata is sent to a Redis stream where other applications can read the data. For more information, see the <a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-238cd4a8-7f1d-42b4-9b0a-a949ade92845/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-238cd4a8-7f1d-42b4-9b0a-a949ade92845/\">Video Storage Toolkit with Metropolis Microservices demo videos</a>.</p>\n\n\n\n<p>Second, as a generative AI application requires some external interface such as prompts, you need the application to take REST API requests.&nbsp;</p>\n\n\n\n<p>Lastly, the application must be containerized to integrate seamlessly with other microservices. Figure 5 shows an example of NanoOwl object detection and metadata output on Redis.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"181\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-running.gif\" alt=\"GIF shows the generative AI application detecting various objects like boxes, pallets, and people.\u00a0\" class=\"wp-image-76969\"/><figcaption class=\"wp-element-caption\"><em>Figure 5. Generative AI application running</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Prepare the generative AI application</h2>\n\n\n\n<p>This reference example uses NanoOwl. However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe. For more information about the full implementation, see the reference example on the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai\">/NVIDIA-AI-IOT/mmj_genai</a> GitHub project.</p>\n\n\n\n<p>To prepare a generative AI model for integration with Metropolis Microservices, take the following steps:&nbsp;</p>\n\n\n\n<ol>\n<li>Call the <code>predict</code> function for model inference</li>\n\n\n\n<li>Add RTSP I/O using the <code>jetson-utils</code> library.</li>\n\n\n\n<li>Add a REST endpoint for prompt updates with Flask.</li>\n\n\n\n<li>Use <code>mmj_utils</code> to generate overlays.</li>\n\n\n\n<li>Use <code>mmj_utils</code> to interact with VST to get streams.</li>\n\n\n\n<li>Use <code>mmj_utils</code> to output metadata to Redis.</li>\n</ol>\n\n\n\n<h3 class=\"wp-block-heading\">Call the predict function for model inference</h3>\n\n\n\n<p>NanoOwl wraps the generative AI model in an <code>OwlPredictor</code> class. When this class is instantiated, it loads the model into memory. To make an inference on an image and text input, call the <code>predict</code> function to get the output.&nbsp;</p>\n\n\n\n<p>In this case, the output is a list of bounding boxes and labels for the detected objects.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport PIL.Image\nimport time\nimport torch\nfrom nanoowl.owl_predictor import OwlPredictor\n\nimage = PIL.Image.open(&quot;my_image.png&quot;)\nprompt = &#91;&quot;an owl&quot;, &quot;a person&quot;]\n\n#Load model\n predictor = OwlPredictor(\n       &quot;google/owlvit-base-patch32&quot;,\n        image_encoder_engine=&quot;../data/owlvit_image_encoder_patch32.engine&quot;\n    )\n#Embed Text\ntext_encodings = predictor.encode_text(text)\n\n#Inference\noutput = predictor.predict(\nimage=image, \n        \ttext=prompt, \n        \ttext_encodings=text_encodings,\n       \t threshold=0.1,\n        \tpad_square=False)\n</pre></div>\n\n\n<p>Most generative AI models have similar Python interfaces. There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the <code>OwlPredictor</code> class.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Add RTSP I/O using the jetson-utils library</h3>\n\n\n\n<p>You can add RTSP video stream input using the <a href=\"https://github.com/dusty-nv/jetson-utils\">jetson-utils</a> library. This library provides <code>videoSource</code> and <code>videoOutput</code> classes that can be used to capture frames from an RTSP stream and output frames on a new RTSP stream.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom jetson_utils import videoSource, videoOutput\n\nstream_input = &quot;rtsp://0.0.0.0:8554/input&quot;\nstream_output = &quot;rtsp://0.0.0.0:8555/output&quot;\n\n#Create stream I/O\nv_input = videoSource(stream_input)\nv_output = videoOutput(stream_output)\n\nwhile(True):\n\timage = v_input.Capture() #get image from stream\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\n\tnew_image = postprocess(output)\n\n\tv_output.Render(new_image) #write image to stream \n</pre></div>\n\n\n<p>This code example captures frames from an RTSP stream, which can then be passed to a model inference function. A new image is created from the model outputs and rendered to an output RTSP stream.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Add a REST endpoint for prompt updates with Flask</h3>\n\n\n\n<p>Many generative AI models accept some kind of prompt or text input. To enable a user or another service to update the prompt dynamically, add a REST endpoint using <a href=\"https://flask.palletsprojects.com/en/3.0.x/\">Flask</a> that accepts prompt updates and passes them to the model.&nbsp;</p>\n\n\n\n<p>To make the Flask server integrate more easily with your model, create a wrapper class that can be called to launch a Flask server in its own thread. For more information, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai\">/NVIDIA-AI-IOT/mmj_genai</a> GitHub project.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom flask_server import FlaskServer\n\n#Launch flask server and connect queue to receive prompt updates \nflask_queue = Queue() #hold prompts from flask input \nflask = FlaskServer(flask_queue)\nflask.start_flask()\n\nwhile(True):\n\t...\n\n\tif not flask_queue.empty(): #get prompt update\n            prompt = flask_queue.get()\n\t\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t...\n</pre></div>\n\n\n<p>Connect your main script and the Flask endpoint through a queue that holds any incoming prompt updates. When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Use mmj_utils to generate overlays</h3>\n\n\n\n<p>For computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1086\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils.png\" alt=\"Image shows a factory floor with people, pallets, and equipment delineated by labeled bounding boxes.\u00a0\" class=\"wp-image-76970\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-300x163.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-625x340.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-179x97.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-768x417.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-1536x834.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-645x350.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-500x272.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-362x197.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-202x110.png 202w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-1024x556.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Detection overlay generated with mmj_utils</em></figcaption></figure></div>\n\n\n<p>To do this, use the utility class called <code>DetectionGenerationCUDA</code> from the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_utils\">mmj_utils library</a>. This library depends on <code>jetson_utils</code>, which provides CUDA-accelerated functions used to generate the overlay.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom mmj_utils.overlay_gen import DetectionOverlayCUDA\n\noverlay_gen = DetectionOverlayCUDA(draw_bbox=True, draw_text=True, text_size=45) #make overlay object\n\nwhile(True):\n\t...\n\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t\n\t#Generate overlay and output\n       text_labels = &#91;objects&#91;x] for x in output.labels]\n       bboxes = output.boxes.tolist()\n       image = overlay_gen(image, text_labels, bboxes)#generate overlay\n       v_output.Render(image)\n</pre></div>\n\n\n<p>You can instantiate the <code>DetectionGenerationCUDA</code> object with several keyword arguments to adjust the text size, bounding box size, and colors to suit your needs. For more information about overlay generation with <code>mmj_utils</code>, see <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_utils\">/NVIDIA-AI-IOT/mmj_utils</a> GitHub repo.</p>\n\n\n\n<p>To generate the overlay, call the object and pass the input image, list of labels, and bounding boxes generated by the model. It then draws the labels and bounding boxes on the input image and returns the modified image with the overlay. This modified image can then be rendered out on the RTSP stream.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Use mmj_utils to interact with VST to get streams</h3>\n\n\n\n<p>VST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the <a href=\"https://docs.nvidia.com/moj/vst/VST_API_Guide.html\">VST REST API</a>.&nbsp;</p>\n\n\n\n<p>Instead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom mmj_utils.vst import VST\n\nvst = VST(&quot;http://0.0.0.0:81&quot;)\nvst_rtsp_streams = vst.get_rtsp_streams()\nstream_input = vst_rtsp_streams&#91;0]\n\nv_input = videoSource(stream_input)\n...\n</pre></div>\n\n\n<p>This connects to VST and grabs the first valid RTSP link. More complex logic could be added here to connect to a specific source or change the inputs dynamically.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Use mmj_utils to output metadata to Redis</h3>\n\n\n\n<p>Generative AI models generate\u200c metadata that can be used downstream by other services for generating analytics and insights. </p>\n\n\n\n<p>In this case, NanoOwl outputs bounding boxes on detected objects. You can output this information in <a href=\"https://docs.nvidia.com/moj/emdx/metadata.html\">Metropolis Schema</a> on a Redis stream, which can be captured by an analytic service. In the <code>mmj_utils</code> library, there is a class to help produce detection metadata on Redis.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom mmj_utils.schema_gen import SchemaGenerator\n\n\nschema_gen = SchemaGenerator(sensor_id=1, sensor_type=&quot;camera&quot;, sensor_loc=&#91;10,20,30])\nschema_gen.connect_redis(aredis_host=0.0.0.0, redis_port=6379, redis_stream=&quot;owl&quot;)\n\nwhile True:\n\t...\n\toutput = predictor.predict(image=image, text=prompt, ...)\n\t\n\t#Output metadata\n       text_labels = &#91;objects&#91;x] for x in output.labels]\nschema_gen(text_labels, bboxes)\n</pre></div>\n\n\n<p>You can instantiate a <code>SchemaGenerator</code> object with information about the input camera stream and connect to Redis. The object can then be called by passing in text labels and bounding boxes produced by the model. The detection information gets converted to Metropolis Schema and output to Redis to be used by other microservices.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1309\" height=\"596\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app.png\" alt=\"The diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream.\" class=\"wp-image-76971\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app.png 1309w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-300x137.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-625x285.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-768x350.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-645x294.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-500x228.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-362x165.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-242x110.png 242w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-1024x466.png 1024w\" sizes=\"(max-width: 1309px) 100vw, 1309px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Generative AI application</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Application deployment</h2>\n\n\n\n<p>To deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through <code>docker compose</code>.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>With the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices.&nbsp;</p>\n\n\n\n<ol>\n<li>Containerize the generative AI application.</li>\n\n\n\n<li>Set up the necessary platform services.</li>\n\n\n\n<li>Launch the application with <code>docker compose</code>.</li>\n\n\n\n<li>View outputs in real time.</li>\n</ol>\n\n\n\n<h3 class=\"wp-block-heading\">Containerize the generative AI application</h3>\n\n\n\n<p>The first step for deployment is to containerize the generative AI application using Docker.&nbsp;</p>\n\n\n\n<p>An easy way to do this is to use the <a href=\"https://github.com/dusty-nv/jetson-containers\">jetson-containers</a> project. This project provides an easy way to build Docker containers for Jetson to support machine learning applications including generative AI models. Use jetson-containers to make a container with the necessary dependencies and then customize the container further to include the application code and any other packages needed to run your generative AI model.&nbsp;</p>\n\n\n\n<p>For more information about how to build the container for the NanoOwl example, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/src/README.md\" data-type=\"link\" data-id=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/src/README.md\">/src/readme</a> file in the GitHub project.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Set up the necessary platform services</h3>\n\n\n\n<p>Next, set up the necessary platform services provided by Metropolis Microservices. These platform services provide many features needed to deploy an application with Metropolis Microservices.&nbsp;</p>\n\n\n\n<p>This reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with <code>systemctl</code>.&nbsp;</p>\n\n\n\n<p>For more information about how to install and launch the necessary platform services, see the <a href=\"https://docs.nvidia.com/moj/index.html\">Metropolis Microservices for Jetson Quickstart Guide</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Launch the application with docker compose</h3>\n\n\n\n<p>With the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using <code>docker compose</code>.&nbsp;</p>\n\n\n\n<p>To do this, create a <code>docker-compose.yaml</code> file that defines the containers to be launched along with any necessary launch options. After you define the <code>docker compose</code> file, you can start or stop your application using the <code>docker compose up</code> and<em> </em><code>docker compose down</code> commands.&nbsp;</p>\n\n\n\n<p>For more information about docker deployment, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/deploy/README.md\" data-type=\"link\" data-id=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/deploy/README.md\">/deploy/readme</a> file in the GitHub project.</p>\n\n\n\n<h3 class=\"wp-block-heading\">View outputs in real time</h3>\n\n\n\n<p>After the application is deployed, you can add an RTSP stream through VST and interact with the generative AI model through the REST API to send prompt updates and view the detections change in real time by watching the RTSP output. You can also see the metadata output on Redis.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/DTVYAND5X8Q?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Transform Edge AI Applications with Generative AI Using Metropolis Microservices for Jetson</em></figcaption></figure>\n\n\n\n<h1 class=\"wp-block-heading\">Conclusion</h1>\n\n\n\n<p>This post explained how to take a generative AI model and integrate it with Metropolis Microservices for Jetson. With generative AI and Metropolis Microservices, you can rapidly build intelligent video analytic applications that are both flexible and accurate.</p>\n\n\n\n<p>For more information about the provided services, see the <a href=\"https://developer.nvidia.com/metropolis-microservices\">Metropolis Microservices for Jetson</a> product page. To view the full reference application and more detailed steps about how to build and deploy it for yourself, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai\">/NVIDIA-AI-IOT/mmj_genai</a> GitHub project.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches. This post explains how to develop and deploy generative AI\u2013powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that &hellip; <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/\">Continued</a></p>\n", "protected": false}, "author": 1925, "featured_media": 77068, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1338055", "discourse_permalink": "https://forums.developer.nvidia.com/t/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/279875", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 3110, 63], "tags": [453, 1950], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-iva-gen-ai-featured.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jWv", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76663"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1925"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76663"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76663/revisions"}], "predecessor-version": [{"id": 77120, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76663/revisions/77120"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77068"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76663"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76663"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76663"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 77003, "date": "2024-01-22T16:34:34", "date_gmt": "2024-01-23T00:34:34", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=77003"}, "modified": "2024-01-25T10:25:28", "modified_gmt": "2024-01-25T18:25:28", "slug": "model-monday-query-graphs-with-optimized-deplot-model", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/model-monday-query-graphs-with-optimized-deplot-model/", "title": {"rendered": "Model Monday: Query Graphs with Optimized DePlot Model"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\" target=\"_blank\">NVIDIA Foundation Models and Endpoints</a> provides access to a curated set of community and NVIDIA-built generative AI models to experience, customize, and deploy in enterprise applications.&nbsp;&nbsp;</p>\n\n\n\n<p>On Mondays throughout the year, we\u2019ll be releasing new models. This week, we released the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\" data-type=\"link\" data-id=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\">NVIDIA-optimized DePlot model</a>, which you can experience directly from your browser.</p>\n\n\n\n<p>If you haven\u2019t already, try the leading models like <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nemo-8b-qa\" target=\"_blank\">Nemotron-3</a>, <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b\" target=\"_blank\">Mixtral 8X7B</a>, <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/llama2-70b\" target=\"_blank\">Llama 2</a>, and <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/sdxl\" target=\"_blank\">Stable Diffusion</a> in the NVIDIA AI playground.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">DePlot&nbsp;</h2>\n\n\n\n<p>A leap in visual language reasoning, <a href=\"https://research.google/pubs/deplot-one-shot-visual-language-understanding-by-plot-to-text-translation/\" target=\"_blank\" rel=\"noreferrer noopener\">DePlot</a> by Google Research enables comprehension of charts and plots when coupled with a large language model (LLM). As opposed to prior multimodal LLMs that are trained end-to-end for plot de-rendering and numerical reasoning, this approach breaks down the problem into the following steps:&nbsp;</p>\n\n\n\n<ul>\n<li>Plot-to-text translation using a pretrained image-to-text model&nbsp;</li>\n\n\n\n<li>Textual reasoning using an LLM&nbsp;</li>\n</ul>\n\n\n\n<p>Specifically, DePlot refers to the image-to-text Transformer model in the first step, used for modality conversion from a plot to the text format. The linearized tables generated by DePlot can be directly ingested as part of a prompt into an LLM in the second step to facilitate reasoning.&nbsp;</p>\n\n\n\n<p>Previous state-of-the-art (SOTA) models required at least tens of thousands of human-written examples to accomplish such plot or chart comprehension while still being limited in their reasoning capabilities on complex queries.&nbsp;&nbsp;</p>\n\n\n\n<p>Using this plug-and-play approach, the DePlot+LLM pipeline achieves over 29.4% improvement over the previous SOTA on the <a href=\"https://arxiv.org/abs/2203.10244\" target=\"_blank\" rel=\"noreferrer noopener\">ChartQA</a> benchmark with just one-shot prompting!&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"484\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-625x484.png\" alt=\"Screenshot of the DePlot playground dashboard shows a bar chart with configuration settings.\" class=\"wp-image-77077\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-625x484.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-300x232.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-148x115.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-768x595.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-645x500.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-387x300.png 387w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-116x90.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-362x280.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1-142x110.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Deplot-Screenshot-b-1.png 1003w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. DePlot converts a plot into a structured table</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows how DePlot converts a plot into a structured table that can be used as context for LLMs to answer reasoning-based questions. Try <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\" data-type=\"link\" data-id=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\">DePlot</a> now.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Using the model in a browser&nbsp;</h2>\n\n\n\n<p>You can now experience DePlot directly from your browser using a simple user interface on the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\" data-type=\"link\" data-id=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\">DePlot playground on the NGC catalog</a>. The following video shows the results generated from the models running on a fully accelerated stack.&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-video aligncenter\"><video controls src=\"https://developer.download.nvidia.com/video/devblog/Deplot_video.mp4\"></video><figcaption class=\"wp-element-caption\"><em>Video 1. NVIDIA AI Foundation model interface</em></figcaption></figure>\n\n\n\n<p>The video shows the NVIDIA AI Foundation model interface used to extract information from a graph using DePlot running on a fully accelerated stack.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Using the model with the API&nbsp;</h2>\n\n\n\n<p>If you would rather use the API to test out the model, we\u2019ve got you covered. After you sign in to the <a href=\"https://catalog.stg.ngc.nvidia.com/orgs/nvidia/models/deplot\" target=\"_blank\" rel=\"noreferrer noopener\">NGC catalog</a>, you have access to NVIDIA cloud credits that enable you to truly experience the models at scale by connecting your application to the API endpoint.&nbsp;</p>\n\n\n\n<p>The following Python example uses <code>base64</code> and requests modules to encode the plot image and issue requests to the API endpoint. Before proceeding, ensure that you have an environment capable of executing Python code, such as a Jupyter notebook.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Obtain the NGC catalog API key&nbsp;</h3>\n\n\n\n<p>On the <strong>API</strong> tab, select <strong>Generate Key</strong>. If you haven&#8217;t registered yet, you are prompted to sign up or sign in.&nbsp;</p>\n\n\n\n<p>Set the API key in your code:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n# Will be used to issue requests to the endpoint\u00a0\nAPI_KEY = \u201cnvapi-xxxx\u201c\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Encode your chart or graph in base64 format&nbsp;</h3>\n\n\n\n<p>To provide an image input as part of your request, you must encode it in <code>base64</code> format.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n# Fetch an example chart from ChartQA dataset\u00a0\n!wget -cO -\u00a0 https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png &gt; chartQA-example.png\u00a0\n\n# Encode the image into base64 format\u00a0\u00a0\nimport base64\u00a0\n\nwith open(os.path.join(os.getcwd(), &quot;chartQA-example.png&quot;), &quot;rb&quot;) as image_file:\u00a0\n\u00a0\u00a0\u00a0 encoded_string = base64.b64encode(image_file.read())\n</pre></div>\n\n\n<p>As an option, you can visualize the chart:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nfrom IPython import display\u00a0\ndisplay.Image(base64.b64decode(encoded_string))\n</pre></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"311\" height=\"478\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example.png\" alt=\"Bar chart show the breakdown of support for government economic assistance across various European countries. This example chart is used to feed the model and generate values as a table.\" class=\"wp-image-77078\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example.png 311w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example-195x300.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example-75x115.png 75w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example-59x90.png 59w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example-72x110.png 72w\" sizes=\"(max-width: 311px) 100vw, 311px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Barchart example for table value generation</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Send an inference request&nbsp;</h3>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nimport requests   \n\ninvoke_url = &quot;https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/3bc390c7-eeec-40f7-a64d-0c6a719985f7&quot; \nfetch_url_format = &quot;https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/&quot; \n\n# Ensure that you have configured API_KEY \nheaders = { \n    &quot;Authorization&quot;: &quot;Bearer {}&quot;.format(API_KEY), \n    &quot;Accept&quot;: &quot;application/json&quot;, \n} \n\n# To re-use connections \nsession = requests.Session()  \n\n# The payload consists of a base64 encoded image accompanied by header text. \npayload = { \n  &quot;messages&quot;: &#91; \n    { \n        &quot;content&quot;: &quot;Generate underlying data table of the figure below:&lt;img src=\\&quot;data:image/png;base64,{}\\&quot; /&gt;&quot;.format(encoded_string.decode('UTF-8')), \n        &quot;role&quot;: &quot;user&quot; \n    } \n  ], \n  &quot;temperature&quot;: 0.1, \n  &quot;top_p&quot;: 0.7, \n  &quot;max_tokens&quot;: 1024, \n  &quot;stream&quot;: False \n} \n\nresponse = session.post(invoke_url, headers=headers, json=payload)   \n\nwhile response.status_code == 202: \n    request_id = response.headers.get(&quot;NVCF-REQID&quot;) \n    fetch_url = fetch_url_format + request_id \n    response = session.get(fetch_url, headers=headers) \n\nresponse.raise_for_status() \nresponse_body = response.json() \nprint(response_body)\n</pre></div>\n\n\n<p>The output looks like the following:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n{'id': '4423f30d-2d88-495d-83cc-710da97889e3', 'choices': &#91;{'index': 0, 'message': {'role': 'assistant', 'content': &quot;Entity | Individuals responsibility | Government's responsibility &lt;0x0A&gt; MEDIAN | 39.0 | 55.0 &lt;0x0A&gt; Germany | nan | 35.0 &lt;0x0A&gt; UK | 45.0 | 49.0 &lt;0x0A&gt; Sweden | 37.0 | 53.0 &lt;0x0A&gt; Denmark | 42.0 | 54.0 &lt;0x0A&gt; France | 38.0 | 55.0 &lt;0x0A&gt; Netherla nns | 40.0 | 58.0 &lt;0x0A&gt; Spain | 29.0 | 63.0 &lt;0x0A&gt; Italy | 22.0 | 74.0&quot;}, 'finish_reason': 'stop'}], 'usage': {'completion_tokens': 149, 'prompt_tokens': 0, 'total_tokens': 149}}\u00a0\n</pre></div>\n\n\n<p>The response body includes the output of DePlot along with additional metadata. The output is generated left-to-right autoregressively as a textual sequence in Markdown format, with separators <code>-</code>, <code>|</code>, and <code>&lt;0x0A&gt;</code> (newline).&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Visualize the output table&nbsp;</h3>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nresponse_table = response_body&#91;'choices']&#91;0]&#91;'message']&#91;'content']\u00a0\n\n# Replace the &lt;0x0A&gt; with \\n for better readability\u00a0\nprint(response_table.replace(&quot;&lt;0x0A&gt;&quot;, &quot;\\n&quot;))\u00a0\n</pre></div>\n\n\n<p>The output looks like the following:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nEntity | Individuals responsibility | Government's responsibility  \nMEDIAN | 39.0 | 55.0  \nGermany | nan | 35.0  \nUK | 45.0 | 49.0  \nSweden | 37.0 | 53.0  \nDenmark | 42.0 | 54.0  \nFrance | 38.0 | 55.0  \nNetherla nns | 40.0 | 58.0  \nSpain | 29.0 | 63.0  \nItaly | 22.0 | 74.0\n</pre></div>\n\n\n<p>In this case, the response was largely accurate. This output can be used as part of the input context to an LLM for a downstream task like question answering (QA).&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Enterprise-grade AI runtime for model deployments&nbsp;</h2>\n\n\n\n<p>Security, reliability, and enterprise support are critical when AI models are ready to deploy for business operations.&nbsp;&nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI Enterprise</a>, an end-to-end AI runtime software platform, is designed to accelerate the data science pipeline and streamline the development and deployment of production-grade generative AI applications.&nbsp;&nbsp;</p>\n\n\n\n<p>NVIDIA AI Enterprise provides the security, support, stability, and manageability to improve the productivity of AI teams, reduce the total cost of AI infrastructure, and ensure a smooth transition from POC to production.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started&nbsp;</h2>\n\n\n\n<p>Try the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\" data-type=\"link\" data-id=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot\">DePlot model</a> through the UI or the API. If this model is the right fit for your application, optimize the model with <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\" target=\"_blank\">NVIDIA TensorRT-LLM</a>.&nbsp;</p>\n\n\n\n<p>If you\u2019re building an enterprise application, sign up for an <a href=\"https://www.nvidia.com/en-us/data-center/free-trial-nvidia-test-drive-register-now/?landing=ai\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI Enterprise trial</a> to get support for taking your application to production.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Foundation Models and Endpoints provides access to a curated set of community and NVIDIA-built generative AI models to experience, customize, and deploy in enterprise applications.&nbsp;&nbsp; On Mondays throughout the year, we\u2019ll be releasing new models. This week, we released the NVIDIA-optimized DePlot model, which you can experience directly from your browser. If you haven\u2019t &hellip; <a href=\"https://developer.nvidia.com/blog/model-monday-query-graphs-with-optimized-deplot-model/\">Continued</a></p>\n", "protected": false}, "author": 953, "featured_media": 77074, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1337475", "discourse_permalink": "https://forums.developer.nvidia.com/t/model-monday-query-graphs-with-optimized-deplot-model/279789", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110], "tags": [3544, 1078, 453, 2158], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-tech-blog-deplot-blog-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-k1Z", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77003"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/953"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=77003"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77003/revisions"}], "predecessor-version": [{"id": 77196, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/77003/revisions/77196"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77074"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=77003"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=77003"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=77003"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76929, "date": "2024-01-22T07:00:00", "date_gmt": "2024-01-22T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76929"}, "modified": "2024-01-25T10:17:30", "modified_gmt": "2024-01-25T18:17:30", "slug": "benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/", "title": {"rendered": "Benchmarking Camera Performance on Your Workstation with NVIDIA Isaac Sim"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Robots are typically equipped with cameras. When designing a digital twin simulation, it\u2019s important to replicate its performance in a simulated environment accurately.</p>\n\n\n\n<p>However, to make sure the simulation runs smoothly, it\u2019s crucial to check the performance of the workstation that is running the simulation. In this blog post, we explore the steps to setting up and running a camera benchmark on your workstation on NVIDIA Isaac Sim.</p>\n\n\n\n<p>Before starting, we explore how Isaac Sim is designed to add cameras and how to export the outputs for ROS 2.\u00a0</p>\n\n\n\n<p>NVIDIA Isaac Sim can simulate multiple types of sensors, starting from range sensors, like LIDAR, ultrasonic, generic range sensors, contact sensors, and IMU force and proximity sensors.&nbsp;</p>\n\n\n\n<p>The camera sensor is the most advanced simulation that you can find in Isaac Sim and you can add this sensor quickly using the user interface.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1446\" height=\"896\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm.png\" alt=\"A screenshot example of adding a camera to Isaac Sim\" class=\"wp-image-76930\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm.png 1446w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-768x476.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-645x400.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-484x300.png 484w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Camera-Setup-Isaac-SIm-1024x635.png 1024w\" sizes=\"(max-width: 1446px) 100vw, 1446px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Example of adding a Camera on NVIDIA Isaac Sim, selecting configuration and output</em></figcaption></figure></div>\n\n\n<p>Also, you can use a Python script to load your camera or multiple cameras from a Python object, like in the following example.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ncamera = Camera(\n    prim_path=&quot;/World/camera&quot;,\n    position=np.array(&#91;0.0, 0.0, 25.0]),\n    frequency=20,\n    resolution=(256, 256),\n    orientation=rot_utils.euler_angles_to_quats(np.array(&#91;0, 90, 0]), degrees=True),\n)\n</pre></div>\n\n\n<p>For more details about the NVIDIA Isaac Sim camera, review our <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/features/sensors_simulation/isaac_sim_sensors_camera.html\">documentation</a>.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Set up your demo</h2>\n\n\n\n<p>You&#8217;ll need a workstation with an NVIDIA RTX GPU and the most current version of NVIDIA Isaac Sim 2023.1.0 installed to use this benchmark. This demo is only compatible with Linux-based machines running Ubuntu 20.04 or Ubuntu 22.04.</p>\n\n\n\n<p>This script doesn&#8217;t require the user interface to run Isaac Sim. It can be done with a terminal. The script will launch the simulator, load all the necessary cameras, and display the output in the terminal.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Installation and run</h2>\n\n\n\n<p>Clone the demo isaac_camera_benchmark:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>git clone https://github.com/nvidia_iot/isaac_camera_benchmark.git\ncd isaac_camera_benchmark</code></pre>\n\n\n\n<p>This repository contains all the scripts and files to run the demo on your screen, but before running the script, remember to download NVIDIA Isaac Sim.&nbsp;</p>\n\n\n\n<p>For guidance, follow the instructions on <a href=\"https://www.nvidia.com/en-us/omniverse/download/\">NVIDIA Omniverse</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"650\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page.png\" alt=\"A screenshot of an NVIDIA Omniverse library page.\" class=\"wp-image-76931\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-300x163.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-625x339.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-179x97.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-768x416.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-645x349.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-500x271.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-362x196.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-203x110.png 203w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/NVIDIA-Omniverse-library-page-1024x555.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA Omniverse, library page, and NVIDIA Isaac Sim download button</em></figcaption></figure></div>\n\n\n<p>From this page, select NVIDIA Isaac Sim 2023.1.0 and download. Then go to your terminal and run the script:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>./run_camera_benchmark.sh</code></pre>\n\n\n\n<p>This script will automatically start the latest NVIDIA Isaac Sim and initialize a simulation.</p>\n\n\n\n<p>In this simulation, three cameras move in a clockwise direction around three different locations within a small virtual warehouse. At the same time, the script displays the current frame rate and the average ROS 2 frequency output on the terminal.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1254\" height=\"1131\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs.png\" alt=\"A screenshot of an example camera output from Isaac Sim.\" class=\"wp-image-76932\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs.png 1254w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-300x271.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-625x564.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-768x693.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-645x582.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-333x300.png 333w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-100x90.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-362x326.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-122x110.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-outputs-1024x924.png 1024w\" sizes=\"(max-width: 1254px) 100vw, 1254px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. An example of camera outputs on a terminal&nbsp;</em></figcaption></figure></div>\n\n\n<p>The FPS output may vary depending on your workstation configuration and may differ from your hardware setup.</p>\n\n\n\n<p>To store the ROS2 output coming from this benchmark, run:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>./ros2record.sh</code></pre>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"756\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording.png\" alt=\"A screenshot of the ros2record terminal script.\" class=\"wp-image-76933\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-300x113.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-625x236.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-179x68.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-768x290.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-1536x581.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-645x244.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-500x189.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-160x61.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-362x137.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-291x110.png 291w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/script-recording-1024x387.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. A script recording of the output from all configured cameras </em></figcaption></figure></div>\n\n\n<p>All ros2bag files are available in the folder <code>isaac_camera_benchmark/rosbag</code>.</p>\n\n\n\n<p>The next chapter explains the script&#8217;s inner workings and how to change its configuration to&nbsp;test multiple cameras or use a different resolution.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Change the default configuration</h2>\n\n\n\n<p>There are three main files, and you can design your configuration by writing a configuration file.</p>\n\n\n\n<p>The first script is <code>run_camera_benchmark.sh</code>, which loads the latest version of Isaac Sim, passes a script of all the configurations, and runs this demo.</p>\n\n\n\n<p>The main script of this repository is <code>camera_benchmark.py</code> script, which runs Isaac Sim with only the ROS2_bridge extension, and by default, it loads the&nbsp;<code>warehouse_with_forklifts.usd</code> environment that contains a small virtual warehouse with two forklifts. The simulation will run in ray-tracing lighting.</p>\n\n\n\n<p>When the environment is loaded, add every camera configured in a file called <strong>config.json</strong> or load three cameras at 640&#215;480 resolution.</p>\n\n\n\n<p>This script automatically adds a camera object in the environment and also builds a graph that reads the output coming from Isaac Sim, fixes a resolution, and publishes on ROS 2 output.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1113\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph.png\" alt=\"A screenshot of the generated graph from script.\" class=\"wp-image-76935\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-300x167.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-625x348.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-768x428.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-1536x855.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-645x359.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-500x278.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-362x202.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-198x110.png 198w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-graph-1024x570.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. A generated graph from</em> <code>camera_benchmar.py</code></figcaption></figure></div>\n\n\n<p>At the same time, the <code>camera_benchmar.py</code> loads a ROS 2 node called <strong>benchmark_camera_node</strong> that is connected from a ROS 2 camera output and measures the topic frequency average.&nbsp;</p>\n\n\n\n<p>This script also reads from Isaac Sim the camera fps and publishes the output on the terminal.</p>\n\n\n\n<p>To change the default configuration, you can simply create a new JSON file called <strong>config.json</strong>. This file must contain the configuration of the cameras. There are different options that you can set:</p>\n\n\n\n<ul>\n<li><strong>Translate:</strong> Where the camera should be located in the environment, this variable must be a vector with three coordinates like [0.0 0.0 0.0].</li>\n\n\n\n<li><strong>Resolution:</strong> Camera output resolution is an integer vector with a camera resolution such as 640 \u00d7 480. Suggested resolutions include:\n<ul>\n<li>640 \u00d7 480</li>\n\n\n\n<li>1024 \u00d7 768</li>\n\n\n\n<li>1920 \u00d7 1080 (FHD)</li>\n\n\n\n<li>2560 \u00d7 1440 (2K)</li>\n\n\n\n<li>3840 \u00d7 2160 (4K)</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>The following is an example of a new <code>config.json</code>.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n{\n   &quot;camera&quot;: &#91;\n       {&quot;translate&quot;: &#91;0.0, 0.0, 3.0], &quot;resolution&quot;: &#91;640,480]}\n       {&quot;translate&quot;: &#91;-1.0, 0.0, 6.0], &quot;resolution&quot;: &#91;1024,768]}\n   ]\n}\n</pre></div>\n\n\n<p>This output generates a new benchmark like the following image.</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1424\" height=\"1063\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim.png\" alt=\"A screenshot showing the camera output from Isaac Sim.\" class=\"wp-image-76936\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim.png 1424w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-300x224.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-625x467.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-154x115.png 154w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-768x573.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-645x481.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-402x300.png 402w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-121x90.png 121w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-362x270.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-147x110.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/camera-output-Isaac-Sim-1024x764.png 1024w\" sizes=\"(max-width: 1424px) 100vw, 1424px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Example output cameras and on terminal FPs ros topic and Isaac Sim fps</em></figcaption></figure>\n\n\n\n<p>You can add a configuration to the config.json file to specify the type of simulation you want to start on Isaac Sim.</p>\n\n\n\n<ul>\n<li><strong>renderer</strong>: Pick the type of render. The default is <em>RayTracedLighting</em></li>\n\n\n\n<li><strong>headless: </strong>To run Isaac Sim without a user interface, you can change this boolean variable with <em>True</em></li>\n</ul>\n\n\n\n<p>See the following for another example of the configuration file.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n{\n   &quot;simulation&quot;: {&quot;renderer&quot;: &quot;RayTracedLighting&quot;, &quot;headless&quot;: true},\n   &quot;camera&quot;: &#91;\n       {&quot;translate&quot;: &#91;0.0, 0.0, 3.0], &quot;resolution&quot;: &#91;640,480]},\n       {&quot;translate&quot;: &#91;-1.0, 0.0, 6.0], &quot;resolution&quot;: &#91;1024,768]}\n   ]\n}\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>This script enables you to monitor the camera output and the performance of a ROS topic on your workstation. It also provides an example of how to create a new Isaac Sim script with ROS 2.</p>\n\n\n\n<p>Be sure to join our NVIDIA Isaac office hours every Wednesday on <a href=\"https://youtube.com/playlist?list=PL3jK4xNnlCVePQFJ9zVOvMofngfK49xki&amp;si=ww_XNxNTxh3e5sSA\">YouTube</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Robots are typically equipped with cameras. When designing a digital twin simulation, it\u2019s important to replicate its performance in a simulated environment accurately. However, to make sure the simulation runs smoothly, it\u2019s crucial to check the performance of the workstation that is running the simulation. In this blog post, we explore the steps to setting &hellip; <a href=\"https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/\">Continued</a></p>\n", "protected": false}, "author": 1255, "featured_media": 76940, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1337192", "discourse_permalink": "https://forums.developer.nvidia.com/t/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/279688", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [2375, 453, 3584], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Isaac-Sim-Workstation-Benchmark.png", "jetpack_shortlink": "https://wp.me/pcCQAL-k0N", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76929"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1255"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76929"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76929/revisions"}], "predecessor-version": [{"id": 76957, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76929/revisions/76957"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76940"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76929"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76929"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76929"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75640, "date": "2024-01-18T13:45:18", "date_gmt": "2024-01-18T21:45:18", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75640"}, "modified": "2024-01-25T10:17:30", "modified_gmt": "2024-01-25T18:17:30", "slug": "generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/", "title": {"rendered": "Generate Synthetic Data for Deep Object Pose Estimation Training with NVIDIA Isaac ROS"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>For robotic agents to interact with objects in their environment, they must know the position and orientation of objects around them. This information describes the six degrees of freedom (DOF) pose of a rigid body in 3D space, detailing the translational and rotational state.</p>\n\n\n\n<p>Accurate pose estimation is necessary to determine how to orient a robotic arm to grasp or place objects in a specific way. Use cases include robotic manipulation for pick-and-place operations, especially applicable in warehouse scenarios for tasks like box packing, part loading, and food packaging. Knowing an object\u2019s pose is also crucial for robot-to-human handoff and is useful in healthcare, retail, and household scenarios.</p>\n\n\n\n<p>NVIDIA developed Deep Object Pose Estimation (DOPE) to find the six DOF pose of an object.&nbsp;In this post, we show how to generate synthetic data to train a DOPE model for an object.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Deep Object Pose Estimation</h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"621\" height=\"142\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation.png\" alt=\"Four images of a collection of different objects. Each image shows 3D bounding boxes and corresponding pose axes around each object. \" class=\"wp-image-75641\" style=\"aspect-ratio:4.373239436619718;object-fit:cover;width:934px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation.png 621w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation-300x69.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation-179x41.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation-500x114.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation-160x37.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation-362x83.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Object-pose-estimation-481x110.png 481w\" sizes=\"(max-width: 621px) 100vw, 621px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. DOPE estimating six DOF poses of objects from an RGB image</em></figcaption></figure></div>\n\n\n<p>DOPE is a one-shot DNN developed by NVIDIA that estimates six DOF poses of objects of interest from an RGB image to enable robotic manipulation of objects in an environment. It is trained only on synthetic data and requires a textured 3D model. It provides enough accuracy for real-world grasping and gripper manipulation, with a tolerance of 2 cm.</p>\n\n\n\n<p>DOPE is an instance-level model, meaning a DOPE model must be trained specifically for each object type within a class. For example, we can\u2019t train a single DOPE model to detect all types of chairs and instead must train one model per chair type.&nbsp;</p>\n\n\n\n<p>As another example, if an application is detecting four geometrically similar boxes of different colors, four instances of DOPE models are required for inference\u2014one trained specifically on each colored box.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Advantages of DOPE</h2>\n\n\n\n<ul>\n<li>It can be trained entirely on synthetic data, reducing data collection and annotation costs.</li>\n\n\n\n<li>Handles object occlusion.</li>\n\n\n\n<li>Reduces the reality gap challenge by combining domain randomized and photorealistic synthetic data for training.</li>\n\n\n\n<li>It works on different camera intrinsics without retraining through using the Perspective-n-point (PnP) algorithm.&nbsp;</li>\n\n\n\n<li>DOPE is supported in NVIDIA Isaac ROS to provide GPU-accelerated object pose estimation.&nbsp;</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Reality gap challenge</h2>\n\n\n\n<p>Networks trained only on synthetic data often perform poorly on real-world data. Techniques like fine-tuning or domain randomization help improve performance. </p>\n\n\n\n<p>Domain randomization is the method of varying parameters like scene lighting, scale, pose, color, and texture of objects in a simulation environment. This is done to provide a sufficient variety of domain parameters to the neural network to improve generalization to real-world environments. This way, real data appears as just another variation to the network.</p>\n\n\n\n<p>DOPE bridges the reality gap by combining domain randomized and photorealistic synthetic data for training, and generalizes well to real-world use cases.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Architecture</h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1024\" height=\"240\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-1024x240.png\" alt=\"A diagram showing an overview of the DOPE network architecture. It shows VGG19 as a block followed by multiple convolution layers.\" class=\"wp-image-75644\" style=\"aspect-ratio:4.271367521367521;width:800px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-1024x240.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-300x70.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-625x146.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-179x42.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-768x180.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-1536x360.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-645x151.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-500x117.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-160x37.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-362x85.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture-470x110.png 470w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOPE-network-architecture.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Overview of DOPE network architecture</em></figcaption></figure></div>\n\n\n<p>DOPE is a one-shot fully convolutional neural network, inspired by <a href=\"https://arxiv.org/pdf/1602.00134.pdf\">convolutional pose machines</a> (CPMs) and a <a href=\"https://arxiv.org/pdf/1611.08050.pdf\">multi-person pose estimator</a>. The architecture consists of a standard CNN such as VGG19 or RESNET with additional convolution layers.&nbsp;</p>\n\n\n\n<p>For a comprehensive understanding of the DOPE architecture and data generation pipeline, refer to <a href=\"https://arxiv.org/pdf/1809.10790.pdf\">Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Dataset</h2>\n\n\n\n<p>NVIDIA provides <a href=\"https://drive.google.com/drive/folders/1DfoA3m_Bm0fW8tOWXGVxi4ETlLEAgmcg\">pretrained DOPE models</a> trained on the <a href=\"https://github.com/swtyree/hope-dataset\">NVIDIA Household Objects for Pose Estimation</a> (HOPE) dataset. It is a collection of 28 toy grocery objects in varying environments and is part of the <a href=\"https://bop.felk.cvut.cz/home/\">Benchmark for 6D Object Pose Estimation</a>.</p>\n\n\n\n<p>Being instance-level, DOPE must be trained with a dataset targeting objects of interest relevant to the application. To generate a dataset for training DOPE, a 3D model of the object is required. 3D object models can be generated using <a href=\"https://github.com/NVlabs/BundleSDF\">BundleSDF</a>. The method, developed by NVIDIA, uses monocular RGBD cameras and removes the need for expensive 3D sensors.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Data generation</h2>\n\n\n\n<p>Synthetic data can be generated for DOPE using NVIDIA Isaac Sim for domain randomization. We focus on two datasets\u2014MESH and DOME\u2014and implement randomization techniques similar to those shown for these datasets in the <a href=\"https://arxiv.org/pdf/2105.13962.pdf\">NViSII paper</a>.&nbsp;</p>\n\n\n\n<p>These datasets add flying distractors to the scene around the object of interest and randomize lighting conditions, distractors\u2019 colors, and materials. DOME uses fewer distractors than MESH and provides more realistic backgrounds.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"886\" height=\"247\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data.png\" alt=\"Two example images from the DOME dataset and MESH dataset, respectively.\" class=\"wp-image-75654\" style=\"aspect-ratio:3.5870445344129553;width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data.png 886w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-300x84.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-625x174.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-179x50.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-768x214.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-645x180.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-500x139.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-160x45.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-362x101.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/DOME-vs-MESH-data-395x110.png 395w\" sizes=\"(max-width: 886px) 100vw, 886px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Example of DOME data (left) and MESH data (right)</em></figcaption></figure></div>\n\n\n<p>Information on how to use Isaac Sim to create training data for DOPE is available in <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials/tutorial_replicator_offline_pose_estimation.html\">NVIDIA docs</a>.</p>\n\n\n\n<p>You can specify the number of images you want to generate of each type (MESH and DOME). A good MESH / DOME split depends on the use case. Experiment to find heuristics that work well for your model (for instance, 25 / 75 between MESH / DOME). If you\u2019re generating data and training DOPE on a single object, a training dataset of around 20k images is generally enough.&nbsp;&nbsp;</p>\n\n\n\n<p>The generated dataset includes images and annotated JSON files. Each JSON file contains information about the object, including object class, position, orientation, and visibility in the corresponding image. Visibility represents how much of the object is visible (in the case of occlusions) and can be used to filter images for training.&nbsp;&nbsp;</p>\n\n\n\n<p>This data generation method using Isaac Sim can also write data in a format similar to the <a href=\"https://www.v7labs.com/open-datasets/ycb-video\">YCB Video Dataset</a>, which can then be used to train other 6D pose estimation models.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Object symmetry</h2>\n\n\n\n<p>DOPE is trained on cuboid corners that bound an object of interest. Rotational symmetries in this object could result in multiple frames that are identical pixel-wise but marked by different cuboid corners.&nbsp;</p>\n\n\n\n<p>Watch this <a href=\"https://github.com/NVlabs/Deep_Object_Pose/tree/master/scripts/nvisii_data_gen#cylinder-object\">Deep Object Pose video</a> on GitHub to learn more.&nbsp;</p>\n\n\n\n<p>The Isaac Sim data generation method doesn&#8217;t explicitly handle rotational symmetries at the moment. However, NVIDIA also provides <a href=\"https://github.com/NVlabs/Deep_Object_Pose/tree/master/scripts/nvisii_data_gen\">synthetic data generation scripts</a> using <a href=\"https://github.com/owl-project/NVISII\">NViSII</a> that can handle symmetry.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Training DOPE</h2>\n\n\n\n<p>After you\u2019ve generated your training dataset, NVIDIA provides a <a href=\"https://github.com/NVlabs/Deep_Object_Pose/tree/master/scripts/train2\">script to train DOPE</a>. You can point the script to your training data and specify the batch size and number of epochs you want to train your model for.</p>\n\n\n\n<p>The script saves useful training information (including loss graphs and belief maps) which you can view using TensorBoard.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Inference and evaluation</h2>\n\n\n\n<p>After you\u2019ve trained your DOPE model, you can run inference on a test dataset. Depending on the images in your test data, you can specify configuration parameters in the provided configuration files or write your own.&nbsp;</p>\n\n\n\n<p>Include the physical dimensions of the object of interest in the object config file (I used a <a href=\"https://3dviewer.net/\">3D viewer</a> online to load the 3D model and find the dimensions). The inference workflow uses these dimensions to generate results with bounding boxes around the detected objects.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1024\" height=\"1024\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE.png\" alt=\"An example output from DOPE. The image shows an object with a 3D bounding box around it, oriented accurately using the pose estimate from DOPE. \" class=\"wp-image-75658\" style=\"aspect-ratio:1;object-fit:cover;width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-625x625.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-768x768.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-645x645.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bounding-box-DOPE-110x110.png 110w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Qualitative result showing a bounding box around an object with an accurate pose from DOPE</em></figcaption></figure></div>\n\n\n<p>After running inference, we provide an evaluation workflow to evaluate the performance of your model quantitatively. Ground truth data, predicted results from the inference step, and a 3D model of the object of interest (in .obj format) are required for evaluation. The object\u2019s 3D model is rendered to calculate the 3D error between ground truth and predicted results.</p>\n\n\n\n<p>The ADD metric is used and we provide two options for calculating error:</p>\n\n\n\n<ul>\n<li><strong>Average distance</strong> (ADD) is the average distance calculated using the closest point distance between the predicted pose and the ground-truth pose.</li>\n\n\n\n<li><strong>Cuboid distance</strong> calculates the average distance using the eight cuboid points of the 3D models (ground truth) and predicted cuboid points. This is faster to calculate than ADD but less accurate.</li>\n</ul>\n\n\n\n<p>With domain randomized data alone for an arbitrary object, the highest area under the curve (AUC) observed was 66.64 for 300k images. 62.94 AUC was observed when using a dataset of 600k photorealistic images alone. Accuracy was highest when domain randomized and photorealistic synthetic images were combined (77.00 AUC).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"784\" height=\"635\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE.png\" alt=\"Image showing the Accuracy-threshold curve for DOPE compared to PoseCNN. The highest value of 76.06 was observed when using DOPE with domain randomized and photorealistic data. This value is observed to be 66.07 for the same object using PoseCNN.\" class=\"wp-image-75662\" style=\"aspect-ratio:1.2346456692913386;object-fit:cover;width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE.png 784w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-300x243.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-625x506.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-142x115.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-768x622.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-645x522.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-370x300.png 370w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-111x90.png 111w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-362x293.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Accuracy-threshold-curve-DOPE-136x110.png 136w\" sizes=\"(max-width: 784px) 100vw, 784px\" /><figcaption class=\"wp-element-caption\"><span id=\"docs-internal-guid-3dd8492f-7fff-0887-24e2-385b64fa97a6\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; background-color: transparent; font-style: italic; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\">Figure 5. Accuracy-threshold curve for DOPE compared with </span><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: rgb(17, 85, 204); background-color: transparent; font-style: italic; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline;\">PoseCNN</span><span style=\"font-size: 11pt; font-family: Arial, sans-serif; background-color: transparent; font-style: italic; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline;\"> for an object on the YCB-Video dataset</span></span></figcaption></figure></div>\n\n\n<p>DOPE has been trained on synthetic images only. Yet it still performs well on scenes captured with a different camera, even when there are occlusions and extreme lighting changes. Its performance is better than PoseCNN and BB8, which have been trained on real data or a combination of synthetic and real data. </p>\n\n\n\n<p>For a direct comparison, five objects were selected from the YCB dataset, and DOPE achieved a higher AUC than PoseCNN for four of the five objects.&nbsp;</p>\n\n\n\n<p>More details can be found in the <a href=\"https://arxiv.org/pdf/1809.10790.pdf\">DOPE paper</a>. Check out our GitHub for information on <a href=\"https://github.com/andrewyguo/dope_training/tree/344e4376cc0f6432b28b9b6997e69b29b3bbcf2f/inference\">inference</a> and <a href=\"https://github.com/andrewyguo/dope_training/tree/344e4376cc0f6432b28b9b6997e69b29b3bbcf2f/evaluate\">evaluation</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Using \u200cIsaac ROS pose estimation</h2>\n\n\n\n<p>Isaac ROS provides a ROS 2 package for pose estimation using DOPE. It performs GPU-accelerated inference using <a href=\"https://github.com/triton-inference-server/server\">NVIDIA Triton</a> or <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> with <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_dnn_inference\">Isaac ROS DNN Inference</a>.&nbsp;</p>\n\n\n\n<p>After training your DOPE model, you can run inference using this package on <a href=\"https://developer.nvidia.com/embedded-computing\">NVIDIA Jetson</a> or a system with an NVIDIA GPU.</p>\n\n\n\n<p>You can also perform inference on live images from a camera stream, however, this is a compute-intensive task. Pose estimation is done at a lower frame rate than the camera input rate. Our DOPE graph runs at 39.8 FPS on an NVIDIA Jetson AGX Orin and 89.2 FPS on an NVIDIA RTX 4060 Ti\u2014based on the <a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_benchmark#list-of-isaac-ros-benchmarks\">Isaac ROS Benchmark</a> workflow.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"514\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/image1-2.gif\" alt=\"GIF showing the camera view of a bottle of mustard being moved around and Isaac ROS DOPE being used to estimate the pose of this bottle. The output pose results are visualized as 3D axis on an RViz window.\" class=\"wp-image-75668\"/><figcaption class=\"wp-element-caption\"><em>Fig 6. Running Isaac ROS DOPE with live camera input and visualizing pose on RViz</em></figcaption></figure></div>\n\n\n<p>The graph includes three components and steps:</p>\n\n\n\n<ul>\n<li><strong>The DNN image encoder node</strong> turns a raw image into a resized, normalized tensor.</li>\n\n\n\n<li><strong>TensorRT node</strong> converts an input tensor into a tensor of belief maps.</li>\n\n\n\n<li><strong>The DOPE decoder node</strong> converts a belief map into an array of poses.</li>\n</ul>\n\n\n\n<p>Learn more about the performance of different Isaac ROS packages and benchmarking methodology in the <a href=\"https://nvidia-isaac-ros.github.io/performance/index.html\">performance summary</a>. Check out Isaac ROS Pose Estimation\u00a0<a rel=\"noreferrer noopener\" href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_pose_estimation\" target=\"_blank\">on GitHub.</a></p>\n", "protected": false}, "excerpt": {"rendered": "<p>For robotic agents to interact with objects in their environment, they must know the position and orientation of objects around them. This information describes the six degrees of freedom (DOF) pose of a rigid body in 3D space, detailing the translational and rotational state. Accurate pose estimation is necessary to determine how to orient a &hellip; <a href=\"https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/\">Continued</a></p>\n", "protected": false}, "author": 890, "featured_media": 76763, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1335645", "discourse_permalink": "https://forums.developer.nvidia.com/t/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/279416", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63, 503], "tags": [453, 1394, 2571, 1718], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Isaac-ROS-DOPE.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jG0", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75640"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/890"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75640"}], "version-history": [{"count": 39, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75640/revisions"}], "predecessor-version": [{"id": 76928, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75640/revisions/76928"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76763"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75640"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75640"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75640"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76593, "date": "2024-01-18T10:00:00", "date_gmt": "2024-01-18T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76593"}, "modified": "2024-01-18T10:36:11", "modified_gmt": "2024-01-18T18:36:11", "slug": "webinar-quantum-espresso-on-gpus-porting-strategy-and-results", "status": "publish", "type": "post", "link": "https://bit.ly/openacc_QEnvn", "title": {"rendered": "Webinar: Quantum ESPRESSO on GPUs: Porting Strategy and Results"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Explore the status of Quantum ESPRESSO porting strategies that enable state-of-the-art performance on HPC systems.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Explore the status of Quantum ESPRESSO porting strategies that enable state-of-the-art performance on HPC systems.</p>\n", "protected": false}, "author": 1115, "featured_media": 76594, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1335596", "discourse_permalink": "https://forums.developer.nvidia.com/t/webinar-quantum-espresso-on-gpus-porting-strategy-and-results/279402", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://bit.ly/openacc_QEnvn", "_links_to_target": "_blank"}, "categories": [503], "tags": [52], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/quantum-espresso-webinar-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jVn", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76593"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76593"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76593/revisions"}], "predecessor-version": [{"id": 76596, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76593/revisions/76596"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76594"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76593"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76593"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76593"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76597, "date": "2024-01-17T15:05:40", "date_gmt": "2024-01-17T23:05:40", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76597"}, "modified": "2024-01-25T10:17:31", "modified_gmt": "2024-01-25T18:17:31", "slug": "release-pytorch-geometric-container-for-gnns-on-ngc", "status": "publish", "type": "post", "link": "https://nvda.ws/3SgrAdP", "title": {"rendered": "Release: PyTorch Geometric Container for GNNs on NGC"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The NVIDIA PyG container, now generally available, packages PyTorch Geometric with accelerations for GNN models, dataloading, and pre-processing using cuGraph-Ops, cuGraph, and cuDF from <a href=\"https://developer.nvidia.com/rapids\">NVIDIA RAPIDS</a>, all with an effortless out-of-the-box experience.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The NVIDIA PyG container, now generally available, packages PyTorch Geometric with accelerations for GNN models, dataloading, and pre-processing using cuGraph-Ops, cuGraph, and cuDF from NVIDIA RAPIDS, all with an effortless out-of-the-box experience.</p>\n", "protected": false}, "author": 1963, "featured_media": 76602, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1334944", "discourse_permalink": "https://forums.developer.nvidia.com/t/release-pytorch-geometric-container-for-gnns-on-ngc/279315", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3SgrAdP", "_links_to_target": "_blank"}, "categories": [2724, 1464, 696, 1968, 503], "tags": [3312, 453, 3052, 1958, 559, 369], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/pyg-container-ngc-release-featured-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jVr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76597"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1963"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76597"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76597/revisions"}], "predecessor-version": [{"id": 76669, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76597/revisions/76669"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76602"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76597"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76597"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76597"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76567, "date": "2024-01-17T13:00:00", "date_gmt": "2024-01-17T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76567"}, "modified": "2024-01-25T10:17:32", "modified_gmt": "2024-01-25T18:17:32", "slug": "simulating-railroads-with-openusd", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simulating-railroads-with-openusd/", "title": {"rendered": "Simulating Railroads with OpenUSD"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Railroad simulation is important in modern transportation and logistics, providing a virtual testing ground for the intricate interplay of tracks, switches, and rolling stock. It serves as a crucial tool for engineers and developers to fine-tune and optimize railway systems, ensuring efficiency, safety, and cost-effectiveness.&nbsp;</p>\n\n\n\n<p>Physically realistic simulations enable comprehensive scenario analysis, predictive maintenance, and the exploration of innovative solutions, ultimately contributing to the advancement and sustainability of rail infrastructure.</p>\n\n\n\n<p>However, simulating railroads effectively and accurately is no easy feat. A wide range of interconnected and dynamic components must be modeled precisely. Real-world physics, safety protocols, and the diversity of operational scenarios must also be considered.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Realistic rail simulations with railOmniverse extension&nbsp;</h2>\n\n\n\n<p>Within the sector initiative <a href=\"https://digitale-schiene-deutschland.de/en\">Digitale Schiene Deutschland</a>, <a href=\"https://digitale-schiene-deutschland.de/en\">Deutsche Bahn</a> (DB) aims to use digitalization to improve the capacity, quality, and efficiency of their rail network. One measure is the creation of a physically accurate emulation of the country-wide rail system in Germany.\u00a0</p>\n\n\n\n<p>railOmniverse is a C++ extension developed in <a href=\"https://developer.nvidia.com/omniverse\">NVIDIA Omniverse</a> by <a href=\"https://trendverlag.com/\">Trend Verlag</a> (Trend) with Deutsche Bahn to facilitate the simulation of their highly complex railroad systems. Based on the <a href=\"https://www.trendverlag.com/Trax/Doc/BlogArticle/traxLibrary.html\">Trend Trax Track Library</a>, the extension provides tools for creating and managing track systems, motor models, brakes, and wheel friction in a physical railroad simulation. railOmniverse uses <a href=\"https://developer.nvidia.com/physx-sdk\">NVIDIA PhysX</a> to support the realistic simulation of physical interactions, forces, and dynamics between the various components of a railroad system.</p>\n\n\n\n<p>Omniverse is a development computing platform that enables developers to build interoperable 3D workflows and tools based on <a href=\"https://developer.nvidia.com/usd\">Universal Scene Description (OpenUSD)</a>. OpenUSD is an extensible open-source framework for describing, composing, and collaborating within 3D worlds.</p>\n\n\n\n<p>Integrating the railOmniverse extension into their digital twins has enabled DB to take advantage of the Trend TrackJoint and other features in their Trax Track Library. This saves countless hours in creating realistic railway simulations.</p>\n\n\n\n<p>&#8220;We believe [railOmniverse] is the best approach so far, as it allows us smooth train movement that can still derail in extreme conditions,\u201d said Jose Minguez, senior software developer at DB Systel GmbH during his GTC 2023 session, <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51560/?start=998&amp;end=1016\">Building a Digital Twin of the German Rail Network to Deliver Next-Generation Railway Systems</a>. \u201cIt also gives us the freedom to tweak some parameters to influence speed changes based on variables like different weather conditions or different load weights in the train.\u201d&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/sewNEojh5sI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Physical simulation in NVIDIA Omniverse, using the railOmniverse extension</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Fine-tuning physical simulations for trains with TrackJoint and NVIDIA PhysX</h2>\n\n\n\n<p>Numerous experiments were conducted by Trend, and later by DB, exploring collisions of various kinds, additional forces, positional corrections, and so on. The goal was to confine train movement to the track. However, these approaches consistently compromised the realism of the simulation and often led to stability issues.&nbsp;</p>\n\n\n\n<p>The introduction of the TrackJoint feature provided a robust solution. TrackJoint is a component that connects the wheel frames of a train to the track system. It restricts the movements of the wheel frames relative to the track while maintaining stability and realism in the simulation.&nbsp;</p>\n\n\n\n<p>After numerous experiments, the Trax Library was designed to omit simulating the intricate physical details of each spinning wheel, but instead focus on maintaining a per-bogie granularity. Bogies are physical bodies that run along the track.</p>\n\n\n\n<p>Even the train&#8217;s drive can be implemented as a constraint, seamlessly integrating a motor model that encompasses acceleration, braking, and friction into the simulation. This model accommodates virtually all types of propulsion engines, along with their gearboxes and wheel configurations, simulated by a traction force characteristic curve tailored to the specifics of a particular rolling stock.</p>\n\n\n\n<p>Integration into NVIDIA PhysX was achieved using the custom joint interface within Omniverse. The implementation of the TrackJoint mirrors the approach used for built-in joints, such as rotational or spherical. This involves defining a series of constraints for relative translational and rotational velocities, which then are provided to the PhysX constraint solver for direct use.</p>\n\n\n\n<p>When dealing with a wheel frame and its connections to the main body of a rolling stock in Omniverse, users can leverage the utilities provided by the platform. For instance, rotational joints can be employed for bogie hinges, and distance joints for couplings between wagons.&nbsp;</p>\n\n\n\n<p>The wheel frame itself is represented by an NVIDIA PhysX body equipped with a TrackJoint, similar to any other PhysX joint. This implies that the new elements introduced by railOmniverse can seamlessly integrate with all the standard simulation tools used in Omniverse and PhysX.</p>\n\n\n\n<p>PhysX, with its focus on relative velocities in the solver, enables tracks that are not restricted to the static environment, but also attached to a physical body. This opens up possibilities including turntables or trains running on other rolling stock (such as adapter wagons or a train ferry ship).</p>\n\n\n\n<h2 class=\"wp-block-heading\">Collaboration and custom schemas with OpenUSD</h2>\n\n\n\n<p>The OpenUSD interchange proved to be an effective medium for collaboration between DB and Trend. To operate a train on a track requires tracks and switches first. DB and Trend agreed on custom USD primitive definitions for these entities, which enabled creating from DB-provided sample data using the functionality of the Trax Library.&nbsp;</p>\n\n\n\n<p>Leveraging the schema mechanism, the Trend development team defined and registered new primitives for tracks, switches, the TrackJoint, and various aspects of the motor model. This facilitated simultaneous development on both ends. For instance, DB provided track geometry data using Omniverse Bezier BasisCurves, while Trend expanded the system with a specialized spline curve, parametrized by arc length, enabling its use in simulations with the TrackJoint.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Exploring the Trax Library</h2>\n\n\n\n<p>Libraries such as the <a href=\"https://www.trendverlag.com/Trax/Doc/BlogArticle/traxLibrary.html\">Trend Trax Track Library</a> are crucial to saving time and effort when creating realistic simulations. Using a library that pre-solves the fundamental problems is essential. With the right library, you aren&#8217;t starting your simulation project tomorrow; you started years ago when the Trax Library began.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1049\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram.png\" alt=\"A technical diagram of Trend Verlag\u2019s USD implementation and Omniverse connection, including track data, the Trax Library, custom USD prims, PhysX, and Isaac Sim.\" class=\"wp-image-76575\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-625x328.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-768x403.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-1536x806.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-645x338.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-500x262.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-210x110.png 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trend-verlag-usd-implementation-omniverse-connection-diagram-1024x537.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. The Trend Verlag USD implementation and Omniverse connection, featuring custom USD primitives for various aspects of the Trax Library</em></em></figcaption></figure></div>\n\n\n<p>The library includes numerous features such as the TrackJoint that have been tested in real-world use cases and are ready to use in your own railway simulations. These include the following:</p>\n\n\n\n<ul>\n<li><strong>Track System Creation Tools:</strong> Tools for defining the geometry of the tracks, specifying curves and twists, and managing switches and transitions between different track segments.</li>\n\n\n\n<li><strong>Motor Model:</strong> The model enables simulating the behavior of train motors. It establishes a relationship between velocity and the fraction of maximal motor force that can be applied at a specific velocity. This ensures realistic acceleration and deceleration of the train.</li>\n\n\n\n<li><strong>Friction and Brakes:</strong> Models for wheel-related friction and brakes. This enables simulating the effects of friction on the train&#8217;s movement and apply braking forces to slow down or stop the train.</li>\n\n\n\n<li><strong>Bogies and RailRunners:</strong> Interfaces for representing bogies. The RailRunner interface allows you to connect multiple bogies together to form a train. It also includes models for hinged bogies and rolling stock configurations.</li>\n\n\n\n<li><strong>Sensors and Signals:</strong> Mechanisms for detecting events along the track. Sensors can be used to detect the passage of wheelsets or other specific events, while signals convey information to be sent to a train as it moves along a track range.</li>\n\n\n\n<li><strong>Curve Theory:</strong> The library incorporates mathematical concepts from Curve Theory, which describes curves in terms of curvature and torsion. This supports accurate representation of curves and velocities while moving along them.</li>\n</ul>\n\n\n\n<p>Trend plans to release a public version of railOmniverse in early 2024. For in-depth information, we maintain a detailed text called the Trax Book, outlining the basic principles used by the Trax Library. Updates on the development of railOmniverse can be found in <a href=\"https://www.trendverlag.com/Trax/Doc/Book/chapter12.html\">Chapter 12 of the Trax Book</a>.</p>\n\n\n\n<p>We are eager to learn about your use cases. If you have questions or suggestions, email horstmann.marc@trendverlag.de. To sign up for the railOmniverse newsletter, include &#8216;railNewsletter\u2019 in the subject line.</p>\n\n\n\n<p>Join NVIDIA for <a href=\"https://www.nvidia.com/gtc/sessions/openusd-day/\">OpenUSD Day at GTC 2024</a> for a full day of expert-led sessions and panels. <a href=\"https://www.nvidia.com/gtc/pricing/\">Register for GTC 2024</a> and attend in person or virtually to learn the latest in AI with in-depth sessions, workshops, and training on building OpenUSD-based extensions, apps, and services on Omniverse.&nbsp;</p>\n\n\n\n<p><em>Get started with NVIDIA Omniverse by downloading the standard license </em><a href=\"https://www.nvidia.com/en-us/omniverse/download/\"><em>free</em></a><em>, access </em><a href=\"https://developer.nvidia.com/usd\"><em>OpenUSD</em></a><em> resources, and learn how </em><a href=\"https://www.nvidia.com/en-us/omniverse/enterprise/\"><em>Omniverse Enterprise</em><em> can connect your team</em></a><em>. Stay up to date on </em><a href=\"https://www.instagram.com/nvidiaomniverse/\"><em>Instagram</em></a><em>, </em><a href=\"https://medium.com/@nvidiaomniverse\"><em>Medium</em></a><em> and </em><a href=\"https://twitter.com/nvidiaomniverse\"><em>Twitter</em></a><em>. For more, join the </em><a href=\"https://www.nvidia.com/en-us/omniverse/community/\"><em>Omniverse community</em></a><em> on the&nbsp; </em><a href=\"https://forums.developer.nvidia.com/c/omniverse/300\"><em>forums</em></a><em>, </em><a href=\"https://discord.com/invite/XWQNJDNuaC\"><em>Discord server</em></a><em>, </em><a href=\"https://www.twitch.tv/nvidiaomniverse\"><em>Twitch</em></a><em> and </em><a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\"><em>YouTube</em></a><em> channels.&nbsp;</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Railroad simulation is important in modern transportation and logistics, providing a virtual testing ground for the intricate interplay of tracks, switches, and rolling stock. It serves as a crucial tool for engineers and developers to fine-tune and optimize railway systems, ensuring efficiency, safety, and cost-effectiveness.&nbsp; Physically realistic simulations enable comprehensive scenario analysis, predictive maintenance, and &hellip; <a href=\"https://developer.nvidia.com/blog/simulating-railroads-with-openusd/\">Continued</a></p>\n", "protected": false}, "author": 1982, "featured_media": 76571, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1334925", "discourse_permalink": "https://forums.developer.nvidia.com/t/simulating-railroads-with-openusd/279308", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1235, 503], "tags": [501, 2375, 453, 36, 1409, 3096], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trains-on-tracks-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jUX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76567"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1982"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76567"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76567/revisions"}], "predecessor-version": [{"id": 76697, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76567/revisions/76697"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76571"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76567"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76567"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76567"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76636, "date": "2024-01-16T10:29:16", "date_gmt": "2024-01-16T18:29:16", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76636"}, "modified": "2024-01-25T10:17:32", "modified_gmt": "2024-01-25T18:17:32", "slug": "new-support-for-dutch-and-persian-released-by-nemo-asr", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/", "title": {"rendered": "New Support for Dutch and Persian Released by NVIDIA NeMo ASR"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Breaking barriers in speech recognition, <a href=\"https://github.com/NVIDIA/NeMo\">NVIDIA NeMo</a> proudly presents pretrained models tailored for Dutch and Persian\u2014languages often overlooked in the AI landscape.</p>\n\n\n\n<p>These models leverage the recently introduced <a href=\"https://arxiv.org/abs/2305.05084\">FastConformer</a> architecture and were trained simultaneously with CTC and transducer objectives to maximize each model\u2019s accuracy.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/essential-guide-to-automatic-speech-recognition-technology/\">Automatic speech recognition</a> (ASR) is a fundamental technology for conversational AI applications, as it enables users to communicate with AI systems and other devices using voice. It\u2019s also widely adopted in conversational analytics and audio captioning, resulting in broader content accessibility.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Persian speech recognition model</h2>\n\n\n\n<p>The Persian model was trained on <a href=\"https://commonvoice.mozilla.org/en/datasets\">Mozilla\u2019s Common Voice (MCV)</a> 15.0 Persian data. Notably, two techniques helped maximize the model\u2019s performance: initialization from a pretrained English checkpoint and a custom train-test split that allowed the use of an extra 300 hours of MCV-validated recordings.&nbsp;</p>\n\n\n\n<p>This model achieves a 13.16% word error rate (WER) and 3.85% character error rate (CER) in evaluation. While WER is a standard metric for ASR, it does not necessarily reflect ASR performance in the Persian language well due to flexibility in compound word notation. This means a compound word may not be separated by a whitespace. In these cases, CER may be a more realistic indication of an ASR system&#8217;s accuracy.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Dutch speech recognition model</h2>\n\n\n\n<p>The Dutch model is trained on 40 hours of <a href=\"https://commonvoice.mozilla.org/en/datasets\">MCV</a> data, 547 hours of&nbsp; <a href=\"http://www.openslr.org/94/\">Multilingual LibriSpeech (MLS)</a>, and 34 hours of <a href=\"https://aclanthology.org/2021.acl-long.80/\">VoxPopuli</a> data.&nbsp;</p>\n\n\n\n<p>This model achieves a 9.2% and 12.1% word error rate on MCV and MLS in evaluation, which is among the top of the available open-source Dutch models. This model can also produce transcripts with punctuation and capitalization.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Try the models</h2>\n\n\n\n<p>These models are permissively licensed with a <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-4.0 BY license</a> that enables commercial use. They are available to download at both NGC and HuggingFace:</p>\n\n\n\n<ul>\n<li><a href=\"https://catalog.ngc.nvidia.com/\" data-type=\"link\" data-id=\"https://catalog.ngc.nvidia.com/\">NGC: Complete list of ASR models offered by NVIDIA NeMo</a>\n<ul>\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_nl_fastconformer_hybrid_large_pc_1024\">Dutch</a></li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_fa_fastconformer_hybrid_large\">Persian</a></li>\n</ul>\n</li>\n\n\n\n<li><a href=\"https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&amp;library=nemo&amp;sort=trending\" data-type=\"link\" data-id=\"https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&amp;library=nemo&amp;sort=trending\">HuggingFace: Complete list of ASR models offered by NVIDIA NeMo</a>\n<ul>\n<li><a href=\"https://huggingface.co/nvidia/stt_nl_fastconformer_hybrid_large_pc\">Dutch</a></li>\n\n\n\n<li><a href=\"https://huggingface.co/nvidia/stt_fa_fastconformer_hybrid_large\">Persian</a></li>\n</ul>\n</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Breaking barriers in speech recognition, NVIDIA NeMo proudly presents pretrained models tailored for Dutch and Persian\u2014languages often overlooked in the AI landscape. These models leverage the recently introduced FastConformer architecture and were trained simultaneously with CTC and transducer objectives to maximize each model\u2019s accuracy. Automatic speech recognition (ASR) is a fundamental technology for conversational AI &hellip; <a href=\"https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/\">Continued</a></p>\n", "protected": false}, "author": 1983, "featured_media": 76777, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1334040", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-support-for-dutch-and-persian-released-by-nvidia-nemo-asr/279161", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050], "tags": [453, 1066, 1976, 3166, 106], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/speech-ai-riva-fsi-abm-devnews-1920x10801-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jW4", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76636"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1983"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76636"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76636/revisions"}], "predecessor-version": [{"id": 76658, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76636/revisions/76658"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76777"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76636"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76636"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76636"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74321, "date": "2024-01-16T09:02:00", "date_gmt": "2024-01-16T17:02:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74321"}, "modified": "2024-01-25T10:17:33", "modified_gmt": "2024-01-25T18:17:33", "slug": "robust-scene-text-detection-and-recognition-inference-optimization", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/", "title": {"rendered": "Robust Scene Text Detection and Recognition: Inference Optimization"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In this post, we delve deeper into the inference optimization process to improve the performance and efficiency of our machine learning models during the inference stage. We discuss the techniques employed, such as inference computation graph simplification, quantization, and lowering precision. We also showcase the benchmarking results of our scene text detection and recognition models, comparing the performance of the <a href=\"https://github.com/microsoft/onnxruntime\" data-type=\"link\" data-id=\"https://github.com/microsoft/onnxruntime\">ONNX Runtime</a> and <a href=\"https://developer.nvidia.com/tensorrt\" data-type=\"link\" data-id=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> using <a href=\"https://developer.nvidia.com/triton-inference-server\" data-type=\"link\" data-id=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>.&nbsp;</p>\n\n\n\n<p>Finally, we summarize the importance of optimizing deep learning models for inference and the benefits of using an end-to-end NVIDIA software solution, <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, for building efficient and robust scene-text-OCR systems.</p>\n\n\n\n<p class=\"has-text-align-right\"><em>The first post in this series, <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\">Robust Scene Text Detection and Recognition: Introduction</a>, discussed the importance of robust scene text detection and recognition (STDR) in various industries and the challenges. The second post, </em><a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation//\">Robust Scene Text Detection and Recognition: Implementation</a><em>, discussed the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning.</em></p>\n\n\n\n<h2 class=\"wp-block-heading\">Inference optimization</h2>\n\n\n\n<p>Inference optimization is done to improve the performance and efficiency of machine learning models during the inference stage. It helps in reducing the time, computational resources, and cost required for making predictions, and can also improve accuracy in some cases.&nbsp;</p>\n\n\n\n<p>We have used techniques like inference computation graph simplification, quantization, and lowering precision for inference optimization. These models were originally trained using the PyTorch library, exported in torchScript format, converted to the <a href=\"https://github.com/microsoft/onnxruntime\" data-type=\"link\" data-id=\"https://github.com/microsoft/onnxruntime\">ONNX</a> format, and then transformed into an <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> engine.&nbsp;</p>\n\n\n\n<p>To carry out the ONNX to TensorRT conversion, we used the NGC container image for TensorRT, version 22.07. Following the conversion process, we deployed the model for inference using the NVIDIA Triton Inference Server, version 22.07. System performance was benchmarked on an NVIDIA A5000 laptop GPU with 16 GB of GPU memory.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1129\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow.png\" alt=\"The diagram shows model optimization using ONNX and TensorRT Optimizer and then model inference using NVIDIA TensorRT runtime in a deployment environment.\u00a0\" class=\"wp-image-74569\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-625x353.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-768x434.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-1536x868.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-optimization-flow-1024x578.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Inference optimization flow</em></figcaption></figure></div>\n\n\n<p>We discuss the details of the optimization of each building block of scene text detection and recognition (STDR) later in this post.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Scene text detection</h3>\n\n\n\n<p>Scene text detection is a crucial component of our scene-text-OCR system. This component takes an image of a scene as input and outputs the locations of text fields within the image. In this article, we are using the pretrained CRAFT model for general scene text detection tasks. This model, which is trained on a diverse set of images, is capable of handling dynamic input images and accurately locating text fields. The average width of the images used as input in our deployments is around 720 points. Here, we have benchmarked two image input sizes: (3,720,720) and (3,1200,1200).</p>\n\n\n\n<p>Our benchmark shows around 2.3x speed-up with TensorRT compared to TorchScript for inference.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1127\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark.png\" alt=\"A graph comparing the performance of three modes of text detection model inference (on an NVIDIA A5000 mobile GPU) using Triton Server: PyTorchScript, ONNX with CUDA and TensorRT, tested on two image sizes (3,700,700 and 3,1200,1200).\" class=\"wp-image-74570\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-1536x866.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/triton-benchmark-1024x577.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\">Figure<em> 2. Triton Inference Server benchmark comparison of scene text detection CRAFT model on image sizes</em></figcaption></figure></div>\n\n\n<p>The deployed CRAFT model is a TensorRT engine with FP32 precision. The following code examples are a quick guide for conversion.</p>\n\n\n\n<p>Create a conda environment:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ conda create \u2013n &lt;your_env_name&gt; python=3.8\n$ conda activate &lt;your_env_name&gt;\n</pre></div>\n\n\n<p>Clone the CRAFT repo and install requirement.txt:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ git clone https://github.com/clovaai/CRAFT-pytorch.git\n$ cd CRAFT-pytorch\n$ pip install \u2013r requirement.txt\n</pre></div>\n\n\n<p>Load the model and convert it to an .onnx format that takes dynamic shapes:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ninput_tensor_detec = torch.randn((1, 3, 768, 768), requires_grad=False)\ninput_tensor_detec=input_tensor_detec.to(device=&quot;cuda\u201d)\n\n# Load net\nnet = CRAFT()\nnet.load_state_dict(copyStateDict(torch.load(model_path)))\nnet = net.cuda()\nnet.eval()\n\n# Convert the model into ONNX\ntorch.onnx.export(net, input_tensor_detec, output_dir,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 verbose=False, opset_version=11,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 do_constant_folding= True,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 export_params=True,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_names=&#91;&quot;input&quot;],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_names=&#91;&quot;output&quot;, &quot;output1&quot;], dynamic_axes={&quot;input&quot;: {0: &quot;batch&quot;, 2: &quot;height&quot;, 3: &quot;width&quot;}})\n</pre></div>\n\n\n<p>Simplify the ONNX graph.<a href=\"https://github.com/daquexian/onnx-simplifier\"> Use ONNX Simplifier</a> to simplify the ONNX model. It infers the whole computation graph and then replaces the redundant operators with their constant outputs (also known as constant folding). The following code example shows the operation folding report for a graph simplification of a CRAFT model:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ onnxsim &lt;path to non_simplified onnx model&gt; &lt;path to simplified onnx model&gt;\n</pre></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-625x352.png\" alt=\"The screenshot shows simplification in the Cast (from 3 to 0), Concat (from 10 to 7), and Constant (from 21 to 0) values.\" class=\"wp-image-74571\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-1536x866.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-1024x577.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. onnxsim report for CRAFT model</em></figcaption></figure></div>\n\n\n<p>For this post, use NVIDIA TensorRT pre-configured Docker containers to convert the ONNX model to a TensorRT serialized plan file. The following code example works with the tensorrt:22.07-py3 NGC container:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n~$ docker run -it --gpus all -v &lt;path to onnx model&gt;:/models \\\nnvcr.io/nvidia/tensorrt:22.07-py3\nroot@576df0ec3a49:/workspace#$ trtexec --onnx=/models/craft.onnx \\\n--explicitBatch --workspace=5000 --minShapes=input:1x3x256x256 \\\n--optShapes=input:1x3x700x700 --maxShapes=input:1x3x1200x1200 \\\n--buildOnly \u2013saveEngine=/models/craft.engine\n</pre></div>\n\n\n<p>The following code example shows the config.pbtxt file for the scene text detection model:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nname: &quot;craft&quot;\ndefault_model_filename: &quot;detec_trt.engine&quot;\nplatform: &quot;tensorrt_plan&quot;\nmax_batch_size : 1\ninput &#91;\n  {\n\tname: &quot;input&quot;\n\tdata_type: TYPE_FP32\n\tdims: &#91; 3, -1, -1 ]\n  }\n]\noutput &#91;\n  {\n\tname: &quot;output&quot;\n\tdata_type: TYPE_FP32\n\tdims: &#91; -1, -1, 2 ]\n  },\n  {\n\tname: &quot;output1&quot;\n\tdata_type: TYPE_FP32\n\tdims: &#91; 32, -1, -1 ]\n  }\n]\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Scene text recognition</h3>\n\n\n\n<p>Scene text recognition is an integral module of the STDR pipeline. We used the PARseq algorithm, a state-of-the-art technique for efficient and customizable text recognition to achieve accurate results.&nbsp;</p>\n\n\n\n<p>To maximize the performance of our pipeline, we converted the PARseq TorchScript model to ONNX and then further converted it to a TensorRT engine, ensuring low latency in text recognition, as each image may contain multiple text fields.</p>\n\n\n\n<p>We found that using an input size of 3x32x128 for the model proved to be the optimal balance between inference time and accuracy. Figure 4 shows the benchmarking results for the PARseq model. We benchmarked around 3x acceleration compared to TorchScript inference.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1127\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq.png\" alt=\"A graph comparing the performance of three modes of text recognition model inference (on NVIDIA A5000 mobile GPU) using Triton Server: PyTorchScript, ONNX with CUDA and TensorRT, tested on a fixed image size of (3,32,128).\" class=\"wp-image-74572\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-1536x866.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/scene-text-recognition-parseq-1024x577.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Triton Inference Server benchmark comparison of scene text recognition PARseq model</em></figcaption></figure></div>\n\n\n<p>The<a href=\"https://github.com/baudm/parseq/releases/download/v1.0.0/parseq-bb5792a6.pt\"> pretrained models</a> published by the authors work well with most of the cases. You can also fine-tune the model if you want to get more accurate output on a custom dataset. The following code examples show the important steps for conversion.</p>\n\n\n\n<p>Install PARSeq:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ git clone https://github.com/baudm/parseq.git\n$ pip install -r requirements.txt\n$ pip install -e .\n</pre></div>\n\n\n<p>You can use your own fine-tuned model or pretrained model from the <a href=\"https://github.com/baudm/parseq/releases\">model repository</a> &nbsp;and convert it into .onnx format. Use an ONNX version later than 1.12.0.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom strhub.models.utils import load_from_checkpoint\n\n# To ONNX\ndevice = &quot;cuda&quot;\nckpt_path = &quot;...&quot;\nonnx_path = &quot;...&quot;\nimg = ...\n\nparseq = load_from_checkpoint(ckpt_path)\nparseq.refine_iters = 0\nparseq.decode_ar = False\nparseq = parseq.to(device).eval()\n\nparseq.to_onnx(onnx_path, img, do_constant_folding=True, opset_version=14)  # opset v14 or newer is required\n\n# check\nonnx_model = onnx.load(onnx_path)\nonnx.checker.check_model(onnx_model, full_check=True) ==&gt; pass\n</pre></div>\n\n\n<p>To convert to TensorRT format, simplify the ONNX model using<a href=\"https://github.com/daquexian/onnx-simplifier\"> onnx-simplifier</a>:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ onnxsim &lt;path to non_simplified onnx model&gt; &lt;path to simplified onnx model&gt;\n</pre></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1127\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq.png\" alt=\"A screenshot of model simplification changed values. Fifteen values were lowered as a result of simplification.\" class=\"wp-image-74573\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-1536x866.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/onnxsim-report-parseq-1024x577.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. onnxsim report for the PARSeq model</em></figcaption></figure></div>\n\n\n<p>After converting the model to a simplified ONNX format, use the <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-ovr\">trtexec</a> tool for the conversion. This conversion is done inside the TensorRT container version 22.07.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n~$ docker run -it --gpus all -v &lt;path to onnx model repository&gt;:/models nvcr.io/nvidia/tensorrt:22.07-py3\nroot@576df0ec3a49:/workspace# trtexec --onnx=/models/parseq_simple.onnx --fp16 \\\n--workspace=1024 --saveEngine=/models/parseq_fp16.trt --minShapes=input:1x3x32x128 \\\n--optShapes=input:4x3x32x128 --maxShapes=input:16x3x32x128\n</pre></div>\n\n\n<p>The following code example shows the <code>config.pbtxt</code> file for the scene text recognition model:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nname: &quot;parseq&quot;\nmax_batch_size: 16\nplatform: &quot;tensorrt_plan&quot;\ndefault_model_filename: &quot;parseq_exp_fp32.trt&quot;\n \ninput {\n\tname: &quot;input&quot;\n\tdata_type: TYPE_FP32\n\tdims: &#91;3, 32, 128]\n}\n \noutput {\n\tname: &quot;output&quot;\n\tdata_type: TYPE_FP32\n\tdims: &#91;26, 95]\n}\n \ninstance_group &#91;\n\t{\n  \tcount: 1\n  \tkind: KIND_GPU\n\t}\n]\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Orchestrator</h3>\n\n\n\n<p>The orchestrator module is a <a href=\"https://github.com/triton-inference-server/python_backend\">Python backend</a> that maintains flow and performs pre-processing for the STDR pipeline. To do the pipeline benchmarking, we used four different images with different image sizes to create custom inputs for <code>perf_analyzer</code>.&nbsp;</p>\n\n\n\n<p>We created two versions of the pipeline, one pipeline using the ONNX Runtime CPU/ GPU backend and another using TensorRT plans, so that the pipeline can work in both GPU and non-GPU environments. We benchmarked the <code>onnx_backend</code> pipeline and <code>tensorrt_plan</code> pipeline on an NVIDIA RTX A5000 laptop GPU (16 GB) using NVIDIA Triton Inference Server.</p>\n\n\n\n<p>The input sample for the benchmark has four different images with sizes (3x472x338), (3x3280x2625), (3x512x413), and (3x1600x1200).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1127\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt.png\" alt=\"Bar chart shows 1x throughput for ONNX-FP32 and 1.5x throughput for TensorRT-FP32.\" class=\"wp-image-74574\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-1536x866.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/stdr-benchmark-onnx-tensorrt-1024x577.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Triton Inference Server benchmark comparison of scene text recognition and detection on ONNX runtime and TensorRT plan</em></figcaption></figure></div>\n\n\n<p>The orchestrator is a Python backend module that coordinates between the scene text detection and scene text recognition models. The configuration file for the orchestrator is as follows:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nname: &quot;pipeline&quot;\nbackend: &quot;python&quot;\nmax_batch_size: 1\ninput &#91;\n  {\n\tname: &quot;input&quot;\n\tdata_type: TYPE_UINT8\n\tdims: &#91; -1, -1, 3 ]\n  }\n]\noutput &#91;\n  {\n\tname: &quot;output&quot;\n\tdata_type: TYPE_STRING\n\tdims: &#91; -1 ]\n  }\n]\n \ninstance_group &#91;\n\t{\n  \tcount: 1\n  \tkind: KIND_GPU\n\t}\n]\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>In summary, the deployment of scene text detection and recognition systems requires careful consideration of real-world scenarios, and optimizing deep learning models for inference is crucial.&nbsp;</p>\n\n\n\n<p>To ensure production-ready optimization and performance, NVIDIA offers an end-to-end software solution, NVIDIA AI Enterprise, that consists of best-in-class AI software and tools including TensorRT and Triton Inference Server for easy access to build enterprise AI applications. The solution is instrumental in achieving low latency and high-performance inference across various devices. </p>\n\n\n\n<p>By using these technologies, you can build efficient and robust scene-text-OCR systems for a range of applications.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In this post, we delve deeper into the inference optimization process to improve the performance and efficiency of our machine learning models during the inference stage. We discuss the techniques employed, such as inference computation graph simplification, quantization, and lowering precision. We also showcase the benchmarking results of our scene text detection and recognition models, &hellip; <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/\">Continued</a></p>\n", "protected": false}, "author": 1945, "featured_media": 76047, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1334001", "discourse_permalink": "https://forums.developer.nvidia.com/t/robust-scene-text-detection-and-recognition-inference-optimization/279157", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758], "tags": [453, 38, 126], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/wipro-series-featured-part3.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jkJ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74321"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1945"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74321"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74321/revisions"}], "predecessor-version": [{"id": 76634, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74321/revisions/76634"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76047"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74321"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74321"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74321"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74323, "date": "2024-01-16T09:01:00", "date_gmt": "2024-01-16T17:01:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74323"}, "modified": "2024-01-25T10:17:33", "modified_gmt": "2024-01-25T18:17:33", "slug": "robust-scene-text-detection-and-recognition-implementation", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/", "title": {"rendered": "Robust Scene Text Detection and Recognition: Implementation"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>To make scene text detection and recognition work on irregular text or for specific use cases, you must have full control of your model so that you can do incremental learning or fine-tuning as per your use cases and datasets. Keep in mind that this pipeline is the main building block of scene understanding, AI-based inspection, and document processing platforms. It should be accurate and have low latency.&nbsp;</p>\n\n\n\n<p class=\"has-text-align-right\"><em>The first post in this series, <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\">Robust Scene Text Detection and Recognition: Introduction</a>, discussed the importance of robust scene text detection and recognition (STDR) in various industries and the challenges. The third post, </em><a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/\">Robust Scene Text Detection and Recognition: Inference Optimization</a><em>, covers production-ready optimization and performance for your STDR pipeline</em>.</p>\n\n\n\n<p>For this post, we decided to use state-of-the-art, highly accurate, deep-learning models. To ensure accuracy and keep low end-to-end latency, we performed model inference optimization using tools and frameworks like <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> and <a href=\"https://github.com/microsoft/onnxruntime\">ONNX Runtime</a>. To ensure standard model deployment and execution, in addition to high-performing inference with scalability, we decided to use <a href=\"https://developer.nvidia.com/nvidia-triton-inference-server\">NVIDIA Triton Inference Server</a>.</p>\n\n\n\n<p>To train our models, we used Docker container images from the <a href=\"https://catalog.ngc.nvidia.com/?filters=&amp;orderBy=weightPopularDESC&amp;query=\">NGC catalog</a>, a hub for GPU-optimized AI and ML software. NGC containers leverage the power of NVIDIA GPUs and can run in virtual machines (VMs) configured with NVIDIA virtual GPU (vGPU) software in NVIDIA vGPU and GPU pass-through deployments. These containers are pre-configured with optimized libraries for SDKs like PyTorch and TensorRT.&nbsp;</p>\n\n\n\n<p>To enable high-performance inference across the cloud, on-premises, and at the edge, we also made use of the Triton Inference Server Docker container. This container enables multiple models from different frameworks to be executed simultaneously on a single GPU or CPU. On a multi-GPU server, Triton Inference Server automatically creates an instance of each model on each GPU to maximize utilization.&nbsp;</p>\n\n\n\n<p>There are three building blocks of the STDR pipeline:</p>\n\n\n\n<ul>\n<li>Scene text detection&nbsp;</li>\n\n\n\n<li>Scene text recognition&nbsp;</li>\n\n\n\n<li>Orchestration</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/Gps6uIqluPFDf4Pa4MO54SQ1Y6KjqnULZb1nVNtFcr5rKpKjIJRI-MDSpbpEY4k2UujOcw-wGtw4vtllfUE-VgdpzpVKq0GgUKcZnVFb7xC386qnxZ5nq9jOgOh-dYix1PloPzd_CnrTwdaJiGah29k\" alt=\"A diagram of an end-to-end OCR pipeline using Triton Inference Server.\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Architecture of a scene text detection pipeline</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/wFzefq-wbiYQ6-hFWeWDlG2Yj1rbi7zLHiTK7vMTHtUTSCfPpZTWWS87GsrBrddk1mjzc71u5y6PG2kf05V9q1MWBlV-JFyF8azsZ1VL-9WPI8wklLvde1Do24jlCZNYh0WtOKGylCzjm3BqNTpVSoE\" alt=\"OCR pipeline flow diagram showing text recognition on the product label.\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Scene text detection flow of text detection and recognition</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Scene text detection</h2>\n\n\n\n<p>In the current pipeline, there are the following options for text detection algorithms:&nbsp;</p>\n\n\n\n<ul>\n<li>FCENet</li>\n\n\n\n<li>CRAFT</li>\n\n\n\n<li>TextFuseNet</li>\n</ul>\n\n\n\n<p>You can train and fine-tune FCENet and TextFuseNet for particular use cases. However, CRAFT cannot be trained or fine-tuned as Clova AI has not published the training code for IP reasons. Our general-purpose pipeline uses the pretrained CRAFT model, which is trained on synthText, IC13, and IC17. For more information, see <a href=\"https://arxiv.org/abs/1904.01941\">Character Region Awareness for Text Detection</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/sQfKuuC08JkscQDr0gvreYgmugV1iTzoHEcilkiQohw-JO2oBLbuB2kpCSrtVcQn7po5NvhVGW_P0p9drhV6CimhLEpeGckgeK7UZ3QMzm2Qv_OCXJUG7MLLHB-Ck-hpL7U5SB00IYbG-0XiIFxDHbE\" alt=\"Diagram shows the region score, affinity score, UpConv blocks and Conv stages.\"/><figcaption class=\"wp-element-caption\"><em>Figure 3. Schematic illustration of CRAFT architecture</em></figcaption></figure></div>\n\n\n<p>This network uses a fully convolutional network architecture based on the VGG-16 model, which encodes the input into a distinct feature representation. The decoding segment of CRAFT is similar to that of UNet and includes skip connections that aggregate low-level features.&nbsp;</p>\n\n\n\n<p>CRAFT predicts two separate scores for each character:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>Region score:</strong> Provides information regarding the area of the character.</li>\n\n\n\n<li><strong>Affinity score:</strong> Reveals the degree to which characters are combined into a single entity.</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/RIv5AKnC3JJIHdWA6eMGI2WDtLL4GwC09dL_gqzYVyDhv0aEjXpRhhUdm9gYf0oqv5aVzKjjAyqA__9YrAYv0DDxoAiePjclCD9IO0Y0ET1Ll6BnAiyRb7CsIKjlIwZh1sygKGMuGgQvMr0410uRDjY\" alt=\"Each column shows the input image and the region score and affinity maps.\"/><figcaption class=\"wp-element-caption\"><em>Figure 4. Two scene text detection photos with region score and affinity maps</em> (Image: <a href=\"https://arxiv.org/pdf/1904.01941.pdf\">Character Region Awareness for Text Detection</a>)</figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Scene text recognition</h2>\n\n\n\n<p>In this post, we use the state-of-the-art Scene Text Recognition with the Permuted Autoregressive Sequence (PARseq) algorithm. For more information, see <a href=\"https://arxiv.org/pdf/2207.06966.pdf\">Scene Text Recognition with Permuted Autoregressive Sequence Models</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/wFYXUOOcdWbBMCShnaOzmnZkZYuLrry32DIky7y8jKO_bISB9iAgU93YSz7dTuXGl9iYtvsG5IoLYHoHdFLlucto81JIKHXuCKo048P9ZbMkHyuo_GHgaAO2Xp79r0cvcQzDPYaNDZScIQlsqQBWsPw\" alt=\"Diagram shows the position queries, visio-lingual decoder, input context, permutations, input image, output logits, and ground-truth label.\"/><figcaption class=\"wp-element-caption\"><em>Figure 5. PARSeq architecture and training overview</em></figcaption></figure></div>\n\n\n<p><a href=\"https://github.com/baudm/parseq/releases\">Published, pretrained models</a> are trained on several datasets like MJSynth and SynthText, COCO-Text, RCTW17, Uber-Text, ArT, LSVT, and MLT19. We also used incremental learning techniques to fine-tune pretrained models on custom datasets.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Orchestrator</h2>\n\n\n\n<p>The orchestrator module is the control unit of the pipeline. This module is responsible for coordinating between scene text detection and scene text recognition.&nbsp;</p>\n\n\n\n<ul>\n<li>It receives an input image from a request.&nbsp;</li>\n\n\n\n<li>That image is pre-processed and sent to the scene text detection module.&nbsp;</li>\n\n\n\n<li>The detection module returns the locations of the text fields present in the input image.&nbsp;</li>\n\n\n\n<li>The Orchestrator crops out the text fields from the input image into a list of <code>ndarrays</code>.</li>\n\n\n\n<li>&nbsp;It creates batches from the cropped text images of a predefined batch size and sends one batch at a time to the text recognition module.&nbsp;</li>\n\n\n\n<li>The recognition module returns STR output with a confidence score for each of the cropped text images within that batch.&nbsp;</li>\n</ul>\n\n\n\n<p>The orchestrator maintains a track of each text field location, corresponding STR output, and confidence score. Using all this information, it creates a response JSON.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/sVTEVs-a_8fEZAvWvdVQwvk-SWC5uuI88CVJoxpaXyod2FqBRl8QIRRuhSZeC3D5r1uwng8RSlTQrgv1nBw9HJysx4rmBbtMWeNkDrPMTlX9-LRTduaQprUditkuY8lJ4rOmXIqaDV5uSGrP8zz0VSg\" alt=\"Diagram shows the scene image input for scene text detection on a TensorRT backend, the orchestrator on a Python backend, and scene text recognition on a TensorRT backend, all leading to the output JSON.\"/><figcaption class=\"wp-element-caption\"><em>Figure 6. Orchestrator flow</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>In this post, we discussed the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning. We used the CRAFT algorithm for text detection and the PARSeq algorithm for text recognition. We designed a distinct orchestration module to facilitate coordination between text detection and recognition. This post also highlighted the use of NVIDIA TensorRT, ONNX Runtime, and NVIDIA Triton Inference Server for model optimization and high-performance inference serving.</p>\n\n\n\n<p>For more information, see the <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\">Robust Scene Text Detection and Recognition: Introduction</a> and <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/\">Robust Scene Text Detection and Recognition: Inference Optimization</a> posts in this series.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>To make scene text detection and recognition work on irregular text or for specific use cases, you must have full control of your model so that you can do incremental learning or fine-tuning as per your use cases and datasets. Keep in mind that this pipeline is the main building block of scene understanding, AI-based &hellip; <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/\">Continued</a></p>\n", "protected": false}, "author": 1945, "featured_media": 76046, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1334000", "discourse_permalink": "https://forums.developer.nvidia.com/t/robust-scene-text-detection-and-recognition-implementation/279156", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758], "tags": [453, 38, 126], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/wipro-series-featured-part2.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jkL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74323"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1945"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74323"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74323/revisions"}], "predecessor-version": [{"id": 76632, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74323/revisions/76632"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76046"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74323"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74323"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74323"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74322, "date": "2024-01-16T09:00:00", "date_gmt": "2024-01-16T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74322"}, "modified": "2024-01-25T10:17:34", "modified_gmt": "2024-01-25T18:17:34", "slug": "robust-scene-text-detection-and-recognition-introduction", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/", "title": {"rendered": "Robust Scene Text Detection and Recognition: Introduction"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Identification and recognition of text from natural scenes and images become important for use cases like video caption text recognition, detecting signboards from vehicle-mounted cameras, information retrieval, scene understanding, vehicle number plate recognition, and recognizing text on products.&nbsp;</p>\n\n\n\n<p>Most of these use cases require near real-time performance. The common technique for text extraction includes using an optical character recognition (OCR) system. However, most of the free and commercially available OCR systems are trained to recognize text from documents. There are many challenges when it comes to recognizing text from natural scenes or captioned videos like image perspective, reflections, blurriness, and so on.</p>\n\n\n\n<p class=\"has-text-align-right\"><em>The next post in this series, </em><a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation//\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation//\">Robust Scene Text Detection and Recognition: Implementation</a><em>, discusses the implementation of an STDR pipeline using state-of-the-art deep learning algorithms and techniques like incremental learning and fine-tuning. The third post, </em><a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/\">Robust Scene Text Detection and Recognition: Inference Optimization</a><em>, covers <em>production-ready optimization and performance for your STDR pipeline</em>.</em></p>\n\n\n\n<p>Typically, the text extraction process involves the following steps:&nbsp;</p>\n\n\n\n<ol>\n<li>Text fields are detected from the bigger scene by text detection algorithms.</li>\n\n\n\n<li>This text is extracted and recognized using a custom OCR technique.&nbsp;</li>\n</ol>\n\n\n\n<p>The recognition of irregular text in natural scene images can be challenging due to the variability in text appearance, such as curvature, orientation, and distortion. To overcome this, sophisticated deep-learning architectures and fine-grained annotations are often required.&nbsp;</p>\n\n\n\n<p>However, these can lead to optimization and latency challenges when creating and deploying these algorithms. Despite these challenges, advancements in computer vision have made significant strides in text detection and recognition, providing a powerful tool for various industries. To further optimize inference, you can use specialized optimization tools to reduce latency and improve performance.</p>\n\n\n\n<p>In this post, we describe these challenges and our approach for optimization and acceleration of inference. We emphasize that deploying a scene text detection and recognition (STDR) pipeline requires careful consideration of real-world scenarios and conditions. To meet these needs, we have used state-of-the-art deep learning algorithms and leveraged techniques like incremental learning and fine-tuning for specific use cases.&nbsp;</p>\n\n\n\n<p>To ensure low latency, we used the following model inference optimization tools:&nbsp;</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/microsoft/onnxruntime\">ONNX Runtime</a> is a cross-platform machine-learning model accelerator that offers flexibility for integrating hardware-specific libraries. It can be used with models from PyTorch, TensorFlow and Keras, TensorFlow Lite, scikit-learn, and other frameworks.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT SDK</a> is used for high-performance deep learning inference, providing a deep learning inference optimizer and runtime that guarantees low latency and high throughput for inference applications.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/nvidia-triton-inference-server\">NVIDIA Triton Inference Server</a> is used for high-performance inference serving across cloud, on-premises, and edge devices.&nbsp;</li>\n</ul>\n\n\n\n<p>TensorRT and Triton Inference Server are included in <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, the software layer of the NVIDIA AI platform.</p>\n\n\n\n<h2 class=\"wp-block-heading\">STDR applications</h2>\n\n\n\n<p>Recognizing text from images and videos is used in various industries.</p>\n\n\n\n<p><strong>Healthcare and Life Sciences:</strong> Scene text detection and recognition are used in the healthcare industry to scan and store the medical history of patients on a computer, including reports, X-rays, previous diseases, treatments, diagnostics, and hospital records. It is also required in medical device and drug manufacturing for logistics and warehouse operations.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/P5lfJGYKE8ShyP_nVzHi49cPu0_t9-mdX4EEe-ZG3TXQhsgyJZoWCx0v-5jLYrUAKu-6tO4jrcLZmEkN5LAryk4l2Ay0CfIBC6eEzBTLKnVZSYCRyxURAwmg8G1T1MDGS_cvCDsbYEG7GPb-SUuk9IE\" alt=\"Picture of four medicine bottles with prescription labels.\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Sample of medicine package and bottles</em> (Image: <a href=\"https://www.thetimes.co.uk/article/medicine-labels-are-given-dose-of-plain-english-j6f87sq3qqn\">The Times</a>)</figcaption></figure></div>\n\n\n<p><strong>Manufacturing Supply Chain/Logistics:</strong> Scene text detection and recognition play a crucial role in the food, drink, and cosmetics industries for quality control throughout the supply chain. It is used to track products and read product codes, batch codes, expiry dates, and serial numbers. This information can be used to ensure compliance with safety and anti-counterfeiting laws and to locate products within the supply chain at any given time. OCR is often used in conjunction with barcoding to maximize information collection accuracy.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/3GLdyn1bllDYKi8fscWEsByrA-CsZX_UqMG4jwzysNn9MaxSRAnHn8z66LeGgwFHR6vw7cJ2qHlY3hFDI3wU7JXQOp6aYleTobzbxJAWmO43oCijN3A1Db5NNfFM3dV5rXGxPMDeavXFqCeMlQtBSEY\" alt=\"Warehouse shelves full of boxes with package labels.\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Samples of warehouse packages</em> (Image: shelving.com)</figcaption></figure></div>\n\n\n<p><strong>Banking:</strong> Scene text detection and recognition is widely used in the banking industry to automate know-your-customer (KYC) documents like birth certificates, marriage certificates, and so on.</p>\n\n\n\n<p><strong>Automotive and utilities:</strong> Self-driving cars and utility line maintenance drives often require scene images to be identified and data to be extracted (for example, street names, establishment names, utility pole numbers, and transformer and generator details). Usually, the text appears for a fraction of a time as the vehicle is moving, creating a motion blur. In that case, manual detection becomes impossible.</p>\n\n\n\n<h2 class=\"wp-block-heading\">STDR challenges</h2>\n\n\n\n<p>The biggest challenge in detecting and extracting text from complex images taken from videos and mobile phones is that the text in such images is often irregular and overlayed on varied backgrounds like glass, plastics, rubber, and so on.&nbsp;</p>\n\n\n\n<p>Also, even if the machine learning model is developed with decent accuracy, the expectation is that the model should process images live or in near real time. Thus, catering to both accuracy and performance expectations requires highly refined models that can work optimally in the cloud as well as edge devices. These challenges are described in detail in this post.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Creating robust models</h3>\n\n\n\n<p>Often, the leading reason for accuracy concerns in scene text models is the number of variations in the input data. Here are some of the data variations.</p>\n\n\n\n<p><strong>Text size-scale-blur:</strong> Text in natural scenes can appear in various sizes and scales. The distance from the camera also plays an important role in scaling the text. The angle from the camera brings the perspective distortions. Also, lighting conditions create the reflections and shadows around the text. The moving objects or camera movements add to the blur effects. All these conditions contribute to the size-scale-blur distortions in images.</p>\n\n\n\n<p><strong>Text orientation, color, and font: </strong>Text may appear horizontally, vertically, diagonally, and even circularly. This variation in text orientation can make it difficult for algorithms to correctly detect and recognize text. The color, transparency, and font style used also cause challenges when not reflected by the data used in training.</p>\n\n\n\n<p><strong>Background and overlays:</strong> Text in natural scenes can appear with various backgrounds, such as buildings, trees, vehicles, and so on, and is often overlayed on glass, metal objects, plastics, or stickers. It can also be embossed or debossed onto various kinds of materials.</p>\n\n\n\n<p><strong>Multiple languages</strong>: Real-world images contain text in multiple scripts and languages. Often, signage or restaurant menus are written in mixed languages.</p>\n\n\n\n<p>Another typical challenge in ML projects is the availability of labeled data to train the model. However, for this pipeline, we used a pretrained <a href=\"https://drive.google.com/open?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ\">CRAFT model</a> for text detection, which is trained on the <a href=\"https://www.robots.ox.ac.uk/~vgg/data/scenetext/\">SynthText</a>, <a href=\"https://rrc.cvc.uab.es/?ch=2\">IC13</a>, and <a href=\"https://rrc.cvc.uab.es/?ch=8&amp;com=downloads\">IC17</a> datasets.&nbsp;</p>\n\n\n\n<p>For text recognition, we used the <a href=\"https://github.com/baudm/parseq/releases/download/v1.0.0/parseq-bb5792a6.pt\">PARseq model</a>, which is trained on various datasets (<a href=\"https://www.robots.ox.ac.uk/~vgg/data/text/\">MJSynth</a>, <a href=\"https://www.robots.ox.ac.uk/~vgg/data/scenetext/\">SynthText</a>,<a href=\"https://vision.cornell.edu/se3/coco-text-2/\"> COCO-Text</a>, <a href=\"https://rctw.vlrlab.net/dataset\">RCTW17</a>, <a href=\"https://s3-us-west-2.amazonaws.com/uber-common-public/ubertext/index.html\">Uber-Text</a>, <a href=\"http://rrc.cvc.uab.es/?ch=14\">ArT</a>, <a href=\"https://rrc.cvc.uab.es/?ch=16\">LSVT</a>, <a href=\"https://rrc.cvc.uab.es/?ch=15\">MLT19</a>, and <a href=\"https://rrc.cvc.uab.es/?ch=12\">ReCTS</a>, <a href=\"https://textvqa.org/textocrZXh0b2NyLw&amp;ntb=1\">TextOCR</a>) and finetuned with in-house data.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Meeting the performance expectations</h3>\n\n\n\n<p>Deploying a scene text detection solution can also present various challenges.</p>\n\n\n\n<p><strong>Computational resources</strong>: Today, modern STDR systems use complex deep learning algorithms. These models have an abundance of parameters, making them computationally expensive to run. Consequently, it can be difficult to deploy these solutions on devices with limited computational resources, such as smartphones or Internet of Things (IoT) devices.</p>\n\n\n\n<p><strong>Latency and response time</strong>: In many scenarios, scene text detection and recognition must be real-time to be effective. Deep learning models can offer excellent accuracy, but their high number of parameters can lead to increased inference time compared to models having a low number of parameters, resulting in unacceptable latency and response time. To optimize accuracy, state-of-the-art algorithms must be used, while inference time can be reduced through optimization techniques such as quantization, lowering precision, and pruning. These optimizations may reduce the accuracy of the model.</p>\n\n\n\n<p><strong>Data privacy and security</strong>: The privacy and security of the data used for training and running the model are important when deploying the solutions in real-world scenarios. The model needs to be protected from malicious attacks and data breaches. Compliance with data privacy regulations must be ensured.</p>\n\n\n\n<p>The deployment of scene text detection solutions demands meticulous consideration of the real-world scenarios and conditions in which the solution will be employed. This process is a crucial step that necessitates thorough testing, evaluation, and fine-tuning.</p>\n\n\n\n<p>Consider a package delivery company that requires a label-reading application on a conveyor belt. In this case, high accuracy is critical, as any error can cause delays and result in additional costs for the company. The speed of the conveyor belt is another essential factor to consider, as it affects the overall time required to process the packages.&nbsp;</p>\n\n\n\n<p>Achieving high accuracy may require complex deep-learning models that can be computationally expensive and impact system latency. To optimize performance, it\u2019s important to consider the specific requirements and constraints of the deployment scenario, such as conveyor belt speed and computational resources, and adjust the deep learning models accordingly to strike a balance between accuracy, latency, and resources.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>In this post, we discussed the importance of robust scene text detection and recognition (STDR) in various industries. We highlighted the challenges faced in STDR, including creating accurate models, meeting performance expectations, and dealing with real-world scenarios and conditions.</p>\n\n\n\n<p>For more information, see the next posts in this series:</p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/\">Robust Scene Text Detection and Recognition: Implementation</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/\">Robust Scene Text Detection and Recognition: Inference Optimization</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Identification and recognition of text from natural scenes and images become important for use cases like video caption text recognition, detecting signboards from vehicle-mounted cameras, information retrieval, scene understanding, vehicle number plate recognition, and recognizing text on products.&nbsp; Most of these use cases require near real-time performance. The common technique for text extraction includes using &hellip; <a href=\"https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/\">Continued</a></p>\n", "protected": false}, "author": 1945, "featured_media": 76044, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1333999", "discourse_permalink": "https://forums.developer.nvidia.com/t/robust-scene-text-detection-and-recognition-introduction/279155", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758], "tags": [453, 38, 126], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/wipro-series-featured-part1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jkK", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74322"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1945"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74322"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74322/revisions"}], "predecessor-version": [{"id": 76631, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74322/revisions/76631"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76044"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74322"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74322"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74322"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76545, "date": "2024-01-12T13:00:00", "date_gmt": "2024-01-12T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76545"}, "modified": "2024-01-25T10:17:35", "modified_gmt": "2024-01-25T18:17:35", "slug": "webinar-state-of-ray-tracing-and-nvrtx-5-3", "status": "publish", "type": "post", "link": "https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia", "title": {"rendered": "Webinar: State of Ray Tracing and NvRTX 5.3"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Get up to speed on the current state of ray tracing in the NVIDIA RTX Branch of Unreal Engine and what\u2019s coming next.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Get up to speed on the current state of ray tracing in the NVIDIA RTX Branch of Unreal Engine and what\u2019s coming next.</p>\n", "protected": false}, "author": 1480, "featured_media": 76553, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia", "_links_to_target": "_blank"}, "categories": [1235], "tags": [637, 453, 1944, 3536, 483, 582], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/game-dev-ray-tracing-nvrtx.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jUB", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76545"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1480"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76545"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76545/revisions"}], "predecessor-version": [{"id": 76563, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76545/revisions/76563"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76553"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76545"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76545"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76545"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]