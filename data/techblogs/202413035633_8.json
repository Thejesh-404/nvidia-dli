[{"id": 70509, "date": "2023-09-12T14:00:00", "date_gmt": "2023-09-12T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70509"}, "modified": "2023-10-05T11:18:12", "modified_gmt": "2023-10-05T18:18:12", "slug": "power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai/", "title": {"rendered": "Power Your Business with NVIDIA AI Enterprise 4.0 for Production-Ready Generative AI"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Crossing the chasm and reaching its iPhone moment, <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a> must scale to fulfill exponentially increasing demands. Reliability and uptime are critical for building generative AI at the enterprise level, especially when AI is core to conducting business operations. NVIDIA is investing its expertise into building a solution for those enterprises ready to take the leap.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Introducing NVIDIA AI Enterprise 4.0</h2>\n\n\n\n<p>The latest version of NVIDIA AI Enterprise accelerates development through multiple facets with production-ready support, manageability, security, and reliability for enterprises innovating with generative AI.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Quickly train, customize, and deploy LLMs at scale with NVIDIA NeMo&nbsp;</h3>\n\n\n\n<p>Generative AI models have billions of parameters and require an efficient data training pipeline. The complexity of training models, customization for domain-specific tasks, and deployment of models at scale require expertise and compute resources.&nbsp;</p>\n\n\n\n<p>NVIDIA AI Enterprise 4.0 now includes <a href=\"https://developer.nvidia.com/blog/unlocking-the-power-of-enterprise-ready-llms-with-nemo/\">NVIDIA NeMo</a>, an end-to-end, cloud-native framework for data curation at scale, accelerated training and customization of large language models (LLMs), and optimized inference on user-preferred platforms. From cloud to desktop workstations, NVIDIA NeMo provides easy-to-use recipes and optimized performance with accelerated infrastructure, greatly reducing time to solution and increasing ROI.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Build generative AI applications faster with AI workflows</h3>\n\n\n\n<p>NVIDIA AI Enterprise 4.0 introduces two new <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/\">AI workflows</a> for building generative AI applications: AI chatbot with retrieval augmented generation and spear phishing detection.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/generative-ai-chatbots/\">generative AI knowledge base chatbot</a> workflow, leveraging Retrieval Augmented Generation, accelerates the development and deployment of generative AI chatbots tuned on your data. These chatbots accurately answer domain-specific questions, retrieving information from a company\u2019s knowledge base and generating real-time responses in natural language. It uses pretrained LLMs, <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NeMo</a>, <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a>, along with third-party tools including Langchain and vector database, for training and deploying the knowledge base question-answering system.</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/spear-phishing/\">spear phishing detection AI workflow</a> uses <a href=\"https://developer.nvidia.com/morpheus-cybersecurity\">NVIDIA Morpheus</a> and generative AI with NVIDIA NeMo to train a model that can detect up to 90% of spear phishing e-mails before they hit your inbox.&nbsp;</p>\n\n\n\n<p>Defending against spear-phishing e-mails is a challenge. Spear phishing e-mails are indistinguishable from benign e-mails, with the only difference between the scam and legitimate e-mail being the intent of the sender. This is why traditional mechanisms for detecting spear phishing fall short.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Develop AI anywhere&nbsp;&nbsp;</h3>\n\n\n\n<p>Enterprise adoption of AI can require additional skilled AI developers and data scientists. Organizations will need a flexible high-performance infrastructure consisting of optimized hardware and software to maximize productivity and accelerate AI development. Together with <a href=\"https://www.nvidia.com/en-us/design-visualization/rtx-6000/\">NVIDIA RTX 6000 Ada Generation GPUs</a> for workstations, NVIDIA AI Enterprise 4.0 provides AI developers a single platform for developing AI applications and deploying them in production.&nbsp;</p>\n\n\n\n<p>Beyond the desktop, NVIDIA offers a complete infrastructure portfolio for AI workloads including NVIDIA H100, L40S, L4 GPUs, and accelerated networking with NVIDIA BlueField data processing units. With <a href=\"https://www.hpe.com/us/en/hpe-machine-learning-data-management-software.html\">HPE Machine Learning Data Management</a>, <a href=\"https://www.hpe.com/us/en/hpe-machine-learning-development-environment.html\">HPE Machine Learning Development Environment</a>, <a href=\"https://ubuntu.com/blog/ubuntu-kvm-supports-nvidia-ai-enterprise\">Ubuntu KVM</a> and Nutanix AHV virtualization support, organizations can use on-prem infrastructure to power AI workloads.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Manage AI workloads and infrastructure</h3>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/scaling-deep-learning-deployments-with-nvidia-triton-management-service\">NVIDIA Triton Management Service</a>, an exclusive addition to NVIDIA AI Enterprise 4.0, automates the deployment of multiple Triton Inference Servers in Kubernetes with GPU resource-efficient model orchestration. It simplifies deployment by loading models from multiple sources and allocating compute resources. Triton Management Service is available for lab experience on <a href=\"https://www.nvidia.com/en-us/launchpad/ai/model-orchestration-with-triton-management-service/\">NVIDIA LaunchPad</a>.</p>\n\n\n\n<p>NVIDIA AI Enterprise 4.0 also includes cluster management software, <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/base-command-manager-essentials/\">NVIDIA Base Command Manager Essentials</a>, for streamlining cluster provisioning, workload management, infrastructure monitoring, and usage reporting. It facilitates the deployment of AI workload management with dynamic scaling and policy-based resource allocation, providing cluster integrity.</p>\n\n\n\n<h3 class=\"wp-block-heading\">New AI software, tools, and pretrained foundation models</h3>\n\n\n\n<p>NVIDIA AI Enterprise 4.0 brings more frameworks and tools to advance AI development. <a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus </a>is a framework for building, training, and fine-tuning physics-machine learning models with a simple Python interface.&nbsp;</p>\n\n\n\n<p>Using Modulus, users can bolster engineering simulations with AI and build models for enterprise-scale digital twin applications across multiple physics domains, from CFD and Structural to Electromagnetics. The \u200b\u200b<a href=\"https://catalog.ngc.nvidia.com/orgs/n\">Deep Graph Library</a> container is designed to implement and train Graph Neural Networks that can help scientists research the graph structure of molecules or financial services to detect fraud.&nbsp;</p>\n\n\n\n<p>Lastly, three exclusive <a href=\"https://developer.nvidia.com/tao-toolkit\">pretrained foundation models</a>, part of NVIDIA TAO, speed time to production for industry applications such as vision AI, defect detection, and retail loss prevention.&nbsp;</p>\n\n\n\n<p>NVIDIA AI Enterprise 4.0 is the most comprehensive upgrade to the platform to date. With enterprise-grade security, stability, manageability, and support, enterprises can expect reliable AI uptime and uninterrupted AI excellence.</p>\n\n\n\n<h1 class=\"wp-block-heading\">Get started with NVIDIA AI Enterprise</h1>\n\n\n\n<p>Three ways to get accelerated with NVIDIA AI Enterprise:</p>\n\n\n\n<ul>\n<li>Sign up for <a href=\"https://www.nvidia.com/en-us/launchpad/ai/\">NVIDIA LaunchPad</a> for \u200cshort-term access to sets of hands-on labs.</li>\n\n\n\n<li>Sign up for a free 90-day <a href=\"http://www.nvidia.com/ai-enterprise-eval\">evaluation</a> for existing on-prem or cloud infrastructure.</li>\n</ul>\n\n\n\n<p>Purchase through <a href=\"https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/\">NVIDIA Partner Network</a> or major Cloud Service Providers including <a href=\"https://aws.amazon.com/marketplace/pp/prodview-ozgjkov6vq3l6\">AWS</a>, <a href=\"https://azuremarketplace.microsoft.com/en-us/marketplace/apps/nvidia.nvidia-ai-enterprise\">Microsoft Azure</a>, and <a href=\"https://console.cloud.google.com/marketplace/product/nvidia/nvidia-ai-enterprise-vmi\">Google Cloud</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Crossing the chasm and reaching its iPhone moment, generative AI must scale to fulfill exponentially increasing demands. Reliability and uptime are critical for building generative AI at the enterprise level, especially when AI is core to conducting business operations. NVIDIA is investing its expertise into building a solution for those enterprises ready to take the &hellip; <a href=\"https://developer.nvidia.com/blog/power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai/\">Continued</a></p>\n", "protected": false}, "author": 1395, "featured_media": 70844, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1260035", "discourse_permalink": "https://forums.developer.nvidia.com/t/power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai/266203", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [2584, 3284, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvaie-workflow-gif-.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-ilf", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70509"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1395"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70509"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70509/revisions"}], "predecessor-version": [{"id": 70840, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70509/revisions/70840"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70844"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70509"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70509"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70509"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70728, "date": "2023-09-12T11:00:00", "date_gmt": "2023-09-12T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70728"}, "modified": "2023-10-05T11:18:13", "modified_gmt": "2023-10-05T18:18:13", "slug": "generative-ai-and-accelerated-computing-for-spear-phishing-detection", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/generative-ai-and-accelerated-computing-for-spear-phishing-detection/", "title": {"rendered": "Generative AI and Accelerated Computing for Spear Phishing Detection"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Spear phishing is the largest and most costly form of cyber threat, with an estimated 300,000 reported victims in 2021 representing $44 million in reported losses in the United States alone. Business e-mail compromises led to $2.4 billion in costs in 2021, according to the <a href=\"https://www.ic3.gov/Media/PDF/AnnualReport/2021_IC3Report.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">FBI Internet Crime Report</a>. In the period from June 2016 to December 2021, costs related to phishing and spear phishing totaled $43 billion for businesses, according to <a href=\"https://drive.google.com/file/d/1MWU1fmeylg4bul0jdFkAfghdA4NOSeuP/view\" target=\"_blank\" rel=\"noreferrer noopener\">IBM Security Cost of a Data Breach</a>.&nbsp;</p>\n\n\n\n<p>Spear phishing e-mails are indistinguishable from a benign e-mail that a victim would receive. This is also why traditional classification of spear phishing e-mails is so difficult. The content difference between a scam and a legitimate e-mail can be minuscule. Often, the only difference between the two is the intent of the sender: is the invoice legitimate, or is it a scam?&nbsp;</p>\n\n\n\n<p>This post details a two-fold approach to improve spear phishing detection by boosting the signals of intent using <a href=\"https://github.com/nv-morpheus/Morpheus\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Morpheus</a> to run data processing and inferencing.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Generating e-mails with new phishing intent</h2>\n\n\n\n<p>The first step involves using <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\" target=\"_blank\" rel=\"noreferrer noopener\">generative AI</a> to create large, varied corpora of e-mails with various intents associated with spear phishing and scams. As new threats emerge, the NVIDIA Morpheus team uses the NVIDIA <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework\" target=\"_blank\" rel=\"noreferrer noopener\">NeMo</a> framework to generate a new corpus of e-mails with such threats. Following the generation of new e-mails with the new type of phishing intent, the team trains a new language model to recognize the intent. In traditional phishing detection mechanisms, such models would require a significant number of human-labeled e-mails.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1164\" height=\"651\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology.png\" alt=\"Diagram showing an overview of the spear phishing detection methodology. AI- generated e-mails with specific intents are used to train intent models that label incoming user e-mails. These labels are joined with past sender behavior (if any) and e-mail metadata to classify the e-mail as spear phishing or not.\" class=\"wp-image-70756\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology.png 1164w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-300x168.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-625x350.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-768x430.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-645x361.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-500x280.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-362x202.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-197x110.png 197w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-1.-Overview-of-the-spear-phishing-detection-methodology-1024x573.png 1024w\" sizes=\"(max-width: 1164px) 100vw, 1164px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Overview of the spear phishing detection methodology</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Detecting sender intent</h2>\n\n\n\n<p>The first step targets the intent behind the e-mail. The next step targets the intent of the sender. To defend against spear phishing attacks that use spoofing, known senders, or longer cons that do not express their true intent immediately, we construct additional signals by building up behavioral sketches from senders or groups of senders.&nbsp;</p>\n\n\n\n<p>Building on the intent work described above, known senders\u2019 past observed intents are recorded. For example, the first time a known sender asks for money can be a signal to alert the user.&nbsp;</p>\n\n\n\n<p>Syntax usage is also observed and recorded. The syntax of new e-mails is compared to the syntax history of the sender. A deviation from the observed syntax could indicate a possible spoofing attack.&nbsp;</p>\n\n\n\n<p>Finally, the temporal patterns of a sender&#8217;s e-mails are collected and cross-referenced when a new e-mail arrives to check for out-of-pattern behavior. Is the sender sending an e-mail for the first time at midnight on a Saturday? If so, that becomes a signal in the final prediction. These signals in aggregate are used to classify e-mails. They are also presented to the end user as an explanation for why an e-mail may be malicious.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Adapting to new attacks and improving protection</h2>\n\n\n\n<p>Existing machine learning (ML) methods rely nearly entirely on human-labeled data and cannot adapt to emerging threats quickly. The biggest benefit to detecting spear phishing e-mails using the approach presented here is how quickly the model can be adapted to new attacks. When a new attack emerges, generative AI is leveraged to create a training corpus for the attack. Intent models are trained to detect its presence in received e-mails.&nbsp;</p>\n\n\n\n<p>Using models built with NeMo generates thousands of high-quality, on-topic e-mails in just a few hours. The new intents are added to the existing spear phishing detector. The entire end-to-end workflow of creating new phishing attack e-mails and updating the existing models happens in less than 24 hours. Once the models are in place, \u200ce-mail processing and inferencing become a Morpheus pipeline to provide near real-time protection against spear phishing threats.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Results</h2>\n\n\n\n<p>To illustrate the flexibility of this approach, a model was trained using only money, banking, and personal identifying information (PII) intents. Next, cryptocurrency-flavored phishing e-mails were generated using models built with NeMo. These e-mails were incorporated into the original training and validation subsets.&nbsp;</p>\n\n\n\n<p>The validation set, now containing the new crypto attacks, was then passed into the original model. Then a second model was trained incorporating the crypto attack intents. Figure 2 shows how the models compare in their detection.&nbsp;</p>\n\n\n\n<p>After training for the attack, the F1 score increased from 0.54 to 0.89 (Figure 3). This illustrates how quickly new attacks can be trained for and adapted to using NVIDIA Morpheus and NeMo.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1113\" height=\"595\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1.png\" alt=\"Chart showing how the models compare in their detection.\" class=\"wp-image-70758\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1.png 1113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-625x334.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-768x411.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-645x345.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-500x267.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-362x194.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-206x110.png 206w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-2.-Differences-in-detection-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-1024x547.png 1024w\" sizes=\"(max-width: 1113px) 100vw, 1113px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Differences in detection between an untrained model and the model trained for a cryptocurrency-based spear phishing attack</em></figcaption></figure></div>\n\n\n<div style=\"height:26px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1124\" height=\"592\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1.png\" alt=\"Chart showing the F1-score difference between the models\" class=\"wp-image-70759\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1.png 1124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-625x329.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-768x404.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-645x340.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-362x191.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-209x110.png 209w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-3.-F1-score-difference-between-an-untrained-model-and-the-model-trained-for-a-cryptocurrency-based-spear-phishing-attack-1-1024x539.png 1024w\" sizes=\"(max-width: 1124px) 100vw, 1124px\" /><figcaption class=\"wp-element-caption\">Figure 3. F1-score difference between an untrained model and the model trained for a cryptocurrency-based spear phishing attack</figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-left\">Get started with NVIDIA Morpheus</h2>\n\n\n\n<p class=\"has-text-align-left\">Watch the video, <a href=\"https://www.youtube.com/watch?v=57dEPP67XrY\" target=\"_blank\" rel=\"noreferrer noopener\">Improve Spear Phishing Detection with Generative AI</a> for more details. Learn more about how to use NVIDIA Morpheus to detect spear phishing e-mails faster and with greater accuracy using the <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/spear-phishing/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI workflow</a> example. You can also apply to <a href=\"https://www.nvidia.com/en-us/launchpad/ai/build-ai-based-cybersecurity-solutions/\" target=\"_blank\" rel=\"noreferrer noopener\">try NVIDIA Morpheus in LaunchPad</a> and request a 90-day free trial to<a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/#evaluation-form\" target=\"_blank\" rel=\"noreferrer noopener\"> test drive NVIDIA Morpheus</a>, part of the NVIDIA AI Enterprise software family.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Spear phishing is the largest and most costly form of cyber threat, with an estimated 300,000 reported victims in 2021 representing $44 million in reported losses in the United States alone. Business e-mail compromises led to $2.4 billion in costs in 2021, according to the FBI Internet Crime Report. In the period from June 2016 &hellip; <a href=\"https://developer.nvidia.com/blog/generative-ai-and-accelerated-computing-for-spear-phishing-detection/\">Continued</a></p>\n", "protected": false}, "author": 1869, "featured_media": 70768, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1259959", "discourse_permalink": "https://forums.developer.nvidia.com/t/generative-ai-and-accelerated-computing-for-spear-phishing-detection/266189", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696, 3110], "tags": [1511, 453, 2877, 3270], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/security-promo-pack-spear-phishing-ai-workflow-2908572-blog-1480x830-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ioM", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70728"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1869"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70728"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70728/revisions"}], "predecessor-version": [{"id": 70771, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70728/revisions/70771"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70768"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70728"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70728"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70728"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70814, "date": "2023-09-12T09:41:22", "date_gmt": "2023-09-12T16:41:22", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70814"}, "modified": "2023-12-05T11:21:54", "modified_gmt": "2023-12-05T19:21:54", "slug": "event-recsys-at-work-best-practices-and-insights", "status": "publish", "type": "post", "link": "https://nvda.ws/45LZPhj", "title": {"rendered": "Event: RecSys at Work: Best Practices and Insights"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>On Sept. 27, join us to learn recommender systems best practices for building, training, and deploying at any scale.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>On Sept. 27, join us to learn recommender systems best practices for building, training, and deploying at any scale.</p>\n", "protected": false}, "author": 1115, "featured_media": 70817, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/45LZPhj", "_links_to_target": "_blank"}, "categories": [696, 1968], "tags": [452, 453, 1185], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/recsys-summit-2023-blog-2912182-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iqa", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70814"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70814"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70814/revisions"}], "predecessor-version": [{"id": 70819, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70814/revisions/70819"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70817"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70814"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70814"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70814"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70336, "date": "2023-09-12T08:00:00", "date_gmt": "2023-09-12T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70336"}, "modified": "2023-10-05T11:18:14", "modified_gmt": "2023-10-05T18:18:14", "slug": "selecting-the-right-camera-for-the-nvidia-jetson-and-other-embedded-systems", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/selecting-the-right-camera-for-the-nvidia-jetson-and-other-embedded-systems/", "title": {"rendered": "Selecting the Right Camera for the NVIDIA Jetson and Other Embedded Systems"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The camera module is the most integral part of an AI-based embedded system. With so many camera module choices on the market, the selection process may seem overwhelming. This post breaks down the process to help make the right selection for an embedded application, including the <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\">NVIDIA Jetson</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Camera selection considerations</h2>\n\n\n\n<p>Camera module selection involves consideration of three key aspects: sensor, interface (connector), and optics.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Sensor&nbsp;</h3>\n\n\n\n<p>The two main types of electronic image sensors are the charge-coupled device (CCD) and the active-pixel sensor (CMOS). For a CCD sensor, pixel values can only be read on a per-row basis. Each row of pixels is shifted, one by one, into a readout register. For a CMOS sensor, each pixel can be read individually and in parallel.&nbsp;</p>\n\n\n\n<p>CMOS is less expensive and consumes less energy without sacrificing image quality, in most cases. It can also achieve higher frame rates due to the parallel readout of pixel values. However, there are some specific scenarios in which CCD sensors still prevail\u2014for example, when long exposure is necessary and very low-noise images are required, such as in astronomy.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Electronic shutter&nbsp;</h4>\n\n\n\n<p>There are two options for the electronic shutter: global or rolling. A global shutter exposes each pixel to incoming light at the same time. A rolling shutter exposes the pixel rows in a certain order (top to bottom, for example) and can cause distortion (Figure 1).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"787\" height=\"479\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion.png\" alt=\"Two images of a helicopter showing distortion of moving blades caused by rolling shutter.\n\" class=\"wp-image-70349\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion.png 787w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-300x183.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-625x380.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-768x467.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-645x393.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-493x300.png 493w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-362x220.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/helicopter-blade-distortion-181x110.png 181w\" sizes=\"(max-width: 787px) 100vw, 787px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Distortion of rotor blades caused by rolling shutter</em></figcaption></figure>\n\n\n\n<p>The global shutter is not impacted by motion blur and distortion due to object movement. It is much easier to sync multiple cameras with a global shutter because there is a single point in time when exposure starts. However, sensors with a global shutter are much more expensive than those with a rolling shutter.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Color or monochrome&nbsp;</h4>\n\n\n\n<p>In most cases, a monochrome image sensor is sufficient for typical machine vision tasks like fault detection, presence monitoring, and recording measurements.</p>\n\n\n\n<p>With a monochrome sensor, each pixel is usually described by eight bits. With a color sensor, each pixel has eight bits for the red channel, eight bits for the green channel, and eight bits for the blue channel. The color sensor requires processing three times the amount of data, resulting in a higher processing time and, consequently, a slower frame rate.&nbsp;&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Dynamic range&nbsp;</h4>\n\n\n\n<p>Dynamic range is the ratio between the maximum and minimum signal that is acquired by the sensor. At the upper limit, pixels appear white for higher values of intensity (saturation), while pixels appear black at the lower limit and below. An HDR of at least 80db is needed for indoor application and up to 140db is needed for outdoor application.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Resolution&nbsp;</h4>\n\n\n\n<p>Resolution is a sensor\u2019s ability to reproduce object details. It can be influenced by factors such as the type of lighting used, the sensor pixel size, and the capabilities of the optics. The smaller the object detail, the higher the required resolution.&nbsp;</p>\n\n\n\n<p>Pixel resolution translates to how many millimeters each pixel is equal to on the image. The higher the resolution, the sharper your image will be. The camera or sensor\u2019s resolution should enable coverage of a feature\u2019s area of at least two pixels.&nbsp;</p>\n\n\n\n<p>CMOS sensors with high resolutions tend to have low frame rates. While a sensor may achieve the resolution you need, it will not capture the quality images you need without achieving enough frames per second. It is important to evaluate the speed of the sensor.&nbsp;</p>\n\n\n\n<p>A general rule of thumb to determine the resolution needed for the use case is shown below and in Figure 2. \u202fThe multiplier (2) represents the typical desire to have a minimum two pixels on an object in order to successfully detect it.</p>\n\n\n\n<p class=\"has-text-align-center has-medium-font-size\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Resolution+%3D+2%5Ctimes+%5Cfrac%7BField+%5C+of+%5C+View+%28FOV%29%7D%7BSize+%5C+of+%5C+feature+%5C+of+%5C+interest%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"Resolution = 2&#92;times &#92;frac{Field &#92; of &#92; View (FOV)}{Size &#92; of &#92; feature &#92; of &#92; interest}\" class=\"latex\" /></p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1320\" height=\"820\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram.png\" alt=\"Diagram showing the representation of a person and the working distance from an object as an example of minimum object feature size of interest in the field of view. \n\" class=\"wp-image-70354\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram.png 1320w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-625x388.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-768x477.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-645x401.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-483x300.png 483w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-362x225.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-177x110.png 177w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-resolution-diagram-1024x636.png 1024w\" sizes=\"(max-width: 1320px) 100vw, 1320px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Sensor resolution required is determined by lens field of view and feature of interest size</em></figcaption></figure>\n\n\n\n<p>For example, suppose you have an image of an injury around the eye of a boxer.&nbsp;</p>\n\n\n\n<ul>\n<li><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Resolution%3D+2%5Ctimes+%5Cfrac%7B2000%7D%7B4%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"Resolution= 2&#92;times &#92;frac{2000}{4}\" class=\"latex\" /></li>\n\n\n\n<li>FOV, mm = 2000mm&nbsp;</li>\n\n\n\n<li>Size of feature of interest (the eye), mm = 4mm</li>\n</ul>\n\n\n\n<p>Based on the calculation, 1000 x 1000, a one-megapixel camera should be sufficient to detect the eye using a CV or AI algorithm.&nbsp;</p>\n\n\n\n<p>Note that a sensor is made up of multiple rows of pixels. These pixels are also called photosites. The number of photons collected by a pixel is directly proportional to the size of the pixel. Selecting a larger pixel may seem tempting but may not be the optimal choice in all the cases.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td>Small pixel&nbsp;</td><td>Sensitive to noise (-)&nbsp;</td><td>Higher spatial resolution for same sensor size (+)&nbsp;</td></tr><tr><td>Large pixel&nbsp;</td><td>Less sensitive to noise (+)&nbsp;</td><td>Less spatial resolution for same sensor size (-)&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1.&nbsp; Pros and cons of small and large pixel size</em></figcaption></figure>\n\n\n\n<p>Back-illuminated sensors maximize the amount of light being captured and converted by each photodiode. In front-illuminated sensors, metal wiring above the photodiodes blocks off some photons, hence reducing the amount of light captured.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"618\" height=\"290\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section.png\" alt=\"On the left, a diagram of a front-illuminated structure with substrate, photodiodes, metal wiring, and microlenses. On the right, a diagram of a back-illuminated structure with metal wiring, photodiodes, and microlenses.\n\" class=\"wp-image-70368\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section.png 618w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section-500x235.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/sensor-cross-section-234x110.png 234w\" sizes=\"(max-width: 618px) 100vw, 618px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Cross-section of a front-illuminated structure (left) and a back-illuminated structure (right)</em></figcaption></figure>\n\n\n\n<h4 class=\"wp-block-heading\">Frame rate and shutter speed&nbsp;</h4>\n\n\n\n<p>The frame rate\u202frefers to the number of frames (or images captured) per second (FPS). The frame rate should be determined based on the number of inspections required per second. This correlates with the shutter speed (or exposure time), which is the time that the camera sensor is exposed to capture the image.&nbsp;</p>\n\n\n\n<p>Theoretically, the maximum frame rate is equal to the inverse of the exposure time. But achievable FPS is lower because of latency introduced by frame readout, sensor resolution, and the data transfer rate of the interface including cabling.&nbsp;</p>\n\n\n\n<p>FPS can be increased by reducing the need for large exposure times by adding additional lighting, binning the pixels.&nbsp;</p>\n\n\n\n<p>CMOS sensors can achieve higher FPS, as the process of reading out each pixel can be done more quickly than with the charge transfer in a CCD sensor\u2019s shift register.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Interface</h3>\n\n\n\n<p>There are multiple ways to connect the camera module to an embedded system. Typically, for evaluation purposes, cameras with USB and Ethernet interfaces are used because custom driver development is not needed.&nbsp;</p>\n\n\n\n<p>Other important parameters for interface selection are transmission length, data rate, and operating conditions. Table 2 lists the most popular interfaces. Each option has its pros and cons.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Features</strong>&nbsp;</td><td><strong>USB 3.2</strong>&nbsp;</td><td><strong>Ethernet (1 GbE)</strong>&nbsp;</td><td><strong>MIPI CSI-2</strong>&nbsp;</td><td><strong>GMSL2</strong>&nbsp;</td><td><strong>FPDLINK III</strong>&nbsp;</td></tr><tr><td><strong>Bandwidth</strong>&nbsp;</td><td>10Gbps&nbsp;</td><td>1Gbps&nbsp;</td><td>DPHY 2.5 Gbps/lane&nbsp;CPHY 5.71 Gbps/lane&nbsp;</td><td>6Gbps&nbsp;</td><td>4.2Gbps&nbsp;</td></tr><tr><td><strong>Cable length supported</strong>&nbsp;</td><td>&lt; 5m&nbsp;</td><td>Up to 100m&nbsp;</td><td>&lt;30cm&nbsp;</td><td>&lt;15m&nbsp;</td><td>&lt;15m&nbsp;</td></tr><tr><td><strong>Plug-and-play</strong>&nbsp;</td><td>Supported&nbsp;</td><td>Supported&nbsp;</td><td>Not supported&nbsp;</td><td>Not supported&nbsp;</td><td>Not supported&nbsp;</td></tr><tr><td><strong>Development costs</strong>&nbsp;</td><td>Low&nbsp;</td><td>Low&nbsp;</td><td>Medium to high&nbsp;</td><td>Medium to high&nbsp;</td><td>Medium to high&nbsp;</td></tr><tr><td><strong>Operating environment</strong>&nbsp;</td><td>Indoor&nbsp;</td><td>Indoor&nbsp;</td><td>Indoor&nbsp;</td><td>Indoor and outdoor&nbsp;</td><td>Indoor and outdoor&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Comparison of various camera interfac</em>es</figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Optics&nbsp;</h3>\n\n\n\n<p>The basic purpose of an optical lens is to collect the light scattered by an object and recreate an image of the object on a light-sensitive image sensor (CCD or CMOS). The following factors should be considered when selecting an optimized lens-focal length, sensor format, field of view, aperture, chief ray angle, resolving power, and distortion.&nbsp;</p>\n\n\n\n<p>Lenses are manufactured with a limited number of standard focal lengths. Common lens focal lengths include 6mm, 8mm, 12.5mm, 25mm, and 50mm.&nbsp;</p>\n\n\n\n<p>Once you choose a lens with a focal length closest to the focal length required by your imaging system, you need to adjust the working distance to get the object under inspection in focus. Lenses with short focal lengths (less than 12mm) produce images with a significant amount of distortion.&nbsp;</p>\n\n\n\n<p>If your application is sensitive to image distortion, try to increase the working distance and use a lens with a higher focal length. If you cannot change the working distance, you are somewhat limited in choosing an optimized lens.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td>&nbsp;</td><td><strong>Wide-angle lens</strong>&nbsp;</td><td><strong>Normal lens</strong>&nbsp;</td><td><strong>Telephoto lens</strong>&nbsp;</td></tr><tr><td><strong>Focal length</strong>&nbsp;</td><td>&lt;=35mm</td><td>50mm</td><td>&gt;=70mm&nbsp;</td></tr><tr><td><strong>Use case</strong>&nbsp;</td><td>Nearby scenes&nbsp;</td><td>Same as human eye&nbsp;</td><td>Far-away scenes&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. Main types of camera lenses</em></figcaption></figure>\n\n\n\n<p>To attach a lens to a camera requires some type of mounting system. Both mechanical stability (a loose lens will deliver an out-of-focus image) and the distance to the sensor must be defined.&nbsp;</p>\n\n\n\n<p>To ensure compatibility between different lenses and cameras, the following standard lens mounts are defined.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td>&nbsp;</td><td><strong>Most popular</strong></td><td><strong>For industrial applications</strong></td></tr><tr><td><strong>Lens mount</strong></td><td><strong>M12/S mount</strong></td><td><strong>C-mount</strong></td></tr><tr><td><strong>Flange focal length</strong></td><td>Non-standard</td><td>17.526mm</td></tr><tr><td><strong>Threads (per mm)</strong></td><td>0.5&nbsp;</td><td>0.75&nbsp;</td></tr><tr><td><strong>Sensor size accommodated (inches)</strong></td><td>Up to \u2154</td><td>Up to 1</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 4. Common lens mounts used in embedded space</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA camera module partners&nbsp;</h2>\n\n\n\n<p>NVIDIA maintains a rich ecosystem of partnerships with highly competent camera module makers all over the world. See <a href=\"https://developer.nvidia.com/embedded/jetson-partner-supported-cameras?t1_supported-jetson-products=Orin\">Jetson Partner Supported Cameras</a> for details. These partners can help you design imaging systems for your application from concept to production for the <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\">NVIDIA Jetson</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1195\" height=\"735\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries.png\" alt=\"Graphic showing NVIDIA Jetson with camera modules for various use cases and industries. \" class=\"wp-image-70391\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries.png 1195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-300x185.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-625x384.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-768x472.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-645x397.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-488x300.png 488w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-362x223.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson-with-camera-modules-industries-1024x630.png 1024w\" sizes=\"(max-width: 1195px) 100vw, 1195px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. NVIDIA Jetson in combination with camera modules can be used across industries for various needs</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>This post has explained the most important camera characteristics to consider when selecting a camera for an embedded application. Although the selection process may seem daunting, the first step is to understand your key constraints based on design, performance, environment, and cost.&nbsp;</p>\n\n\n\n<p>Once you understand the constraints, then focus on the characteristics most relevant to your use case. For example, if the camera will be deployed away from the compute or in a rugged environment, consider using the GMSL interface. If the camera will be used in low-light conditions, consider a camera module with larger pixel and sensor sizes. If the camera will be used in a motion application, consider using a camera with a global shutter.&nbsp;</p>\n\n\n\n<p>To learn more, watch <a href=\"https://www.e-consystems.com/webinars/optimize-your-edge-application-unveiling-the-right-combination-of-jetson-processors-and-cameras.asp\">Optimize Your Edge Application: Unveiling the Right Combination of Jetson Processors and Cameras</a>. For detailed specs on AI performance, GPU, CPU, and more for both Xavier and Orin-based Jetson modules, visit <a href=\"https://developer.nvidia.com/embedded/jetson-modules\">Jetson Modules</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn the most important camera characteristics to consider when selecting a camera for an embedded application like the NVIDIA Jetson.</p>\n", "protected": false}, "author": 1862, "featured_media": 70342, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1259858", "discourse_permalink": "https://forums.developer.nvidia.com/t/selecting-the-right-camera-for-the-nvidia-jetson-and-other-embedded-systems/266170", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63], "tags": [115, 453, 347, 2571], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-jetson.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iis", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70336"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1862"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70336"}], "version-history": [{"count": 59, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70336/revisions"}], "predecessor-version": [{"id": 70727, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70336/revisions/70727"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70342"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70336"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70336"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70336"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70717, "date": "2023-09-11T15:00:00", "date_gmt": "2023-09-11T22:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70717"}, "modified": "2023-11-02T13:05:09", "modified_gmt": "2023-11-02T20:05:09", "slug": "webinar-nvidia-rtx-caustics-branch-of-unreal-engine", "status": "publish", "type": "post", "link": "https://nvda.ws/3rciiVt", "title": {"rendered": "Webinar: NVIDIA RTX Caustics Branch of Unreal Engine"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Explore how ray-traced caustics combined with NVIDIA RTX features can enhance the performance of your games.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Explore how ray-traced caustics combined with NVIDIA RTX features can enhance the performance of your games.</p>\n", "protected": false}, "author": 1480, "featured_media": 70719, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3rciiVt", "_links_to_target": "_blank"}, "categories": [1235], "tags": [453, 582], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/RTX-Caustics-DLSS.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ioB", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70717"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1480"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70717"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70717/revisions"}], "predecessor-version": [{"id": 70722, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70717/revisions/70722"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70719"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70717"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70717"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70717"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 69885, "date": "2023-09-11T09:00:00", "date_gmt": "2023-09-11T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=69885"}, "modified": "2023-10-12T22:56:24", "modified_gmt": "2023-10-13T05:56:24", "slug": "accelerating-vector-search-fine-tuning-gpu-index-algorithms", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/", "title": {"rendered": "Accelerating Vector Search: Fine-Tuning GPU Index Algorithms"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In this post, we dive deeper into each of the GPU-accelerated indexes mentioned in <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/\">part 1</a> and give a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior.</p>\n\n\n\n<p>We then go through a simple end-to-end example to demonstrate RAFT\u2019s Python APIs on a question-and-answer problem with a pretrained large language model and provide a performance comparison of RAFT\u2019s algorithms against HNSW for a few different scenarios involving different numbers of query vectors being passed to the search algorithm concurrently.</p>\n\n\n\n<p>This post provides:</p>\n\n\n\n<ul>\n<li>An overview of vector search index algorithms that can be used with GPUs</li>\n\n\n\n<li>An end-to-end example demonstrating how easy it can be to run vector search on the GPU with Python</li>\n\n\n\n<li>Performance comparison of vector search on the GPU against HNSW, the current state-of-the-art method on the CPU</li>\n</ul>\n\n\n\n<p>The first post in this series introduced vector search indexes, explained the role they play in enabling a widespread range of important applications, and provided a brief overview of vector search on the GPU with the <a href=\"https://github.com/rapidsai/raft\">RAFT</a> library. For more information, see <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/\">Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT</a>.</p>\n\n\n\n<p>The third post in this series focuses on IVF-Flat, an ANN algorithm found in <a href=\"https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/\">RAPIDS RAFT</a>. We discuss how the algorithm works, and demonstrate the usage of both the <a rel=\"noreferrer noopener\" href=\"https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat\" target=\"_blank\">Python</a> and <a rel=\"noreferrer noopener\" href=\"https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/\" target=\"_blank\">C++ APIs</a> in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. For more information, see <a href=\"https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/\">Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Vector search indexes</h2>\n\n\n\n<p>When working with vector search, the vectors are often converted to an indexed format that is optimized for fast lookups. Choosing the right indexing algorithm is important as it can affect both index build and search times. Furthermore, each different index type comes with its own set of knobs for fine-tuning the behavior, trading off index construction time, storage cost, search quality, and search speed.</p>\n\n\n\n<p>When the right indexing algorithm is paired with the correct parameter settings, vector search on the GPU provides both faster build and search times for all levels of recall.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>IVF-Flat</h3>\n\n\n\n<p>As it\u2019s the simplest index type, start with the <a href=\"https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat\">IVF-Flat algorithm</a>. In this algorithm, a set of training vectors are first split into some clusters and then stored in the GPU memory organized by their closest cluster centers. The index-building step is faster than that of other algorithms presented in this post, even at high numbers of clusters.</p>\n\n\n\n<p>To search an IVF-Flat index, the closest clusters to each query vector are selected, and the k-nearest neighbors (k-NN) are computed from each of those closest clusters. Because IVF-Flat stores the vectors in an exact, or <em>flat</em> format, meaning without compression, it has the advantage of computing exact distances within each of the clusters it searches. As we describe later in this post, this provides an advantage that often has a higher recall than IVF-PQ when the same number of closest clusters are searched. IVF-Flat index is a good choice when the full index can fit in GPU memory.</p>\n\n\n\n<p>RAFT\u2019s IVF-Flat index contains a couple of parameters to help trade off the query performance and accuracy: </p>\n\n\n\n<ul>\n<li>When training the index, the <code>n_lists</code> parameter determines the number of clusters to partition the training dataset. </li>\n\n\n\n<li>The search parameter <code>n_probes</code> determines the number of closest clusters to search through to compute the nearest neighbors for a set of query points.</li>\n</ul>\n\n\n\n<p>In general, a smaller number of probes leads to a faster search at the expense of recall. When the number of probes is set to the number of lists, exact results are computed. However, in that case, a call to <a href=\"https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_brute_force/\">RAFT\u2019s brute-force search</a> is more performant.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>IVF-PQ</h3>\n\n\n\n<p>When your dataset becomes too large to fit on the GPU, you gain some mileage by compressing the vectors using the <a href=\"https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-pq\">IVF-PQ index type</a>. Like IVF-Flat, IVF-PQ splits the points into a number of clusters (also specified by a parameter called <code>n_lists</code>) and searches the closest clusters to compute the nearest neighbors (also specified by a parameter called <code>n_probes</code>), but it shrinks the sizes of the vectors using a technique called <a href=\"https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/\">product quantization</a>.</p>\n\n\n\n<p>Compressing the index ultimately allows for more vectors to be stored on the GPU. The amount of compression can be controlled with tuning parameters, which we describe later in this post, but higher levels of compression can provide a faster lookup time at the cost of recall. IVF-PQ is currently RAFT\u2019s most memory-efficient vector index.</p>\n\n\n\n<p>RAFT\u2019s IVF-PQ provides two parameters that control memory usage:</p>\n\n\n\n<ul>\n<li><code>pq_dim</code> sets the target dimensionality of the compressed vector.</li>\n\n\n\n<li><code>pq_bits</code> sets the number of bits for each vector element after compression.</li>\n</ul>\n\n\n\n<p>We recommend setting the former to a multiple of 32 while the latter is limited to a range of 4-8 bits. By default, RAFT selects a dimensionality value that minimizes quantization loss according to <code>pq_bits</code>, but this value can be adjusted to lower the memory footprint for each vector. It is useful to play with these parameters to see which settings work best for you.</p>\n\n\n\n<p>When using large amounts of compression, an additional <a href=\"https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#candidate-refinement\">refinement step</a> can be performed by querying the IVF-PQ index for a larger number of neighbors than needed and computing an exact search over the resulting neighbors to reduce the set down to the final desired number. The refinement step requires the original uncompressed dataset on the host memory.</p>\n\n\n\n<p>For more information about building an IVF-PQ index, with in-depth details and recommendations, see the <a href=\"https://github.com/rapidsai/raft/blob/branch-23.08/notebooks/tutorial_ivf_pq.ipynb\">complete guide to RAFT IVF-PQ</a> notebook on our GitHub repo.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>CAGRA</h3>\n\n\n\n<p><a href=\"https://docs.rapids.ai/api/raft/nightly/pylibraft_api/neighbors/#cagra\">CAGRA</a> is RAFT\u2019s new state-of-the-art ANN index. It is a high-performance, GPU-accelerated, graph-based method that has been specifically optimized for small-batch cases, where each lookup contains only one or a few query vectors. Like other popular graph-based methods, such as hierarchical navigable small-world graphs (HNSW) and SONG, an optimized k-NN graph is built at index training time with various qualities that yield efficient search at reasonable levels of recall.</p>\n\n\n\n<p>CAGRA performs a search by first randomly selecting candidate vertices from the graph and then expanding, or traversing, those vertices to compute distances to their children, storing off the nearest neighbors along the way (Figure 1). Each time it traverses a set of vertices, it has performed one iteration.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks.png\" alt=\"Diagram shows how CAGRA can map subgraphs to separate thread blocks, enabling parallelism even for a single query.\" class=\"wp-image-69911\" width=\"725\" height=\"323\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks.png 966w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-300x134.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-625x278.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-179x80.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-768x342.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-645x287.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-500x223.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-160x71.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-362x161.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/cagra-multiple-thread-blocks-247x110.png 247w\" sizes=\"(max-width: 725px) 100vw, 725px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. CAGRA using multiple thread blocks</em></figcaption></figure></div>\n\n\n<p>In Figure 1, CAGRA is using multiple thread blocks to visit more graph nodes in parallel. This is maximizing GPU utilization for single-query searches.</p>\n\n\n\n<p>Because CAGRA returns the approximate nearest neighbors like the algorithms described earlier, it also provides a few parameters to control the recall and the speed.</p>\n\n\n\n<p>The main parameter that can be adjusted to trade off search speed is <code>itopk_size</code>, which specifies the size of an internal sorted list that stores the nodes that can be explored in the next iteration. Higher values of <code>itopk_size</code> keep a larger search context in memory that improves recall at the cost of more time spent in maintaining the queue.</p>\n\n\n\n<p>The parameter <code>search_width</code> defines the number of the closest parent vertices that are traversed to expand their children in each search iteration.</p>\n\n\n\n<p>Another useful parameter is the number of iterations to perform. The setting is selected automatically by default, but this can be changed to a higher or lower value to trade off recall for a faster search.</p>\n\n\n\n<p>CAGRA\u2019s optimized graph is fixed-degree, which is tuned using the parameter <code>graph_degree</code>. The fixed-degree makes better use of GPU resources by keeping the number of computations uniform when searching the graph. It builds the initial k-NN graph by computing an actual k-NN, for example by using IVF-PQ explained earlier, to compute the nearest neighbors of all the points in the training dataset.</p>\n\n\n\n<p>The number of <em>k</em>-nearest neighbors (<em>k</em>) of this intermediate k-NN graph can be tuned using a parameter called <code>intermediate_graph_degree</code> to trade off the quality of the final searchable CAGRA graph.</p>\n\n\n\n<p>A higher quality graph can be built with a larger <code>intermediate_graph_degree</code> value, which means that the final optimized graph is more likely to find nearest neighbors that yield a high recall. RAFT provides several useful parameters to tune the CAGRA algorithm. For more information, see the <a href=\"https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_cagra/\">CAGRA API documentation</a>.</p>\n\n\n\n<p>Again, this parameter can be used to control how thoroughly the overall space is covered by the search but again this comes at the cost of having to search more to find the nearest neighbors, which reduces the search performance.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Getting started with pylibraft</h2>\n\n\n\n<p><a href=\"https://docs.rapids.ai/api/raft/nightly/pylibraft_api\">Pylibraft</a> is the lightweight Python library of RAFT and enables you to use RAFT\u2019s ANN algorithms for vector search right in Python. Pylibraft can accept any object that supports <code>__cuda_array_interface__</code>, such as a <a href=\"https://pytorch.org/\">Torch</a> or <a href=\"https://cupy.dev/\">CuPy</a> array.</p>\n\n\n\n<p>The following example briefly demonstrates how you can build and query a RAFT CAGRA index with Pylibraft.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nfrom pylibraft.neighbors import cagra\nimport cupy as cp\n\n# On small batch sizes, using &quot;multi_cta&quot; algorithm is efficient\nindex_params = cagra.IndexParams(graph_degree=32)\nsearch_params = cagra.SearchParams(algo=&quot;multi_cta&quot;)\n\ncorpus_embeddings = cp.random.random((1500,96), dtype=cp.float32)\nquery_embeddings = cp.random.random((1,96), dtype=cp.float32)\n\ncagra_index = cagra.build(index_params, corpus_embeddings)\n# Find the 10 closest vectors\nhits = cagra.search(search_params, cagra_index, query_embeddings, k=10)\n</pre></div>\n\n\n<p>With the recent success of LLMs, semantic search is a perfect way to showcase vector similarity search in action using RAFT. In the following example, a <a href=\"https://huggingface.co/docs/transformers/model_doc/distilbert\">DistilBERT</a> transformer model combined with each of the three ANN indexes is used to solve a simple question retrieval problem. The Simple English Wikipedia dataset  is used to answer the user\u2019s search query.</p>\n\n\n\n<p>The language model first transforms the training sentences into vector embeddings that are inserted into a RAFT ANN index. The inference is done by encoding the query and using our trained ANN index to find vectors similar to the encoded query vector. The answer that you return to the user is the nearest article in Simple Wikipedia, which you fetch using the closest vector from the similarity search.</p>\n\n\n\n<p>You can get started with RAFT by using pylibraft and this notebook for a question-retrieval task:</p>\n\n\n\n<p><style>.gist table { margin-bottom: 0; }</style><div style=\"tab-size: 8\" id=\"gist123573514\" class=\"gist\">\n    <div class=\"gist-file\" translate=\"no\">\n      <div class=\"gist-data\">\n        <div class=\"js-gist-file-update-container js-task-list-container file-box\">\n  <div id=\"file-vectorsearch_questionretrieval-ipynb\" class=\"file my-2\">\n    \n    <div itemprop=\"text\" class=\"Box-body p-0 blob-wrapper data type-jupyter-notebook  \">\n\n          <div class=\"render-wrapper \">\n    <div class=\"render-container is-render-pending js-render-target \"\n      data-identity=\"b606acc0-27c3-44a0-ba24-00a0bc7ef6a0\"\n      data-host=\"https://notebooks.githubusercontent.com\"\n      data-type=\"ipynb\">\n      <svg style=\"box-sizing: content-box; color: var(--color-icon-primary);\" width=\"64\" height=\"64\" viewBox=\"0 0 16 16\" fill=\"none\" data-view-component=\"true\" class=\"octospinner mx-auto anim-rotate\">\n  <circle cx=\"8\" cy=\"8\" r=\"7\" stroke=\"currentColor\" stroke-opacity=\"0.25\" stroke-width=\"2\" vector-effect=\"non-scaling-stroke\" fill=\"none\" />\n  <path d=\"M15 8a7.002 7.002 0 00-7-7\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" vector-effect=\"non-scaling-stroke\" />\n</svg>\n      <div class=\"render-viewer-error\">Sorry, something went wrong. <a class=\"Link--inTextBlock\" href=\"https://gist.github.com/lowener/08eef6aca69cae5c2151224c801521b0.json\">Reload?</a></div>\n      <div class=\"render-viewer-fatal\">Sorry, we cannot display this file.</div>\n      <div class=\"render-viewer-invalid\">Sorry, this file is invalid so it cannot be displayed.</div>\n      <iframe\n        class=\"render-viewer \"\n        src=\"https://notebooks.githubusercontent.com/view/ipynb?bypass_fastly=true&amp;color_mode=auto&amp;commit=5b3bde53c2e8fa16cee07f25d0b8b5367a5cdb3c&amp;docs_host=https%3A%2F%2Fdocs.github.com&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f676973742f6c6f77656e65722f30386565663661636136396361653563323135313232346338303135323162302f7261772f356233626465353363326538666131366365653037663235643062386235333637613563646233632f566563746f725365617263685f5175657374696f6e52657472696576616c2e6970796e62&amp;logged_in=false&amp;nwo=lowener%2F08eef6aca69cae5c2151224c801521b0&amp;path=VectorSearch_QuestionRetrieval.ipynb&amp;repository_id=123573514&amp;repository_type=Gist#b606acc0-27c3-44a0-ba24-00a0bc7ef6a0\"\n        sandbox=\"allow-scripts allow-same-origin allow-top-navigation\"\n        title=\"File display\"\n        name=\"b606acc0-27c3-44a0-ba24-00a0bc7ef6a0\"\n      >\n          Viewer requires iframe.\n      </iframe>\n    </div>\n  </div>\n \n    </div>\n\n  </div>\n</div>\n\n      </div>\n      <div class=\"gist-meta\">\n        <a href=\"https://gist.github.com/lowener/08eef6aca69cae5c2151224c801521b0/raw/5b3bde53c2e8fa16cee07f25d0b8b5367a5cdb3c/VectorSearch_QuestionRetrieval.ipynb\" style=\"float:right\" class=\"Link--inTextBlock\">view raw</a>\n        <a href=\"https://gist.github.com/lowener/08eef6aca69cae5c2151224c801521b0#file-vectorsearch_questionretrieval-ipynb\" class=\"Link--inTextBlock\">\n          VectorSearch_QuestionRetrieval.ipynb\n        </a>\n        hosted with &#10084; by <a class=\"Link--inTextBlock\" href=\"https://github.com\">GitHub</a>\n      </div>\n    </div>\n</div>\n</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Benchmarks</h2>\n\n\n\n<p>Using GPU as a hardware accelerator for your vector search application can lead to an increase in performance, and it is best showcased on large datasets. The benchmarks can be fully reproduced by following <a href=\"https://docs.rapids.ai/api/raft/nightly/raft_ann_benchmarks/\">RAFT\u2019s end-to-end benchmark documentation</a>. Our benchmarks consider that the data is already available for computation, which means that data transfer is not taken into consideration, although this should not be a significant difference thanks to the high transfer speed of recent NVIDIA hardware (over 25 GB/s).</p>\n\n\n\n<p>We used the <a href=\"http://big-ann-benchmarks.com/neurips21.html\">DEEP-100M</a> dataset on an H100 GPU to compare RAFT indexes with HNSW running on an Intel Xeon Platinum 8480CL CPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b.png\" alt=\"Bar chart compares the throughput of HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a single query at a time at various levels of recall.\" class=\"wp-image-70427\" width=\"878\" height=\"549\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b.png 1170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-645x404.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-1-b-1024x641.png 1024w\" sizes=\"(max-width: 878px) 100vw, 878px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. Comparing ANN algorithms at various levels of recall and throughput, batch of 1</em></figcaption></figure></div>\n\n\n<p>Figure 2 compares ANN algorithms at various levels of recall and throughput for a single query. At high levels of recall, RAFT\u2019s methods demonstrate higher throughput than other alternative libraries.</p>\n\n\n\n<p>We ran a performance comparison on queries for a single vector at a time, called <em>online search</em>. It&#8217;s one of the main use cases for vector search. RAFT-based indexes provide a higher throughput, measured in queries-per-second (QPS), than other libraries that are using CPU or GPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b.png\" alt=\"Bar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a query of 10 vectors at a time.\" class=\"wp-image-70426\" width=\"878\" height=\"549\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b.png 1170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-645x404.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10-b-1024x641.png 1024w\" sizes=\"(max-width: 878px) 100vw, 878px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Comparing ANN algorithms at various levels of recall and throughput, batch of 10</em></figcaption></figure></div>\n\n\n<p>Figure 3 compares ANN algorithms at various levels of recall and throughput with a batch size of 10 queries. RAFT\u2019s methods demonstrate higher throughput than HNSW for all experiments.</p>\n\n\n\n<p>The benefits of using GPU for vector search applications are most prevalent at higher batch sizes. The performance gap between CPU and GPU is significant and can scale up easily. Figure 3 shows that for a batch size of 10, only RAFT-based indexes are relevant when comparing the number of queries per second. For a batch size of 10k (Figure 4), CAGRA outperforms all other indexes by far.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b.png\" alt=\"Bar chart compares throughput for HNSW, the state-of-the-art on CPU, against RAFT\u2019s ANN algorithms for a query of 10k vectors at a time.\" class=\"wp-image-70424\" width=\"878\" height=\"549\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b.png 1170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-645x404.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ann-throughput-batch-10k-b-1024x641.png 1024w\" sizes=\"(max-width: 878px) 100vw, 878px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 4. Comparing ANN algorithms at various levels of recall and throughput</em></figcaption></figure></div>\n\n\n<p>Figure 4 compares ANN algorithms at various levels of recall and throughput with a batch size of 10K query. RAFT\u2019s methods demonstrate higher throughput than HNSW for all experiments.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Summary</h2>\n\n\n\n<p>Each different vector search index type has benefits and drawbacks which ultimately depend on your needs. This post outlined some of those benefits and drawbacks, providing a brief explanation of how each different algorithm works, along with a few of the most important parameters that can be tuned to trade off storage costs, build times, search quality, and search performance. In all cases, GPUs can improve both index construction and search performance. </p>\n\n\n\n<p>RAPIDS RAFT is fully open source and available on the <a href=\"https://github.com/rapidsai/raft\">/rapidsai/raft</a> GitHub repo. You can get started with RAFT by reading through the <a href=\"https://docs.rapids.ai/api/raft/nightly/\">docs</a>, running the <a href=\"https://docs.rapids.ai/api/raft/nightly/raft_ann_benchmarks/\">reproducible benchmarking</a> suite, or building upon the example vector search <a href=\"https://github.com/rapidsai/raft/tree/branch-23.10/cpp/template\">template project</a>. Also be sure to look for options to enable RAFT indexes in Milvus, Redis, and FAISS. Finally, you can follow us on Twitter at<a href=\"https://twitter.com/rapidsai\"> @rapidsai</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In this post, we dive deeper into each of the GPU-accelerated indexes mentioned in part 1 and give a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. We then go through a simple end-to-end example to demonstrate RAFT\u2019s Python APIs on a question-and-answer problem with &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/\">Continued</a></p>\n", "protected": false}, "author": 1476, "featured_media": 70680, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1258746", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-vector-search-fine-tuning-gpu-index-algorithms/266074", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1050, 1464, 852, 696, 3110, 1968], "tags": [9, 453, 1953, 126, 3496], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-part-2-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ibb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69885"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1476"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=69885"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69885/revisions"}], "predecessor-version": [{"id": 71647, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69885/revisions/71647"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70680"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=69885"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=69885"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=69885"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 69884, "date": "2023-09-11T08:59:00", "date_gmt": "2023-09-11T15:59:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=69884"}, "modified": "2023-12-05T10:56:13", "modified_gmt": "2023-12-05T18:56:13", "slug": "accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/", "title": {"rendered": "Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In the AI landscape of 2023, vector search is one of the hottest topics due to its applications in <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language models</a> (LLM) and <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a>. Semantic vector search enables a broad range of important tasks like detecting fraudulent transactions, recommending products to users, using contextual information to augment full-text searches, and finding actors that pose potential security risks.</p>\n\n\n\n<p>Data volumes continue to soar and traditional methods for comparing items one by one have become computationally infeasible. Vector search methods use approximate lookups, which are more scalable and can handle massive amounts of data more efficiently. As we show in this post, accelerating vector search on the GPU provides not only faster search times, but the index building times can also be substantially faster.</p>\n\n\n\n<p>This post provides:</p>\n\n\n\n<ul>\n<li>An introduction to vector search with a brief review of popular applications</li>\n\n\n\n<li>An overview of the RAFT library for accelerating vector search on the GPU</li>\n\n\n\n<li>Performance comparison of GPU-accelerated vectors search indexes against the state-of-the-art on the CPU</li>\n</ul>\n\n\n\n<p>The second post in this series dives deeper into each of the GPU-accelerated indexes mentioned in this post and gives a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. For more information, see <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/\">Accelerating Vector Search: Fine-Tuning GPU Index Algorithms</a>.</p>\n\n\n\n<p>The third post in this series focuses on <a href=\"https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/\">IVF-Flat</a>, an ANN algorithm found in <a href=\"https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/\">RAPIDS RAFT</a>. We discuss how the algorithm works, and demonstrate the usage of both the <a rel=\"noreferrer noopener\" href=\"https://docs.rapids.ai/api/raft/stable/pylibraft_api/neighbors/#ivf-flat\" target=\"_blank\">Python</a> and <a rel=\"noreferrer noopener\" href=\"https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_ivf_flat/\" target=\"_blank\">C++ APIs</a> in RAFT. We cover setting parameters for index building and give tips on how to configure GPU-accelerated IVF-Flat search. For more information, see <a href=\"https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/\">Accelerated Vector Search: Approximating with RAPIDS RAFT IVF-Flat</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>What is vector search?</h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list.png\" alt=\"Diagram shows a list of vectors that may have been encoded from sources like images, documents, or videos and a query vector for which you would like to find the closest vectors from the list.\" class=\"wp-image-69902\" width=\"647\" height=\"356\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list.png 862w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-300x165.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-625x344.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-768x423.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-645x355.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-500x276.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-362x199.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/vector-search-list-200x110.png 200w\" sizes=\"(max-width: 647px) 100vw, 647px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Vector search process</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows that vector search entails creating an index of vectors and performing lookups to find some number of vectors in the index that are closest to a query vector. The vectors could be as small as three-dimensional points from a lidar point cloud or larger embeddings from text documents, images, or videos.</p>\n\n\n\n<p>Vector search is the process of querying a database to find the most similar vectors. This similarity search is done on numerical vectors that can represent any type of object (Figure 2). These vectors are often embeddings created from multimedia like images, video, and text fragments or entire documents that went through a deep learning model to encode their semantic characteristics into a vector form.</p>\n\n\n\n<p>Embedding vectors typically have the advantage of being a smaller object than the original document (lower dimensionality), while maintaining as much information about the source as possible. Therefore, two documents that are similar often have similar embeddings.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c.jpg\" alt=\"Image of a 3D point cloud such as one created from LIDAR.\" class=\"wp-image-70685\" width=\"371\" height=\"399\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c.jpg 741w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-279x300.jpg 279w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-625x672.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-107x115.jpg 107w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-645x694.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-84x90.jpg 84w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-362x389.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/rapids-blog-images-vector-search-c-102x110.jpg 102w\" sizes=\"(max-width: 371px) 100vw, 371px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Vectors represent data points in higher dimensions</em></figcaption></figure></div>\n\n\n<p>The points in Figure 2 are 3D but they could be 500 dimensions or even higher.</p>\n\n\n\n<p>This makes it easier to compare objects, as the embedding vectors are smaller and retain most of the information. When two documents share similar characteristics, their embedding vectors are often spatially close, or similar.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>Approximate methods for vector search</h3>\n\n\n\n<p>To handle larger datasets efficiently, approximate nearest neighbor (ANN) methods are often used for vector search. ANN methods speed up the search by approximating the closest vectors. This avoids the exhaustive distance computation often required by an exact brute-force approach, which requires comparing the query against every single vector in the database.</p>\n\n\n\n<p>In addition to the search compute cost, storing many vectors can also consume a large amount of memory. To ensure both fast searches and low memory usage, you must index vectors in an efficient way. As we outline a bit later, this can sometimes benefit from compression. A vector index is a space-efficient data structure built on mathematical models that is used for efficiently querying several vectors at a time.</p>\n\n\n\n<p>Updating the indexes, such as from inserting and deleting vectors, can cause problems when indexes take hours or even days to build. It turns out that these indexes can often be built much faster on the GPU. We showcase this performance later in the post.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Vector search in LLMs</h2>\n\n\n\n<p>LLMs have become popular for capturing and preserving the semantic meaning and context of the original documents. This means that the vectors resulting from LLM models can be searched using <em>vector similarity search</em>. This search finds items that happen to contain similar words, shapes, or moving objects. It also finds vectors that contextually and semantically mean similar things.</p>\n\n\n\n<p>This semantic search doesn\u2019t rely on exact word matching. For example, searching for the term, &nbsp;\u201cI would like to buy a muscle car\u201d in an image database should be able to contextualize the sentence to understand the following:</p>\n\n\n\n<ul>\n<li>Buying a car is different from renting a car, so you\u2019d expect to find vectors closer to car dealerships and reviews from car purchasers, rather than car rental companies.</li>\n\n\n\n<li>A muscle car is different from a bodybuilder so you\u2019d expect to find vectors about Dodge Chargers and not Arnold Schwarzenegger.</li>\n\n\n\n<li>Buying a muscle car is different from buying muscle relaxers or economy vehicles.</li>\n</ul>\n\n\n\n<p>More recently, large language transformer-based models like ChatGPT, LLaMa, <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NeMo</a>, and BERT have provided significant technical leaps that are increasing the contextual awareness of the models and making them even more useful and applicable to more industries. </p>\n\n\n\n<p>In addition to creating embedding vectors that can be stored and later searched, these new LLM models use semantic search in pipelines that generate new content from context gleaned by finding similar vectors. This content generation process, shown in Figure 3, is known as <em>retrieval-augmented generative AI.</em></p>\n\n\n\n<h3 class=\"wp-block-heading\">Using vector search in a vector database</h3>\n\n\n\n<p>A vector database stores high-dimensional vectors (for example, embeddings), and facilitates fast and accurate search and retrieval based on vector similarity (for example, ANN algorithms). Some databases are purpose-built for vector search (for example, Milvus). Other databases include vector search capabilities as an additional feature (for example, Redis).</p>\n\n\n\n<p>Choosing which vector database to use depends on the requirements of your workflow.</p>\n\n\n\n<p>Retrieval-augmented language models allow pretrained models to be customized for specific products, services, or other domain-specific use cases by augmenting a search with additional context that has been encoded into vectors by the LLM and stored in a vector database.</p>\n\n\n\n<p>More specifically, a search is encoded into vector form and similar vectors are found in the vector database to augment the search. The vectors are then used with the LLM to formulate an appropriate response. Retrieval-augmented LLMs are a form of generative AI and they have revolutionized the industry of chatbots and semantic text search.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED.png\" alt=\"Workflow diagram shows how vector search is often combined with LLMs to perform semantic search. \" class=\"wp-image-70681\" width=\"617\" height=\"388\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED.png 822w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-300x189.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-625x393.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-768x483.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-645x406.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-477x300.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-362x228.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/text-retrieval-application-workflow-CORRECTED-175x110.png 175w\" sizes=\"(max-width: 617px) 100vw, 617px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Example workflow of a text retrieval application using RAPIDS RAFT for vector search</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\"><a></a><a></a>Other applications of vector similarity search</h3>\n\n\n\n<p>In addition to retrieval-augmented LLMs for generative AI, vector embeddings have been around for some time and have found many useful applications in the real world:</p>\n\n\n\n<ul>\n<li><strong>Recommender systems:</strong> Provide personalized suggestions according to what a user has shown interest in or interacted with.</li>\n\n\n\n<li><strong>Finance:</strong> Fraud detection models vectorize user transactions, making it possible to determine whether those transactions are similar to typical fraudulent activities.</li>\n\n\n\n<li><strong>Cybersecurity:</strong> Uses embeddings to model and search behaviors of bad actors and anomalous activities.</li>\n\n\n\n<li><strong>Genomics:</strong> Finds similar genes and cell structures in genomics analysis, such as single-cell RNA analysis.</li>\n\n\n\n<li><strong>Chemistry:</strong> Models molecular descriptors or fingerprints of chemical structures to compare them or find similar structures in a database.</li>\n</ul>\n\n\n\n<p>We are always interested in learning about your use cases so don\u2019t hesitate to leave a comment if you either use vector search already or would like to discuss how it could benefit your application.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>RAPIDS RAFT library for vector search</h2>\n\n\n\n<p>RAFT is a library of composable building blocks for accelerating machine learning algorithms on the GPU, such as those used in nearest neighbors and vector search. ANN algorithms are among the core building blocks that comprise vector search libraries. Most importantly, these algorithms can greatly benefit from GPU acceleration.</p>\n\n\n\n<p>For more information about RAFT\u2019s core APIs and the various accelerated building blocks that it contains, see <a href=\"https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/\">Reusable Computational Patterns for Machine Learning and Data Analytics with RAPIDS RAFT</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>ANN for fast searches</h2>\n\n\n\n<p>In addition to <a href=\"https://docs.rapids.ai/api/raft/stable/cpp_api/neighbors_brute_force/\">brute-force for exact search</a>, RAFT currently provides three different algorithms for ANN search:</p>\n\n\n\n<ul>\n<li>IVF-Flat</li>\n\n\n\n<li>IVF-PQ</li>\n\n\n\n<li>CAGRA</li>\n</ul>\n\n\n\n<p>The choice of the algorithm can depend upon your needs, as they each offer different advantages. Sometimes, brute force can even be the better option. More are being added in upcoming releases.</p>\n\n\n\n<p>Because these algorithms are not doing an exact search, it is possible that some highly similar vectors are missed. The <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">recall</a> metric can be used to represent how many neighbors in the results are actual nearest neighbors of the query. Most of our benchmarks target recall levels of 85% and higher, meaning 85% (or more) of the relevant vectors were retrieved. </p>\n\n\n\n<p>To tune the resulting indexes for different levels of recall, use various settings, or hyperparameters, when training approximate nearest-neighbors algorithms. Reducing the recall score often increases the speed of your searches and increasing the recall decreases the speed. This is known as the recall-speed tradeoff.</p>\n\n\n\n<p>For more information, see <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/\">Accelerating Vector Search: Fine-Tuning GPU Index Algorithms</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>Performance comparison</h3>\n\n\n\n<p>GPUs excel at processing a lot of data at one time. All the algorithms just mentioned can outperform corresponding algorithms on the CPU when computing the nearest neighbors for thousands or tens of thousands of points at a time.</p>\n\n\n\n<p>However, CAGRA was specifically engineered with online search in mind, which means that it outperforms the CPU even when only querying the nearest neighbors for a few data points at a time.</p>\n\n\n\n<p>Figure 4 and Figure 5 show benchmarks that we performed by building an index on 100M vectors and querying only 10 vectors at a time. In Figure 4, CAGRA outperforms HNSW, which is one of the most popular indexes for vector search on CPU, in raw search performance even for an extremely small batch size of 10 vectors. This speed comes at a memory cost, however. In Figure 5, you can see that CAGRA\u2019s memory footprint is a bit higher than the other nearest neighbors methods.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b.png\"><img decoding=\"async\" loading=\"lazy\" width=\"802\" height=\"502\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b.png\" alt=\"Bar chart compares throughput performance (queries per second) for RAFT\u2019s GPU algorithms against HNSW on the CPU.\" class=\"wp-image-70696\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b.png 802w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-768x481.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-645x404.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-479x300.png 479w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-362x227.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-throughput-deep100m-b-176x110.png 176w\" sizes=\"(max-width: 802px) 100vw, 802px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 4. Vector search throughput at 95% recall on DEEP-100M dataset, batch size of 10</em></figcaption></figure></div>\n\n\n<p>In Figure 5, the host memory of IVF-PQ is for the optional refinement step.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage.png\" alt=\"Bar chart compares memory usage for RAFT\u2019s GPU algorithms against HNSW on the CPU.\" class=\"wp-image-69907\" width=\"900\" height=\"557\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/gpu-memory-usage-1024x633.png 1024w\" sizes=\"(max-width: 900px) 100vw, 900px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. GPU memory usage</em></figcaption></figure></div>\n\n\n<p>Figure 6 presents a comparison of the index build times and shows that indexes can often be built faster on the GPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b.png\"><img decoding=\"async\" loading=\"lazy\" width=\"725\" height=\"448\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b.png\" alt=\"Bar chart compares index build time for RAFT\u2019s GPU algorithms against HNSW on the CPU.\" class=\"wp-image-70698\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b.png 725w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-300x185.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/index-build-time-b-178x110.png 178w\" sizes=\"(max-width: 725px) 100vw, 725px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 6. Index build time for the best-performing index at 95% recall</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Summary</h2>\n\n\n\n<p>From feature stores to generative AI, vector similarity search can be applied in every industry. Vector search on the GPU performs at lower latency and achieves higher throughput for every level of recall for both online and batch processing.</p>\n\n\n\n<p>RAFT is a set of composable building blocks that can be used to accelerate vector search in any data source. It has pre-built APIs for Python and C++. Integration for RAFT is underway for Milvus, Redis, and FAISS. We encourage database providers to try RAFT and consider integrating it into their data sources.</p>\n\n\n\n<p>In addition to state-of-the-art ANN algorithms, RAFT contains other GPU-accelerated building blocks, such as matrix and vector operations, iterative solvers, and clustering algorithms. The second post in this series dives deeper into each of the GPU-accelerated indexes mentioned in this post and gives a brief explanation of how the algorithms work, along with a summary of important parameters to fine-tune their behavior. For more information, see <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/\">Accelerating Vector Search: Fine-Tuning GPU Index Algorithms</a>. </p>\n\n\n\n<p>RAPIDS RAFT is fully open source and available on the <a href=\"https://github.com/rapidsai/raft\">/rapidsai/raft</a> GitHub repo. You can also follow us on Twitter at <a href=\"https://twitter.com/rapidsai\">@rapidsai</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the AI landscape of 2023, vector search is one of the hottest topics due to its applications in large language models (LLM) and generative AI. Semantic vector search enables a broad range of important tasks like detecting fraudulent transactions, recommending products to users, using contextual information to augment full-text searches, and finding actors that &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/\">Continued</a></p>\n", "protected": false}, "author": 1476, "featured_media": 70679, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1258745", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/266073", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1050, 1464, 852, 696, 3110, 1968], "tags": [9, 453, 3550, 1953, 126, 3496], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/vector-search-part-1-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iba", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69884"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1476"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=69884"}], "version-history": [{"count": 17, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69884/revisions"}], "predecessor-version": [{"id": 71506, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69884/revisions/71506"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70679"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=69884"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=69884"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=69884"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70291, "date": "2023-09-11T08:57:41", "date_gmt": "2023-09-11T15:57:41", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70291"}, "modified": "2023-10-25T16:51:22", "modified_gmt": "2023-10-25T23:51:22", "slug": "creating-immersive-events-with-openusd-and-digital-twins", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/creating-immersive-events-with-openusd-and-digital-twins/", "title": {"rendered": "Creating Immersive Events with OpenUSD and Digital Twins"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a rel=\"noreferrer noopener\" href=\"https://momentfactory.com/home\" target=\"_blank\">Moment Factory</a> is a global multimedia entertainment studio that combines specializations in video, lighting, architecture, sound, software, and interactivity to create immersive experiences for audiences around the world.&nbsp;</p>\n\n\n\n<p>From live performances and multimedia shows to interactive installations, Moment Factory is known for some of the most awe-inspiring and entertaining experiences that bring people together in the real world. These include dazzling visuals at <a rel=\"noreferrer noopener\" href=\"https://momentfactory.com/work/all/all/billie-eilish-happier-than-ever-world-tour\" target=\"_blank\">Billie Eilish\u2019s <em>Happier Than Ever</em> world tour</a>, <a rel=\"noreferrer noopener\" href=\"https://momentfactory.com/lumina/en/\" target=\"_blank\">Lumina Night Walks</a> at natural sites around the world, and digital placemaking at the <a rel=\"noreferrer noopener\" href=\"https://momentfactory.com/work/destinations/public-spaces/att-discovery-district\" target=\"_blank\">AT&amp;T Discovery District</a>.</p>\n\n\n\n<p>With a team of over 400 professionals and offices in Montreal, Tokyo, Paris, New York City, and Singapore, Moment Factory has become a global leader in the entertainment industry.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"960\" height=\"768\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory.jpg\" alt=\"Billie Eilish on stage during her Happier Than Ever world tour\n\" class=\"wp-image-70298\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory.jpg 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-300x240.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-625x500.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-144x115.jpg 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-768x614.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-645x516.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-375x300.jpg 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-113x90.jpg 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-362x290.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/billie-eilish-on-stage-moment-factory-138x110.jpg 138w\" sizes=\"(max-width: 960px) 100vw, 960px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Billie Eilish engaged Moment Factory to oversee creative direction, stage design, and content creation for her </em>Happier Than Ever<em><em> </em>world tour</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Streamlining immersive experience development with OpenUSD</h2>\n\n\n\n<p>Bringing these experiences to life requires large teams of highly skilled experts with diverse specialties, all using unique tools. To achieve optimal efficiency in their highly complex production processes, Moment Factory looked to implement an interoperable open data format and development platform that could seamlessly integrate all aspects, from concept to operation.</p>\n\n\n\n<p>Moment Factory chose <a href=\"https://developer.nvidia.com/usd\" target=\"_blank\" rel=\"noreferrer noopener\">Universal Scene Description, also known as OpenUSD</a>, as the solution. OpenUSD is an extensible framework and ecosystem for describing, composing, simulating, and collaborating within 3D worlds. <a href=\"https://developer.nvidia.com/omniverse\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Omniverse</a> is a software platform that enables teams to develop OpenUSD-based 3D workflows and applications. It provides the unified environment to visualize and collaborate on digital twins in real time with live connections to Moment Factory\u2019s tools.</p>\n\n\n\n<p>Using OpenUSD with Omniverse enables Moment Factory to unify data from their diverse digital content creation (DCC) tools to form a <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/\" target=\"_blank\">digital twin</a> of a real-world environment. Every member of the team can interact with this digital twin and iterate on their aspect of the project without affecting other elements</p>\n\n\n\n<p>For example, a scenographer can work on a base set and unique scene pieces using <a rel=\"noreferrer noopener\" href=\"https://www.vectorworks.net/en-US\" target=\"_blank\">Vectorworks</a>, 3D design software. At the same time in the same scene, an AV (audio visual) and lighting designer can take care of lighting and projectors with Moment Factory\u2019s proprietary live entertainment operating system and virtual projection mapping software, <a href=\"https://momentfactory.com/about/x-agora-multimedia-software\" target=\"_blank\" rel=\"noreferrer noopener\">X-Agora</a>.</p>\n\n\n\n<p>Simultaneously, artists and designers can render and create eye-catching visuals in the scene using tools like Epic Games Unreal Engine, Blender, and Adobe Photoshop\u2014without affecting layers of the project still in progress.</p>\n\n\n\n<p>\u201cUSD is unique in that it can be fragmented into smaller pieces that enable people to work on their own unique parts of a project while staying connected,\u201d said Arnaud Grosjean, solution architect and project lead for Moment Factory\u2019s Innovation Team. \u201cIts flexibility and interoperability allows us to create powerful, custom 3D pipelines.\u201d</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"982\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic.png\" alt=\"Diagram of USD scenes composition, including nondestructive layers such as venue, scenography, AV, and sensor data from diverse data sources.\" class=\"wp-image-70303\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-768x377.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-1536x755.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-645x317.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-500x246.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-224x110.png 224w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/usd-scene-composition-graphic-1024x503.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. USD scenes are composed of nondestructive layers such as venue, scenography, AV, and sensor data from diverse data sources</em></em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Digital twins simulate real-world experiences&nbsp;</h2>\n\n\n\n<p>To simulate immersive events before deploying them in the real world, Moment Factory is developing digital twins of their installations in <a href=\"https://www.nvidia.com/en-us/omniverse/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Omniverse</a>. Omniverse, a computing platform that enables teams to develop OpenUSD-based 3D workflows and applications, provides the unified environment to visualize and collaborate on digital twins in real time with live <a href=\"https://www.nvidia.com/en-us/omniverse/ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\">connections to DCC tools</a>.</p>\n\n\n\n<p>The first digital twin they\u2019ve created is that of Blackbox, which serves as an experimentation and prototyping space where they can preview fragments of immersive experiences before real-world deployment. It is a critical space for nearly every phase of the project lifecycle, from conception and design to integration and operation.&nbsp;</p>\n\n\n\n<p>To build the digital twin of the Blackbox, Moment Factory used <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/omniverse/apps/create/\" target=\"_blank\">USD Composer</a>, a fully customizable foundation application built on NVIDIA Omniverse.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"2560\" height=\"1440\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2.png\" alt=\"Moment Factory digital twin\" class=\"wp-image-70664\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2.png 2560w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-2048x1152.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/bbxVStwin-cleaned-2-1024x576.png 1024w\" sizes=\"(max-width: 2560px) 100vw, 2560px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Live video projection and lighting in the Blackbox is reflected in real time in the digital twin of the Blackbox, shown in the screen on the right</em></figcaption></figure></div>\n\n\n<p>The virtual replica of the installation enables the team to run innumerable iterations on the project to test for various factors. They can also better sell concepts for immersive experiences to prospective customers, who can see the show before live production in a virtual environment.</p>\n\n\n\n<p>One of the key challenges in the process for building large-scale immersive experiences is reaching a consensus among various stakeholders and managing changes.&nbsp;</p>\n\n\n\n<p>\u201cEveryone has their own idea of how a scene should be structured, so we needed a way to align everyone contributing to the project in a unified, dynamic environment\u201d explained Grosjean. \u201cWith the digital twin, potential ideas can be tested and simulated with stakeholders across every core expertise.\u201d</p>\n\n\n\n<p>As CAD drafters, AV designers, interactive designers, and others contribute to the digital twin of the Blackbox, artists and 2D/3D designers can render and experiment with beauty shots of the immersive experience in action.</p>\n\n\n\n<p>To see the digital twin of the Blackbox in action, join the <a href=\"https://www.addevent.com/event/qR18485194\">Omniverse Livestream with Moment Factory on Wednesday, September 13</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Developing Omniverse Connectors and extensions</h2>\n\n\n\n<p>Moment Factory is continuously building and testing extensions for Omniverse to bring new functionalities and possibilities into their digital twins.</p>\n\n\n\n<p>They developed an Omniverse Connector for X-Agora, their proprietary multi-display software that allows you to design, plan and operate shows. The software now has a working implementation of a Nucleus connection, USD import/export, and an early live mode implementation.</p>\n\n\n\n<p>Video projection is a key element of immersive events. The team will often experiment with mapping and projecting visual content onto architectural surfaces, scenic elements, and sometimes even moving objects, transforming static spaces into dynamic and captivating environments.</p>\n\n\n\n<p><a href=\"https://ndi.video/\" target=\"_blank\" rel=\"noreferrer noopener\">NDI</a>, which stands for Network Design Interface, is a popular IP video protocol developed by NewTek that allows for efficient live video production and streaming across interconnected devices and systems. In their immersive experiences, Moment Factory typically connects a media system to physical projectors using video cables. With NDI, they can replicate this connection within a virtual venue, effectively simulating the entire experience digitally.&nbsp;</p>\n\n\n\n<p>To enable seamless connectivity between the Omniverse RTX Renderer and their creative content, Moment Factory developed an NDI extension for Omniverse. The extension supports more than just video projection and allows the team to simulate LED walls, screens, and pixel fields to mirror their real-world setup in the digital twin.</p>\n\n\n\n<p>The extension, which was developed with <a href=\"https://docs.omniverse.nvidia.com/dev-guide/latest/kit-architecture.html\" target=\"_blank\" rel=\"noreferrer noopener\">Omniverse Kit</a>, also enables users to use video feeds as dynamic textures. Developers at Moment Factory used the<a href=\"https://github.com/jshrake-nvidia/kit-cv-video-example\" target=\"_blank\" rel=\"noreferrer noopener\"> kit-cv-video-example</a> and <a href=\"https://github.com/jshrake-nvidia/kit-dynamic-texture-example\" target=\"_blank\" rel=\"noreferrer noopener\">kit-dynamic texture-example</a> to develop the extension.</p>\n\n\n\n<p>Anyone can access and use Moment Factory&#8217;s <a rel=\"noreferrer noopener\" href=\"https://github.com/MomentFactory/Omniverse-NDI-extension\" target=\"_blank\">Omniverse-NDI-extension</a> on GitHub, and install it on the Omniverse Launcher or launch with:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ ./link_app.bat --app create\n$ ./app/omni.create.bat --/rtx/ecoMode/enabled=false --ext-folder exts --enable mf.ov.ndi\n</code></pre>\n\n\n\n<p><a href=\"https://docs.omniverse.nvidia.com/dev-guide/latest/extensions.html\" target=\"_blank\" rel=\"noreferrer noopener\">Extensions in Omniverse</a> serve as reusable components or tools that developers can build to accelerate and add new functionalities for 3D workflows. They can be built for simple tasks like randomizing objects or used to enable more complex workflows like visual scripting.</p>\n\n\n\n<p>The team also developed an extension for converting <a href=\"https://vesa.org/featured-articles/vesa-completes-specifications-for-new-multiple-projector-common-data-interchange-standard-mpcdi/\" target=\"_blank\" rel=\"noreferrer noopener\">MPDCI</a>, a VESA standard describing multiprojector rigs, to USD called the <a href=\"https://github.com/MomentFactory/Omniverse-MPCDI-converter\">Omniverse-</a><a href=\"https://github.com/MomentFactory/Omniverse-MPCDI-converter\" target=\"_blank\" rel=\"noreferrer noopener\">MPCDI</a><a href=\"https://github.com/MomentFactory/Omniverse-MPCDI-converter\">-converter</a>. They are currently testing extensions for <a href=\"https://gdtf-share.com/help/en/help/mvr/index.html\" target=\"_blank\" rel=\"noreferrer noopener\">MVR (My Virtual Rig)</a> and <a href=\"https://gdtf-share.com/\" target=\"_blank\" rel=\"noreferrer noopener\">GDTF (General Device Type Format) </a>Converters to import lighting fixtures and rigs into their digital twins.&nbsp;</p>\n\n\n\n<p>Even more compelling is a lidar UDP simulator extension, which is being developed to enable sensor simulation in Omniverse and connect synthetic data to lidar-compatible software.</p>\n\n\n\n<p>You can use Moment Factory\u2019s NDI and MPDCI extensions today in your workflows. Stay tuned for new extensions coming soon.</p>\n\n\n\n<p>To build extensions like Moment Factory, <a href=\"https://developer.nvidia.com/omniverse/get-started\" target=\"_blank\" rel=\"noreferrer noopener\">get started with all the Omniverse Developer Resources</a> you\u2019ll need, like&nbsp; documentation, tutorials, <a href=\"https://developer.nvidia.com/usd\" target=\"_blank\" rel=\"noreferrer noopener\">USD resources</a>, GitHub samples, and more.</p>\n\n\n\n<p><em>Get started with NVIDIA Omniverse by downloading the standard license </em><a href=\"https://www.nvidia.com/en-us/omniverse/download/\"><em>free</em></a><em>, or learn how </em><a href=\"https://www.nvidia.com/en-us/omniverse/enterprise/\"><em>Omniverse Enterprise</em><em> can connect your team</em></a><em>.&nbsp;</em></p>\n\n\n\n<p><em>Developers can check out these </em><a href=\"https://developer.nvidia.com/omniverse/get-started/\"><em>Omniverse resources</em></a><em> to begin building on the platform.&nbsp;</em></p>\n\n\n\n<p><em>Stay up to date on the platform by subscribing to the </em><a href=\"https://nvda.ws/3u5KPv1\"><em>newsletter</em></a><em> and following NVIDIA Omniverse on </em><a href=\"https://www.instagram.com/nvidiaomniverse/\"><em>Instagram</em></a><em>, </em><a href=\"https://www.linkedin.com/showcase/nvidia-omniverse\"><em>LinkedIn</em></a><em>, </em><a href=\"https://medium.com/@nvidiaomniverse\"><em>Medium</em></a><em>, </em><a href=\"https://www.threads.net/@nvidiaomniverse\"><em>Threads</em></a><em>, and </em><a href=\"https://twitter.com/nvidiaomniverse\"><em>Twitter</em></a><em>.</em></p>\n\n\n\n<p><em>For more, check out our </em><a href=\"https://forums.developer.nvidia.com/c/omniverse/300\"><em>forums</em></a><em>, </em><a href=\"https://discord.com/invite/XWQNJDNuaC\"><em>Discord server</em></a><em>, </em><a href=\"https://www.twitch.tv/nvidiaomniverse\"><em>Twitch</em></a><em>, and </em><a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\"><em>YouTube</em></a><em> channels.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Moment Factory is a multimedia studio that combines specializations in video, lighting, architecture, sound, software, and interactivity to create immersive experiences for audiences around the world.</p>\n", "protected": false}, "author": 1868, "featured_media": 70653, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1258743", "discourse_permalink": "https://forums.developer.nvidia.com/t/creating-immersive-events-with-openusd-and-digital-twins/266071", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 503], "tags": [2375, 453, 1956, 1409, 3096], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Moment-Factory-Tech-Blog-Feature-Image_16x91.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ihJ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70291"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1868"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70291"}], "version-history": [{"count": 17, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70291/revisions"}], "predecessor-version": [{"id": 70683, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70291/revisions/70683"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70653"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70291"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70291"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70291"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70549, "date": "2023-09-09T10:00:00", "date_gmt": "2023-09-09T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70549"}, "modified": "2023-11-07T14:27:14", "modified_gmt": "2023-11-07T22:27:14", "slug": "nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/", "title": {"rendered": "NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Large language models (LLMs) offer incredible new capabilities, expanding the frontier of what is possible with AI. However, their large size and unique execution characteristics can make them difficult to use in cost-effective ways.&nbsp;</p>\n\n\n\n<p>NVIDIA has been working closely with leading companies, including <a href=\"https://ai.meta.com/llama/\">Meta</a>, <a href=\"https://www.anyscale.com/\">Anyscale</a>, <a href=\"https://cohere.com/\">Cohere</a>, <a href=\"https://deci.ai/\">Deci</a>, <a href=\"https://www.grammarly.com/\">Grammarly</a>, <a href=\"https://mistral.ai/\">Mistral AI</a>, <a href=\"https://www.mosaicml.com/\">MosaicML</a> (now a part of <a href=\"https://www.databricks.com/blog/databricks-mosaicml\">Databricks</a>), <a href=\"https://octoml.ai/\">OctoML</a>, <a href=\"https://www.perplexity.ai/\">Perplexity</a>, <a href=\"https://www.tabnine.com/\">Tabnine</a>, and <a href=\"https://together.ai/\">Together AI</a>, to accelerate and optimize LLM inference.\u00a0</p>\n\n\n\n<div class='stb-container stb-style-info stb-no-caption'><div class='stb-caption'><div class='stb-logo'><img class='stb-logo__image' src='data:image/png;base64,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' alt='img'/></div><div class='stb-caption-content'></div><div class='stb-tool'></div></div><div class='stb-content'>As of October 19, 2023, NVIDIA TensorRT-LLM is now public and free to use for all as an open-source library on the <a style=\"color: #0000ff;\" href=\"https://github.com/NVIDIA/TensorRT-LLM\">/NVIDIA/TensorRT-LLM</a> GitHub repo and as part of the <a style=\"color: #0000ff;\" href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo framework</a>.</div></div>\n\n\n\n<p>Those innovations have been integrated into the open-source <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/tensorrt-llm-early-access\" target=\"_blank\">NVIDIA TensorRT-LLM</a> software, available for NVIDIA Ampere, NVIDIA Lovelace, and NVIDIA Hopper GPUs. TensorRT-LLM consists of the TensorRT deep learning compiler and includes optimized kernels, pre\u2013 and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs. It enables you to experiment with new LLMs, offering peak performance and quick customization capabilities, without requiring a deep knowledge of C++ or NVIDIA CUDA.</p>\n\n\n\n<p>TensorRT-LLM improves ease of use and extensibility through an open-source modular Python API for defining, optimizing, and executing new architectures and enhancements as LLMs evolve, and can be customized easily.&nbsp;&nbsp;</p>\n\n\n\n<p>For example, MosaicML has seamlessly added specific features that it needs on top of TensorRT-LLM and integrated them into inference serving. Naveen Rao, vice president of engineering at Databricks, said, \u201cIt has been an absolute breeze.\u201d</p>\n\n\n\n<p>\u201cTensorRT-LLM is easy to use, feature-packed with streaming of tokens, in-flight batching, <a href=\"https://vllm.ai/\" target=\"_blank\" rel=\"noreferrer noopener\">paged-attention</a>, quantization, and more, and is efficient,&#8221; Rao said. \u201cIt delivers state-of-the-art performance for LLM serving using NVIDIA GPUs and allows us to pass on the cost savings to our customers.\u201d</p>\n\n\n\n<h2 class=\"wp-block-heading\">Performance comparison</h2>\n\n\n\n<p>Summarizing articles is just one of the many applications of <a href=\"https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\">LLMs</a>. The following benchmarks show performance improvements brought by TensorRT-LLM on the latest NVIDIA Hopper architecture.&nbsp;</p>\n\n\n\n<p>The following figures reflect article summarization using an NVIDIA A100 and NVIDIA H100 GPUs with CNN/Daily Mail, a well-known dataset for evaluating summarization performance.&nbsp;&nbsp;</p>\n\n\n\n<p>In Figure 1, the NVIDIA H100 GPU alone is 4x faster than the A100 GPU. Adding TensorRT-LLM and its benefits, including in-flight batching, results in an 8x total increase to deliver the highest throughput.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"832\" height=\"666\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance.png\" alt=\"GPT-J performance comparison between A100 and H100 with and without TensorRT-LLM.\n\" class=\"wp-image-70666\" style=\"width:624px;height:500px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance.png 832w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-768x615.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/8X-Inference-Performance-137x110.png 137w\" sizes=\"(max-width: 832px) 100vw, 832px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. GPT-J-6B&nbsp; A100 compared to H100 with and without TensorRT-LLM</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\">Text summarization, variable I/O length, CNN / DailyMail dataset | A100 FP16 PyTorch eager mode | H100 FP8 | H100 FP8, in-flight batching, TensorRT-LLM</p>\n\n\n\n<p>On Llama 2\u2014a popular language model released recently by Meta and used widely by organizations looking to incorporate generative AI\u2014TensorRT-LLM can accelerate inference performance by 4.6x compared to A100 GPUs.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"832\" height=\"666\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2.png\" alt=\"Llama 2 70B performance comparison between A100 and H100 with and without TensorRT-LLM.\n\" class=\"wp-image-70668\" style=\"width:624px;height:500px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2.png 832w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-768x615.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/4.6X-Performance-Llama2-137x110.png 137w\" sizes=\"(max-width: 832px) 100vw, 832px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Llama 2 70B, A100 compared to H100 with and without TensorRT-LLM</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\">Text summarization, variable I/O length, CNN / DailyMail dataset | A100 FP16 PyTorch eager mode| H100 FP8 | H100 FP8, in-flight batching, TensorRT-LLM</p>\n\n\n\n<h2 class=\"wp-block-heading\">TCO and energy efficiency improvements</h2>\n\n\n\n<p>Minimizing total cost of ownership (TCO) and energy consumption in the data center are key goals for customers adopting AI and LLMs in particular, given their explosive increase in computational requirements. Customers don\u2019t just look at the cost of a single server when it comes to AI platform expenditures. Rather, they have to look at aggregate capital and operational costs:&nbsp;</p>\n\n\n\n<ul>\n<li>Cost of GPU servers</li>\n\n\n\n<li>Management head nodes (CPU servers to coordinate all the GPU servers)</li>\n\n\n\n<li>Networking equipment (fabric, Ethernet, and cabling) </li>\n\n\n\n<li>Storage</li>\n\n\n\n<li>Data center IT staff and software</li>\n\n\n\n<li>Equipment maintenance</li>\n\n\n\n<li>Data center rent and electricity</li>\n</ul>\n\n\n\n<p>Taken at a holistic level of the actual costs incurred by a data center, significant performance speedups reduce the equipment and maintenance requirements, leading to sizable capital and operational expense savings.</p>\n\n\n\n<p>Figure 3 shows that an 8x performance speedup on small language models like GPT-J 6B leads to a 5.3x reduction in TCO and a 5.6x reduction in energy (electricity bill savings) over the A100 GPU baseline.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"769\" height=\"620\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM.png\" alt=\"This bar chart graph shows a GPT-J-6B performance comparison between A100 and H100 TCO and energy benefits.\" class=\"wp-image-70712\" style=\"width:577px;height:465px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM.png 769w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-300x242.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-625x504.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-645x520.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-372x300.png 372w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-362x292.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/A100-compared-to-H100-with-TensorRT-LLM-136x110.png 136w\" sizes=\"(max-width: 769px) 100vw, 769px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. A100 compared to H100 with TensorRT-LLM showing TCO and energy cost benefits</em></figcaption></figure></div>\n\n\n<p>Similarly, on state-of-the-art LLMs like Llama2, even with 70B parameters, you can realize a 4.6x performance speedup, which results in a 3x reduction in TCO and a 3.2x reduction in energy consumed compared to the A100 baseline.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"764\" height=\"616\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits.png\" alt=\"This bar chart graph shows a Llama 2 70B  performance comparison between A100 and H100 TCO and energy benefits\" class=\"wp-image-70713\" style=\"width:573px;height:462px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits.png 764w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-300x242.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-625x504.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-645x520.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-372x300.png 372w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-362x292.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/Figure-4.-A100-compared-to-H100-with-TensorRT-LLM-TCO-and-Energy-cost-benefits-136x110.png 136w\" sizes=\"(max-width: 764px) 100vw, 764px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. A100 compared to H100 with TensorRT-LLM TCO and energy cost benefits</em></figcaption></figure></div>\n\n\n<p>In addition to TCO, there are substantial labor costs associated with software development that can easily exceed infrastructure costs themselves. Investments made by NVIDIA in TensorRT, TensortRT-LLM, Triton Inference Server, and the NVIDIA NeMo framework save you a great deal of time as well as reduce time to market. You must factor in these labor costs, which can easily exceed capital and operational costs, to develop a true picture of your aggregate AI expenditures.</p>\n\n\n\n<h2 class=\"wp-block-heading\">LLM ecosystem explosion</h2>\n\n\n\n<p>The ecosystem is innovating rapidly, developing new and diverse model architectures. Larger models unleash new capabilities and use cases. Some of the largest, most advanced language models, like Meta\u2019s 70B-parameter Llama 2, require multiple GPUs working in concert to deliver responses in real time. Previously, developers looking to achieve the best performance for LLM inference had to rewrite and manually split the AI model into fragments and coordinate execution across GPUs.</p>\n\n\n\n<p>TensorRT-LLM uses <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/1909.08053\" target=\"_blank\">tensor parallelism</a>, a type of model parallelism in which individual weight matrices are split across devices. This enables efficient inference at scale\u2014with each model running in parallel across multiple GPUs connected through NVLink and across multiple servers\u2014without developer intervention or model changes.</p>\n\n\n\n<p>As new models and model architectures are introduced, you can optimize models with the latest NVIDIA AI kernels available as open source in TensorRT-LLM. The supported kernel fusions include cutting-edge implementations of <code>FlashAttention</code> and masked multi-head attention for the context and generation phases of GPT model execution, along with many others.</p>\n\n\n\n<p>TensorRT-LLM also includes fully optimized, ready-to-run versions of many LLMs widely used in production today, all of which can be implemented with the simple-to-use TensorRT-LLM Python API:</p>\n\n\n\n<ul>\n<li>Meta Llama 2</li>\n\n\n\n<li>OpenAI GPT-2 and GPT-3</li>\n\n\n\n<li>Falcon</li>\n\n\n\n<li>Mosaic MPT</li>\n\n\n\n<li>BLOOM</li>\n\n\n\n<li>&#8230;and a dozen others</li>\n</ul>\n\n\n\n<p>These capabilities help you create customized LLMs faster and more accurately to meet the needs of virtually any industry.</p>\n\n\n\n<h2 class=\"wp-block-heading\">In-flight batching</h2>\n\n\n\n<p>Today\u2019s LLMs are extremely versatile. A single model can be used simultaneously for a variety of different tasks. From a simple question-and-answer response in a chatbot to the summarization of a document or the generation of a long chunk of code, workloads are highly dynamic, with outputs varying in size by several orders of magnitude.&nbsp;</p>\n\n\n\n<p>This versatility can make it difficult to batch requests and execute them in parallel effectively\u2014a common optimization for serving neural networks, which could result in some requests finishing much earlier than others.</p>\n\n\n\n<p>To manage these dynamic loads, TensorRT-LLM includes an optimized scheduling technique called <em>in-flight batching</em>. This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model.&nbsp;</p>\n\n\n\n<p>With in-flight batching, rather than waiting for the whole batch to finish before moving on to the next set of requests, the TensorRT-LLM runtime immediately evicts finished sequences from the batch. It then begins executing new requests while other requests are still in flight. </p>\n\n\n\n<p>In-flight batching and the additional kernel-level optimizations enable improved GPU usage and minimally double the throughput on a benchmark of real-world LLM requests on NVIDIA H100 Tensor Core GPUs, helping to reduce energy costs and minimize TCO.</p>\n\n\n\n<h2 class=\"wp-block-heading\">H100 Transformer Engine with FP8</h2>\n\n\n\n<p>LLMs contain billions of model weights and activations, typically trained and represented with 16-bit floating point (FP16 or BF16) values where each value occupies 16 bits of memory. At inference time, however, most models can be effectively represented at lower precision, like 8-bit or even 4-bit integers (INT8 or INT4), using modern quantization techniques.&nbsp;</p>\n\n\n\n<p><em>Quantization</em> is the process of reducing the precision of a model\u2019s weights and activations without sacrificing accuracy. Using lower precision means that each parameter is smaller, and the model takes up less space in GPU memory. This enables inference on larger models with the same hardware while spending less time on memory operations during execution.&nbsp;</p>\n\n\n\n<p><a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/data-center/h100/\" target=\"_blank\">NVIDIA H100</a> GPUs with TensorRT-LLM give you the ability to convert model weights into a new FP8 format easily and compile models to take advantage of optimized FP8 kernels automatically. This is made possible through NVIDIA Hopper <a rel=\"noreferrer noopener\" href=\"https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/\" target=\"_blank\">Transformer Engine</a> technology and done without having to change any model code.&nbsp;</p>\n\n\n\n<p>The FP8 data format introduced by the H100 enables you to quantize your models and radically reduce \u200cmemory consumption without degrading model accuracy. FP8 quantization retains higher accuracy compared to other data formats like INT8 or INT4 while achieving the fastest performance and offering the simplest implementation.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>LLMs are advancing rapidly. Diverse model architectures are being developed daily and contribute to a growing ecosystem. In turn, larger models unleash new capabilities and use cases, driving adoption across all industries.</p>\n\n\n\n<p>LLM inference is also reshaping the data center. Higher performance with increased accuracy yields better TCO for enterprises. Model innovations enable better customer experiences, translating into higher revenue and earnings.</p>\n\n\n\n<p>When planning inference deployment projects, there are still many other considerations to achieve peak performance using state-of-the-art LLMs. Optimization rarely happens automatically. You must consider fine-tuning factors such as parallelism, end-to-end pipelines, and advanced scheduling techniques. Those factors require a computing platform that can handle mixed precision without diminishing accuracy.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Get started with TensorRT-LLM</h3>\n\n\n\n<p>NVIDIA TensorRT-LLM is now available as the <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">open-source library /NVIDIA/TensorRT-LLM</a> on GitHub and the NVIDIA NeMo framework\u2014part of <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" target=\"_blank\">NVIDIA AI Enterprise</a>, an enterprise-grade AI software platform with security, stability, manageability, and support. </p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) offer incredible new capabilities, expanding the frontier of what is possible with AI. However, their large size and unique execution characteristics can make them difficult to use in cost-effective ways.&nbsp; NVIDIA has been working closely with leading companies, including Meta, Anyscale, Cohere, Deci, Grammarly, Mistral AI, MosaicML (now a part of &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\">Continued</a></p>\n", "protected": false}, "author": 1550, "featured_media": 70556, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1257481", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/265841", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1968, 1903], "tags": [296, 453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ilT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70549"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1550"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70549"}], "version-history": [{"count": 73, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70549/revisions"}], "predecessor-version": [{"id": 74945, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70549/revisions/74945"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70556"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70549"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70549"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70549"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70450, "date": "2023-09-09T09:00:00", "date_gmt": "2023-09-09T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70450"}, "modified": "2023-09-22T09:17:33", "modified_gmt": "2023-09-22T16:17:33", "slug": "leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/", "title": {"rendered": "Leading MLPerf Inference v3.1 Results with NVIDIA GH200 Grace Hopper Superchip Debut"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>AI is transforming computing, and inference is how the capabilities of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/artificial-intelligence/\">AI</a> are deployed in the world\u2019s applications. Intelligent chatbots, image and video synthesis from simple text prompts, personalized content recommendations, and medical imaging are just a few examples of AI-powered applications.</p>\n\n\n\n<p>Inference workloads are both computationally demanding and diverse, requiring that platforms be able to process many predictions on never-seen-before data quickly as well as run inference on a breadth of AI models. Organizations looking to deploy AI need a way to evaluate the performance of infrastructure objectively across a breadth of workloads, environments, and deployment scenarios. This is true for both AI training and inference.</p>\n\n\n\n<p><a href=\"https://nvda.ws/44rwR52\">MLPerf</a> Inference v3.1, developed by the MLCommons consortium, is the latest edition of an industry-standard AI inference benchmark suite. It complements MLPerf Training and MLPerf <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">HPC</a>. MLPerf Inference v3.1 measures inference performance across a variety of important workloads, including image classification, object detection, natural language processing, speech recognition, and recommender systems, across common data center and edge deployment scenarios.</p>\n\n\n\n<p>MLPerf Inference v3.1 includes two important updates to better reflect modern AI use cases:</p>\n\n\n\n<ul>\n<li>The addition of a <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language model</a> (LLM) test based on GPT-J\u2013an open source, 6B-parameter LLM\u2013to represent text summarization, a form of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a>.</li>\n\n\n\n<li>An updated DLRM test with a new model architecture and a substantially larger dataset that mirrors the DLRM update introduced in <a href=\"https://developer.nvidia.com/blog/breaking-mlperf-training-records-with-nvidia-h100-gpus/\">MLPerf Training v3.0</a>. The update better reflects the scale and complexity of modern recommender systems.</li>\n</ul>\n\n\n\n<p>Powered by the full&nbsp;<a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/ai-inference-software\" target=\"_blank\">NVIDIA AI Inference software stack</a>, including the latest TensorRT 9.0, NVIDIA made submissions in MLPerf Inference v3.1 using a wide array of products. These included the debut submission of the&nbsp;<a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/data-center/dgx-gh200/\" target=\"_blank\">NVIDIA GH200 Grace Hopper Superchip</a>, which extended the great per-accelerator performance delivered by the <em><a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/data-center/h100/\" target=\"_blank\">NVIDIA H100 Tensor Core GPU</a>.</em> NVIDIA also submitted the <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/data-center/l4/\" target=\"_blank\">NVIDIA L4 Tensor Core GPU</a>&nbsp;for mainstream servers, as well as both the&nbsp;<a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\" target=\"_blank\">NVIDIA Jetson AGX Orin and Jetson Orin NX</a>&nbsp;platforms for edge AI and robotics.&nbsp;&nbsp;</p>\n\n\n\n<p>The rest of this post provides highlights of the NVIDIA submissions as well as a peek into how these exceptional results were achieved.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Grace Hopper Superchip extends NVIDIA Hopper inference performance</h2>\n\n\n\n<p>The NVIDIA GH200 Grace Hopper Superchip combines the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper GPU</a> and the <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\">NVIDIA Grace CPU</a> through the coherent <a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\">NVLink-C2C</a> at 900 GB/s to create a single superchip. That\u2019s 7x higher than PCIe Gen5 at 5x lower power. It also incorporates up to 576 GB of fast access memory through the combination of 96 GB of HBM3 GPU memory and up to 480 GB of low-power, high-bandwidth LPDDR5X memory.</p>\n\n\n\n<p>The GH200 Grace Hopper Superchip has integrated power management features that enable the GH200 to take advantage of the <a href=\"https://www.google.com/url?q=https://www.nvidia.com/en-us/glossary/energy-efficiency/&amp;sa=D&amp;source=docs&amp;ust=1693504353066770&amp;usg=AOvVaw2sxJXIKnnursu0DDrvLRQO\">energy </a><a href=\"https://www.google.com/url?q=https://www.nvidia.com/en-us/glossary/energy-efficiency/&amp;sa=D&amp;source=docs&amp;ust=1693504353066770&amp;usg=AOvVaw2sxJXIKnnursu0DDrvLRQO\">efficiency</a> of the Grace CPU to balance efficiency and performance. For more information, see <a href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\">NVIDIA Grace Hopper Superchip Architecture In-Depth</a> and the <a href=\"https://nvda.ws/3frjtKQ\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Grace Hopper Superchip Architecture</a> whitepaper.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip.png\" alt=\"Diagram shows the GH200 with 96 GB HBM3 was used for MLPerf Inference v3.1 submission.\" class=\"wp-image-70461\" width=\"1000\" height=\"514\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-625x321.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-768x395.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-1536x789.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-645x331.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-500x257.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-362x186.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-214x110.png 214w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/logical-overview-gh200-grace-hopper-superchip-1024x526.png 1024w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Logical overview of the NVIDIA GH200 Grace Hopper Superchip</em></figcaption></figure></div>\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA GH200 Grace Hopper Superchip</a> is designed for the versatility required to deliver leading performance across compute and memory-intensive workloads. It also delivers substantially higher performance on the most demanding frontier workloads, such as large transformer-based models with hundreds of billions or trillions of parameters, recommender systems with multi-terabyte embedding tables, and vector databases.</p>\n\n\n\n<p>In addition to being built for the most intensive AI workloads, the GH200 Grace Hopper Superchip also shines on the popular, mainstream workloads tested by MLPerf Inference. It ran every test, demonstrating its seamless support for the full NVIDIA software stack. It extended the exceptional performance achieved by NVIDIA&#8217;s single H100 SXM submission on every workload.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1.jpg\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1.jpg\" alt=\"Bar chart shows that NVIDIA Grace Hopper delivered up to 17% better performance than H100 SXM with the help of larger memory capacity, wider memory bandwidth, and sustaining higher GPU clock frequency.\" class=\"wp-image-70542\" width=\"1030\" height=\"446\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1.jpg 1373w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-300x130.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-625x270.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-179x77.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-768x332.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-645x279.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-500x216.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-160x69.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-362x157.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-254x110.jpg 254w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/GH200-perf-increases-1-1024x443.jpg 1024w\" sizes=\"(max-width: 1030px) 100vw, 1030px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA Grace Hopper MLPerf Inference data center performance results compared to DGX H100 SXM</em></figcaption></figure></div>\n\n\n<p class=\"has-text-align-left has-small-font-size\">MLPerf Inference: Datacenter v3.1, Closed. Submission IDs: NVIDIA 3.1-0107(1xH100 SXM), 3.1-0110(1xGH200 Grace Hopper Superchip)<br>The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. For more information, see <a href=\"https://www.mlcommons.org\">www.mlcommons.org</a>.</p>\n\n\n\n<p>The GH200 Grace Hopper Superchip incorporates 96 GB of HBM3 and provides up to four TB/s of HBM3 memory bandwidth, compared to 80 GB and 3.35 TB/s for H100 SXM, respectively. This larger memory capacity, as well as greater memory bandwidth, enabled the use of larger batch sizes for workloads on the NVIDIA GH200 Grace Hopper Superchip compared to the NVIDIA H100 SXM. For example, both RetinaNet and DLRMv2 ran with up to double the batch sizes in the Server scenario and 50% greater batch sizes in the Offline scenario.</p>\n\n\n\n<p>The GH200 Grace Hopper Superchip\u2019s high-bandwidth NVLink-C2C link between the NVIDIA Hopper GPU and the Grace CPU enables fast communication between the CPU and GPU, which can help boost performance.</p>\n\n\n\n<p>For example, in the MLPerf DLRMv2 workload, transferring a batch of tensors over PCIe takes approximately 22% of the batch inference time on H100 SXM. The GH200 Grace Hopper Superchip, however, performed the same transfer using just 3% of the inference time as a result of NVLink-C2C.</p>\n\n\n\n<p>Thanks to higher memory bandwidth and larger memory capacity, the Grace Hopper Superchip delivered up to 17% higher per-chip performance advantage compared to the H100 GPU on MLPerf Inference v3.1 workloads. These results showcase the performance and versatility of both the GH200 Grace Hopper Superchip and the NVIDIA software stack.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Optimizing GPT-J 6B for LLM inference</h2>\n\n\n\n<p>To represent LLM inference workloads, MLPerf Inference v3.1 introduces a new test based on the GPT-J 6B model: an LLM with 6B parameters. The task tested by the new benchmark is text summarization using the <a href=\"https://huggingface.co/datasets/cnn_dailymail\">CNN/DailyMail dataset</a>.</p>\n\n\n\n<p>The NVIDIA platform delivered strong results on the GPT-J workload, with GH200 Grace Hopper Superchip delivered the highest per-accelerator performance on both the Offline and Server scenarios on a per-accelerator basis. The NVIDIA L4 GPU also delivered strong performance, outpacing the best CPU-only result up to 6x in a 1-slot PCIe card with a thermal design power (TDP) of just 72 Watts.</p>\n\n\n\n<p>To achieve these results, NVIDIA software for LLM inference intelligently applies both FP8 and FP16 precisions to increase performance while also meeting target accuracy requirements.</p>\n\n\n\n<p>A key challenge for performing GPT-J inference is the high memory consumption of the key-value (KV) cache in the transformer block. By storing the KV cache in the FP8 data format, the NVIDIA submission significantly increased the batch size used. This boosted GPU memory utilization and enabled better use of the immense compute performance of NVIDIA GPUs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture.png\" alt=\"Diagram shows the architecture of the GPT-J model, including input, output, and internal mechanism.\" class=\"wp-image-70464\" width=\"883\" height=\"820\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture.png 1765w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-300x279.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-625x581.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-124x115.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-768x714.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-1536x1427.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-645x599.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-323x300.png 323w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-97x90.png 97w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-362x336.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-118x110.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/gpt-j-architecture-1024x951.png 1024w\" sizes=\"(max-width: 883px) 100vw, 883px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. GPT-J architecture</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Enabling DLRM-DCNv2 submissions</h2>\n\n\n\n<p>MLPerf Inference v3.1 introduced an update to the DLRMv1 model used in prior versions of the benchmark. This DLRMv2 model replaces the interactions layer with a three-layer <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/2008.13535\" target=\"_blank\">DCNv2 cross network</a>. DLRMv2 also uses multi-hot categorical inputs rather than one-hot, which are synthetically generated from the <a rel=\"noreferrer noopener\" href=\"https://labs.criteo.com/2013/12/download-terabyte-click-logs/\" target=\"_blank\">Criteo Terabyte Click Logs Dataset</a>.</p>\n\n\n\n<p>One of the challenges of recommender inference&nbsp;arises from fitting the embedding tables on the system. By converting the model to FP16 precision, including the embedding table, we could both improve performance and halve&nbsp;the memory footprint of the embedding table, reducing it to 49 GB. This enables the entire embedding table to fit within a single H100 GPU.&nbsp;</p>\n\n\n\n<p>To enable our submission on the L4 GPU, which has 24 GB of memory, NVIDIA software intelligently splits the embedding table between GPU and host memory using row-frequency data obtained by analyzing the training dataset. Using this data, NVIDIA software can minimize memory transfers between the host CPU and GPU&nbsp; by storing the most frequently used embedding table rows on the GPU.</p>\n\n\n\n<p>The NVIDIA platform demonstrated exceptional results on DLRMv2, with GH200 showing up to a 17% increase compared to the great performance delivered by H100 SXM.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Maximizing parallelism on NVIDIA Jetson Orin with Programmable Vision Accelerator</h2>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">Jetson AGX Orin series and Jetson Orin NX series</a> are embedded modules for edge AI and robotics, based on the NVIDIA Orin system-on-chip (SoC). To deliver exceptional AI performance and efficiency across a range of use cases, Jetson Orin incorporates many compute engines:</p>\n\n\n\n<ul>\n<li>A GPU based on the <a href=\"https://www.nvidia.com/en-us/data-center/ampere-architecture/\">NVIDIA Ampere Architecture</a>, with third-generation Tensor Cores</li>\n\n\n\n<li>Two second-generation, fixed-function <a href=\"https://developer.nvidia.com/deep-learning-accelerator\">NVIDIA Deep Learning Accelerators</a> (NVDLA v2.0)</li>\n\n\n\n<li>One second-generation Programmable Vision Accelerator (PVA v2.0).</li>\n</ul>\n\n\n\n<p>These accelerators can be used to offload the GPU and enable additional AI inference performance on the Jetson Orin modules.</p>\n\n\n\n<p>NVDLA is a fixed-function accelerator optimized for deep learning operations and is designed to do full hardware acceleration of convolutional neural network inferencing.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1.png\" alt=\"Diagram of the NVIDIA Orin SoC shows the individual blocks, including CPU, GPU, dedicated accelerators, cache, and memory interface.\" class=\"wp-image-70466\" width=\"703\" height=\"659\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1.png 937w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-300x281.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-625x586.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-123x115.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-768x720.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-645x605.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-320x300.png 320w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-96x90.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-362x340.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/nvidia-orin-soc-1-117x110.png 117w\" sizes=\"(max-width: 703px) 100vw, 703px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. NVIDIA Orin system-on-chip</em></figcaption></figure></div>\n\n\n<p>For the first time in MLPerf Inference v3.1, we demonstrate the concurrent use of the PVA alongside GPU and DLA for inference. The second-generation PVA provides dedicated hardware for various computer vision kernels such as filtering, warping, and fast Fourier transforms (FFT). It also supports advanced programmed kernels, which can serve as the backend runtime of TensorRT custom plug-ins.</p>\n\n\n\n<p>With the 23.08 Jetson CUDA-X AI Developer Preview, we\u2019ve included a sample PVA SDK. This package provides runtime support for a non-maximum suppression (NMS) layer. It demonstrates that the PVA can serve as a highly capable accelerator, complementing the powerful Jetson Orin GPU.</p>\n\n\n\n<p>NVIDIA has developed a TensorRT custom NMS PVA plug-in as a reference for Jetson Orin users and it was included as part of the NVIDIA MLPerf Inference v3.1 submission.</p>\n\n\n\n<p>In the NVIDIA MLPerf Inference v3.0 RetinaNet submission on NVIDIA Orin platforms, the GPU handled all outputs from the ResNext + FPN backbone from the GPU as well as from the two DLAs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output.png\" alt=\"Diagram shows how inference queries are sent to the GPU and DLAs, and then the GPU handles the outputs from the DLAs.\" class=\"wp-image-70467\" width=\"690\" height=\"468\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output.png 920w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-300x203.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-625x424.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-170x115.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-768x521.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-645x437.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-442x300.png 442w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-133x90.png 133w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-362x246.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-inference-v3-gpu-output-162x110.png 162w\" sizes=\"(max-width: 690px) 100vw, 690px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 5. GPU responsible for GPU and DLA outputs in MLPerf Inference v3.0</em></figcaption></figure></div>\n\n\n<p>Figure 5 shows how, in MLPerf Inference v3.0 submissions, the GPU was responsible for outputs from the ResNext+FPN backbone from both the GPU and the DLAs.</p>\n\n\n\n<p>By using the NMS PVA plug-in, the NMS operator is now offloaded from GPU to PVA, enabling three fully parallel inference flows on Jetson Orin AGX and Jetson Orin NX. The output from the ResNext and the FPN backbone running on the two DLAs is now consumed by the two PVAs running the NMS PVA plug-in inside the end-to-end RetinaNet TensorRT engine.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output.png\" alt=\"Diagram shows how inference queries are sent to the GPU and DLAs, the PVAs consume the outputs from the DLAs, and then the GPU and two PVAs create output.\" class=\"wp-image-70468\" width=\"666\" height=\"469\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output.png 888w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-300x211.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-625x440.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-163x115.png 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-768x541.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-645x454.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-426x300.png 426w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-362x255.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/fully-parallel-computations-v3.1-output-156x110.png 156w\" sizes=\"(max-width: 666px) 100vw, 666px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Fully parallel computations in MLPerf Inference v3.1</em></figcaption></figure></div>\n\n\n<p>In Figure 6, the NVIDIA MLPerf Inference v3.1 submission enables computations to run fully in parallel through optimized use of Jetson Orin PVAs.</p>\n\n\n\n<p>This careful use of PVA along with GPU and DLA boosts performance by 30% on both the Jetson AGX Orin 64GB and the Jetson Orin NX 16GB modules. When this use of PVA is coupled with a newly optimized NMS Opt GPU plug-in, Jetson AGX Orin delivers 61% higher performance and 38% better power efficiency on the RetinaNet workload. The Jetson Orin NX 16GB showed an even larger gain, with an 84% performance boost on the same test.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Algorithmic optimizations further improve BERT performance</h2>\n\n\n\n<p>In MLPerf Inference v3.1, NVIDIA made a submission on the BERT Large workload using the L4 GPU in the open division using techniques developed by the OmniML team. OmniML is a startup acquired by NVIDIA in early 2023 that brought expertise in machine learning algorithmic model optimization for use cases spanning cloud platforms to edge devices.</p>\n\n\n\n<p>The open division submission on BERT applied structured pruning with distillation techniques to improve the performance by up to 4.7x while maintaining 99% accuracy. This submission demonstrates the potential of algorithmic optimizations for enhancing significantly the already exceptional performance of the NVIDIA platform.</p>\n\n\n\n<p>NVIDIA deployed a proprietary, automatic, structured pruning tool that uses a gradient-based sensitivity analysis to prune the model to the given target FLOPs and fine-tune it with distillation to recover most of the accuracy. The number of transformer layers, attention heads, and linear layer dimensions were pruned in all the transformer layers in the model while the embedding dimension was kept unchanged.</p>\n\n\n\n<p>Compared to the original MLPerf Inference BERT INT8 model, our pruned model reduced the number of parameters by 4x and the number of FLOPs by 5.6x. This model has a varying number of heads and linear layer dimensions in each layer. The resulting TensorRT engine built from the pruned model is 3.4x smaller, 177 MB compared to 607 MB.</p>\n\n\n\n<p>The fine-tuned model is quantized to INT8 precision using the same technique employed in the NVIDIA closed division submission. The submission also employed distillation during quantization-aware training (QAT) to achieve an accuracy that is 99% or higher.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Scenario</strong></td><td><strong>Closed Division</strong></td><td><strong>Open Division</strong></td><td><strong>Speedup</strong></td></tr><tr><td>Offline samples/sec</td><td>1029</td><td>4609</td><td>4.5x</td></tr><tr><td>Server samples/sec</td><td>899</td><td>4265</td><td>4.7x</td></tr><tr><td>Single Stream p90 Latency (ms)</td><td>2.58</td><td>0.82</td><td>3.1x</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. BERT Large performance metrics for both closed division and open division</em></figcaption></figure>\n\n\n\n<p>To understand better how each of the model optimizations affects performance, NVIDIA performed a stacking analysis and applied different model optimization methods individually (Figure 8).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis.png\" alt=\"Diagram stacks quantization performance on optimization and pruning (Closed Division) and then on distillation (Open Division). The accuracy baseline is the FP32 model (not listed).\" class=\"wp-image-70470\" width=\"620\" height=\"207\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis.png 620w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis-300x100.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis-500x167.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis-362x121.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/stacking-performance-analysis-329x110.png 329w\" sizes=\"(max-width: 620px) 100vw, 620px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 7. Stacking performance analysis</em></figcaption></figure></div>\n\n\n<p>Figure 7 shows that, through model pruning and distillation, the NVIDIA open division submission on the BERT workload using L4 provides a 4.5x speedup compared to the same GPU running the closed division workload in the offline scenario.</p>\n\n\n\n<p>Each model optimization method applied can be easily integrated with each other. Together, they yielded a substantial performance improvement compared to the baseline model.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>NVIDIA accelerated computing boosts performance for inference and AI training workloads</h2>\n\n\n\n<p>In its MLPerf debut, the GH200 Grace Hopper Superchip turned in exceptional performance on all workloads and scenarios in the closed division of the data center category, boosting performance by up to 17% on the NVIDIA single-chip H100 SXM submission. The NVIDIA software stack fully supports the GH200 Grace Hopper Superchip today.&nbsp;</p>\n\n\n\n<p>For mainstream servers, the L4 GPU showed delivery of a large performance leap over CPU-only offerings in a compact, low-power, PCIe add-in card.</p>\n\n\n\n<p>For edge AI and robotics applications, the Jetson AGX Orin and Jetson Orin NX modules achieved great performance. Software optimizations helped to further unlock the potential of the powerful NVIDIA Orin SoC that powers those modules. It boosted performance on RetinaNet, a popular AI network for object detection, by up to 84%.</p>\n\n\n\n<p>In this round, NVIDIA also submitted results in the open division, providing a first look at the potential for model optimizations to speed inference performance dramatically while still achieving excellent accuracy.</p>\n\n\n\n<p>The latest MLPerf Inference v3.1 benchmarks show that the NVIDIA accelerated computing platform continues to deliver leadership performance and versatility. There\u2019s innovation at every layer of the technology stack, from cloud to edge, at the speed of light.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI is transforming computing, and inference is how the capabilities of AI are deployed in the world\u2019s applications. Intelligent chatbots, image and video synthesis from simple text prompts, personalized content recommendations, and medical imaging are just a few examples of AI-powered applications. Inference workloads are both computationally demanding and diverse, requiring that platforms be able &hellip; <a href=\"https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/\">Continued</a></p>\n", "protected": false}, "author": 1355, "featured_media": 70471, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1258747", "discourse_permalink": "https://forums.developer.nvidia.com/t/leading-mlperf-inference-v3-1-results-with-nvidia-gh200-grace-hopper-superchip-debut/266075", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [296, 453, 2932, 973, 187], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/mlperf-v3.1-inference-benchmark-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iki", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70450"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1355"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70450"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70450/revisions"}], "predecessor-version": [{"id": 70707, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70450/revisions/70707"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70471"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70450"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70450"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70450"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70645, "date": "2023-09-08T14:00:00", "date_gmt": "2023-09-08T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70645"}, "modified": "2023-09-21T10:53:42", "modified_gmt": "2023-09-21T17:53:42", "slug": "webinar-boost-your-ai-development-with-clearml-and-nvidia-tao", "status": "publish", "type": "post", "link": "https://nvda.ws/45Grdxp", "title": {"rendered": "Webinar: Boost Your AI Development with ClearML and NVIDIA TAO"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>On Sept. 19, learn how NVIDIA TAO integrates with the ClearML platform to deploy and maintain machine learning models in production environments.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>On Sept. 19, learn how NVIDIA TAO integrates with the ClearML platform to deploy and maintain machine learning models in production environments.</p>\n", "protected": false}, "author": 1115, "featured_media": 70655, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/45Grdxp", "_links_to_target": "_blank"}, "categories": [2724, 2758], "tags": [452, 453, 1595, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ClearML_NVIDIA_TAO_Sept-promo.png", "jetpack_shortlink": "https://wp.me/pcCQAL-inr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70645"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70645"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70645/revisions"}], "predecessor-version": [{"id": 70656, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70645/revisions/70656"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70655"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70645"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70645"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70645"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70476, "date": "2023-09-08T09:00:00", "date_gmt": "2023-09-08T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70476"}, "modified": "2023-09-21T10:56:12", "modified_gmt": "2023-09-21T17:56:12", "slug": "workshop-fundamentals-of-deep-learning", "status": "publish", "type": "post", "link": "https://nvda.ws/3Z5H5qE", "title": {"rendered": "Workshop: Fundamentals of Deep Learning"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn key techniques and tools required to train a deep learning model in this virtual hands-on workshop. </p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn key techniques and tools required to train a deep learning model in this virtual hands-on workshop.</p>\n", "protected": false}, "author": 1115, "featured_media": 70545, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3Z5H5qE", "_links_to_target": "_blank"}, "categories": [2724, 1050, 696], "tags": [3312, 2964, 453, 1731, 375], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/ai-devnews-data-science-techblogs-computers-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ikI", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70476"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70476"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70476/revisions"}], "predecessor-version": [{"id": 70482, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70476/revisions/70482"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70545"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70476"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70476"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70476"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70493, "date": "2023-09-07T12:10:21", "date_gmt": "2023-09-07T19:10:21", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70493"}, "modified": "2023-09-21T10:56:27", "modified_gmt": "2023-09-21T17:56:27", "slug": "cuda-toolkit-symbol-server", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/cuda-toolkit-symbol-server/", "title": {"rendered": "NVIDIA CUDA Toolkit Symbol Server"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA has already made available a <a href=\"https://developer.nvidia.com/nvidia-driver-symbol-server\">GPU driver binary symbols server for Windows</a>. Now, NVIDIA is making available a repository of <a href=\"https://cudatoolkit-symbols.nvidia.com/\">CUDA Toolkit symbols for Linux</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>What are we providing?</h2>\n\n\n\n<p>NVIDIA is introducing CUDA Toolkit symbols for Linux for an application development enhancement. During application development, you can now download obfuscated symbols for NVIDIA libraries that are being debugged or profiled in your application. This is shipping initially for the CUDA Driver (<code>libcuda.so</code>) and the CUDA Runtime (<code>libcudart.so</code>), with more libraries to be added.</p>\n\n\n\n<p>For instance, when an issue appears to relate to a CUDA API, it may not always be possible to provide NVIDIA with a reproducing example, core dump, or unsymbolized stack traces with all library load information. Providing a symbolized call stack can help speed up the debug process.</p>\n\n\n\n<p>We are only hosting symbol files, so debug data will not be distributed. The symbol files contain obfuscated symbol names.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Quickstart guide</h2>\n\n\n\n<p>There are two recommended ways to use the obfuscated symbols for each library:</p>\n\n\n\n<ul>\n<li>By unstripping the library</li>\n\n\n\n<li>By deploying the .sym file as a symbol file for the library</li>\n</ul>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n# Determine the symbol file to fetch and obtain it\n$ readelf -n /usr/local/cuda/lib64/libcudart.so\n\n# ... Build ID: 70f26eb93e24216ffc0e93ccd8da31612d277030\n# Browse to https://cudatoolkit-symbols.nvidia.com/libcudart.so/70f26eb93e24216ffc0e93ccd8da31612d277030/index.html to determine filename to download\n$ wget https://cudatoolkit-symbols.nvidia.com/libcudart.so/70f26eb93e24216ffc0e93ccd8da31612d277030/libcudart.so.12.2.128.sym\n\n# Then with appropriate permissions, either unstrip,\n$ eu-unstrip /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.128 libcudart.so.12.2.128.sym \u2013o /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.128\n\n# Or, with appropriate permissions, deploy as symbol file\n# By splitting the Build ID into first two characters as directory, then remaining with &quot;.debug&quot; extension\n$ cp libcudart.so.12.2.128.sym /usr/lib/debug/.build-id/70/f26eb93e24216ffc0e93ccd8da31612d277030.debug\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\"><a></a>Example: Symbolizing</h3>\n\n\n\n<p>Here is a simplified example to show the uses of symbolizing. The sample application test_shared has a data corruption that leads to an invalid handle being passed to the CUDA Runtime API cudaStreamDestroy. With a default install of CUDA Toolkit and no obfuscated symbols, the output in gdb might look like the following:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nThread 1 &quot;test_shared&quot; received signal SIGSEGV, Segmentation fault.\n0x00007ffff65f9468 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n(gdb) bt\n#0  0x00007ffff65f9468 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n#1  0x00007ffff6657e1f in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n#2  0x00007ffff6013845 in ?? () from /usr/local/cuda/lib64/libcudart.so.12\n#3  0x00007ffff604e698 in cudaStreamDestroy () from /usr/local/cuda/lib64/libcudart.so.12\n#4  0x00005555555554e3 in main ()\n</pre></div>\n\n\n<p>After applying the obfuscated symbols in one of the ways described earlier, it would give a stack trace like the following example:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nThread 1 &quot;test_shared&quot; received signal SIGSEGV, Segmentation fault.\n0x00007ffff65f9468 in libcuda_8e2eae48ba8eb68460582f76460557784d48a71a () from /lib/x86_64-linux-gnu/libcuda.so.1\n(gdb) bt\n#0  0x00007ffff65f9468 in libcuda_8e2eae48ba8eb68460582f76460557784d48a71a () from /lib/x86_64-linux-gnu/libcuda.so.1\n#1  0x00007ffff6657e1f in libcuda_10c0735c5053f532d0a8bdb0959e754c2e7a4e3d () from /lib/x86_64-linux-gnu/libcuda.so.1\n#2  0x00007ffff6013845 in libcudart_43d9a0d553511aed66b6c644856e24b360d81d0c () from /usr/local/cuda/lib64/libcudart.so.12\n#3  0x00007ffff604e698 in cudaStreamDestroy () from /usr/local/cuda/lib64/libcudart.so.12\n#4  0x00005555555554e3 in main ()\n</pre></div>\n\n\n<p>The symbolized call stack can then be documented as part of the bug description provided to NVIDIA for analysis.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Conclusion</h2>\n\n\n\n<p>When you have to profile and debug applications using CUDA and want to share a call stack with NVIDIA for analysis, use the CUDA symbol server. Profiling and debugging will be faster and easier.</p>\n\n\n\n<p>For questions or issues, dive into the forum at <a href=\"https://forums.developer.nvidia.com/c/developer-tools/other-tools/127\">Developer Tools</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA has already made available a GPU driver binary symbols server for Windows. Now, NVIDIA is making available a repository of CUDA Toolkit symbols for Linux. What are we providing? NVIDIA is introducing CUDA Toolkit symbols for Linux for an application development enhancement. During application development, you can now download obfuscated symbols for NVIDIA libraries &hellip; <a href=\"https://developer.nvidia.com/blog/cuda-toolkit-symbol-server/\">Continued</a></p>\n", "protected": false}, "author": 1867, "featured_media": 70500, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1256889", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-cuda-toolkit-symbol-server/265748", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [1932, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/cuda-driver-symbol-server-featured-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ikZ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70493"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1867"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70493"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70493/revisions"}], "predecessor-version": [{"id": 70503, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70493/revisions/70503"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70500"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70493"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70493"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70493"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70261, "date": "2023-09-07T10:09:18", "date_gmt": "2023-09-07T17:09:18", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70261"}, "modified": "2023-09-07T11:30:59", "modified_gmt": "2023-09-07T18:30:59", "slug": "unlocking-multi-gpu-model-training-with-dask-xgboost", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/unlocking-multi-gpu-model-training-with-dask-xgboost/", "title": {"rendered": "Unlocking Multi-GPU Model Training with Dask XGBoost"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>As data scientists, we often face the challenging task of training large models on huge datasets. One commonly used tool, <a href=\"https://www.nvidia.com/en-us/glossary/data-science/xgboost/\">XGBoost</a>, is a robust and efficient <a href=\"https://developer.nvidia.com/blog/gradient-boosting-decision-trees-xgboost-cuda/#:~:text=Gradient%20boosting%20is%20a%20powerful%20machine%20learning%20algorithm%20used%20to%20achieve%20state%2Dof%2Dthe%2Dart%20accuracy%20on%20a%20variety%20of%20tasks%20such%20as%20regression%2C%20classification%C2%A0and%20ranking.\">gradient-boosting</a> framework that\u2019s been widely adopted due to its speed and performance for large tabular data.&nbsp;</p>\n\n\n\n<p>Using multiple GPUs should theoretically provide a significant boost in computational power, resulting in faster model training. Yet, many users have found it challenging when attempting to leverage this power through <a href=\"https://www.nvidia.com/en-us/glossary/data-science/dask/\">Dask</a> XGBoost. Dask is a flexible open-source Python library for parallel computing and XGBoost provides Dask APIs to train CPU or GPU <a href=\"https://docs.dask.org/en/stable/dataframe.html\">Dask DataFrames</a>.</p>\n\n\n\n<p>A common hurdle of training Dask XGBoost is handling out of memory (OOM) errors at different stages, including</p>\n\n\n\n<ul>\n<li>Loading the training data</li>\n\n\n\n<li>Converting the DataFrame into XGBoost&#8217;s DMatrix format</li>\n\n\n\n<li>During the actual model training</li>\n</ul>\n\n\n\n<p>Addressing these memory issues can be challenging, but very rewarding because the potential benefits of multi-GPU training are enticing.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Top takeaways</h2>\n\n\n\n<p>This post explores how you can optimize Dask XGBoost on multiple GPUs and manage memory errors. Training XGBoost on large datasets presents a variety of challenges. I use the <a href=\"https://www.kaggle.com/competitions/otto-group-product-classification-challenge/data\">Otto Group Product Classification Challenge </a>dataset to demonstrate the OOM problem and how to fix it. The dataset has 180 million rows and 152 columns, totaling 110 GB when loaded into memory.</p>\n\n\n\n<p>The key issues we tackle include:&nbsp;</p>\n\n\n\n<ul>\n<li>Installation using the latest version of <a href=\"https://developer.nvidia.com/rapids\">RAPIDS</a> and the correct version of XGBoost.</li>\n\n\n\n<li>Setting environment variables.</li>\n\n\n\n<li>Dealing with OOM errors.</li>\n\n\n\n<li>Utilizing UCX-py for more speedup.</li>\n</ul>\n\n\n\n<p>Be sure to follow along with the accompanying Notebooks for each section.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Prerequisites</h2>\n\n\n\n<p>An initial step in leveraging the power of RAPIDS for multi-GPU training is the correct installation of RAPIDS libraries. It&#8217;s critical to note that there <a href=\"https://docs.rapids.ai/install\">are several ways to install these libraries\u2014pip, conda, docker, and building from source</a>, each compatible with Linux and Windows Subsystem for Linux.&nbsp;</p>\n\n\n\n<p>Each method has unique considerations. For this guide, I recommend using Mamba, while adhering to the conda install instructions. <a href=\"https://mamba.readthedocs.io/en/latest/installation.html\">Mamba</a> provides similar functionalities as conda but is much faster, especially for dependency resolution. Specifically, I opted for <a href=\"https://mamba.readthedocs.io/en/latest/installation.html#fresh-install\">a fresh installation of mamba</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Install the latest RAPIDS version</h3>\n\n\n\n<p>As a best practice, always install the latest RAPIDS libraries available to use the latest features. You can find up-to-date install instructions in the <a href=\"https://docs.rapids.ai/install#install-rapids\">RAPIDS Installation Guide</a>.&nbsp;</p>\n\n\n\n<p>This post uses version 23.04, which can be installed with the following command:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>mamba create -n rapids-23.04 -c rapidsai -c conda-forge -c nvidia&nbsp; \\\n\n&nbsp;&nbsp;&nbsp;&nbsp;rapids=23.04 python=3.10 cudatoolkit=11.8</code></pre>\n\n\n\n<p>This instruction installs all the libraries required including Dask, Dask-cuDF, XGBoost, and more. In particular, you\u2019ll want to check the XGBoost library installed using the command: </p>\n\n\n\n<p><code>mamba list xgboost</code></p>\n\n\n\n<p>The output is listed in Table 1:</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td><strong>Name</strong></td><td><strong>Version</strong></td><td><strong>Build</strong></td><td><strong>Channel</strong></td></tr><tr><td>XGBoost</td><td>1.7.1dev.rapidsai23.04</td><td>cuda_11_py310_3</td><td>rapidsai-nightly</td></tr></tbody></table><figcaption class=\"wp-element-caption\">Tabel 1. Install the correct XGBoost whose channel should be rapidsai or rapidsai-nightly</figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Avoid manual updates for XGBoost</h3>\n\n\n\n<p>Some users might notice that the version of XGBoost is not the latest, which is 1.7.5. Manually updating or installing XGBoost using pip or conda-forge is\u200c problematic when training XGBoost together with UCX.&nbsp;</p>\n\n\n\n<p>The error message will read something like the following:<br><em>Exception: &#8220;XGBoostError(&#8216;[14:14:27] /opt/conda/conda-bld/work/rabit/include/rabit/internal/utils.h:86: Allreduce failed&#8217;)&#8221;</em></p>\n\n\n\n<p>Instead, use the XGBoost installed from RAPIDS. A quick way to verify the correctness of the XGBoost version is <code>mamba list xgboost</code> and check the \u201cchannel\u201d of the xgboost, which should be \u201crapidsai\u201d or \u201crapidsai-nightly\u201d.&nbsp;&nbsp;</p>\n\n\n\n<p>XGBoost in the rapidsai channel is built with the RMM plug-in enabled and delivers the best performance regarding multi-GPU training.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Multi-GPU training walkthrough&nbsp;</h2>\n\n\n\n<p>First, I\u2019ll walk through a multi-GPU training notebook for the Otto dataset and cover the steps to make it work. Later on, we will talk about some advanced optimizations including UCX and spilling.&nbsp;</p>\n\n\n\n<p>You can also find the <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/XGB-186-CLICKS-DASK.ipynb\">XGB-186-CLICKS-DASK</a> Notebook on GitHub. Alternatively, we provide a <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/XGB-186-CLICKS-DASK.py\">python script</a> with full command line configurability.</p>\n\n\n\n<p>The main libraries we are going to use are xgboost, dask, dask_cuda, and dask-cudf.&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import os\n\nimport dask\n\nimport dask_cudf\n\nimport xgboost as xgb\n\nfrom dask.distributed import Client\n\nfrom dask_cuda import LocalCUDACluster</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Environment set up</h3>\n\n\n\n<p>First, let&#8217;s set our environment variables to make sure our GPUs are visible. This example uses eight GPUs with 32 GB of memory on each GPU, which is the minimum requirement to run this notebook without OOM complications. In Section Enable memory spilling below we will discuss techniques to lower this requirement to 4 GPUs.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>GPUs = ','.join(&#091;str(i) for i in range(0,8)])\nos.environ&#091;'CUDA_VISIBLE_DEVICES'] = GPUs</code></pre>\n\n\n\n<p>Next, define a helper function to create a local GPU cluster for a mutli-GPU single node.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>def get_cluster():\n\n&nbsp;&nbsp;&nbsp;&nbsp;cluster = LocalCUDACluster()\n\n&nbsp;&nbsp;&nbsp;&nbsp;client = Client(cluster)\n\n&nbsp;&nbsp;&nbsp;&nbsp;return client</code></pre>\n\n\n\n<p>Then, create a Dask client for your computations.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>client = get_cluster()</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Loading data</h3>\n\n\n\n<p>Now, let&#8217;s load the Otto dataset. Use <code>dask_cudf read_parquet</code> function, which uses multiple GPUs to read the parquet files into a <strong>dask_cudf.DataFrame</strong>.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>users = dask_cudf.read_parquet('/raid/otto/Otto-Comp/pqs/train_v152_*.pq').persist()</code></pre>\n\n\n\n<p>The dataset consists of 152 columns that represent engineered features, providing information about the frequency with which specific product pairs are viewed or purchased together. The objective is to predict which product the user will click next based on their browsing history. The details of this dataset can be found in this <a href=\"https://www.kaggle.com/competitions/otto-recommender-system/discussion/383013\">writeup</a>.</p>\n\n\n\n<p>Even at this early stage, out of memory errors can occur. This issue often arises due to excessively large row groups in \u200cparquet files. To resolve this, we recommend rewriting the parquet files with smaller row groups. For a more in-depth explanation, refer to the <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/parquet_large_row_group_demo.ipynb\">Parquet Large Row Group Demo</a> Notebook.</p>\n\n\n\n<p>After loading the data, we can check its shape and memory usage.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>users.shape&#091;0].compute()\n\nusers.memory_usage().sum().compute()/2**30</code></pre>\n\n\n\n<p>The &#8216;clicks&#8217; column is our target, which means if the recommended item was clicked by the user. We ignore the ID columns and use the rest columns as features.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>FEATURES = users.columns&#091;2:]\n\nTARS = &#091;'clicks']\n\nFEATURES = &#091;f for f in FEATURES if f not in TARS]</code></pre>\n\n\n\n<p>Next, we create a <a href=\"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.dask.DaskQuantileDMatrix\">DaskQuantileDMatrix</a> which is the input data format for training xgboost models. <a href=\"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.dask.DaskQuantileDMatrix\">DaskQuantileDMatrix</a> is a drop-in replacement for the DaskDMatrix when the histogram tree method is used. It helps reduce overall memory usage.&nbsp;</p>\n\n\n\n<p>This step is critical to avoid OOM errors. If we use the DaskDMatrix OOM occurs even with 16 GPUs. In contrast, <a href=\"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.dask.DaskQuantileDMatrix\">DaskQuantileDMatrix</a> enables training xgboot with eight GPUs or less without OOM errors.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>dtrain = xgb.dask.DaskQuantileDMatrix(client, users&#091;FEATURES], users&#091;'clicks'])</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">XGBoost model training</h3>\n\n\n\n<p>We then set our XGBoost model parameters and start the training process. Given the target column &#8216;clicks&#8217; is binary, we use the binary classification objective.&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>xgb_parms = {&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;'max_depth':4,&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;'learning_rate':0.1,&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;'subsample':0.7,\n\n&nbsp;&nbsp;&nbsp;&nbsp;'colsample_bytree':0.5,&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;'eval_metric':'map',\n\n&nbsp;&nbsp;&nbsp;&nbsp;'objective':'binary:logistic',\n\n&nbsp;&nbsp;&nbsp;&nbsp;'scale_pos_weight':8,\n\n&nbsp;&nbsp;&nbsp;&nbsp;'tree_method':'gpu_hist',\n\n&nbsp;&nbsp;&nbsp;&nbsp;'random_state':42\n\n}</code></pre>\n\n\n\n<p>Now, you&#8217;re ready to train the XGBoost model using all eight GPUs.&nbsp;&nbsp;</p>\n\n\n\n<p><strong>Output:</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#091;99] train-map:0.20168\n\nCPU times: user 7.45 s, sys: 1.93 s, total: 9.38 s\n\nWall time: 1min 10s</code></pre>\n\n\n\n<p>That\u2019s it! You&#8217;re done with training the XGBoost model using multiple GPUs.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Enable memory spilling</h2>\n\n\n\n<p>In the previous <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/XGB-186-CLICKS-DASK.ipynb\">XGB-186-CLICKS-DASK</a> Notebook, training the XGBoost model on the Otto dataset required a minimum of eight GPUs. Given that this dataset occupies 110GB in memory, and each V100 GPU offers 32GB, the data-to-GPU-memory ratio amounts to a mere 43% (calculated as 110/(32*8)).&nbsp;</p>\n\n\n\n<p>Optimally, we&#8217;d halve this by using just four GPUs. Yet, a straightforward reduction of GPUs in our previous setup invariably leads to OOM errors. This issue arises from the creation of temporary variables needed to generate the <code>DaskQuantileDMatrix</code> from the Dask cuDF dataframe and in other steps of training XGBoost. These variables themselves consume a substantial share of the GPU memory.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Optimize the same GPU resources to train larger datasets</h3>\n\n\n\n<p>In the <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/XGB-186-CLICKS-DASK-SPILL.ipynb\">XGB-186-CLICKS-DASK-SPILL</a> Notebook, I introduce minor tweaks to the previous setup. By enabling spilling, you can now train on the same dataset using just four GPUs. This technique allows you to train much larger data with the same GPU resources.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/on-demand/session/daskdistributedsummit2021-ds2105/#:~:text=An%20Introduction%20to%20Memory%20Spilling\">Spilling</a> is the technique that moves data automatically when an operation that would otherwise succeed runs out of memory due to other dataframes or series taking up needed space in GPU memory. It enables out-of-core computations on datasets that don&#8217;t fit into memory. RAPIDS cuDF and dask-cudf now support spilling from GPU to CPU memory&nbsp;</p>\n\n\n\n<p>Enabling spilling is surprisingly easy, where we just need to reconfigure the cluster with two new parameters, <code>device_memory_limit</code> and <code>jit_unspil</code>l:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>def get_cluster():\n\n&nbsp;&nbsp;&nbsp;&nbsp;ip = get_ip()\n\n&nbsp;&nbsp;&nbsp;&nbsp;cluster = LocalCUDACluster(ip=ip,&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device_memory_limit='10GB',\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jit_unspill=True)\n\n&nbsp;&nbsp;&nbsp;&nbsp;client = Client(cluster)\n\n&nbsp;&nbsp;&nbsp;&nbsp;return client</code></pre>\n\n\n\n<p><code>device_memory_limit='10GB\u2019</code> sets a limit on the amount of GPU memory that can be used by each GPU before spilling is triggered. Our configuration intentionally assigns a <code>device_memory_limit</code> of 10GB, substantially less than the total 32GB of the GPU. This is a deliberate strategy designed to preempt OOM errors during XGBoost training.\u00a0</p>\n\n\n\n<p>It&#8217;s also important to understand that memory usage by XGBoost isn&#8217;t managed directly by Dask-CUDA or Dask-cuDF. Therefore, to prevent memory overflow, Dask-CUDA and Dask-cuDF need to initiate the spilling process before the memory limit is reached by XGBoost operations.</p>\n\n\n\n<p><code>Jit_unspill</code> enables Just-In-Time un-spilling, which means that the cluster will automatically spill data from GPU memory to main memory when GPU memory is running low, and unspill it back just in time for a computation.</p>\n\n\n\n<p>And that\u2019s it! The rest of the notebook is identical to the previous notebook. Now it can train with just four GPUs, saving 50% of computing resources.<br></p>\n\n\n\n<p>Refer to the <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/XGB-186-CLICKS-DASK-SPILL.ipynb\">XGB-186-CLICKS-DASK-SPILL</a> Notebook for details.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Use Unified Communication X (UCX) for optimal data transfer</h2>\n\n\n\n<p><a href=\"https://github.com/rapidsai/ucx-py\">UCX-py</a> is a high-performance communication protocol that provides optimized data transfer capabilities, which is particularly useful for GPU-to-GPU communication.</p>\n\n\n\n<p>To use UCX effectively, we need to set another environment variable <code>RAPIDS_NO_INITIALIZE</code>:&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>os.environ&#091;\"RAPIDS_NO_INITIALIZE\"] = \"1\"</code></pre>\n\n\n\n<p>It stops cuDF from running various diagnostics on import which requires the creation of an NVIDIA CUDA context. When running distributed and using UCX, we have to bring up the networking stack before a CUDA context is created (for various reasons). By setting that environment variable, any child processes that import cuDF do not create a CUDA context before UCX has a chance to do so.&nbsp;</p>\n\n\n\n<p>Reconfigure the cluster:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>def get_cluster():\n\n&nbsp;&nbsp;&nbsp;&nbsp;ip = get_ip()\n\n&nbsp;&nbsp;&nbsp;&nbsp;cluster = LocalCUDACluster(ip=ip,&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device_memory_limit='10GB',\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jit_unspill=True,\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;protocol=\"ucx\",&nbsp;\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rmm_pool_size=\"29GB\"\n\n)\n\n&nbsp;&nbsp;&nbsp;&nbsp;client = Client(cluster)\n\n&nbsp;&nbsp;&nbsp;&nbsp;return client</code></pre>\n\n\n\n<p>The protocol=&#8217;ucx&#8217; parameter specifies UCX to be the communication protocol used for transferring data between the workers in the cluster.</p>\n\n\n\n<p>Use the prmm_pool_size=&#8217;29GB&#8217; parameter to set the size of the RAPIDS Memory Manager (RMM) pool for each worker. RMM allows for efficient use of GPU memory. In this case, the pool size is set to 29GB which is less than the total GPU memory size of 32GB. This adjustment is crucial as it accounts for the fact that XGBoost creates certain intermediate variables that exist outside the control of the RMM pool.</p>\n\n\n\n<p>By simply enabling UCX, we experienced a substantial acceleration in our training times\u2014a significant speed boost of 20% with spilling, and an impressive 40.7% speedup when spilling was not needed. Refer to the <a href=\"https://github.com/daxiongshu/Kaggle-OTTO-Comp/blob/dask_xgb/train/ranker_models/XGB-186-CLICKS-DASK-UCX-SPILL.ipynb\">XGB-186-CLICKS-DASK-UCX-SPILL</a> Notebook for details.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Configure local_directory</h2>\n\n\n\n<p>There are times when warning messages emerge, such as, &#8220;UserWarning: Creating scratch directories is taking a surprisingly long time.&#8221; This is a signal indicating that \u200cdisk performance is becoming a bottleneck.&nbsp;</p>\n\n\n\n<p>To circumvent this issue, we could set <code>local_directory</code> of <code>dask-cuda</code>, which specifies the path on the local machine to store temporary files. These temporary files are used during Dask&#8217;s spill-to-disk operations.&nbsp;</p>\n\n\n\n<p>A recommended practice is to set the <code>local_directory</code> to a location on a fast storage device. For instance, we could set <code>local_directory</code> to <code>/raid/dask_dir </code>if it is on a high-speed local SSD. Making this simple change can significantly reduce the time it takes for scratch directory operations, optimizing your overall workflow.&nbsp;</p>\n\n\n\n<p>The final cluster configuration is as follows:<br></p>\n\n\n\n<pre class=\"wp-block-code\"><code>def get_cluster():\n\n    ip = get_ip()\n\n    cluster = LocalCUDACluster(ip=ip, \n\n                               local_directory=\u2019/raid/dask_dir\u2019               \n\n                               device_memory_limit='10GB',\n\n                               jit_unspill=True,\n\n                               protocol=\"ucx\", \n\n                                 rmm_pool_size=\"29GB\"\n\n)\n\n    client = Client(cluster)\n\n    return client</code></pre>\n\n\n\n<h2 class=\"wp-block-heading\">Results</h2>\n\n\n\n<p>As shown in Table 2, the two main optimization techniques are UCX and spilling. We managed to train XGBoost with just four GPUs and 128GB of memory. We will also show the performance scales nicely to more GPUs.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td></td><td><strong>Spilling off</strong></td><td><strong>Spilling on</strong></td></tr><tr><td><strong>UCX off</strong></td><td>135s / 8GPUs / 256 GB</td><td>270s / 4GPUs / 128 GB</td></tr><tr><td><strong>UCX on</strong></td><td>80s / 8GPUs /&nbsp; 256 GB&nbsp;</td><td>217s / 4GPUs / 128 GB</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Overview of four combinations of optimizations</em></figcaption></figure>\n\n\n\n<p>In each cell, the numbers represent end-to-end execution time, the minimum number of GPUs required, and the total GPU memory available. All four demos accomplish the same task of loading and training 110 GB of Otto data.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>In conclusion, leveraging Dask and XGBoost with multiple GPUs can be an exciting adventure, despite the occasional bumps like out of memory errors.</p>\n\n\n\n<p>You can mitigate these memory challenges and tap into the potential of multi-GPU model training by:</p>\n\n\n\n<ul>\n<li>Carefully configuring parameters such as row group size in the input parquet files</li>\n\n\n\n<li>Ensuring the correct installation of RAPIDS and XGBoost</li>\n\n\n\n<li>Utilizing Dask Quantile DMatrix</li>\n\n\n\n<li>Enabling spilling</li>\n</ul>\n\n\n\n<p>Furthermore, by applying advanced features such as UCX-Py, you can significantly speed up training times.</p>\n\n\n\n<div class='stb-container stb-style-info stb-no-caption'><div class='stb-caption'><div class='stb-logo'><img class='stb-logo__image' src='data:image/png;base64,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' alt='img'/></div><div class='stb-caption-content'></div><div class='stb-tool'></div></div><div class='stb-content'>Sign up for the latest <a style=\"color: #0000ff;\" href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-analytics/news/\">Data Science news</a>. Get the latest announcements, notebooks, hands-on tutorials, events, and more in your inbox once a month from NVIDIA.</div></div>\n", "protected": false}, "excerpt": {"rendered": "<p>As data scientists, we often face the challenging task of training large models on huge datasets. One commonly used tool, XGBoost, is a robust and efficient gradient-boosting framework that\u2019s been widely adopted due to its speed and performance for large tabular data.&nbsp; Using multiple GPUs should theoretically provide a significant boost in computational power, resulting &hellip; <a href=\"https://developer.nvidia.com/blog/unlocking-multi-gpu-model-training-with-dask-xgboost/\">Continued</a></p>\n", "protected": false}, "author": 564, "featured_media": 70266, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1256849", "discourse_permalink": "https://forums.developer.nvidia.com/t/unlocking-multi-gpu-model-training-with-dask-xgboost/265733", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696], "tags": [453, 49, 413], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/multi-gpu-training.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ihf", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70261"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/564"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70261"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70261/revisions"}], "predecessor-version": [{"id": 70528, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70261/revisions/70528"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70266"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70261"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70261"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70261"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70514, "date": "2023-09-07T09:51:14", "date_gmt": "2023-09-07T16:51:14", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70514"}, "modified": "2023-09-13T14:08:35", "modified_gmt": "2023-09-13T21:08:35", "slug": "ask-me-anything-winning-formula-for-the-best-multilingual-recommender-systems", "status": "publish", "type": "post", "link": "https://nvda.ws/44824dq", "title": {"rendered": "Ask Me Anything: Winning Formula for the Best Multilingual Recommender Systems"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>On Sept. 13, connect with the winning multilingual recommender systems Kaggle Grandmaster team of KDD\u201923.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>On Sept. 13, connect with the winning multilingual recommender systems Kaggle Grandmaster team of KDD\u201923.</p>\n", "protected": false}, "author": 1115, "featured_media": 70515, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/44824dq", "_links_to_target": "_blank"}, "categories": [1050, 696, 1968], "tags": [3425, 452, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/AMA-Kaggle.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ilk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70514"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70514"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70514/revisions"}], "predecessor-version": [{"id": 70535, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70514/revisions/70535"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70515"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70514"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70514"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70514"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 68160, "date": "2023-09-06T12:08:55", "date_gmt": "2023-09-06T19:08:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=68160"}, "modified": "2023-09-07T11:33:23", "modified_gmt": "2023-09-07T18:33:23", "slug": "supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions/", "title": {"rendered": "Supercharge Ransomware Detection with AI-Enhanced Cybersecurity Solutions"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><br>Ransomware attacks have become increasingly popular, more sophisticated, and harder to detect. For example, in 2022, a destructive <a href=\"https://www.ibm.com/downloads/cas/3R8N1DZJ\">ransomware attack</a> took 233 days to identify and 91 days to contain, for a total lifecycle of 324 days. Going undetected for this amount of time can cause irreversible damage. Faster and smarter detection capabilities are critical to addressing these attacks.&nbsp;</p>\n\n\n\n<p><strong>Behavioral ransomware detection with NVIDIA DPUs and GPUs&nbsp;</strong></p>\n\n\n\n<p>Adversaries and malware are evolving faster than defenders, making it hard for security teams to track changes and maintain signatures for known threats.&nbsp; To address this, a combination of AI and advanced security monitoring is needed. Developers can build solutions for detecting ransomware attacks faster using advanced technologies including <a href=\"https://www.nvidia.com/en-us/networking/products/data-processing-unit/\">NVIDIA BlueField Data Processing Units (DPUs),</a> the NVIDIA DOCA SDK with <a href=\"https://docs.nvidia.com/doca/sdk/app-shield-programming-guide/index.html\">DOCA App Shield,</a> and <a href=\"https://developer.nvidia.com/morpheus-cybersecurity\">NVIDIA Morpheus</a> cybersecurity AI framework.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Intrusion detection with BlueField DPU</h2>\n\n\n\n<p>BlueField DPUs are ideal for enabling best-in-class, zero-trust security, and extending that security to include host-based protection. With built-in isolation, this creates a separate trust domain from the host system, where intrusion detection system (IDS) security agents are deployed. If a host is compromised, the isolation layer between the security control agents on the DPU and the host prevents the attack from spreading throughout the data center.</p>\n\n\n\n<p>DOCA App-Shield is one of the libraries provided with the <a href=\"https://developer.nvidia.com/networking/doca\">NVIDIA DOCA software framework</a>. It is a security framework for host monitoring, enabling cybersecurity vendors to create IDS solutions that can quickly identify an attack on any physical server or virtual machine.</p>\n\n\n\n<p>DOCA App-Shield runs on the NVIDIA DPU as an out-of-band (OOB) device in a separate domain from the host CPU and OS and is:</p>\n\n\n\n<ol>\n<li>Resilient against attacks on a host machine.</li>\n\n\n\n<li>Least disruptive to the execution of host applications.</li>\n</ol>\n\n\n\n<p>DOCA App Shield exposes an API to users developing security applications. For detecting malicious activities from the DPU Arm processor, it uses DMA without involving the host OS or CPU. In contrast, a standard agent of anti-virus or endpoint-detection-response runs on the host and can be seen or\u200c compromised by an attacker or malware.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh5.googleusercontent.com/oXRXwpOCtKrMAS_ZC-NCB1RlfJchH6-6JspbyCMezkr49DtFZ7prm0h7JaIWfTXdxnYaeiweO0NpXDM3xn1T9JuEatoPsFhRixenP9Mpm0Lq0F0nTrr8CjsN4KEwcC8xWqpB9i-J68qPrVi1E_6Cp4c\" alt=\"Image of an NVIDIA BlueField-3 DPU.\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA BlueField-3 </em><a href=\"https://www.nvidia.com/en-us/networking/products/data-processing-unit\"><em>DPU</em></a><em> 400 Gb/s infrastructure compute platform</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Morpheus AI framework for cybersecurity&nbsp;</h2>\n\n\n\n<p>Morpheus is part of the NVIDIA AI Enterprise software product family and is designed to build complex ML and AI-based pipelines. It provides significant acceleration of AI pipelines to deal with high data volumes, classify data, and identify anomalies, vulnerabilities, phishing, compromised machines, and many other security issues.&nbsp;</p>\n\n\n\n<p>Morpheus can be deployed on-premise with a GPU-accelerated server like the <a href=\"https://www.nvidia.com/en-us/data-center/products/egx/\">NVIDIA EGX Enterprise Platform</a>, and it is also accessible through <a href=\"https://docs.nvidia.com/morpheus/cloud_deployment_guide.html\">cloud deployment</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img decoding=\"async\" loading=\"lazy\" width=\"1024\" height=\"666\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-1024x666.png\" alt=\"A workflow showing Morpheus consisting of a GPU-accelerated server with SmartNic/DPU and software stack of RAPIDS, Cyber Logs Accelerator, NVIDIA Triton, and NVIDIA TensorRT for real-time telemetry from BlueField DPUs.\" class=\"wp-image-69282\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-1024x666.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-300x195.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-625x406.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-177x115.png 177w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-768x499.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-1536x999.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-2048x1331.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-645x419.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-461x300.png 461w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-138x90.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-362x235.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/NVIDIA-Morpheus-Bluefield-DPU-169x110.png 169w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA Morpheus with BlueField DPU Telemetry</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Addressing ransomware with AI</h2>\n\n\n\n<p>One of the pretrained AI models in Morpheus is the <a href=\"https://docs.nvidia.com/morpheus/examples/ransomware_detection/readme.html\">ransomware detection pipeline</a> that leverages NVIDIA DOCA App-Shield as a data source. This brings a new level of security for detecting ransomware attacks that were previously impossible to detect in real time.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-1024x576.png\" alt=\"Ransomware detection AI pipeline showing a DPU monitoring virtual machines. The Morpheus AI server receives DOCA AppShield events and alerts high anomaly processes.\" class=\"wp-image-69285\" width=\"1024\" height=\"576\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-2048x1152.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Ransomware-detection-AI-pipeline-196x110.png 196w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Ransomware detection AI pipeline</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Inside BlueField DPU</h2>\n\n\n\n<p>BlueField DPU offers the new OS-Inspector app to leverage DOCA App-Shield host monitoring capabilities and enables a constant collection of OS attributes from the monitored host or virtual machine. OS-Inspector app is now available through early access.&nbsp;<a href=\"mailto:networking-support@nvidia.com\">Contact us</a> for more information.</p>\n\n\n\n<p>The collected operating system attributes include processes, threads, libraries, handles, and vads (for a complete API list, see the <a href=\"https://docs.nvidia.com/doca/sdk/app-shield-programming-guide/index.html#capabilities-per-system\">App-Shield programming guide</a>).</p>\n\n\n\n<p>OS-Inspector App then uses DOCA Telemetry Service to stream the attributes to the Morpheus inference server using the Kafka event streaming platform.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Inside the Morpheus Inference Framework</h2>\n\n\n\n<p>The Morpheus ransomware detection AI pipeline processes the data using GPU acceleration and feeds the data to the ransomware detection AI model.</p>\n\n\n\n<p>This tree-based model detects ransomware attacks based on suspicious attributes in the servers. It uses N-gram features to capture the change in attributes through time and detect any suspicious anomaly.&nbsp;</p>\n\n\n\n<p>When an attack is detected, Morpheus generates an inference event and triggers a real-time alert to the security team for further mitigation steps.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://lh4.googleusercontent.com/ZZCFuQJjzUGyH-iY_TXf6QXCdYWMXCSGCOqtFA2RkdvvVfOIzhrsvsHiLGeqqKvGaYLWmBZUX7zlQgAG1lItUmkIzwgZqA1dWoXs_XAHuI6BzRPkmrXRi3NkEf-kga-GDHO4jr1J6gSRQflOYExz13M\" alt=\"A ransomware detection model detects a ransomware process named sample.exe.\" width=\"800\" height=\"412\"/><figcaption class=\"wp-element-caption\"><em>Figure 4. Ransomware detection model</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">FinSec lab use case&nbsp;</h2>\n\n\n\n<p>NVIDIA partner FinSec Innovation Lab, a joint venture between Mastercard and Enel X, demonstrated their solution for combating ransomware attacks at <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51437/\">NVIDIA GTC 2023</a>.</p>\n\n\n\n<p>FinSec ran a POC, which used BlueField DPUs and the Morpheus cybersecurity AI framework to train a model that detected a ransomware attack in less than 12 seconds. This real-time response enabled them to isolate a virtual machine and save 80% of the data on the infected servers.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Learn more</h2>\n\n\n\n<p>BlueField DPU running DOCA App Shield enables OOB host monitoring. Together with Morpheus, developers can quickly build AI models to protect against cyber attacks, better than ever before.\u00a0OS-Inspector app is now available through early access. \u00a0<a rel=\"noreferrer noopener\" href=\"mailto:networking-support@nvidia.com\" target=\"_blank\">Contact us</a>\u00a0for more information.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Ransomware attacks have become increasingly popular, more sophisticated, and harder to detect. For example, in 2022, a destructive ransomware attack took 233 days to identify and 91 days to contain, for a total lifecycle of 324 days. Going undetected for this amount of time can cause irreversible damage. Faster and smarter detection capabilities are critical &hellip; <a href=\"https://developer.nvidia.com/blog/supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions/\">Continued</a></p>\n", "protected": false}, "author": 1851, "featured_media": 61663, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1256099", "discourse_permalink": "https://forums.developer.nvidia.com/t/supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions/265605", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696], "tags": [453, 2127], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/cybersecurity-abstract-image.png", "jetpack_shortlink": "https://wp.me/pcCQAL-hJm", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/68160"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1851"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=68160"}], "version-history": [{"count": 19, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/68160/revisions"}], "predecessor-version": [{"id": 70490, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/68160/revisions/70490"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/61663"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=68160"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=68160"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=68160"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70034, "date": "2023-09-06T09:53:28", "date_gmt": "2023-09-06T16:53:28", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70034"}, "modified": "2023-11-09T17:26:53", "modified_gmt": "2023-11-10T01:26:53", "slug": "gpus-for-etl-optimizing-etl-architecture-for-apache-spark-sql-operations", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/gpus-for-etl-optimizing-etl-architecture-for-apache-spark-sql-operations/", "title": {"rendered": "GPUs for ETL? Optimizing ETL Architecture for Apache Spark SQL Operations"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Extract-transform-load (ETL) operations with GPUs using the <a href=\"https://resources.nvidia.com/en-us-spark\">NVIDIA RAPIDS Accelerator for Apache Spark</a> running on large-scale data can produce both cost savings and performance gains. We demonstrated this in our previous post, <a href=\"https://developer.nvidia.com/blog/gpus-for-etl-run-faster-less-costly-workloads-with-nvidia-rapids-accelerator-for-apache-spark-and-databricks/\">GPUs for ETL? Run Faster, Less Costly Workloads with NVIDIA RAPIDS Accelerator for Apache Spark and Databricks</a>. In this post, we dive deeper to identify precisely <em>which</em> <a href=\"https://www.nvidia.com/en-us/glossary/data-science/apache-spark/\">Apache Spark</a> SQL operations are accelerated for a given processing architecture.&nbsp;</p>\n\n\n\n<p><em>This post is part of a series on GPUs and extract-transform-load (ETL) operations</em>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Migrating ETL to GPUs</strong></h2>\n\n\n\n<p>Should all ETL be migrated to GPUs? Or is there an advantage to evaluating which processing architecture is best suited to specific Spark SQL operations?</p>\n\n\n\n<p>CPUs are optimized for sequential processing with significantly fewer yet faster individual cores. There are clear computational advantages for memory management, handling I/O operations, running operating systems, and so on.</p>\n\n\n\n<p>GPUs are optimized for parallel processing with significantly more yet slower cores. GPUs excel at rendering graphics, training, <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> and <a href=\"https://www.nvidia.com/en-us/glossary/data-science/deep-learning/\">deep learning</a> models, performing matrix calculations, and other operations that benefit from parallelization.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Experimental design</strong></h2>\n\n\n\n<p>We created three large, complex datasets modeled after real client retail sales data using computationally&nbsp;expensive ETL operations:</p>\n\n\n\n<ul>\n<li>Aggregation (SUM + GROUP BY)</li>\n\n\n\n<li>CROSS JOIN</li>\n\n\n\n<li>UNION</li>\n</ul>\n\n\n\n<p>Each dataset was specifically curated to test the limits and value of specific Spark SQL operations. All three datasets were modeled based on a transactional sales dataset from a global retailer. The row size, column count, and type were selected to balance experimental processing costs while performing tests that would demonstrate and evaluate the benefits of both CPU and GPU architectures under specific operating conditions. See Table 1 for data profiles.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Operation&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Rows</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong># COLUMNS: Structured data&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong># COLUMNS: Unstructured data&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Size (MB)</strong></td></tr><tr><td>Aggregation (SUM + GROUP BY)</td><td class=\"has-text-align-center\" data-align=\"center\">94.4 million&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">2</td><td class=\"has-text-align-center\" data-align=\"center\">0</td><td class=\"has-text-align-center\" data-align=\"center\">3,200</td></tr><tr><td>CROSS JOIN</td><td class=\"has-text-align-center\" data-align=\"center\">63 billion&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">6</td><td class=\"has-text-align-center\" data-align=\"center\">1</td><td class=\"has-text-align-center\" data-align=\"center\">983</td></tr><tr><td>UNION</td><td class=\"has-text-align-center\" data-align=\"center\">447 million&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">10</td><td class=\"has-text-align-center\" data-align=\"center\">2</td><td class=\"has-text-align-center\" data-align=\"center\">721</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 1. Summary of experimental datasets</em></em></figcaption></figure>\n\n\n\n<p>The following computational configurations were evaluated for this experiment:&nbsp;</p>\n\n\n\n<ul>\n<li>Worker and driver type</li>\n\n\n\n<li>Workers [minimum and maximum]</li>\n\n\n\n<li>RAPIDS or Photon deployment</li>\n\n\n\n<li>Maximal hourly limits on Databricks units (DBUs)\u2014a proprietary measure of Databricks compute cost&nbsp;&nbsp;</li>\n</ul>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Worker and driver type</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Workers [min/max]</strong></td><td><strong>RAPIDS Accelerator / PHOTON</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Max DBUs / hour</strong></td></tr><tr><td>Standard_NC4as_T4_v3</td><td class=\"has-text-align-center\" data-align=\"center\">1/1</td><td>RAPIDS Accelerator&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">2</td></tr><tr><td>Standard_NC4as_T4_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/8</td><td>RAPIDS Accelerator</td><td class=\"has-text-align-center\" data-align=\"center\">9</td></tr><tr><td>Standard_NC8as_T4_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/2</td><td>RAPIDS Accelerator</td><td class=\"has-text-align-center\" data-align=\"center\">4.5</td></tr><tr><td>Standard_NC8as_T4_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/8</td><td>RAPIDS Accelerator</td><td class=\"has-text-align-center\" data-align=\"center\">14</td></tr><tr><td>Standard_NC16as_T4_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/2</td><td>RAPIDS Accelerator</td><td class=\"has-text-align-center\" data-align=\"center\">7.5</td></tr><tr><td>Standard_NC16as_T4_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/8</td><td>RAPIDS Accelerator</td><td class=\"has-text-align-center\" data-align=\"center\">23</td></tr><tr><td>Standard_E16_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/2</td><td>Photon</td><td class=\"has-text-align-center\" data-align=\"center\">24</td></tr><tr><td>Standard_E16_v3</td><td class=\"has-text-align-center\" data-align=\"center\">2/8</td><td>Photon</td><td class=\"has-text-align-center\" data-align=\"center\">72</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Experimental computational configurations</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Other experimental considerations</strong></h3>\n\n\n\n<p>In addition to building industry-representative test datasets, other experimental factors are listed below.&nbsp;</p>\n\n\n\n<ol>\n<li>Datasets are run using several different worker and driver configurations on pay-as-you-go instances\u2013as opposed to spot instances\u2013as their inherent availability establishes pricing consistency across experiments.</li>\n\n\n\n<li>For GPU testing, we leveraged RAPIDS Accelerator on T4 GPUs, which are optimized for analytics-heavy loads, and carry a substantially lower cost per DBU.</li>\n\n\n\n<li>The CPU worker type is an in-memory optimized architecture which uses&nbsp;Intel Xeon Platinum 8370C (Ice Lake) CPUs.</li>\n\n\n\n<li>We also leveraged Databricks Photon, a native CPU accelerator solution and accelerated version of&nbsp;their traditional Java runtime, rewritten in C++.&nbsp;</li>\n</ol>\n\n\n\n<p>These parameters were chosen to ensure experimental repeatability and applicability to common use cases.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Results</strong></h2>\n\n\n\n<p>To evaluate experimental results in a consistent fashion, we developed a composite metric named adjusted DBUs per minute (ADBUs). ADBUs are based on DBUs and computed as follows:</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Ctext%7B%5Cemph%7BAdjusted+DBUs+per+Minute%7D%7D+%3D+%5Cfrac%7B%5Ctext%7B%5Cemph%7BRuntime+%28mins%29%7D%7D%7D%7B%5Ctext%7B%5Cemph%7BCluster+DBUs+Cost+per+Hour%7D%7D%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;text{&#92;emph{Adjusted DBUs per Minute}} = &#92;frac{&#92;text{&#92;emph{Runtime (mins)}}}{&#92;text{&#92;emph{Cluster DBUs Cost per Hour}}}\" class=\"latex\" /></p>\n\n\n\n<p>Experimental results demonstrate that there is no computational Spark SQL task in which one chipset\u2013GPU or CPU\u2013dominates. As Figure 1 shows, dataset characteristics and the suitability of a cluster configuration have the strongest impact on which framework to choose for a specific task. Although unsurprising, the question remains: which ETL processes should be migrated to GPUs?</p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>UNION operations</strong></h3>\n\n\n\n<p>Although RAPIDS Accelerator on T4 GPUs generate results having both lower costs and execution times with UNION operations, the difference when compared with CPUs is negligible. Moving an existing ETL pipeline from CPUs to GPUs seems unwarranted for this combination of dataset and Spark SQL operation. It is likely\u2013albeit untested by this research\u2013that a larger dataset may generate results that warrant a move to GPUs.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>CROSS JOIN operations</strong></h3>\n\n\n\n<p>For the compute-heavy CROSS JOIN operation, we observed an order of magnitude of both time and cost savings by employing RAPIDS Accelerator (GPUs) over Photon (CPUs).&nbsp;</p>\n\n\n\n<p>One possible explanation for these performance gains is that the CROSS JOIN is a Cartesian product that involves an unstructured data column being multiplied with itself. This leads to exponentially increasing complexity. The performance gains of GPUs are well suited for this type of large-scale parallelizable operation.&nbsp;</p>\n\n\n\n<p>The main driver of cost differences is that the CPU clusters we experimented with had a much higher DBU rating than the chosen GPU clusters.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>SUM + GROUP BY operations</strong></h3>\n\n\n\n<p>For aggregation operations (SUM + GROUP BY), we observed mixed results. Photon (CPUs) delivered notably faster compute times, whereas RAPIDS Accelerator (GPUs) provided lower overall costs. Looking at individual experimental runs, we observed that the higher Photon costs result in higher DBUs, whereas the costs associated with T4s are significantly lower.&nbsp;</p>\n\n\n\n<p>This explains the lower overall cost using RAPIDS Accelerator in this part of the experiment. In summary, if speed is the objective, Photon is the clear winner. More price-conscious users may prefer the longer compute times of RAPIDS Accelerator for notable cost savings.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1212\" height=\"604\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison.png\" alt=\"Bar graphs showing the trade-off between compute time and cost for UNION, CROSS JOIN, and SUM + GROUP operations in Spark SQL for both Photon and RAPIDS Accelerator\" class=\"wp-image-70044\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison.png 1212w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-300x150.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-625x311.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-179x89.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-768x383.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-645x321.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-500x249.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-160x80.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-362x180.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-221x110.png 221w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/mean-compute-time-mean-cost-comparison-1024x510.png 1024w\" sizes=\"(max-width: 1212px) 100vw, 1212px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Comparison of mean compute time and mean cost</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\"><strong>Deciding which architecture to use</strong></h2>\n\n\n\n<p>The CPU cluster gained performance in execution time in the commonly used aggregation (SUM + GROUP BY) experiment. However, this came at the price of higher associated cluster costs. For CROSS JOINs, a less common high-compute and highly-parallelizable operation, GPUs dominated both in higher speed and lower costs. UNIONs showed negligible comparative differences in compute time and cost.&nbsp;</p>\n\n\n\n<p>Where GPUs (and by association RAPIDS Accelerator) will excel depends largely on the data structure, the scale of the data, the ETL operation(s) performed, and the user\u2019s technical depth.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>GPUs for ETL</strong></h3>\n\n\n\n<p>In general, GPUs are well suited to large, complex datasets and Spark SQL operations that are highly parallelizable. The experimental results suggest using GPUs for CROSS JOIN situations, as they are amenable to parallelization, and can also scale easily as data grows in size and complexity.&nbsp;</p>\n\n\n\n<p>It is important to note the scale of data is less important than the complexity of the data and the selected operation, as shown in the SUM + GROUP BY experiment. (This experiment involved more data, but less computational complexity compared to CROSS JOINs.) You can work with <a href=\"http://nvidia.com/spark-tool\">NVIDIA free of charge to estimate expected GPU acceleration gains based on analyses of Spark log files.</a>&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>CPUs for ETL</strong></h3>\n\n\n\n<p>Based on the experiments, certain Spark SQL operations such as UNIONs showed a negligible difference in cost and compute time. A shift to GPUs may not be warranted in this case. Moreover, for aggregations (SUM + GROUP BY), a conscious choice of speed over cost can be made based on situational requirements, where CPUs will execute faster, but at a higher cost.&nbsp;</p>\n\n\n\n<p>In cases where in-memory calculations are straightforward, staying with an established CPU ETL architecture may be ideal.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Discussion and future considerations</strong></h2>\n\n\n\n<p>This experiment explored one-step Spark SQL operations. For example, a singular CROSS JOIN, or a singular UNION, omitting more complex ETL jobs that involve multiple steps. An interesting future experiment might include optimizing ETL processing at a granular level, sending individual SparkSQL operations to CPUs or GPUs in a single job or script, and optimizing for both time and compute cost.</p>\n\n\n\n<p>A savvy Spark user might try to focus on implementing scripting strategies to make the most of the default runtime, rather than implementing a more efficient paradigm. Examples include:</p>\n\n\n\n<ul>\n<li>Spark SQL join strategies (broadcast join, shuffle merge, hash join, and so on)</li>\n\n\n\n<li>High-performing data structures (storing data in parquet files that are highly performant in a cloud architecture as compared to text files, for example)</li>\n\n\n\n<li>Strategic data caching for reuse&nbsp;</li>\n</ul>\n\n\n\n<p>The results of our experiment indicate that leveraging GPUs for ETL can supply additional performance sufficient to warrant the effort to implement a GPU architecture.</p>\n\n\n\n<p>Although supported, RAPIDS Accelerator for <a href=\"https://www.nvidia.com/en-us/glossary/data-science/apache-spark/\">Apache Spark</a> is not available by default on Azure Databricks. This requires the installation of .jar files that may necessitate some debugging. This tech&nbsp;debt was largely paid going forward, as subsequent uses of RAPIDS Accelerator were seamless and straightforward. NVIDIA support was always readily available to help if and when necessary.&nbsp;</p>\n\n\n\n<p>Finally, we opted to keep all created clusters&nbsp;under 100 DBUs per hour to manage experimental costs. We tried only one size of Photon cluster. Experimental results may change by varying the cluster&nbsp;size, number of workers, and other experimental parameters. We feel these results are sufficiently robust and relevant for many typical use cases in organizations running ETL jobs.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Conclusion</strong></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/tesla-t4/\">NVIDIA T4</a> GPUs, designed specifically for analytics workloads, accomplish a leap in the price/performance ratio&nbsp;associated with leveraging GPU-based compute. <a href=\"https://www.nvidia.com/en-us/data-center/tesla-t4/\">NVIDIA RAPIDS Accelerator for Apache Spark</a>, especially when run on NVIDIA T4 GPUs, has the potential to significantly reduce costs and execution times for certain common ETL&nbsp;SparkSQL operations, particularly those that are highly parallelizable.&nbsp;</p>\n\n\n\n<p>To implement this solution on your own Apache Spark workload with no code changes, visit the <a href=\"https://github.com/NVIDIA/spark-rapids-examples\">NVIDIA/spark-rapids-examples</a> GitHub repo or the <a href=\"https://www.nvidia.com/en-us/lp/deep-learning-ai/solutions/data-science/spark-accelerator\">Apache Spark tool</a> page for sample code and applications that showcase the performance and benefits of using RAPIDS Accelerator in your data processing or machine learning pipelines.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn which Apache Spark SQL operations are accelerated for a given processing architecture. </p>\n", "protected": false}, "author": 1801, "featured_media": 70488, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1256037", "discourse_permalink": "https://forums.developer.nvidia.com/t/gpus-for-etl-optimizing-etl-architecture-for-apache-spark-sql-operations/265595", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 2758], "tags": [278, 3273, 453, 126, 695, 579], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/GPUs-for-ETL-Optimizing-ETL-Architecture-for-Apache-Spark-SQL-Operations.png", "jetpack_shortlink": "https://wp.me/pcCQAL-idA", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70034"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1801"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70034"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70034/revisions"}], "predecessor-version": [{"id": 70489, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70034/revisions/70489"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70488"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70034"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70034"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70034"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70441, "date": "2023-09-05T12:30:46", "date_gmt": "2023-09-05T19:30:46", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70441"}, "modified": "2023-12-06T09:00:06", "modified_gmt": "2023-12-06T17:00:06", "slug": "webinar-build-realistic-robot-simulations-with-nvidia-isaac-sim-and-matlab", "status": "publish", "type": "post", "link": "https://nvda.ws/3sK6Kcr", "title": {"rendered": "Webinar: Build Realistic Robot Simulations with NVIDIA Isaac Sim and MATLAB"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>On Sept. 12, learn about the connection between MATLAB and NVIDIA Isaac Sim through ROS.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>On Sept. 12, learn about the connection between MATLAB and NVIDIA Isaac Sim through ROS.</p>\n", "protected": false}, "author": 1115, "featured_media": 70444, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/3sK6Kcr", "_links_to_target": "_blank"}, "categories": [63, 503], "tags": [453, 45, 1410], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/isaac-sim-feature-image.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ik9", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70441"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70441"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70441/revisions"}], "predecessor-version": [{"id": 70446, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70441/revisions/70446"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70444"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70441"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70441"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70441"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70243, "date": "2023-09-01T08:36:30", "date_gmt": "2023-09-01T15:36:30", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70243"}, "modified": "2023-10-25T16:52:32", "modified_gmt": "2023-10-25T23:52:32", "slug": "advanced-api-performance-shaders", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-api-performance-shaders/", "title": {"rendered": "Advanced API Performance: Shaders"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><em>This post covers best practices when working with shaders on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all&nbsp;</em><a href=\"https://developer.nvidia.com/blog/tag/advanced-api-performance\">Advanced API Performance tips</a>.</p>\n\n\n\n<p>Shaders play a critical role in graphics programming by enabling you to control various aspects of the rendering process. They run on the GPU and are responsible for manipulating vertices, pixels, and other data.</p>\n\n\n\n<ul>\n<li>General shaders</li>\n\n\n\n<li>Compute shaders</li>\n\n\n\n<li>Pixel shaders</li>\n\n\n\n<li>Vertex shaders</li>\n\n\n\n<li>Geometry, domain, and hull shaders</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>General shaders</h2>\n\n\n\n<p>These tips apply to all types of shaders.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommended</h3>\n\n\n\n<ul>\n<li>Avoid warp-divergent constant buffer view (CBV) and immediate constant buffer (ICB) reads.\n<ul>\n<li>Constant buffer reads are most effective when threads in a warp access data uniformly. If you need divergent reads, use shader resource view (SRVs).</li>\n\n\n\n<li>Typical cases where SRVs should be preferred over CBVs include the following:\n<ul>\n<li>Bones or skinning data</li>\n\n\n\n<li>Lookup tables, like precomputed random numbers</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li>To optimize buffers and group shared memory, use manual bit packing. When creating structures for packing data, consider the range of values a field can hold and choose the smallest datatype that can encompass this range.</li>\n\n\n\n<li>Optimize control flow by providing hints of the expected runtime behavior.\n<ul>\n<li>Make sure to enable compile flag -all-resources-bound for DXC (or D3DCOMPILE_ALL_RESOURCES_BOUND<em> </em>in FXC<em>) </em>if possible. This enables a larger set of driver-side optimizations.</li>\n\n\n\n<li>Consider using the [FLATTEN] and [BRANCH] keywords where appropriate.\n<ul>\n<li>A conditional branch may prevent the compiler from hoisting long-latency instructions, such as texture fetches.</li>\n\n\n\n<li>The [FLATTEN] keyword hints that the compiler is free to hoist and start the load operations before the statement has been evaluated.</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li>Use Root Signature 1.1 to specify static data and descriptors to enable the driver to make the most optimal shader optimizations.</li>\n\n\n\n<li>Keep the register use to a minimum. Register allocation could limit occupancy and may force the driver to spill registers to memory.</li>\n\n\n\n<li>Prefer the use of gather instructions when loading single channel texture quads.\n<ul>\n<li>This will cut down the expected latency by almost 4x compared to the equivalent operation constructed from consecutive sample instructions.</li>\n</ul>\n</li>\n\n\n\n<li>Prefer structured buffers over raw buffers.\n<ul>\n<li>Structured buffers have stricter alignment requirements, which enables the driver to schedule more efficient load instructions.</li>\n</ul>\n</li>\n\n\n\n<li>Consider using numerical approximations or precomputed lookup tables of transcendental functions (exp, log, sin, cos, sqrt) in math-intensive shaders, for instance, physics simulations and denoisers.</li>\n\n\n\n<li>To promote a fast path in the TEX unit, with up to 2x speedup, use point filtering in certain circumstances:\n<ul>\n<li>Low-resolution textures where point filtering is already an accurate representation.</li>\n\n\n\n<li>Textures that are being accessed at their native resolution.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Not recommended</h3>\n\n\n\n<ul>\n<li>Don\u2019t assume that half-precision floats are always faster than full precision and the reverse.\n<ul>\n<li>On NVIDIA Ampere GPUs, it\u2019s just as efficient to execute FP32 as FP16 instructions. The overhead of converting between precision formats may just end up with a net loss.</li>\n\n\n\n<li>NVIDIA Turing GPUs may benefit from using FP16 math, as FP16 can be issued at twice the rate of FP32.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Compute shaders</h2>\n\n\n\n<p>Compute shaders are used for general-purpose computations, from data processing and simulations to machine learning.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommended</h3>\n\n\n\n<ul>\n<li>Consider using wave intrinsics over group shared memory when possible for communication across threads.\n<ul>\n<li>Wave intrinsics don\u2019t require explicit thread synchronization.</li>\n\n\n\n<li>Starting from SM 6.0, HLSL supports warp-wide wave intrinsics natively without the need for vendor-specific HLSL extensions. Consider using vendor-specific APIs only when the expected functionality is missing. For more information, see <a href=\"https://developer.nvidia.com/unlocking-gpu-intrinsics-hlsl\">Unlocking GPU Intrinsics in HLSL</a>.</li>\n\n\n\n<li>To increase atomic throughput, use wave instructions to coalesce atomic operations across a warp.</li>\n</ul>\n</li>\n\n\n\n<li>To maximize cache locality and to improve L1 and L2 hit rate, try thread group ID swizzling for full-screen compute passes.</li>\n\n\n\n<li>A good starting point is to target a thread group size corresponding to between two or eight warps. For instance, thread group size 8x8x1 or 16x16x1 for full-screen passes. Make sure to profile your shader and tune the dimensions based on profiling results.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Not recommended</h3>\n\n\n\n<ul>\n<li>Do not make your thread group size difficult to scale per platform and GPU architecture.\n<ul>\n<li>Specialization constants can be used in Vulkan to set the dimensions at pipeline creation time whereas HLSL requires the thread group size to be known at shader compile time.</li>\n</ul>\n</li>\n\n\n\n<li>Be careless of thread group launch latency.\n<ul>\n<li>If your CS has early-out conditions that are expected to early out in most cases, it might be better to choose larger thread group dimensions and cut down on the total number of thread groups launched.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Pixel shaders</h2>\n\n\n\n<p>Pixel shaders, also known as fragment shaders, are used to calculate effects on a per-pixel basis.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommended</h3>\n\n\n\n<ul>\n<li>Prefer the use of depth bounds test or stencil and depth testing over manual depth tests in pixel shaders.</li>\n\n\n\n<li>Depth and stencil tests may discard entire 16&#215;16 raster tiles down to individual pixels. Make sure that Early-Z is enabled.</li>\n\n\n\n<li>Be mindful of the use patterns that may force the driver to disable Early-Z testing:\n<ul>\n<li>Conditional z-writes such as clip and discard\n<ul>\n<li>As an alternative consider using null blend ops instead</li>\n</ul>\n</li>\n\n\n\n<li>Pixel shader depth write</li>\n\n\n\n<li>Writing to UAV resources</li>\n</ul>\n</li>\n\n\n\n<li>Consider converting your full screen pass to a compute shader if there&#8217;s a large difference in latency between warps.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Not recommended</h3>\n\n\n\n<ul>\n<li>Don\u2019t use raster order view (ROV) techniques pervasively.\n<ul>\n<li>Guaranteeing order doesn\u2019t come for free.</li>\n\n\n\n<li>Always compare with alternative approaches like advanced blending ops and atomics.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Vertex shaders</h2>\n\n\n\n<p>Vertex shaders are used to calculate effects on a per-vertex basis.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommended</h3>\n\n\n\n<ul>\n<li>Prefer the use of compressed vertex formats.</li>\n\n\n\n<li>Prefer the use of SRVs for skinning data over CBVs. This is a typical case of divergent CBV reads.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Geometry, domain, and hull shaders</h2>\n\n\n\n<p>Geometry, domain, and hull shaders are used to control, evaluate, and generate geometry, enabling tessellation to create a dynamic generation of surfaces and objects.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommended</h3>\n\n\n\n<ul>\n<li>Replace the geometry, domain, and hull shaders with the mesh shading capabilities introduced in NVIDIA Turing.</li>\n\n\n\n<li>Enable the fast geometry path with the following configuration:\n<ul>\n<li><strong>Fixed topology</strong>: Neither an expansion or reduction in the number of vertices.</li>\n\n\n\n<li><strong>Fixed primitive type</strong>: The input primitive type is equal to the output primitive type.</li>\n\n\n\n<li><strong>Immutable per-vertex attributes</strong>: The application cannot change the vertex attributes and can only copy them from the input to the output.</li>\n\n\n\n<li><strong>Mutable per-primitive attributes</strong>: The application can compute a single value for the whole primitive, which then is passed to the fragment shader stage. For example, it can compute the area of the triangle.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Acknowledgments</h2>\n\n\n\n<p><em>Thanks to Ryan Prescott, Ana Mihut, Katherine Sun, and Ivan Fedorov.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>This post covers best practices when working with shaders on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all&nbsp;Advanced API Performance tips. Shaders play a critical role in graphics programming by enabling you to control various aspects of the rendering process. They run on the GPU and are responsible &hellip; <a href=\"https://developer.nvidia.com/blog/advanced-api-performance-shaders/\">Continued</a></p>\n", "protected": false}, "author": 1850, "featured_media": 66457, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1253682", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-api-performance-shaders/265233", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [97, 1235, 503], "tags": [2424, 3516, 453, 693], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/Advanced-API-series.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-igX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70243"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1850"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70243"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70243/revisions"}], "predecessor-version": [{"id": 70335, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70243/revisions/70335"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/66457"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70243"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70243"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70243"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70193, "date": "2023-09-01T08:30:11", "date_gmt": "2023-09-01T15:30:11", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70193"}, "modified": "2023-11-03T00:14:57", "modified_gmt": "2023-11-03T07:14:57", "slug": "speeding-up-text-to-speech-diffusion-models-by-distillation", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/speeding-up-text-to-speech-diffusion-models-by-distillation/", "title": {"rendered": "Speeding Up Text-To-Speech Diffusion Models by Distillation"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Every year, as part of their coursework, students from the University of Warsaw, Poland get to work under the supervision of engineers from the NVIDIA Warsaw office on challenging problems in deep learning and accelerated computing. We present the work of three M.Sc. students\u2014Alicja Ziarko, Pawe\u0142 Pawlik, and Micha\u0142 Siennicki\u2014who managed to significantly reduce the latency in <a href=\"https://arxiv.org/pdf/2305.07243.pdf\">TorToiSe</a>, a multi-stage, diffusion-based, text-to-speech (TTS) model.</p>\n\n\n\n<p>Alicja, Pawe\u0142, and Micha\u0142 first learned about the recent advancements in speech synthesis and diffusion models. They chose the <a href=\"https://arxiv.org/abs/2210.03142\">combination</a> of <a href=\"https://arxiv.org/abs/2207.12598\">classifier-free guidance</a> and <a href=\"https://arxiv.org/abs/2202.00512\">progressive distillation</a>, which performs well in computer vision, and adapted it to speech synthesis, achieving a 5x reduction in diffusion latency without a regression in speech quality. Small perceptual speech tests confirmed the results. Notably, this approach does not require costly training from scratch of the original model.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Why speed up diffusion-based TTS?</h2>\n\n\n\n<p>Since the publication of <a href=\"https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio\">WaveNet</a> in 2016, neural networks have become the primary models for speech synthesis. In simple applications, such as synthesis for AI-based voice assistants, synthetic voices are almost indistinguishable from human speech. Such voices can be synthesized orders of magnitudes faster than real time, for instance with the <a href=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tts/intro.html\">NVIDIA NeMo AI toolkit</a>.</p>\n\n\n\n<p>However, achieving high expressivity or imitating a voice based on a few seconds of recorded speech (few-shot) is still considered challenging.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/training/instructor-led-workshops/generative-ai-with-diffusion-models/\">Denoising Diffusion Probabilistic Models (DDPMs)</a> emerged as a generative technique that enables the generation of images of great quality and expressivity based on input text. DDPMs can be readily applied to TTS because a frequency-based spectrogram, which graphically represents a speech signal, can be processed like an image.</p>\n\n\n\n<p>For instance, in TorToiSe, which is a guided diffusion-based TTS model, a spectrogram is generated by combining the results of two diffusion models (Figure 1). The iterative diffusion process involves hundreds of steps to achieve a high-quality output, significantly increasing latency compared to state-of-the-art TTS methods, which severely limits its applications.</p>\n\n\n\n<p>In Figure 1, the unconditional diffusion model iteratively refines the initial noise until a high-quality spectrogram is obtained. The second diffusion model is further conditioned on the text embeddings produced by the language model.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"599\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise.png\" alt=\"Diagram shows a speech spectrogram generated by combining the results of two diffusion models. After numerous iterations, the expected speech spectrogram is obtained.\" class=\"wp-image-70207\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-300x90.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-625x187.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-179x54.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-768x230.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-1536x460.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-645x193.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-500x150.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-160x48.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-362x108.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-367x110.png 367w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/architecture-tortoise-1024x307.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Architecture of TorToiSe, a diffusion-based neural network for TTS</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Methods for speeding up diffusion<strong></strong></h2>\n\n\n\n<p>Existing latency reduction techniques in diffusion-based TTS can be divided into training-free and training-based methods.</p>\n\n\n\n<p>Training-free methods do not involve training the network used to generate images by reversing the diffusion process. Instead, they only focus on optimizing the multi-step diffusion process. The diffusion process can be seen as solving <a href=\"https://www.cambridge.org/core/books/applied-stochastic-differential-equations/6BB1B8B0819F8C12616E4A0C78C29EAA\">ODE/SDE</a> equations, so one way to optimize it is to create a better solver like <a href=\"https://arxiv.org/abs/2006.11239\">DDPM</a>, <a href=\"https://arxiv.org/abs/2010.02502\">DDIM</a>, and <a href=\"https://arxiv.org/abs/2206.00927\">DPM</a>, which lowers the number of diffusion steps. Parallel sampling methods, such as those based on <a href=\"https://arxiv.org/pdf/2305.16317.pdf\">Picard iterations</a> or <a href=\"https://research.nvidia.com/labs/adlr/RADTTS/\">Normalizing Flows</a>, can parallelize the diffusion process to benefit from parallel computing on GPUs.</p>\n\n\n\n<p>Training-based methods focus on optimizing the network used in the diffusion process. The network can be <a href=\"https://developer.nvidia.com/blog/transfer-learning-toolkit-pruning-intelligent-video-analytics/\">pruned</a>, <a href=\"https://developer.nvidia.com/blog/accelerating-quantized-networks-with-qat-toolkit-and-tensorrt/\">quantized, </a>or <a href=\"https://developer.nvidia.com/blog/sparsity-in-int8-training-workflow-and-best-practices-for-tensorrt-acceleration/\">sparsified</a>, and then fine-tuned for higher accuracy. Alternatively, its neural architecture can be changed manually or automatically using <a href=\"https://arxiv.org/abs/2107.10624\">NAS</a>. Knowledge distillation techniques enable distilling the student network from the teacher network to reduce the number of steps in the diffusion process.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Distillation in diffusion-based TTS</h2>\n\n\n\n<p>Alicja, Pawe\u0142, and Micha\u0142 decided to use the distillation approach based on <a href=\"https://arxiv.org/pdf/2210.03142.pdf\">promising results</a> in computer vision and its potential for an estimated 5x reduction in latency of the diffusion model at inference. They have managed to adapt progressive distillation to the diffusion part of a pretrained TorToiSe model, overcoming problems like the lack of access to the original training data.</p>\n\n\n\n<p>Their approach consists of two knowledge distillation phases:</p>\n\n\n\n<ul>\n<li>Mimicking the guided diffusion model output</li>\n\n\n\n<li>Training another student model</li>\n</ul>\n\n\n\n<p>In the first knowledge distillation phase (Figure 2), the student model is trained to mimic the output of the guided diffusion model at each diffusion step. This phase reduces latency by half by combining the two diffusion models into one model.</p>\n\n\n\n<p>To address the lack of access to the original training data, text embeddings from the language model are passed through the original teacher model to generate synthetic data used in distillation. The use of synthetic data also makes the distillation process more efficient because the entire TTS, guided diffusion pipeline does not have to be invoked at each distillation step.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion.png\" alt=\"Diagram shows a two-step distillation pipeline. First, the student model is trained (distilled) to mimic the output of the guided diffusion model at each diffusion step. In the second phase, the newly trained student model serves as a teacher to another student model, with a reduced number of steps, using progressive distillation.\" class=\"wp-image-70208\" width=\"1000\" height=\"328\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-300x98.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-625x205.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-179x59.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-768x252.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-1536x503.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-645x211.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-500x164.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-160x52.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-362x119.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-336x110.png 336w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distillation-guided-diffusion-1024x336.png 1024w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. Distillation of guided diffusion-based TTS model</em></figcaption></figure></div>\n\n\n<p>In the second progressive distillation phase (Figure 3), the newly trained student model serves as a teacher to train another student model. In this technique, the student model is trained to mimic the teacher model while reducing the number of diffusion steps by a factor of two. This process is repeated many times to further reduce the number of steps, while each time, a new student serves as the teacher for the next round of distillation.</p>\n\n\n\n<p>A progressive distillation with seven iterations reduces the number of inference steps 7^2 times, from 4,000 steps on which the model was trained to 31 steps. This reduction results in a 5x speedup compared to the guided diffusion model, excluding the text embedding calculation cost.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation.png\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation.png\" alt=\"Diagram shows two steps of progressive distillation. In each step, the number of steps required to transform the Gaussian noise to the output speech spectrogram is reduced by a factor of two, from 4 to 2, and then 2 to 1.\" class=\"wp-image-70209\" width=\"1000\" height=\"462\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-300x139.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-625x289.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-179x83.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-768x355.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-1536x710.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-645x298.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-500x231.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-362x167.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-238x110.png 238w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/example-two-iterations-progressive-distillation-1024x473.png 1024w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Example of two iterations of progressive distillation</em></figcaption></figure></div>\n\n\n<p>The perceptual pairwise speech test shows that the distilled model (after the second phase) matches the quality of speech produced by the TTS model based on guided distillation.</p>\n\n\n\n<p>As an example, listen to audio samples in Table 1 generated by the progressive distillation-based TTS model. The samples match the quality of the audio samples from the guided diffusion-based TTS model. If we simply reduced the number of distillation steps to 31, instead of using progressive distillation, the quality of the generated speech deteriorates significantly.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><thead><tr><th class=\"has-text-align-center\" data-align=\"center\"><br><br><br><br>Speaker<br></th><th class=\"has-text-align-center\" data-align=\"center\">Guided diffusion-based TTS model&nbsp;<br>(2&#215;80 diffusion steps)</th><th class=\"has-text-align-center\" data-align=\"center\">Diffusion-based TTS after progressive distillation<br>(31 diffusion steps)</th><th class=\"has-text-align-center\" data-align=\"center\">Guided diffusion-based TTS model&nbsp;<br>(naive reduction to 31 diffusion steps)</th></tr></thead><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\">Female <br>1</td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F1_1.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F1_2.wav\" target=\"_blank\" rel=\"noreferrer noopener\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F1_3.wav\">Audio</a></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Female 2</td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F2_1.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F2_2.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F2_3.wav\">Audio</a></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Female 3</td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F3_1.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/F3_2.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/F3_3.wav\">Audio</a></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Male 1</td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/M1_1.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/M1_2.wav\">Audio</a></td><td class=\"has-text-align-center\" data-align=\"center\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/M1_3.wav\">Audio</a></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1: Audio samples generated by diffusion-based TTS compared to the two baseline models</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Conclusion</h2>\n\n\n\n<p>Collaborating with academia and assisting young students in shaping their future in science and engineering is one of the core NVIDIA values. Alicja, Pawe\u0142, and Micha\u0142\u2019s successful project exemplifies the NVIDIA Warsaw, Poland office partnership with local universities.</p>\n\n\n\n<p>The students managed to solve the challenging problem of speeding up the pretrained, diffusion-based, text-to-speech (TTS) model. They designed and implemented a knowledge distillation-based solution in the complex field of diffusion-based TTS, achieving a 5x speedup of the diffusion process. Most notably, their unique solution based on synthetic data generation is applicable to pretrained TTS models without access to the original training data. </p>\n\n\n\n<p>We encourage you to explore <a href=\"https://www.nvidia.com/en-us/about-nvidia/careers/university-recruiting/\">NVIDIA Academic Programs</a> and try out the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo Framework</a> to create complete conversational AI (TTS, ASR, or NLP/LLM) solutions for the new era of generative AI.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Every year, as part of their coursework, students from the University of Warsaw, Poland get to work under the supervision of engineers from the NVIDIA Warsaw office on challenging problems in deep learning and accelerated computing. We present the work of three M.Sc. students\u2014Alicja Ziarko, Pawe\u0142 Pawlik, and Micha\u0142 Siennicki\u2014who managed to significantly reduce the &hellip; <a href=\"https://developer.nvidia.com/blog/speeding-up-text-to-speech-diffusion-models-by-distillation/\">Continued</a></p>\n", "protected": false}, "author": 1863, "featured_media": 70256, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1253680", "discourse_permalink": "https://forums.developer.nvidia.com/t/speeding-up-text-to-speech-diffusion-models-by-distillation/265232", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [453, 3545, 1976], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Fast_diffusion_tts_blog_post_features_image_1920_1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ig9", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70193"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1863"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70193"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70193/revisions"}], "predecessor-version": [{"id": 70337, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70193/revisions/70337"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70256"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70193"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70193"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70193"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 68307, "date": "2023-08-31T10:46:30", "date_gmt": "2023-08-31T17:46:30", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=68307"}, "modified": "2023-10-25T16:52:33", "modified_gmt": "2023-10-25T23:52:33", "slug": "solving-self-intersection-artifacts-in-directx-raytracing", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/solving-self-intersection-artifacts-in-directx-raytracing/", "title": {"rendered": "Solving Self-Intersection Artifacts in DirectX Raytracing"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Ray and path tracing algorithms construct light paths by starting at the camera or the light sources and intersecting rays with the scene geometry. As objects are hit, new secondary rays are generated on these surfaces to continue the paths.&nbsp;</p>\n\n\n\n<p>In theory, these secondary rays will not yield an intersection with the same triangle again, as intersections at a distance of zero are excluded by the intersection algorithm. In practice, however, the finite floating-point precision used in the actual implementation often leads to false-positive results, known as self-intersections (Figure 2). This creates artifacts, such as shadow acne, where the triangle sometimes improperly shadows itself (Figure 1).&nbsp;</p>\n\n\n\n<p>Self-intersection can be avoided by explicitly excluding the same primitive from intersection using its identifier. In DirectX Raytracing (DXR) this self-intersection check would be implemented in an any-hit shader. However, forcing an any-hit<em> </em>invocation for all triangle hits comes at a significant performance penalty. Furthermore, this method does not deal with false positives against adjacent (near) coplanar triangles.&nbsp;</p>\n\n\n\n<p>The most widespread solutions to work around the issue use various heuristics to offset the ray along either the ray direction or the normal. These methods are, however, not robust enough to handle a variety of common production content and may even require manual parameter tweaking on a per-scene basis, particularly in scenes with heavily translated, scaled or sheared instanced geometry. For more information, see <a href=\"https://www.realtimerendering.com/raytracinggems/unofficial_RayTracingGems_v1.4.pdf\">Ray Tracing Gems: High-Quality and Real-Time Rendering with DXR and Other APIs.&nbsp;</a></p>\n\n\n\n<p>Alternatively, the sources of the numerical imprecision can be numerically bounded at runtime, giving robust error intervals on the intersection test. However, this comes with considerable performance overhead and requires source access to the underlying implementation of the ray/triangle intersection routine, which is not possible in a hardware-accelerated API like DXR.</p>\n\n\n\n<p>This post describes a robust offsetting method for secondary rays spawned from triangles in DXR. The method is based on a thorough numerical analysis of the sources of the numerical imprecision. It involves computing spawn points for secondary rays, safe from self-intersections. The method does not require modification of the traversal and ray/triangle intersection routines and can thus be used with closed-source and hardware-accelerated ray tracing APIs like DXR. Finally, the method does not rely on self-intersection rejection using an any-hit shader and has a fixed overhead per shading point.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1920\" height=\"1080\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering.png\" alt=\"Two renderings of a floating castle. The rendering on the right shows shading artifacts because no self-intersection avoidance is applied.\n\" class=\"wp-image-68311\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering.png 1920w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/self-intersection-avoidance-rendering-1024x576.png 1024w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Rendering with self-intersection avoidance (left) and without self-intersection avoidance (right). Image credit: </em><a href=\"https://sketchfab.com/Sander.Vander.Meiren\"><em>Sander van der Meiren</em></a></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Method overview&nbsp;</h2>\n\n\n\n<p>The spawn point of a secondary ray coincides with the hit point on a triangle of an incoming ray. The goal is to compute a spawn point as close as possible to the hit point in the triangle plane, while still avoiding self-intersections. Too close to the triangle may result in self-intersection artifacts, but too far away may push the spawn point past nearby geometry, causing light leaking artifacts.&nbsp;</p>\n\n\n\n<p>Figure 2 shows the sources of numerical error for secondary rays. In the user shader, the object-space hit point is reconstructed and transformed into world-space. During DXR ray traversal, the world-space ray is transformed back into object-space and intersected against triangles.&nbsp;</p>\n\n\n\n<p>Each of these operations accumulates numerical errors, possibly resulting in self-intersections. This method computes a minimal uncertainty interval centered around the intended ray origin (red dot in Figure 2) on the triangle at each operation. The approximate ray origin (black dot in Figure 2) lies within this uncertainty interval. The ray origin is offset along the triangle normal beyond the final uncertainty interval to prevent self-intersections.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"765\" height=\"392\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1.png\" alt=\"A flow diagram of four stages (in green), showing an approximate computed ray origin as it is constructed, transformed, and finally used for a ray-triangle intersection test.\" class=\"wp-image-68391\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1.png 765w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-625x320.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-645x331.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-500x256.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-diagram-1-215x110.png 215w\" sizes=\"(max-width: 765px) 100vw, 765px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. The sources of numerical error in the user shader (left) and DXR ray traversal and intersection (right)</em></em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Hit point</h2>\n\n\n\n<p>Start by reconstructing the hit point and the geometric triangle normal in object-space (Listing 1).&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>precise float3 edge1 = v1 - v0;\nprecise float3 edge2 = v2 - v0;\n\n// interpolate triangle using barycentrics\n// add in base vertex last to reduce object-space error \nprecise float3 objPosition = v0 + mad(barys.x, edge1, mul(barys.y, edge2));\nfloat3 objNormal = cross(edge1, edge2);\n</code></pre>\n\n\n\n<p>The hit point is computed by interpolating the triangle vertices <em><code>v0</code></em>, <em><code>v1</code>,</em> and <em><code>v2</code></em> using the 2D barycentric hit coordinates <em><code>barys</code></em>. Although it is possible to compute the interpolated hit point using two fused multiply-add operations, adding the base vertex <code>v0</code> last reduces the maximum rounding error on the base vertex, which in practice dominates the rounding error in this computation.&nbsp;</p>\n\n\n\n<p>Use the <code>precise</code> keyword to force the compiler to perform the computations exactly as specified. Enforced precise computation of the normal and the error bounds is not required. The effects of rounding errors on these quantities are vanishingly small and can safely be ignored for self-intersection.&nbsp;</p>\n\n\n\n<p>Next, the object-space position is transformed into world-space (Listing 2).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>const float3x4 o2w = ObjectToWorld3x4(); \n\n// transform object-space position\n// add in translation last to reduce world-space error\nprecise float3 wldPosition; \nwldPosition.x = o2w._m03 + \n\tmad(o2w._m00, objPosition.x, \nmad(o2w._m01, objPosition.y, \nmul(o2w._m02, objPosition.z ))); \nwldPosition.y = o2w._m13 + \nmad(o2w._m10, objPosition.x, \nmad(o2w._m11, objPosition.y, \nmul(o2w._m12, objPosition.z )));\nwldPosition.z = o2w._m23 + \nmad(o2w._m20, objPosition.x , \nmad(o2w._m21, objPosition.y , \nmul(o2w._m22, objPosition.z )));\n</code></pre>\n\n\n\n<p>Instead of using the HLSL matrix mul intrinsic, write out the transformation. This ensures that the translational part <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bm_%7Bi%2C2%7D%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{m_{i,2}}\" class=\"latex\" /> of the transformation is added last. This again reduces the rounding error on the translation, which in practice tends to dominate the error in this computation.&nbsp;</p>\n\n\n\n<p>Finally, transform the object-space normal to world-space and normalize it (Listing 3).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>const float3x4 w2o = WorldToObject3x4();\n\n// transform normal to world-space using \n// inverse transpose matrix\nfloat3 wldNormal = mul(transpose((float3x3)w2o), objNormal); \n\n// normalize world-space normal\nconst float wldScale = rsqrt(dot(wldNormal, wldNormal)); \nwldNormal = mul(wldScale, wldNormal);\n\n// flip towards incoming ray \nif(dot(WorldRayDirection(), wldNormal) &gt; 0) \nwldNormal = -wldNormal;\n</code></pre>\n\n\n\n<p>To support transformations with uneven scaling or shear, the normals are transformed using the inverse transpose transformation. There is no need to normalize the object-space normal before the transformation. It is necessary to normalize again in world-space anyway. Because the inverse length of the world normal is needed again later to appropriately scale the error bounds, normalize manually instead of using the HLSL <code>normalize</code> intrinsic.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Error bounds</h2>\n\n\n\n<p>With an approximate world-space position and triangle normal, continue by computing error bounds on the computed position, bounding the maximum finite precision rounding error. It is necessary to account for the rounding errors in the computations in Listings 1 and 2.&nbsp;</p>\n\n\n\n<p>It is also necessary to account for rounding errors that may occur during traversal (Figure 2). During traversal, DXR will apply a world-to-object transformation and perform a ray-triangle intersection test. Both of these are performed in finite precision and thus introduce rounding errors.&nbsp;</p>\n\n\n\n<p>Start by computing a combined object-space error bound, accounting both for the rounding errors in Listing 1 and rounding errors due to the DXR ray-triangle intersection test (Listing 4).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>const float c0 = 5.9604644775390625E-8f;\nconst float c1 = 1.788139769587360206060111522674560546875E-7f; \n \n// compute twice the maximum extent of the triangle \nconst float3 extent3 = abs(edge1) + abs(edge2) + \n   abs(abs(edge1) - abs(edge2)); \nconst float extent = max(max(extent3.x, extent3.y), extent3.z); \n \n// bound object-space error due to reconstruction and intersection \nfloat3 objErr = mad(c0, abs(v0), mul(c1, extent));\n</code></pre>\n\n\n\n<p>Note that the error on the triangle intersection is bounded by the maximum triangle extent along the three dimensions. A rigorous proof for this bound goes beyond the scope of this post. To provide an intuitive justification, common ray-triangle intersection algorithms reorient the triangle into \u2019ray space\u2019 (by subtracting the ray origin) before performing the intersection test. In the context of self-intersection, the ray origin lies on the triangle. Thus, the magnitude of the remaining triangle vertices in this ray space is bounded by the extent of the triangle along each dimension.&nbsp;</p>\n\n\n\n<p>Furthermore, these intersection algorithms project the triangle into a 2D plane. This projection causes errors along one dimension to bleed over into the other dimensions. Therefore, take the maximum extent along all dimensions, instead of treating the error along the dimensions independently. The exact bound on the ray-triangle intersection test will be hardware-specific. The constant <code>c1</code> is tuned for NVIDIA RTX hardware, but may require some adjusting on different platforms.</p>\n\n\n\n<p>Error bounds for custom intersection primitives depend on the implementation details of their Intersection shader. See <a href=\"https://www.cs.utexas.edu/users/flame/laff/alaff/ALAFF.pdf\">Advanced Linear Algebra: Foundations to Frontiers</a> for a thorough introduction to finite precision rounding error analysis.&nbsp;</p>\n\n\n\n<p>Next, compute the world-space error bound due to the transformation of the hit point from object-space to world-space (Listing 5).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>// bound world-space error due to object-to-world transform \nconst float c2 = 1.19209317972490680404007434844970703125E-7f; \nfloat3 wldErr = mad(c1, mul(abs((float3x3)o2w), abs(objPosition)), \t\t\n     mul(c2, abs(transpose(o2w&#091;3]))));\n</code></pre>\n\n\n\n<p>That leaves the rounding errors in the world-to-object transformation performed by DXR during ray traversal (Listing 6).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>// bound object-space error due to world-to-object transform \nobjErr = mad(c2, mul(abs(w2o), float4(abs(wldPosition), 1)), objErr);\n</code></pre>\n\n\n\n<p>Like the ray-triangle intersection test, the rounding error in the world-to-object transformation depends on the hardware. The constant <code>c2</code> is conservative and should suffice for the various ways of implementing the vector matrix multiplication.</p>\n\n\n\n<p>The finite precision representation of the world-to-object transformation matrix and its inverse are not guaranteed to match exactly. In the analysis, the error in the representation can be attributed to one or the other. Because the object-to-world transformation is performed in user code, the errors are best attributed to the object-to-world transformation matrix, enabling tighter bounds.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Offset</h2>\n\n\n\n<p>The previous section explained how to compute bounds on the rounding errors for secondary ray construction and traversal. These bounds yield an interval around the approximate, finite precision ray origin. The intended, full-precision \u2018true\u2019 ray origin is guaranteed to lie somewhere in this interval.&nbsp;</p>\n\n\n\n<p>The true triangle passes through the true ray origin, so the triangle also passes through this interval. Figure 3 shows how to offset the approximate origin along the triangle normal to guarantee it lies above the true triangle, thus preventing self-intersections.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"667\" height=\"198\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1.png\" alt=\"The approximate computed ray origin, centered inside a 2D uncertainty interval. The true ray origin lies within the uncertainty interval. The true triangle passing through the true ray origin. The triangle normal, passing through the approximate ray origin and a conservatively offset triangle, touching the uncertainty interval. The offset ray origin is the intersection of the triangle normal and the conservatively offset triangle.\n\" class=\"wp-image-68393\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1.png 667w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-300x89.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-625x186.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-179x53.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-645x191.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-500x148.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-160x47.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-362x107.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/approximate-computed-ray-origin-2d-uncertainty-interval-1-371x110.png 371w\" sizes=\"(max-width: 667px) 100vw, 667px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Avoid self-intersection by offsetting the ray origin along the normal to outside the error interval</em></em></figcaption></figure>\n\n\n\n<p>The error bound \u2206 is projected onto the normal n to obtain an offset \u03b4 along the normal</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3D+%5Cfrac%7B%5CDelta+%5Ccdot+%7Babs%7D%5Cleft%28n%5Cright%29%7D%7Bn+%5Ccdot+n%7D%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;delta = &#92;frac{&#92;Delta &#92;cdot {abs}&#92;left(n&#92;right)}{n &#92;cdot n}}\" class=\"latex\" /></p>\n\n\n\n<p>Rounding errors on the normal are of similar magnitude as rounding errors on the computation of the error bounds and offset themselves. These are vanishingly small and can in practice be ignored. Combine the object and world-space offsets into a single world-space offset along the world-space normal (Listing 7).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>// compute world-space self-intersection avoidance offset \nfloat objOffset = dot(objErr, abs(objNormal)); \nfloat wldOffset = dot(wldErr, abs(wldNormal)); \n\nwldOffset = mad(wldScale, objOffset, wldOffset);\n</code></pre>\n\n\n\n<p>Use the already normalized world-space normal <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cbar%7Bn%7D_w%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;bar{n}_w}\" class=\"latex\" /> from Listing 3. The world-space offset <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cdelta_w%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;delta_w}\" class=\"latex\" /> simplifies to <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cdelta_w+%3D+%5CDelta+%5Ccdot+%7Babs%7D%5Cleft%28+%5Cbar%7Bn%7D_w+%5Cright%29+&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;delta_w = &#92;Delta &#92;cdot {abs}&#92;left( &#92;bar{n}_w &#92;right) \" class=\"latex\" />. The object-space offset <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cdelta_o%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;delta_o}\" class=\"latex\" /> along the object-space normal <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bn_o%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{n_o}\" class=\"latex\" /> needs to be transformed into world-space as <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cdelta_o+M+n_o%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;delta_o M n_o}\" class=\"latex\" />.&nbsp;</p>\n\n\n\n<p>Note, however, that the transformed object-space offset is not necessarily parallel to the world-space normal <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bn_w%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{n_w}\" class=\"latex\" />. To obtain a single combined offset along the world-space normal, project the transformed object-space offset onto the world-space normal, as <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cdelta_o+M+n_o+%5Ccdot+n_w%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;delta_o M n_o &#92;cdot n_w}\" class=\"latex\" />. Using that <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bn_w+%3D+M%5E%7B-T%7D+n_o%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{n_w = M^{-T} n_o}\" class=\"latex\" /> this simplifies to:&nbsp;</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7B%5Cdelta_o+M+n_o+%5Ccdot+M%5E%7B-T%7D+n_o+%3D+%5Cdelta_o+%5Cleft%28+M+n_o+%5Cright%29%5ET+M%5E%7B-T%7D+n_o+%3D+%5Cdelta_o+n_o+%5Ccdot+M%5ET+M%5E%7B-T%7D+n_o+%3D+%5Cdelta_o+n_o+%5Ccdot+n_o+%3D+%5CDelta+%5Ccdot+%7Babs%7D%5Cleft%28n%5Cright%29%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{&#92;delta_o M n_o &#92;cdot M^{-T} n_o = &#92;delta_o &#92;left( M n_o &#92;right)^T M^{-T} n_o = &#92;delta_o n_o &#92;cdot M^T M^{-T} n_o = &#92;delta_o n_o &#92;cdot n_o = &#92;Delta &#92;cdot {abs}&#92;left(n&#92;right)}\" class=\"latex\" /></p>\n\n\n\n<p>Finally, use the computed offset to perturb the hit point along the triangle normal (Listing 8).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>// offset along the normal on either side. \nprecise float3 wldFront = mad( wldOffset, wldNormal, wldPosition);\nprecise float3 wldBack  = mad(-wldOffset, wldNormal, wldPosition);\n</code></pre>\n\n\n\n<p>This yields front and back spawn points safe from self-intersection. The derived error bounds (and thus offsets) neither depend on the incoming ray direction nor the outgoing secondary ray direction. It is therefore possible to reuse the same spawn points for all secondary rays originating from this hit point. All reflection rays should use the front spawn point while transmission rays should use the back spawn point.&nbsp;</p>\n\n\n\n<p>Object-to-world and world-to-object transformations of the direction also cause rounding errors in the ray direction. At extreme grazing angles, these rounding errors may cause it to flip sides, orienting it back towards the triangle. The offsetting method in this post does not protect against such rounding errors. It is generally advised to filter out secondary rays at extreme angles.&nbsp;</p>\n\n\n\n<p>Alternatively, similar error bounds can be derived on the ray direction transformations. Offsetting the ray direction along the triangle normal (as for the ray origin) can then guarantee its sidedness. However, as the reflectance distribution of common BRDF models tends towards zero at grazing angles, this problem can be safely ignored in many applications.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Object space</h2>\n\n\n\n<p>As seen in Listing 4, the offset grows linearly in the triangle extent and the magnitude of the triangle base vertex in object-space. For small triangles, the rounding error in the base vertex will dominate the object-space error (Figure 2). It is thus possible to reduce the object-space error by repositioning geometry in object-space, centering it around the object-space origin to minimize the distance to the origin. For geometry with extremely large triangles, such as ground planes, it may be worthwhile to tessellate the geometry and further reduce the rounding errors in the triangle extent.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Camera space</h2>\n\n\n\n<p>As seen in Listings 5 and 6, the magnitude of the offset will grow linearly with the magnitudes of the world-space position. The proportionality constant <code>c2</code> is approximately 1 ulps. Instanced geometry at a distance <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bd%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{d}\" class=\"latex\" /> from the scene origin in world-space will have a maximum rounding error in the order of <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bd2%5E%7B-22%7D%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{d2^{-22}}\" class=\"latex\" />, or 1 mm of offset for every 4 km distance. The offset magnitudes also scale linear with the triangle extent and object-space position.&nbsp;</p>\n\n\n\n<p>For an example secondary ray in Figure 4 spawned on a leaf of 10 cm, in a tree of 20 m (object-space origin at the root) 1 km away from the world space origin, the offset magnitudes due to the triangle extent, object-space position, and world-space position will be in the order of 45 nm, 4 \u00b5m, and 0.25 mm, respectively. In practice, \u200c rounding errors in the world-space position tend to dominate all rounding errors. This is particularly true for large scenes of relatively small objects.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"711\" height=\"493\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1.png\" alt=\"A diagram of a tree, placed 1 km from the world-space scene origin. Measurements are shown for a leaf of 10 cm within the tree, placed 20 m from the object-space origin. A camera icon is close to the tree, oriented towards the leaf.\n\" class=\"wp-image-68394\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1.png 711w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-300x208.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-625x433.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-166x115.png 166w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-645x447.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-433x300.png 433w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-130x90.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-362x251.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/tree-diagram-offset-magnitudes-1-159x110.png 159w\" sizes=\"(max-width: 711px) 100vw, 711px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Offset magnitudes scale linear with the triangle extent, object-space position, and world-space position magnitudes</em></em></figcaption></figure>\n\n\n\n<p>Note that the error is proportional to the world-space distance to the scene origin, not the scene camera. Consequently, if the camera is far away from the scene origin, the offsets for rays spawned from nearby geometry may become prohibitively large, resulting in visual artifacts.&nbsp;</p>\n\n\n\n<p>This problem can be reduced by translating the entire scene into camera space. All instances are repositioned so the camera origin coincides with the world-space origin. Consequently, the distance <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%7Bd%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"{d}\" class=\"latex\" /> becomes the distance to the camera in this camera space and the offset magnitudes will be proportional to the distance to the camera. Rays spawned from geometry near the camera will enjoy relatively small offsets, reducing the likelihood of visual artifacts due to offsetting.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Connection rays</h2>\n\n\n\n<p>This discussion has so far focused on offsetting of the ray origin to prevent self-intersection at the origin. Ray and path tracing algorithms also trace rays to evaluate visibility between two points on different triangles, such as shadow rays connecting a shading point and a light source.</p>\n\n\n\n<p>These rays may suffer from self-intersection on either end of the ray. It is necessary to offset both ends to avoid self-intersections. The offset for the endpoint is computed in a similar fashion as for the ray origin, but using the object-to-world and world-to-object transformation matrices, barycentric and triangle vertices of the endpoint and using the connection ray direction as the incoming ray direction.&nbsp;</p>\n\n\n\n<p>Contrary to scattering rays, it is necessary to account for rounding errors in the world-to-object ray direction transform during traversal. Theoretically, it is also necessary to account for additional rounding error in the ray-triangle intersection test because the ray origin does not lie on the endpoint triangle. However, this additional error scales sublinearly with the world-to-object error, so for simplicity these errors are implicitly combined.&nbsp;</p>\n\n\n\n<p>For the endpoint, the world-to-object transformation error computation in Listing 6 is replaced by (Listing 9).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>// connection ray direction \nprecise float3 wldDir = wldEndPosition - wldOrigin; \n \n// bound endpoint object-space error due to object-to-world transform \nfloat4 absOriginDir = (float4)(abs(wldOrigin) + abs(wldDir), 1); \nobjEndErr = mad(c2, mul(abs(w2oEnd), absOriginDir), objEndErr); \n</code></pre>\n\n\n\n<p>Here, <code>wldOrigin</code> is the connection ray origin in world-space. In DXR, rays are defined using an origin and direction. Instead of offsetting the endpoint and recomputing the ray direction, apply the offset directly to the world-space direction. For endpoint offsetting, Listing 8 thus becomes Listing 10.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>// offset ray direction along the endpoint normal towards the ray origin \nwldDir = mad(wldEndOffset, wldEndNormal, wldDir) ; \n \n// shorten the ray tmax by 1 ulp \nconst float tmax = 0.99999994039f;\n</code></pre>\n\n\n\n<p>Shorten the ray length by 1 ulp to account for rounding errors in the direction computation.</p>\n\n\n\n<p>In practice, a simpler approach of using a cheap approximate offsetting heuristic in combination with identifier-based self-intersection rejection is often sufficient to avoid endpoint self-intersection.The approximate offsetting will avoid most endpoint self-intersections, with identifier-based hit rejection taking care of the remaining self-intersections.&nbsp;</p>\n\n\n\n<p>For secondary scatter rays, avoid identifier based self-intersection rejection, as it requires invoking an any-hit shader for every intersection along the ray, adding significant performance overhead. However, for visibility rays, the additional performance overhead of endpoint identifier-based hit rejection is minimal.&nbsp;</p>\n\n\n\n<p>For visibility rays using the <code>RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH</code> flag there will always be at most two additional reported hits: the rejected endpoint self-intersection and any occluder terminating traversal.&nbsp;</p>\n\n\n\n<p>For visibility rays not using the <code>RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH</code> flag, self-intersections can be rejected in the closest-hit shader instead of the any-hit shader. If the visibility ray invokes the closest-hit shader for the endpoint triangle, no closer hit was found and thus the hit should simply be treated as a miss in the closest-hit shader.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>The method presented in this post offers a robust and easy-to-use solution for self-intersections of secondary rays. The method applies a minimal conservative offset, resolving self-intersection artifacts while reducing light leaking artifacts. Moreover, the method has minimal runtime overhead and integrates easily in common shading pipelines. While this post describes an HLSL implementation for DXR, the approach translates easily to GLSL for Vulkan and CUDA for OptiX.&nbsp;</p>\n\n\n\n<p>For more information, visit <a href=\"https://github.com/NVIDIA/self-intersection-avoidance\">NVIDIA/self-intersection-avoidance</a> on GitHub for HLSL and GLSL sample implementations. And check out the <a href=\"https://github.com/NVIDIA/otk-shader-util/\">OptiX Toolkit ShaderUtil Library</a> for a ready-to-use OptiX header library for self-intersection avoidance.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn an efficient and robust method for avoiding self-intersections for the DXR ray tracing API.</p>\n", "protected": false}, "author": 1808, "featured_media": 68309, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1253044", "discourse_permalink": "https://forums.developer.nvidia.com/t/solving-self-intersection-artifacts-in-directx-raytracing/265082", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 503], "tags": [514, 453, 1944], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/07/farm-island-rendering.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-hLJ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/68307"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1808"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=68307"}], "version-history": [{"count": 36, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/68307/revisions"}], "predecessor-version": [{"id": 70227, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/68307/revisions/70227"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/68309"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=68307"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=68307"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=68307"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 69996, "date": "2023-08-31T10:00:00", "date_gmt": "2023-08-31T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=69996"}, "modified": "2023-09-07T11:38:16", "modified_gmt": "2023-09-07T18:38:16", "slug": "deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/", "title": {"rendered": "Deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA: Quantization-Aware Training to Inference"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">NVIDIA Jetson Orin</a> is the best-in-class embedded platform for AI workloads. One of the key components of the Orin platform is the second-generation <a href=\"https://developer.nvidia.com/deep-learning-accelerator\">Deep Learning Accelerator (DLA)</a>, the dedicated deep learning inference engine that offers one-third of the AI compute on the AGX Orin platforms.&nbsp;</p>\n\n\n\n<p>This post is a deep technical dive into how embedded developers working with Orin platforms can deploy deep neural networks (DNNs) using <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> as a reference. To learn more about how DLA can help maximize the performance of your deep learning applications, see <a href=\"https://developer.nvidia.com/blog/maximizing-deep-learning-performance-on-nvidia-jetson-orin-with-dla/\">Maximizing Deep Learning Performance on NVIDIA Jetson Orin with DLA</a>.</p>\n\n\n\n<p><a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> is an object detection algorithm. Building on the success of v3 and v4, YOLOv5 aims to provide improved accuracy and speed in real-time object detection tasks. YOLOv5 has gained notoriety due to its excellent trade-off between accuracy and speed, making it a popular choice among researchers and practitioners in the field of computer vision. Its open-source implementation enables developers to leverage pretrained models and customize them according to specific goals.</p>\n\n\n\n<p>The following sections walk through an <a href=\"https://github.com/NVIDIA-AI-IOT/cuDLA-samples/\">end-to-end YOLOv5 cuDLA sample</a> that shows you how to:</p>\n\n\n\n<ol>\n<li>Train a YOLOv5 model with Quantization-Aware Training (QAT) and export it for deployment on DLA.</li>\n\n\n\n<li>Deploy the network and run inference using CUDA through TensorRT and cuDLA.&nbsp;</li>\n\n\n\n<li>Execute on-target YOLOv5 accuracy validation and performance profiling.</li>\n</ol>\n\n\n\n<p>Using this sample, we demonstrate how to achieve 37.3 mAP on the COCO dataset with DLA INT8 (official FP32 mAP is 37.4). We also show how to obtain over 400 FPS for YOLOv5 on a single NVIDIA Jetson Orin DLA. (A total of two DLA instances are available on Orin.)&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">QAT training and export for DLA</h2>\n\n\n\n<p>To balance the inference performance and accuracy of YOLOv5, it&#8217;s essential to apply Quantization-Aware-Training (QAT) on the model. Because DLA does not support QAT through TensorRT at the time of writing, it\u2019s necessary to convert the QAT model to a Post-Training Quantization (PTQ) model before inference. The steps are outlined in Figure 1.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"944\" height=\"134\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps.png\" alt=\"Diagram of the key steps in taking a QAT model and converting it a model with post training quantization scales.\n\" class=\"wp-image-70002\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps.png 944w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-300x43.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-625x89.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-179x25.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-768x109.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-645x92.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-500x71.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-160x23.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-362x51.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-to-ptq-model-conversion-steps-775x110.png 775w\" sizes=\"(max-width: 944px) 100vw, 944px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Key steps involved in converting a QAT model to a PTQ model</em></em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">QAT training workflow</h3>\n\n\n\n<p>Use the <a href=\"https://github.com/NVIDIA/TensorRT/tree/release/8.6/tools/pytorch-quantization\">TensorRT pytorch-quantization</a> toolkit to quantize YOLOv5. The first step is to add quantizer modules to the neural network graph. This toolkit provides <a href=\"https://github.com/NVIDIA/TensorRT/tree/release/8.6/tools/pytorch-quantization/pytorch_quantization/nn/modules\">a set of quantized layer modules</a> for common DL operations. If a module is not among the provided quantized modules, you can create a custom quantization module for the right place in the model.&nbsp;</p>\n\n\n\n<p>The second step is to calibrate the model, obtaining the scale values for each Quantization/Dequantization (Q/DQ) module. After the calibration is complete, select a training schedule and fine-tune the calibrated model using the COCO dataset.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1530\" height=\"288\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow.png\" alt=\"Diagram of the steps involved in quantization aware training of a model, from left to right: pretrained model. insert Q/DA node, calibration, fine-tuning with data, quantized model; below: training data.\n\" class=\"wp-image-70009\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow.png 1530w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-300x56.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-625x118.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-768x145.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-645x121.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-500x94.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-160x30.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-362x68.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-584x110.png 584w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/qat-training-workflow-1024x193.png 1024w\" sizes=\"(max-width: 1530px) 100vw, 1530px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Steps of the QAT training workflow</em></em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Adding Q/DQ nodes</h3>\n\n\n\n<p>There are two options for adding Q/DQ nodes to your network:</p>\n\n\n\n<p><strong>Option 1:</strong> Place Q/DQ nodes, as recommended, in <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#tensorrt-process-qdq\">TensorRT Processing of Q/DQ Networks</a>. This method follows TensorRT fusion strategy for Q/DQ layers. These TensorRT strategies are mostly tuned for GPU inference. To make this compatible with DLA, add additional Q/DQ nodes, which can be derived using the scales from their neighboring layers with the <a href=\"https://github.com/NVIDIA/Deep-Learning-Accelerator-SW/tree/main/tools/qdq-translator\">Q/DQ Translator</a>.&nbsp;</p>\n\n\n\n<p>Any missing scales would otherwise result in certain layers running in FP16. This may result in a slight decrease in mAP and possibly a large performance drop. The Orin DLA is optimized for INT8 convolutions, about 15x over FP16 dense performance (or 30x when comparing dense FP16 to INT8 sparse performance).</p>\n\n\n\n<p><strong>Option 2:</strong> Insert Q/DQ nodes at every layer to make sure all tensors have INT8 scales. With this option, all layers&#8217; scales can be obtained during model fine-tuning. However, this method may potentially disrupt TensorRT fusion strategy with Q/DQ layers when running inference on GPU and lead to higher latency on the GPU. For DLA, on the other hand, the rule of thumb with PTQ scales is, \u201cThe more available scales, the lower the latency.\u201d&nbsp;</p>\n\n\n\n<p>As confirmed by experiment, our YOLOv5 model was verified on the COCO 2017 validation dataset with a resolution of 672 x 672 pixels. Option 1 and Option 2, respectively, achieved mAP scores of 37.1 and 37.0.&nbsp;</p>\n\n\n\n<p>Choose the best option based on your needs. If you already have an existing QAT workflow for GPU and would like to preserve it as much as possible, Option 1 is probably better. (You may need to extend Q/DQ Translator to infer more missing scales to achieve optimal DLA latency as well.)&nbsp;</p>\n\n\n\n<p>On the other hand, if you are looking for a QAT training method that inserts Q/DQ nodes into all layers and is compatible with DLA, Option 2 may be your most promising.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Q/DQ Translator workflow</h3>\n\n\n\n<p>The purpose of the Q/DQ Translator is to translate an ONNX graph trained with QAT, to PTQ tensor scales and an ONNX model without Q/DQ nodes.</p>\n\n\n\n<p>For this YOLOv5 model, extract quantization scales from Q/DQ nodes in the QAT model. Use the information of neighboring layers to infer the input/output scales of other layers such as Sigmoid and Mul in YOLOv5\u2019s SiLU or for Concat nodes. After scales are extracted, export the ONNX model without Q/DQ nodes and the (PTQ) calibration cache file such that TensorRT can use them to build a DLA engine.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Deploying network to DLA for inference</h2>\n\n\n\n<p>The next step is to deploy the network and run inference using CUDA through TensorRT and cuDLA.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Loadable build with TensorRT</h3>\n\n\n\n<p>Use TensorRT to build the DLA loadable. This provides an easy-to-use interface for DLA loadable building and seamless integration with GPU if needed. For more information about TensorRT-DLA, see <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#dla_topic\">Working with DLA</a> in the TensorRT Developer Guide.</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec\">trtexec</a> is a convenient tool provided by TensorRT for building engines and benchmarking performance. Note that a DLA loadable is the result of successful DLA compilation through the DLA Compiler, and that TensorRT can package DLA loadables inside of serialized engines.</p>\n\n\n\n<p>First, prepare the ONNX model and the calibration cache generated in the previous section. The DLA loadable can be built with a single command. Pass the <code>--safe</code> option and the entire model can run on DLA. This directly saves the compilation result as a serialized DLA loadable (without a TensorRT engine wrapping around it). For more details about this step, see the <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#using-trtexec-gen-dla-load\">NVIDIA Deep Learning TensorRT Documentation</a>.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>trtexec --onnx=model.onnx --useDLACore=0 --safe --saveEngine=model.loadable  --inputIOFormats=int8:dla_hwc4 --outputIOFormats=fp16:chw16 --int8 --fp16 --calib=qat2ptq.cache</code></pre>\n\n\n\n<p>Note that the input format <code>dla_hwc4</code> is highly recommended from a performance point of view, if your model input qualifies. The input must have at most <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/namespacenvinfer1.html#ac3e115b1a2b1e578e8221ef99d27cd45a71c9eebe99afdbeac36c38438b12e06d\">four input channels and be consumed by a convolution</a>. In INT8, DLA can benefit from a specific hardware and software optimization that is not available if you use <code>--inputIOFormats=int8:chw32</code> instead, for example.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Running inference using cuDLA</h3>\n\n\n\n<p><a href=\"https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html#cudla\">cuDLA</a> is the CUDA runtime interface for DLA, an extension of the CUDA programming model that integrates DLA with CUDA. cuDLA enables you to submit DLA tasks using CUDA programming constructs. You can run inference using cuDLA either implicitly through TensorRT runtime or you can explicitly call the cuDLA APIs. This sample demonstrates the latter approach to explicitly call cuDLA APIs to run inference in <a href=\"https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html#memory-model\">hybrid mode</a> and <a href=\"https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html#memory-model\">standalone mode</a>.</p>\n\n\n\n<p>cuDLA hybrid mode and standalone mode mainly differ in synchronization. In hybrid mode, DLA tasks are submitted to a CUDA stream, so synchronization can be done seamlessly with other CUDA tasks.&nbsp;</p>\n\n\n\n<p>In standalone mode, the <code>cudlaTask</code> structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively, as part of <code>cudlaSubmitTask</code>.&nbsp;</p>\n\n\n\n<p>In short, using cuDLA hybrid mode can give quick integration with other CUDA tasks. Using cuDLA standalone mode can prevent the creation of CUDA context, and thus can save resources if the pipeline has no CUDA context.</p>\n\n\n\n<p>The primary cuDLA APIs used in this YOLOv5 sample are detailed below.</p>\n\n\n\n<ul>\n<li><a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1gf9d00e3a93dfd31814736144764ae478\"><code>cudlaCreateDevice</code></a> creates the DLA device.</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1gcd725924569cec1a3214fd09cb38601d\"><code>cudlaModuleLoadFromMemory</code></a> loads the engine memory for DLA use.</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356\"><code>cudaMalloc</code></a> and <a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1ge07f8bb22373163a0117fc5738a23be0\"><code>cudlaMemRegister</code></a> are called to first allocate memory on GPU, then let the CUDA pointer be registered with the DLA. (Used only for hybrid mode.)</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1gca69cd7ac008500693ffeedb18d7a9c8\"><code>cudlaImportExternalMemory</code></a> and <a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1g12751fbcc295349c16ad3aea0e8bda34\"><code>cudlaImportExternalSemaphore</code></a> are called to import external NvSci buffers and sync objects. (Used only for standalone mode.)</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1g7c7e68c05dbc5a7f7ea011c8e3285a7a\"><code>cudlaModuleGetAttributes</code></a> gets module attributes from the loaded module.</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cudla-api/index.html#group__CUDLA__API_1gc560a614b388d50216bd161c0b3d88cb\"><code>cudlaSubmitTask</code></a> is called to submit the inference task. In hybrid mode, users need to specify the CUDA stream to let cuDLA tasks run on it. In standalone mode, users need to specify the signal event and wait event to let cuDLA wait and signal when the corresponding fence expires.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">On-target validation and profiling</h2>\n\n\n\n<p>It&#8217;s important to note the numerical differences between GPU to DLA. The underlying hardware is different, so the computations are not bit-wise accurate. Because training the network is done on the GPU and then deployed to DLA on the target, it&#8217;s important to validate on the target. This specifically comes into play when it comes to quantization. It&#8217;s also important to compare against a reference baseline.</p>\n\n\n\n<h3 class=\"wp-block-heading\">YOLOv5 DLA accuracy validation&nbsp;</h3>\n\n\n\n<p>We used the COCO dataset to validate. Figure 3 shows the inference pipeline architecture. First, load the image data and normalize it. Extra reformats on the inference inputs and outputs are needed because DLA only supports INT8/FP16.&nbsp;</p>\n\n\n\n<p>After inference, decode the inference result and perform NMS (non-maximum suppression) to get the detection result. Finally, save the result and compute mAP.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1566\" height=\"819\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping.png\" alt=\"Schematic showing the entire pipeline with tasks mapped to DLA, GPU, and CPU.\" class=\"wp-image-70017\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping.png 1566w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-625x327.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-768x402.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-1536x803.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-645x337.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-500x261.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-362x189.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-210x110.png 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/inference-pipeline-dla-gpu-cpu-task-mapping-1024x536.png 1024w\" sizes=\"(max-width: 1566px) 100vw, 1566px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Inference pipeline with tasks mapped to the different compute engines</em></em></figcaption></figure></div>\n\n\n<p>In the case of YOLOv5, the feature maps of the last three convolution layers encode final detection information. When quantized to INT8, the quantization error of the bounding box coordinates becomes noticeable compared to FP16/FP32, thus affecting the final mAP.&nbsp;</p>\n\n\n\n<p>Our experiment shows that running the last three convolution layers in FP16 improves the final mAP from 35.9 to 37.1. Orin DLA has a special hardware design highly optimized for INT8, so we observe a performance drop when these three convolutions run in FP16.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"907\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations.png\" alt=\"Diagram showing different mixed precision configurations of the last three convolution layers of the YOLOv5 engine. \n\" class=\"wp-image-70018\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-300x136.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-625x284.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-768x348.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-1536x697.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-645x293.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-500x227.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-362x164.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-242x110.png 242w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/yolov5-engine-precision-configurations-1024x465.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. YOLOv5 engine with different precision configurations</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td></td><td>Configuration 1</td><td>Configuration 2</td><td>Configuration 3</td></tr><tr><td>Input tensor format</td><td>INT8:DLA_HWC4</td><td>INT8:DLA_HWC4</td><td>INT8:DLA_HWC4</td></tr><tr><td>Output tensor format</td><td>INT8:CHW32</td><td>FP16:CHW16</td><td>FP16:CHW16</td></tr><tr><td>COCO Val mAP</td><td>35.9</td><td>37.1</td><td>37.3</td></tr><tr><td>FPS (DLA 3.14.0, 1x DLA @ 1.33 GHz, EMC @ 3.2 GHz)</td><td>410</td><td>255</td><td>252</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Configurations exploring mixed precision for the last three convolution layers</em></figcaption></figure>\n\n\n\n<p>Note that the mAP results are based on Option 1 described in the preceding section on adding Q/DQ nodes. You can apply the same principle to Option 2 as well.</p>\n\n\n\n<h3 class=\"wp-block-heading\">YOLOv5 DLA performance&nbsp;</h3>\n\n\n\n<p>DLA offers one-third of AI compute on Orin AGX platforms, thanks to the two DLA cores. For a general baseline of Orin DLA performance, see <a href=\"https://github.com/NVIDIA/Deep-Learning-Accelerator-SW#orin-dla-performance\">Deep-Learning-Accelerator-SW</a> on GitHub.</p>\n\n\n\n<p>In the latest release, DLA 3.14.0 (DOS 6.0.8.0 and JetPack 6.0), several performance optimizations were added to the DLA compiler that specifically apply for INT8 CNN architecture-based models: </p>\n\n\n\n<ol>\n<li>Native INT8 Sigmoid (previously ran in FP16 and had to be cast to and from INT8; also applies to Tanh)</li>\n\n\n\n<li>INT8 SiLU fusion into a single DLA HW operation (instead of standalone Sigmoid plus standalone elementwise Mul)</li>\n\n\n\n<li>Fusing the INT8 SiLU HW op with the previous INT8 Conv HW op (also applies to standalone Sigmoid or Tanh)</li>\n</ol>\n\n\n\n<p>These improvements can provide a 6x speedup for YOLO architectures compared to prior releases. For instance, in the case of YOLOv5, the inference performance jumped from 13 ms to 2.4 ms in INT8 (with a few layers running in FP16), which is a 5.4x improvement. Further, you can use the <a href=\"https://github.com/NVIDIA/Deep-Learning-Accelerator-SW/tree/main/samples/cuDLA/cuDLALayerwiseStatsHybrid\">cuDLA sample</a> to profile your DNN layer-wise, identify bottlenecks, and modify your network to improve its performance.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started with DLA</h2>\n\n\n\n<p>This post explains how to run an entire object detection pipeline on Orin in the most efficient way using YOLOv5 on its dedicated Deep Learning Accelerator. Keep in mind that other SoC components such as the GPU are either idling or running at very small load. If you had a single camera producing inputs at 30 fps, one DLA instance would only be loaded at about 10%. So there is plenty of headroom for adding more bells and whistles to your application.</p>\n\n\n\n<p>Ready to dive in? The YOLOv5 sample replicates the entire workflow discussed here. You can use it as a reference point for your own use case.</p>\n\n\n\n<p>For beginners, the <a href=\"https://github.com/NVIDIA-AI-IOT/jetson_dla_tutorial\">Jetson_dla_tutorial</a> on GitHub demonstrates a basic DLA workflow to help you get started deploying a simple model to <a href=\"https://developer.nvidia.com/deep-learning-accelerator\">DLA</a>.&nbsp;</p>\n\n\n\n<p>For additional samples and resources on leveraging DLA to get the most out of <a href=\"https://developer.nvidia.com/drive\">NVIDIA DRIVE</a> or NVIDIA Jetson, visit <a href=\"https://github.com/NVIDIA/Deep-Learning-Accelerator-SW/\">Deep-Learning-Accelerator-SW</a> on GitHub. For more information about cuDLA, visit <a href=\"https://github.com/NVIDIA/Deep-Learning-Accelerator-SW/tree/main/samples/cuDLA\">Deep-Learning-Accelerator-SW/samples/cuDLA</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how to run an entire object detection pipeline on Orin in the most efficient way using YOLOv5 on its dedicated Deep Learning Accelerator.</p>\n", "protected": false}, "author": 1859, "featured_media": 70249, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1253031", "discourse_permalink": "https://forums.developer.nvidia.com/t/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/265080", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63], "tags": [296, 3477, 2850, 453, 2792, 3314, 367], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/YOLO-DLA-FeatureImage.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-icY", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69996"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1859"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=69996"}], "version-history": [{"count": 18, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69996/revisions"}], "predecessor-version": [{"id": 70187, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/69996/revisions/70187"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70249"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=69996"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=69996"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=69996"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70072, "date": "2023-08-31T09:00:00", "date_gmt": "2023-08-31T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70072"}, "modified": "2023-12-05T10:55:58", "modified_gmt": "2023-12-05T18:55:58", "slug": "introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/", "title": {"rendered": "Introduction to Graph Neural Networks with NVIDIA cuGraph-DGL"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://blogs.nvidia.com/blog/2022/10/24/what-are-graph-neural-networks/\">Graph neural networks</a> (GNNs) have emerged as a powerful tool for a variety of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> tasks on graph-structured data. These tasks range from node classification and link prediction to graph classification. They also cover a wide range of applications such as social network analysis, drug discovery in healthcare, fraud detection in financial services, and molecular chemistry.</p>\n\n\n\n<p>In this post, I introduce how to use cuGraph-DGL, a GPU-accelerated library for graph computations. It extends Deep Graph Library (DGL), a popular framework for GNNs that enables large-scale applications.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Basics of graph neural networks</h2>\n\n\n\n<p>Before I dive into cuGraph-DGL, I want to establish some basics. GNNs are a special kind of neural network designed to work with data structured as graphs. Unlike traditional neural networks that assume independence between samples, which doesn\u2019t fit well with graph data, GNNs effectively exploit the rich and complex interconnections within graph data.</p>\n\n\n\n<p>In a nutshell, GNNs work by propagating and transforming node features across the graph structure in multiple steps, often referred to as layers (Figure 1). Each layer updates the features of each node based on its own features and the features of its neighbors.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic.png\" alt=\"Diagram shows the initial graph on the left and the update graph on the right. Node features are respected by using information from its neighbors and graph structure.\" class=\"wp-image-70078\" width=\"1000\" height=\"372\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-300x112.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-625x233.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-179x67.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-768x286.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-1536x572.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-645x240.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-500x186.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-160x60.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-362x135.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-296x110.png 296w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/message-passing-layer-schematic-1024x381.png 1024w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Schematic for the message passing layer </em>(source: <a href=\"https://distill.pub/2021/gnn-intro/\">Distill</a>)</figcaption></figure></div>\n\n\n<p>In Figure 1, the first step \u201cprepares\u201d a message composed of information from an edge and its connected nodes and then \u201cpasses\u201d the message to the node. This process enables the model to learn high-level representations of nodes, edges, and the graph as a whole, which can be used for various downstream tasks like node classification, link prediction, and graph classification.</p>\n\n\n\n<p>Figure 2 shows how a 2-layer GNN is supposed to compute the output of node 5.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"800\" height=\"400\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/sampling-cuGraph-DGL.gif\" alt=\"GIF shows a two-layer GNN updating the seed node\u2019s embeddings. First, immediate neighbors\u2019 embeddings are refreshed using data from neighbors two hops away. Then, these updated embeddings are used to update the seed node\u2019s representation.\" class=\"wp-image-70241\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Update of embeddings on a single node in a 2-layer GNN</em> (source: <a href=\"https://docs.dgl.ai/tutorials/large/L0_neighbor_sampling_overview.html#sphx-glr-tutorials-large-l0-neighbor-sampling-overview-py\">DGL documentation</a>)</figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Bottlenecks when handling large-scale graphs</h2>\n\n\n\n<p>The bottleneck in GNN sampling and training is the lack of an existing implementation that can scale to handle billions or even trillions of edges, a scale often seen in real-world graph problems. For example, if you\u2019re handling a graph with trillions of edges, you must be able to run DGL-based GNN workflows quickly.</p>\n\n\n\n<p>One solution is to use RAPIDS, which already possesses the foundational elements capable of scaling to trillions of edges using GPUs.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>What is RAPIDS cuGraph?</h3>\n\n\n\n<p><a href=\"https://docs.rapids.ai/api/cugraph/stable/\">cuGraph</a> is a part of the RAPIDS AI ecosystem, an open-source suite of software libraries for executing end-to-end data science and analytics pipelines entirely on GPUs. The cuGraph library provides a simple, flexible, and powerful API for graph analytics, enabling you to perform computations on graph data at scale and speed.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>What is DGL?</h3>\n\n\n\n<p><a href=\"https://www.dgl.ai/\">Deep Graph Library (DGL)</a> is a Python library designed to simplify the implementation of graph neural networks (GNNs) by providing intuitive interfaces and high-performance computation.</p>\n\n\n\n<p>DGL supports a broad array of graph operations and structures, enhancing the modeling of complex systems and relationships. It also integrates with popular deep learning frameworks like PyTorch and TensorFlow, fostering seamless development and deployment of GNNs.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>What is cuGraph-DGL?</h3>\n\n\n\n<p>cuGraph-DGL is an extension of cuGraph that integrates with the Deep Graph Library (DGL) to leverage the power of GPUs to run DGL-based GNN workflows at unprecedented speed. This library is a collaborative effort between DGL developers and cuGraph developers.</p>\n\n\n\n<p>In addition to cuGraph-DGL, cuGraph also provides the cugraph-ops library, which enables DGL users to get performance boosts using <a href=\"https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.CuGraphSAGEConv.html#dgl.nn.pytorch.conv.CuGraphSAGEConv\">CuGraphSAGEConv</a>, <a href=\"https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.CuGraphGATConv.html#dgl.nn.pytorch.conv.CuGraphGATConv\">CuGraphGATConv</a>, and <a href=\"https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.CuGraphRelGraphConv.html#dgl.nn.pytorch.conv.CuGraphRelGraphConv\">CuGraphRelGraphConv</a> in place of the default <a href=\"https://docs.dgl.ai/en/0.9.x/generated/dgl.nn.pytorch.conv.SAGEConv.html\">SAGEConv</a>, <a href=\"https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GATConv.html\">GATConv</a>, and <a href=\"https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.RelGraphConv.html#dgl.nn.pytorch.conv.RelGraphConv\">RelGraphConv</a> models. You can also import the SAGEConv, GATConv, and RelGraphConv models directly from the <code>cugraph_dgl</code> library.</p>\n\n\n\n<p>In GNN sampling and training, the major challenge is the absence of an implementation that can manage real-world graph problems with billions or trillions of edges. To address this, use cuGraph-DGL, with its inherent capability to scale to trillions of edges using GPUs.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Setting up cuGraph-DGL</h2>\n\n\n\n<p>Before you dive into the code, make sure that you have cuGraph and DGL installed in your Python environment. To install the cuGraph-DGL-enabled environment, run the following command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nconda install mamba -c conda-forge\u00a0\n\nmamba create -n cugraph_dgl_23_06 -c pytorch -c dglteam/label/cu118 -c rapidsai-nightly -c nvidia -c conda-forge dgl cugraph-dgl=23.10 pylibcugraphops=23.10 cudatoolkit=11.8 torchmetrics ogb\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\"><a></a>Implementing a GNN with cuGraph-DGL</h3>\n\n\n\n<p>With your environment set up, put cuGraph-DGL into action and construct a simple GNN for node classification. Converting an existing DGL workflow to a cuGraph-DGL workflow has the following steps:</p>\n\n\n\n<ol type=\"1\">\n<li>Use cuGraph-ops models such as <code>CuGraphSAGECon</code>, in place of the native DGL model (<code>SAGEConv</code>).</li>\n\n\n\n<li>Create a <code>CuGraphGraph</code> object from a DGL graph.</li>\n\n\n\n<li>Use the <code>cuGraph</code> data loader in place of the native DGL Dataloader.</li>\n</ol>\n\n\n\n<p>Using cugraph-dgl on a 3.2 billion-edge graph, we observed a 3x speedup when using eight GPUs for sampling and training, compared to a single GPU UVA DGL setup. Additionally, we saw a 2x speedup when using eight GPUs for sampling and one GPU for training. </p>\n\n\n\n<p>An upcoming blog post will provide more details on the gains and scalability.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>Create a cuGraph-DGL graph</h3>\n\n\n\n<p>To create a <code>cugraph_dgl</code> graph directly from a DGL graph, run the following code example.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nimport dgl\nimport cugraph_dgl\n\ndataset = dgl.data.CoraGraphDataset()\ndgl_g = dataset&#91;0]\n# Add self loops as cugraph\n# does not support isolated vertices yet \ndgl_g = dgl.add_self_loop(dgl_g)\ncugraph_g = cugraph_dgl.convert.cugraph_storage_from_heterograph(dgl_g, single_gpu=True)\n</pre></div>\n\n\n<p>For more information about creating a <code>cuGraph</code> storage object, see <a href=\"https://docs.rapids.ai/api/cugraph/stable/api_docs/api/cugraph-dgl/cugraph_dgl.cugraph_storage.cugraphstorage/#cugraph_dgl.cugraph_storage.CuGraphStorage\">CuGraphStorage</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><a></a>Create a cuGraph-Ops-based model</h3>\n\n\n\n<p>In this step, the only modification to make is the importation of <code>cugraph_ops</code>-based models. These models are drop-in replacements for upstream models like <code>dgl.nn.SAGECon</code>.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n# Drop in replacement for dgl.nn.SAGEConv\nfrom dgl.nn import CuGraphSAGEConv as SAGEConv\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SAGE(nn.Module):\n    def __init__(self, in_size, hid_size, out_size):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        # three-layer GraphSAGE-mean\n        self.layers.append(SAGEConv(in_size, hid_size, &quot;mean&quot;))\n        self.layers.append(SAGEConv(hid_size, hid_size, &quot;mean&quot;))\n        self.layers.append(SAGEConv(hid_size, out_size, &quot;mean&quot;))\n        self.dropout = nn.Dropout(0.5)\n        self.hid_size = hid_size\n        self.out_size = out_size\n\n    def forward(self, blocks, x):\n        h = x\n        for l_id, (layer, block) in enumerate(zip(self.layers, blocks)):\n            h = layer(block, h)\n            if l_id != len(self.layers) - 1:\n                h = F.relu(h)\n                h = self.dropout(h)\n        return h\n\n\n# Create the model with given dimensions\nfeat_size = cugraph_g.ndata&#91;&quot;feat&quot;]&#91;&quot;_N&quot;].shape&#91;1]\nmodel = SAGE(feat_size, 256, dataset.num_classes).to(&quot;cuda&quot;)\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Train the model</h3>\n\n\n\n<p>In this step, you opt to use <code>cugraph_dgl.dataloading.NeighborSampler</code> and <code>cugraph_dgl.dataloading.DataLoader</code>, replacing the conventional data loaders of upstream DGL.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nimport torchmetrics.functional as MF\nimport tempfile\nimport torch\n\ndef train(g, model):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    features = g.ndata&#91;&quot;feat&quot;]&#91;&quot;_N&quot;].to(&quot;cuda&quot;)\n    labels = g.ndata&#91;&quot;label&quot;]&#91;&quot;_N&quot;].to(&quot;cuda&quot;)\n    train_nid = torch.tensor(range(g.num_nodes())).type(torch.int64)\n    temp_dir_name = tempfile.TemporaryDirectory().name\n\n    for epoch in range(10):\n        model.train()\n        sampler = cugraph_dgl.dataloading.NeighborSampler(&#91;10,10,10])\n        dataloader = cugraph_dgl.dataloading.DataLoader(g, train_nid, sampler,\n                                                batch_size=128,\n                                                shuffle=True,\n                                                drop_last=False,\n                                                num_workers=0,\n                                                sampling_output_dir=temp_dir_name)\n\n        total_loss = 0\n        \n        for step, (input_nodes, seeds, blocks) in enumerate((dataloader)):\n            batch_inputs = features&#91;input_nodes]\n            batch_labels = labels&#91;seeds]\n            batch_pred = model(blocks, batch_inputs)\n            loss = F.cross_entropy(batch_pred, batch_labels)\n            total_loss += loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        sampler = cugraph_dgl.dataloading.NeighborSampler(&#91;-1,-1,-1])\n        dataloader = cugraph_dgl.dataloading.DataLoader(g, train_nid, sampler,\n                                                batch_size=1024,\n                                                shuffle=False,\n                                                drop_last=False,\n                                                num_workers=0,\n                                                sampling_output_dir=temp_dir_name)\n\n\n        acc = evaluate(model, features, labels, dataloader)\n        print(&quot;Epoch {:05d} | Acc {:.4f} | Loss {:.4f} &quot;.format(epoch, acc, total_loss))\n\n\ndef evaluate(model, features, labels, dataloader):\n    with torch.no_grad():\n        model.eval()\n        ys = &#91;]\n        y_hats = &#91;]\n        for it, (in_nodes, out_nodes, blocks) in enumerate(dataloader):\n            with torch.no_grad():\n                x = features&#91;in_nodes]\n                ys.append(labels&#91;out_nodes])\n                y_hats.append(model(blocks, x))\n        num_classes = y_hats&#91;0].shape&#91;1]\n        return MF.accuracy(\n            torch.cat(y_hats),\n            torch.cat(ys),\n            task=&quot;multiclass&quot;,\n            num_classes=num_classes,\n        )\n\ntrain(cugraph_g, model)\n\nEpoch 00000 | Acc 0.3401 | Loss 39.3890 \nEpoch 00001 | Acc 0.7164 | Loss 27.8906 \nEpoch 00002 | Acc 0.7888 | Loss 16.9441 \nEpoch 00003 | Acc 0.8589 | Loss 12.5475 \nEpoch 00004 | Acc 0.8863 | Loss 9.9894 \nEpoch 00005 | Acc 0.8948 | Loss 9.0556 \nEpoch 00006 | Acc 0.9029 | Loss 7.3637 \nEpoch 00007 | Acc 0.9055 | Loss 7.2541 \nEpoch 00008 | Acc 0.9132 | Loss 6.6912 \nEpoch 00009 | Acc 0.9121 | Loss 7.0908\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>By combining the power of GPU-accelerated graph computations with the flexibility of DGL, cuGraph-DGL emerges as an invaluable tool for anyone dealing with graph data.</p>\n\n\n\n<p>This post has only scratched the surface of what you can do with cuGraph-DGL. I encourage you to explore further, experiment with different GNN architectures, and discover how cuGraph-DGL can accelerate your graph-based, machine-learning tasks. </p>\n\n\n\n<p>Read&nbsp;<a rel=\"noreferrer noopener\" href=\"https://medium.com/rapids-ai/intro-to-graph-neural-networks-with-cugraph-pyg-6fe32c93a2d0\" target=\"_blank\">Intro to Graph Neural Networks with cuGraph-PyG</a>&nbsp;for details on how to implement GNNs in the cuGraph-PyG ecosystem</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Graph neural networks (GNNs) have emerged as a powerful tool for a variety of machine learning tasks on graph-structured data. These tasks range from node classification and link prediction to graph classification. They also cover a wide range of applications such as social network analysis, drug discovery in healthcare, fraud detection in financial services, and &hellip; <a href=\"https://developer.nvidia.com/blog/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/\">Continued</a></p>\n", "protected": false}, "author": 1043, "featured_media": 70073, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1253011", "discourse_permalink": "https://forums.developer.nvidia.com/t/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/265076", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 3550, 3052], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/intro-to-gnns-16x9-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iec", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70072"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1043"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70072"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70072/revisions"}], "predecessor-version": [{"id": 70246, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70072/revisions/70246"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70073"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70072"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70072"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70072"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70110, "date": "2023-08-30T12:20:39", "date_gmt": "2023-08-30T19:20:39", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70110"}, "modified": "2023-09-07T11:39:26", "modified_gmt": "2023-09-07T18:39:26", "slug": "how-to-build-a-distributed-inference-cache-with-nvidia-triton-and-redis", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/how-to-build-a-distributed-inference-cache-with-nvidia-triton-and-redis/", "title": {"rendered": "How to Build a Distributed Inference Cache with NVIDIA Triton and Redis"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Caching is as fundamental to computing as arrays, symbols, or strings. Various layers of caching throughout the stack hold instructions from memory while pending on your CPU. They enable you to reload the page quickly and without re-authenticating, should you navigate away. They also dramatically decrease application workloads, and increase throughput by not re-running the same queries repeatedly.</p>\n\n\n\n<p>Caching is not new to <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, which is a system tuned to answering questions in the form of running inferences on tensors. Running inferences is a relatively computationally expensive task that often calls on the same inference to run repeatedly. This naturally lends itself to using a caching pattern.&nbsp;</p>\n\n\n\n<p>The NVIDIA Triton team recently implemented the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/response_cache.html#triton-response-cache\">Triton response cache</a> using the <a href=\"https://github.com/triton-inference-server/local_cache\">Triton local cache</a> library. They have also built a <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/response_cache.md\">cache API</a> to make this caching pattern extensible within Triton. The Redis team then leveraged that API to build the <a href=\"https://github.com/triton-inference-server/redis_cache\">Redis cache for NVIDIA Triton</a>.</p>\n\n\n\n<p>In this post, the Redis team explores the benefits of the new <a href=\"https://github.com/triton-inference-server/redis_cache\">Redis implementation</a> of the Triton Caching API. We explore how to get started and discuss some of the best practices for using Redis to supercharge your NVIDIA Triton instance.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What is Redis?</h2>\n\n\n\n<p>Redis is an acronym for REmote DIctionary Server. It is a NoSQL database that operates as a key-value data structure store. Redis is memory-first, meaning that the entire dataset in Redis is stored in memory, and optionally persisted to disk, based on configuration. Because it is a key-value database completely held in memory, Redis is blazingly fast. Execution times are measured in microseconds, and throughputs in tens of thousands of operations a second.</p>\n\n\n\n<p>The remarkable speed and typical access pattern of Redis make it ideal for caching. Redis is synonymous with caching and is consequentially one of the built-in distributed caches of most major application frameworks across a variety of developer communities.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What is local cache?</h2>\n\n\n\n<p>The local cache is an in-memory derivation of the most common caching pattern out there (cache-aside). It is simple and efficient, making it easy to grasp and implement. After receiving a query, NVIDIA Triton:</p>\n\n\n\n<ol>\n<li>Computes a hash of the input query, including the tensor and some metadata. This becomes the inference key.</li>\n\n\n\n<li>Checks for a previously inferred result for that tensor at that key.</li>\n\n\n\n<li>Returns any results found.&nbsp;</li>\n\n\n\n<li>Performs the inference if no results are found.&nbsp;</li>\n\n\n\n<li>Caches the inference in memory using the key for storage.</li>\n\n\n\n<li>Returns the inference.</li>\n</ol>\n\n\n\n<p>\u2018Local\u2019 means that it is staying local to the process and storing the cache in the system\u2019s main memory. Figure 1 shows the implementation of this pattern.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1028\" height=\"672\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache.png\" alt=\"Diagram showing how the local cache works in NVIDIA Triton. The server checks the cache; if it has the inference, return the inference. If not, run the tensor through the model, cache the outputted inference, and return the outputted inference.\n\" class=\"wp-image-70123\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache.png 1028w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-300x196.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-625x409.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-176x115.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-768x502.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-645x422.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-459x300.png 459w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-138x90.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-362x237.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-168x110.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-local-cache-1024x669.png 1024w\" sizes=\"(max-width: 1028px) 100vw, 1028px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA Triton using the local cache</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Benefits of local cache</h3>\n\n\n\n<p>There are a variety of benefits that flow naturally from using this pattern. Because the queries are cached, they can be retrieved again easily without rerunning the tensor through the models. Because everything is maintained locally in the process memory, there is no need to leave the process or machine to retrieve the cached data. These two in concert can dramatically increase throughput, as well as decrease the cost of this computation.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Drawbacks of local cache</h3>\n\n\n\n<p>This technique does have drawbacks. Because the cache is tied directly into the process memory, each time the Triton process restarts, it starts from square one (generally referred to as a cold start). You will not see the benefits from caching while the cache warms up. Also, because the cache is process-locked, other instances of Triton will not be able to share the cache, leading to duplication of caching across each node.</p>\n\n\n\n<p>The other major drawback concerns resource contention. Since the local cache is tied to the process, it is limited to the resources of the system that Triton runs on. This means that it is impossible to horizontally scale the resources allocated to the cache (distributing the cache across multiple machines), which limits the options for expanding the local cache to vertical scaling. This makes the server running Triton bigger.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Benefits of distributed caching with Redis</h2>\n\n\n\n<p>Unlike local caching, distributed caching leverages an external service (such as Redis) to distribute the cache off the local server. This confers several advantages to the NVIDIA Triton caching API:</p>\n\n\n\n<ul>\n<li>Redis is not bound to the available system resources of the same machine as Triton, or for that matter, a single machine.</li>\n\n\n\n<li>Redis is decoupled from Triton\u2019s process life cycle, enabling multiple Triton instances to leverage the same cache.</li>\n\n\n\n<li>Redis is extremely fast (execution times are typically sub-milliseconds).</li>\n\n\n\n<li>Redis is a significantly more specialized, feature-rich, and tunable caching service compared to the Triton local cache.&nbsp;</li>\n\n\n\n<li>Redis provides immediate access to tried and tested high availability, horizontal scaling, and cache-eviction features out of the box.</li>\n</ul>\n\n\n\n<p>Distributed caching with Redis works much the same way as the local cache. Rather than staying within the same process, it crosses out of the Triton server process to Redis to check the cache and store inferences. After receiving a query, NVIDIA Triton:</p>\n\n\n\n<ol>\n<li>Computes a hash of the input query, including the tensor and some metadata. This becomes the inference key.</li>\n\n\n\n<li>Checks Redis for a previous run inference.</li>\n\n\n\n<li>Returns that inference, if it exists.</li>\n\n\n\n<li>Runs the tensor through Triton if the inference does not exist.</li>\n\n\n\n<li>Stores the inference in Redis.</li>\n\n\n\n<li>Returns the inference.</li>\n</ol>\n\n\n\n<p>Architecturally, this is shown in Figure 2.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1371\" height=\"623\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache.png\" alt=\"Diagram showing how the Triton Inference Server uses Redis as a cache. Similar to how it uses the local cache, but reaching out to the external service Redis for caching.\" class=\"wp-image-70128\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache.png 1371w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-300x136.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-625x284.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-768x349.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-645x293.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-500x227.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-362x164.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-242x110.png 242w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/nvidia-triton-redis-cache-1024x465.png 1024w\" sizes=\"(max-width: 1371px) 100vw, 1371px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA Triton using Redis as its caching layer</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Distributed cache set up and configuration</h2>\n\n\n\n<p>To set up the distributed Redis cache requires two top-level steps:</p>\n\n\n\n<ol>\n<li>Deploy your Redis instance.</li>\n\n\n\n<li>Configure NVIDIA Triton to point at the Redis instance.</li>\n</ol>\n\n\n\n<p>Triton will take care of the rest for you. To learn more about Redis, see <a href=\"https://redis.io/docs/getting-started/\">redis.io</a>, <a href=\"https://docs.redis.com/latest/index.html\">docs.redis.com</a>, and <a href=\"https://university.redis.com/\">Redis University</a>.</p>\n\n\n\n<p>To configure Triton to point at your Redis instance, use the <code>--cache-config</code> options in your start command. In the model config,\u00a0<a rel=\"noreferrer noopener\" href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/response_cache.md#enable-caching-for-a-model\" target=\"_blank\">enable the response cache for the model</a>\u00a0with\u00a0<code>{{response_cache { enable: true }}}</code>.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>tritonserver --cache-config redis,host=localhost --cache-config redis,port=6379</code></pre>\n\n\n\n<p>The Redis cache calls on you to minimally configure the host and port of your Redis instance. For a full enumeration of configuration options, see the <a href=\"https://github.com/triton-inference-server/redis_cache\">Triton Redis Cache</a> GitHub repo.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Best practices with Redis</h2>\n\n\n\n<p>Redis is lightweight, easy to use, and extremely fast. Even with its small footprint and simplicity, there is much you can configure in and around Redis to optimize it for your use case. This section highlights best practices for using and configuring Redis.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Minimize round-trip time</h3>\n\n\n\n<p>The only real drawback of using an external service like Redis over an in-process memory cache is that the queries to Redis will, at least, have to cross process. They typically need to cross server boundaries as well.&nbsp;</p>\n\n\n\n<p>Because of this, minimizing round-trip times (RTT) is of paramount importance in optimizing the use of Redis as a cache. The topic of how to minimize RTT is far too complex a topic to dive into in this post. A couple of key tips: maintain the locality of your Redis servers to your Triton servers and have them physically close to each other. If they are in a data center, try to keep them in the same rack or availability zone.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Scaling and high availability</h3>\n\n\n\n<p><a href=\"https://docs.redis.com/latest/rs/databases/durability-ha/clustering/\">Redis Cluster</a> enables you to scale your Redis instances horizontally over multiple shards. The cluster includes the ability to replicate your Redis instance. If there is a failure in your primary shard, the replica can be promoted for high availability.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Maximum memory and eviction</h3>\n\n\n\n<p>If Redis memory is not capped, it will use all the available memory on the system that the OS will release to it. Set the <a href=\"https://github.com/redis/redis/blob/9b1d4f003de1b141ea850f01e7104e7e5c670620/redis.conf#L1127\">maxmemory</a> configuration key in redis.conf. But what happens if you set maxmemory and Redis runs out of memory? The default is, as you might expect, to stop accepting new writes to Redis.&nbsp;</p>\n\n\n\n<p>However, you can also set an eviction policy. An eviction policy uses some basic intelligence to decide which keys might be good candidates to kick out of Redis. Allowing Redis to evict keys that no longer make sense to store enables it to continue accepting new writes without interruption when the memory fills.</p>\n\n\n\n<p>For a full explanation of different Redis eviction policies, see <a href=\"https://redis.io/docs/reference/eviction/#eviction-policies\">key eviction</a> in the Redis manual.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Durability and persistence</h3>\n\n\n\n<p>Redis is memory-first, meaning everything is stored in memory. If you do not configure persistence and the Redis process dies, it will essentially return to a cold-started state. (The cache will need to \u2018warm up\u2019 before you get the benefits from caching.)&nbsp;</p>\n\n\n\n<p>There are two options for persisting Redis. Taking periodic snapshots of the state of Redis in .rdb files and keeping a log of all write commands in the append-only file. For a full explanation of these methods, see <a href=\"https://redis.io/docs/management/persistence/\">persistence</a> in the Redis manual.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Speed comparison</h2>\n\n\n\n<p>Getting down to brass tacks, this section explores a comprehensive difference between the performance of Triton without Redis and Triton with Redis\u200c. In the interest of simplicity, we leveraged the <a href=\"https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md\">perf_analyzer</a> tool the Triton team built for measuring performance with Triton. We tested with two separate models, DenseNet and Simple.&nbsp;</p>\n\n\n\n<p>We ran Triton Server version 23.06 on a Google Cloud Platform (GCP) n1-standard-4 VM with a single NVIDIA T4 GPU. We also ran a vanilla open-source Redis instance on a GCP n2-standard-4 VM. Finally, we ran the Triton client image in Docker on a GCP e2-medium VM.</p>\n\n\n\n<p>We ran the <code>perf_analyzer</code> tool with both the DenseNet and <a href=\"https://github.com/triton-inference-server/server/tree/main/docs/examples/model_repository/simple\">Simple</a> models, 10 times on each caching configuration, with no caching, with Redis as the cache, and with the local cache as the cache. We then averaged the results of these runs.&nbsp;</p>\n\n\n\n<p>It is important to note that these runs assume a 100% cache-hit rate. So, the measurement is the difference between the performance of Triton when it has encountered the entry in the past and when it has not.</p>\n\n\n\n<p>We used the following command for the DenseNet model:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>perf_analyzer -m densenet_onnx -u triton-server:8000</code></pre>\n\n\n\n<p>We used the following command for the Simple model:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>perf_analyzer -m simple -u triton-server:8000</code></pre>\n\n\n\n<p>In the case of the DenseNet model, the results showed that using either cache was dramatically better than running with no cache. Without caching, Triton was able to handle 80 inferences per second (inference/sec) with an average latency of 12,680 \u00b5s. With Redis, it was about 4x faster, processing 329 inference/sec with an average latency of 3,030 \u00b5s.&nbsp;</p>\n\n\n\n<p>Interestingly, while local caching was somewhat faster than Redis, as you would expect it to be, it was only marginally faster. Local caching resulted in a throughput of 355 inference/sec with a latency of 2,817 \u00b5s, only about 8% faster. In this case, it\u2019s clear that the speed tradeoff of caching locally versus in Redis is a marginal one. Given all the extra benefits that come from using a distributed versus a local cache, \u200cdistributed will almost certainly be the way to go when handling these kinds of data.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2.png\" alt=\"Chart showing the throughput comparison for DenseNet for No Cache, Redis, and Local. No Cache is dramatically lower while Redis and Local are close to parity.\n\" class=\"wp-image-70149\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-throughput-comparison-chart-2-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. DenseNet throughput comparison, demonstrating that Redis throughput is comparable to the local cache for computationally expensive inferences</em></figcaption></figure>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2.png\" alt=\"Chart showing the difference in latency for DenseNet between No Cache, Redis, and Local. Again No cache's latency is quite high while Redis and Local are near parity.\" class=\"wp-image-70151\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/densenet-latency-comparison-chart-2-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. DenseNet latency comparison, demonstrating that Redis latency is comparable to the local cache for computationally expensive inferences</em></figcaption></figure></div>\n\n\n<p>The Simple model tells a slightly more complicated story. In the case of the simple model, not using any cache enabled a throughput of 1,358 inference/sec with a latency of 735 \u00b5s. Redis was somewhat faster with a throughput of 1,639 inference/sec and a latency of 608 \u00b5s. Local was faster than Redis with a throughput of 2,753 inference/sec with a latency of 363 \u00b5s.&nbsp;</p>\n\n\n\n<p>This is an important case to note, as not all uses are created equal. The system of record, in this case, may be fast enough and not worth adding the extra system for the 20% boost in throughput of Redis. Even with the halving of latency in the case of the local cache, it may not be worth the resource contention, depending on other factors such as cache hit rate and available system resources.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart.png\" alt=\"Chart showing the difference in throughput for the simple model between No Cache, Redis, and local cache. No Cache is the slowest with both Redis and Local being somewhat faster.\n\" class=\"wp-image-70143\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-throughput-chart-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Simple model throughput. For computationally inexpensive inferences, there is less of a throughput advantage with Redis over the local cache</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart.png\" alt=\"Chart showing the comparison in latency on the Simple model between no cache, Redis, and local cache. Latency is somewhat higher for no cache than Redis, but not dramatically. Local has about half the latency of no cache.\n\" class=\"wp-image-70144\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/simple-model-latency-comparison-chart-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Simple model latency. For computationally inexpensive inferences, there is less of a latency advantage with Redis over the local cache</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Best practices for managing trade-offs</h2>\n\n\n\n<p>As shown in the experiment, the difference between models, expected inputs, and expected outputs is critically important for assessing what, if any, caching is appropriate for your Triton instance.</p>\n\n\n\n<p>Whether caching adds value is largely a function of how computationally expensive your queries are. The more computationally expensive your queries, the more each query will benefit from caching.</p>\n\n\n\n<p>The relative performance of local versus Redis will largely be a function of how large the output tensors are from the model. The larger the output tensors, the more the transport costs will impact the throughput allowable by Redis.&nbsp;</p>\n\n\n\n<p>Of course, the larger the output tensors are, the fewer output tensors you\u2019ll be able to store in the local cache before you run out of room and begin contending with Triton for resources. Fundamentally, these factors need to be balanced when assessing which caching solution works best for your deployment of Triton.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Benefits</strong></td><td><strong>Drawbacks</strong></td></tr><tr><td>1. Horizontally scalable<br>2. Effectively unlimited memory access<br>3. Enables high availability and disaster recovery<br>4. Removes resource contention&nbsp;<br>5. Minimizes cold starts</td><td>A distributed Redis cache requires calls <br>over the network. Naturally, you can <br>expect somewhat lower throughput and <br>higher latency as compared to the local cache.</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Benefits and drawbacks of using Redis as the caching layer rather than the local cache</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Distributed caching is an old trick that developers use to boost system performance while enabling horizontal scalability and separation of concerns. With the introduction of the Redis Cache for Triton Inference Server, you can now leverage this technique to greatly increase the performance and efficiency of your Triton instance, while managing heavier workloads and enabling multiple Triton instances to share in the same cache. Fundamentally, by offloading caching to Redis, Triton can concentrate its resources on its fundamental role\u2014running inferences.</p>\n\n\n\n<p>Get started with <a href=\"https://github.com/triton-inference-server/redis_cache\">Triton Redis Cache</a> and <a href=\"https://github.com/triton-inference-server/server\">NVIDIA Triton Inference Server</a>. For more information about setting up and administering Redis instances, see <a href=\"https://redis.io/\">redis.io</a> and <a href=\"https://docs.redis.com/latest/index.html\">docs.redis.com</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Explore the benefits of the new Redis implementation of the Triton Caching API, including best practices for using Redis to supercharge your NVIDIA Triton instance.</p>\n", "protected": false}, "author": 1858, "featured_media": 70115, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1252237", "discourse_permalink": "https://forums.developer.nvidia.com/t/how-to-build-a-distributed-inference-cache-with-nvidia-triton-and-redis/264938", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [296, 453, 1953, 1177], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/distributed-triton-cache-redis.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ieO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70110"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1858"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70110"}], "version-history": [{"count": 28, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70110/revisions"}], "predecessor-version": [{"id": 70329, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70110/revisions/70329"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70115"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70110"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70110"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70110"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 70214, "date": "2023-08-30T11:18:33", "date_gmt": "2023-08-30T18:18:33", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=70214"}, "modified": "2023-09-07T11:39:43", "modified_gmt": "2023-09-07T18:39:43", "slug": "workshop-enhancing-data-science-outcomes-with-efficient-workflows", "status": "publish", "type": "post", "link": "https://nvda.ws/3QXKeqo", "title": {"rendered": "Workshop: Enhancing Data Science Outcomes with Efficient Workflows"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn to create an end-to-end machine learning pipeline for large datasets with this virtual, hands-on workshop.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn to create an end-to-end machine learning pipeline for large datasets with this virtual, hands-on workshop.</p>\n", "protected": false}, "author": 1115, "featured_media": 70215, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3QXKeqo", "_links_to_target": "_blank"}, "categories": [696], "tags": [2964, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/08/Enhancing-data-science-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-igu", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70214"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=70214"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70214/revisions"}], "predecessor-version": [{"id": 70229, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/70214/revisions/70229"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/70215"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=70214"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=70214"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=70214"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]