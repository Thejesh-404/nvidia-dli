[{"id": 75493, "date": "2023-12-18T11:44:31", "date_gmt": "2023-12-18T19:44:31", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75493"}, "modified": "2023-12-18T11:44:34", "modified_gmt": "2023-12-18T19:44:34", "slug": "rag-101-demystifying-retrieval-augmented-generation-pipelines", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/", "title": {"rendered": "RAG 101: Demystifying Retrieval-Augmented Generation Pipelines"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large language models</a> (LLMs) have impressed the world with their unprecedented capabilities to comprehend and generate human-like responses. Their chat functionality provides a fast and natural interaction between humans and large corpora of data. For example, they can summarize and extract highlights from data or replace complex queries such as SQL queries with natural language.&nbsp;</p>\n\n\n\n<p>It is tempting to assume that business value can be generated by these models with no extra effort, but this is unfortunately not often the case. Luckily, all that enterprises must do to extract value out of using LLMs is to augment the LLM with their own data. This can be done with retrieval augmented generation (RAG), which is showcased in the <a href=\"https://nvda.ws/41gNtfJ\">NVIDIA Generative AI Examples</a> GitHub repo for developers&nbsp;</p>\n\n\n\n<p>By augmenting an LLM with business data, enterprises can make their AI applications agile and responsive to new developments. For instance:</p>\n\n\n\n<ul>\n<li><strong>Chatbots: </strong>Many enterprises already use AI chatbots to power basic customer interactions on their websites. With RAG, companies can build a chat experience that\u2019s highly specific to their product. For example, questions about product specifications could easily be answered.&nbsp;</li>\n\n\n\n<li><strong>Customer service: </strong>Companies can empower live service representatives to easily answer customer questions with precise, up-to-date information.</li>\n\n\n\n<li><strong>Enterprise search: </strong>Businesses have a wealth of knowledge across the organization, including technical documentation, company policies, IT support articles, and code repositories. Employees could query an internal search engine to retrieve information faster and more efficiently.</li>\n</ul>\n\n\n\n<p>This post explains the benefits of using the RAG technique when building an LLM application, along with the components of a RAG pipeline. For more information after you finish this post, see <a href=\"https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/\">RAG 101: Retrieval-Augmented Generation Questions Answered</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Benefits of RAG</h2>\n\n\n\n<p>There are several advantages of using RAG:</p>\n\n\n\n<ul>\n<li>Empowering LLM solutions with real-time data access</li>\n\n\n\n<li>Preserving data privacy</li>\n\n\n\n<li>Mitigating LLM hallucinations</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Empowering LLM solutions with real-time data access</h3>\n\n\n\n<p>Data is constantly changing in an enterprise. AI solutions that use LLMs can remain up-to-date and current with RAG, which facilitates direct access to additional data resources. These resources can consist of real-time and personalized data.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Preserving data privacy</h3>\n\n\n\n<p>Ensuring data privacy is crucial for enterprises. With a self-hosted LLM (demonstrated in the RAG workflow), sensitive data can be kept on-premises just like the stored data.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Mitigating LLM hallucinations</h3>\n\n\n\n<p>When LLMs are not supplied with factual actual information, they often provide faulty, but convincing responses. This is known as <em>hallucination</em>, and RAG reduces the likelihood of hallucinations by providing the LLM with relevant and factional information.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Building and deploying your first RAG pipeline</h2>\n\n\n\n<p>A typical RAG pipeline consists of several phases. The process of document ingestion occurs offline, and when an online query comes in, the retrieval of relevant documents and the generation of a response occurs.&nbsp;</p>\n\n\n\n<p>Figure 1 shows an accelerated RAG pipeline that can be built and deployed in the <a href=\"https://nvda.ws/41gNtfJ\">/NVIDIA/GenerativeAIExamples</a> GitHub repo.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"2056\" height=\"964\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b.png\" alt=\"Diagram showing retrieval-augmented generation pipeline components.\" class=\"wp-image-75751\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b.png 2056w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-625x293.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-768x360.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-1536x720.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-2048x960.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-645x302.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-500x234.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-235x110.png 235w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-pipeline-ingest-query-flow-b-1024x480.png 1024w\" sizes=\"(max-width: 2056px) 100vw, 2056px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Overview of RAG pipeline components: ingest and query flows</em></figcaption></figure></div>\n\n\n<p>Each logical microservice is separated into containers available in the <a href=\"https://ngc.nvidia.com/\">NGC</a> public catalog. On a high level, the architecture of a RAG system can be distilled down to the pipelines shown in Figure 1:</p>\n\n\n\n<ul>\n<li>A recurring pipeline of document pre-processing, ingestion, and embedding generation</li>\n\n\n\n<li>An inference pipeline with a user query and response generation&nbsp;</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Document ingestion</h3>\n\n\n\n<p>First, raw data from diverse sources, such as databases, documents, or live feeds, is ingested into the RAG system. To pre-process this data, LangChain provides a variety of <a href=\"https://python.langchain.com/docs/integrations/document_loaders\">document loaders</a> that load data of many forms from many different sources.&nbsp;</p>\n\n\n\n<p>The term <em>document loader</em> is used loosely. Source documents do not necessarily need to be what you might think of as standard documents (PDFs, text files, and so on). For example, LangChain supports loading data from Confluence, CSV files, Outlook emails, and <a href=\"https://python.langchain.com/docs/integrations/document_loaders\">more</a>. LlamaIndex also provides a variety of loaders, which can be viewed in <a href=\"https://llamahub.ai/\">LlamaHub</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Document pre-processing</h3>\n\n\n\n<p>After documents have been loaded, they are often transformed. One transformation method is text-splitting, which breaks down long text into smaller segments. This is necessary for fitting the text into the embedding model, <a href=\"https://huggingface.co/intfloat/e5-large-v2\">e5-large-v2</a>, which has a maximum token length of 512. While splitting the text sounds simple, this can be a nuanced process.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Generating embeddings</h3>\n\n\n\n<p>When data is ingested, it must be transformed into a format that the system can efficiently process. Generating embeddings involves converting data into high-dimensional vectors, which represent text in a numerical format.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Storing embeddings in vector databases</h3>\n\n\n\n<p>The processed data and generated embeddings are stored in specialized databases known as vector databases. These databases are optimized to handle vectorized data, enabling rapid search and retrieval operations. Storing the data in <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/\">RAPIDS RAFT accelerated vector databases</a> like <a href=\"https://github.com/milvus-io/milvus\">Milvus</a> guarantees that information remains accessible and can be <a href=\"https://www.globenewswire.com/en/news-release/2023/03/21/2631737/0/en/Milvus-2-3-Massively-Accelerates-AI-Powered-Applications-With-NVIDIA-GPU-Support.html&amp;sa=D&amp;source=docs&amp;ust=1701894993444578&amp;usg=AOvVaw3GcAkITfBguNwsyhlk_ivW\">quickly retrieved during real-time interactions</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">LLMs</h3>\n\n\n\n<p>LLMs form the foundational generative component of the RAG pipeline. These advanced, generalized language models are trained on vast datasets, enabling them to understand and generate human-like text. In the context of RAG, LLMs are used to generate fully formed responses based on the user query and contextual information retrieved from the vector DBs during user queries.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Querying</h3>\n\n\n\n<p>When a user submits a query, the RAG system uses the indexed data and vectors to perform efficient searches. The system identifies relevant information by comparing the query vector with the stored vectors in the vector DBs. The LLMs then use the retrieved data to craft appropriate responses.</p>\n\n\n\n<p>Fast-track your deployment of this system by testing out this example workflow in the <a href=\"https://nvda.ws/41gNtfJ\">/NVIDIA/GenerativeAIExamples</a> GitHub repo.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started building RAG in your enterprise</h2>\n\n\n\n<p>By using RAG, you can provide up-to-date and proprietary information with ease to LLMs and build a system that increases user trust, improves user experiences, and reduces hallucinations.</p>\n\n\n\n<p>Explore the NVIDIA <a href=\"https://nvda.ws/47OvlMU\">AI chatbot RAG workflow</a> to get started building a chatbot that can accurately answer domain-specific questions in natural language using up-to-date information.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) have impressed the world with their unprecedented capabilities to comprehend and generate human-like responses. Their chat functionality provides a fast and natural interaction between humans and large corpora of data. For example, they can summarize and extract highlights from data or replace complex queries such as SQL queries with natural language.&nbsp; &hellip; <a href=\"https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/\">Continued</a></p>\n", "protected": false}, "author": 1973, "featured_media": 75516, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1318758", "discourse_permalink": "https://forums.developer.nvidia.com/t/rag-101-demystifying-retrieval-augmented-generation-pipelines/276500", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/rag-101-featured-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jDD", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75493"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1973"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75493"}], "version-history": [{"count": 24, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75493/revisions"}], "predecessor-version": [{"id": 75807, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75493/revisions/75807"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75516"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75493"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75493"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75493"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74632, "date": "2023-12-18T09:00:00", "date_gmt": "2023-12-18T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74632"}, "modified": "2023-12-18T07:04:25", "modified_gmt": "2023-12-18T15:04:25", "slug": "deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/", "title": {"rendered": "Deploying Retrieval-Augmented Generation Applications on NVIDIA GH200 Delivers Accelerated Performance"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large language model</a> (LLM) applications are essential in enhancing <a href=\"https://blogs.nvidia.com/blog/generative-ai-for-industries/\">productivity across industries</a> through natural language. However, their effectiveness is often limited by the extent of their training data, resulting in poor performance when dealing with real-time events and new knowledge the LLM isn\u2019t trained on.</p>\n\n\n\n<p><a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">Retrieval-augmented generation (RAG)</a> solves these problems. By augmenting models with an external knowledge base, RAG can ground LLMs with relevant data. This enhances the quality of their response, improving accuracy, and reducing hallucinations. RAG not only extends the utility of LLMs but also provides a cheaper alternative to time-consuming re-training runs.&nbsp;</p>\n\n\n\n<p>Deploying and scaling RAG applications for tens of thousands to millions of users comes with its own set of challenges, specifically around GPU memory management. Developers need access to state-of-the-art infrastructure with robust memory capabilities that runs real-time RAG applications performantly and within stringent service level agreements (SLAs).&nbsp;</p>\n\n\n\n<p>This is where the NVIDIA end-to-end full-stack accelerated computing solution shines. In this post, we discuss how the <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA GH200 Grace Hopper Superchip</a> can help solve these issues. </p>\n\n\n\n<p>Comparing the GH200 to NVIDIA A100 Tensor Core GPUs, we observed up to a 2.7x increase in speed for embedding generation, 2.9x for index build, 3.3x for vector search time, and 5.7x for Llama-2-70B (FP8) inference performance.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Memory challenges when deploying RAG applications at scale</h2>\n\n\n\n<p>One significant challenge for developers deploying large-scale RAG applications is managing GPU memory usage. Both GPU memory capacity and bandwidth are vital for optimal inference performance, and when limited, often act as a bottleneck for crucial tasks such as:&nbsp;</p>\n\n\n\n<ul>\n<li>Hosting LLMs</li>\n\n\n\n<li>Processing batched requests</li>\n\n\n\n<li>Handling Key-Value (KV) cache in attention mechanisms</li>\n\n\n\n<li>Facilitating efficient data transfer between the GPU and CPU</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Model hosting</h3>\n\n\n\n<p>While larger models tend to provide more precise and comprehensive responses, they require more GPU memory. This can make deployments for models like Llama-2-70B difficult to manage.&nbsp;</p>\n\n\n\n<p>Llama-2-70B (FP16) has weights that take up 140 GB of GPU memory alone. Developers often resort to techniques like model sharding across multiple GPUs, which ultimately add latency and complexity.&nbsp;</p>\n\n\n\n<p>In addition to hosting the LLM, the GPU must host an embedding model and a vector database. These components may not require much additional GPU memory (about 10 GB depending on the size of the model and knowledge base.) However, generating embeddings and executing vector search at scale are highly parallel operations and can significantly benefit from GPU acceleration and strong bandwidth between the GPU and CPU.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Batch processing and KV cache</h3>\n\n\n\n<p>Batching enables the GPU to handle multiple requests simultaneously in a single pass through the neural network to boost throughput effectively. However, batch size is directly proportional to KV (key-value) cache size, which represents the memory occupied by the caching of self-attention tensors to avoid redundant computation, and results in large GPU memory requirements. For example, when deploying Llama-2-70B (80 layers) with FP16 precision, batch size 32, and context size 4096, the size of the KV cache comes out to around 40 GB.</p>\n\n\n\n<p>In a RAG application that is already GPU memory-constrained from model hosting, there exists an upper limit to the batch size during inference. While larger batch sizes do boost throughput, they can also result in higher latency. To meet SLAs, developers must optimize for this throughput and latency tradeoff according to the RAG application\u2019s intended use.&nbsp;</p>\n\n\n\n<p>For instance, an application generating detailed reports from extensive enterprise data can afford higher latency, and therefore larger batch sizes, compared to an application providing real-time customer support.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Data transfer</h3>\n\n\n\n<p>RAG applications at scale require rapid data transfer between the GPU and CPU. For example, during the embedding generation process, the CPU preprocessing (for example, tokenization, cleaning, and so on) of new data takes place before the data is sent to the GPU for accelerated embedding generation. This requires strong bandwidth between the GPU and CPU, which is essential for applications where delays in processing large amounts of new information at scale can lead to outdated or irrelevant results.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Hardware: NVIDIA GH200 Grace Hopper Superchip</h2>\n\n\n\n<p>The NVIDIA GH200 Grace Hopper Superchip tackles GPU memory deployment challenges head-on with its state-of-the-art memory capabilities.&nbsp;</p>\n\n\n\n<p>GH200 is a high-performance GPU-CPU superchip designed for the world\u2019s most demanding AI inference workloads. The architecture combines the performance of the NVIDIA Hopper GPU and the versatility of the NVIDIA Grace CPU in one superchip. They are connected by a high-bandwidth, memory-coherent NVIDIA NVLink-C2C interconnect. This enables the CPU and GPU to talk to each other at 900 GB/s, which is 7x the bandwidth of traditional PCIe Gen5 lanes and 5x the <a href=\"https://www.nvidia.com/en-us/glossary/power-efficiency/\">power efficiency</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1862\" height=\"1060\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM.png\" alt=\"Diagram showcasing the GH200 architecture and the tight coupling of the CPU-GPU with NVLink-C2C 900 GB/s.\" class=\"wp-image-74667\" style=\"aspect-ratio:1.827485380116959;width:625px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM.png 1862w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-300x171.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-625x356.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-768x437.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-1536x874.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-645x367.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-500x285.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-158x90.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-362x206.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-193x110.png 193w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Screenshot-2023-12-01-at-1.53.08\u202fPM-1024x583.png 1024w\" sizes=\"(max-width: 1862px) 100vw, 1862px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. GH200 Architecture</em></figcaption></figure></div>\n\n\n<p>This design, as shown in Figure 1, expands memory capacity significantly. The GH200 features up to 480 GB of LPDDR5X CPU memory and supports up to 144 GB of HBM3e GPU memory, offering up to 624 GB of fast-access memory on a single GPU-CPU superchip. This ultimately increases developer productivity and performance, enabling concurrent, transparent access to both CPU and GPU-resident memory.&nbsp;</p>\n\n\n\n<p>The expanded memory capacity simplifies algorithms and memory management. It also makes the GH200 ideal for handling large batch sizes, making the GPU-CPU superchip an optimal choice for RAG applications generating content for complex queries at scale. However, \u200c expanded memory capacity isn\u2019t the only feature that addresses GPU memory bottlenecks.</p>\n\n\n\n<p>The GH200 features the <a href=\"https://docs.nvidia.com/deeplearning/transformer-engine/index.html\">NVIDIA Transformer Engine</a> as part of the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">Hopper architecture</a>. It can natively support FP8, which reduces the memory footprint for LLMs like Llama2-70B through <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0#quantization\">quantization methods</a> implemented in optimized software like <a href=\"https://developer.nvidia.com/tensorrt#inference\">NVIDIA TensorRT-LLM</a>. This enables large models to fit onto a single GH200.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Software: NVIDIA NeMo Framework, NVIDIA Triton Inference Server, NVIDIA TensorRT-LLM, and NVIDIA RAFT</h2>\n\n\n\n<p>A RAG pipeline includes various software components working together in harmony.&nbsp;</p>\n\n\n\n<p>Optimized software tools sit within different components of the broader <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/docs/architecture.md\">RAG architecture</a> (Figure 2) from embedding generation, vector search, to LLM inference, and ensure a fully accelerated RAG pipeline that delivers the best performance. It includes elements such as the deployment software optimized for LLMs like <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo Framework</a>, <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server </a>and <a href=\"https://developer.nvidia.com/tensorrt#inference\">TensorRT-LLM</a>, and the GPU-accelerated vector database running <a href=\"https://github.com/rapidsai/raft\">NVIDIA RAFT</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1712\" height=\"874\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture.png\" alt=\"Diagram of a RAG inference pipeline using the NVIDIA software services like TensorRT-LLM, Triton Inference Server, and NeMo.\" class=\"wp-image-74635\" style=\"aspect-ratio:1.9588100686498855;width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture.png 1712w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-300x153.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-625x319.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-768x392.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-1536x784.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-645x329.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-500x255.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-215x110.png 215w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-Architecture-1024x523.png 1024w\" sizes=\"(max-width: 1712px) 100vw, 1712px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. RAG Architecture</em></figcaption></figure></div>\n\n\n<p>While each component of this architecture is designed to be open-source and modular, we recommend implementing this RAG architecture with the best NVIDIA models, libraries, tools, and support for optimal performance.&nbsp;</p>\n\n\n\n<p>For example, TensorRT-LLM can supercharge LLM inference beyond quantization methods by implementing techniques like tensor parallelism, which enables model weights to be split across devices when GPU memory is constrained. Triton Model Analyzer, a tool that automatically evaluates model deployment configurations in Triton Inference Server, helps developers optimize for the best dynamic batching and model concurrency parameters that maximize inference performance under strict latency constraints. The NVIDIA RAFT library also includes widely used NVIDIA CUDA-accelerated algorithms like <a href=\"https://docs.rapids.ai/api/raft/nightly/pylibraft_api/neighbors/#ivf-pq\">IVF-PQ</a> for developers to simplify GPU-accelerated vector search.&nbsp;</p>\n\n\n\n<p>Triton and TensorRT-LLM are part of <a href=\"https://blogs.oracle.com/cloud-infrastructure/post/announcing-support-nvidia-ai-enterprise-oci\">NVIDIA AI Enterprise</a>, which features <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/\">support services</a> along with enterprise-grade stability, security, and manageability for open-sourced containers and frameworks that support the RAG pipeline.&nbsp;</p>\n\n\n\n<p>Further details on NVIDIA best practices for the RAG workflow can be found on <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration\">GitHub</a>.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">GH200 RAG inference performance benchmarks</h2>\n\n\n\n<p>When deploying a single GH200 GPU-CPU superchip optimized with NVIDIA software throughout the RAG pipeline, we observe incredible speedups. This includes an increase of <strong>2.7x</strong> in embedding generation, <strong>2.9x</strong> in index build, <strong>3.3x</strong> in vector search time, and <strong>5.7x</strong> in Llama-2-70B inference performance (2048 input length and 128 output length) running on TensorRT-LLM relative to A100.</p>\n\n\n\n<div class=\"wp-block-image aligncenter \"><figure class=\"size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"737\" height=\"508\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing.png\" alt=\"A bar chart showing GH200 and H100 speedups over A100 from 2.7x in embedding generation to 2.9x in index build for GH200 versus A100 and 1.7x in embedding generation to 1.5x in index build for H100 versus A100.\" class=\"wp-image-75701\" style=\"aspect-ratio:1.4507874015748032;width:800px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing.png 737w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-300x207.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-625x431.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-167x115.png 167w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-645x445.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-435x300.png 435w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-131x90.png 131w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-362x250.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RAG-performance-preprocessing-160x110.png 160w\" sizes=\"(max-width: 737px) 100vw, 737px\" /><figcaption class=\"wp-element-caption\">Figure 3. <em>Accelerated RAG performance (preprocessing) for embedding generation and index build (GH200 and H100 speedup over A100)</em></figcaption></figure></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>Preliminary measured performance per GPU, subject to change.&nbsp;<br>Embedding Generation Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory) | Batch Size = 1024 | Output Vectors = 85M of size 768 (251 GB) | Model = Sentence Transformer Paraphrase Multilingual MPNET Base v2 from Hugging Face | Performance scaled linearly from a measurement of 10K text chunks<br>Index Build Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory)&nbsp;</em></p>\n\n\n\n<p>Figure 3 shows GH200 speedups of 2.7x in embedding generation over A100, saving 9 hours on a sample Wikipedia dataset in the RAG preprocessing pipeline. The index build time on GH200 following the embedding generation process shows speedups of 2.9x over A100. </p>\n\n\n\n<p>These workloads are highly parallel operations, and at scale, can benefit significantly from additional GPUs and a strong connection between the GPU and CPU to reduce data transfer time and prevent bottlenecks.&nbsp;</p>\n\n\n\n<div class=\"wp-block-image aligncenter\"><figure class=\"size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"722\" height=\"572\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG.png\" alt=\"A bar chart showing vector search and Llama-2-70B inference speedups with GH200 and H100 over A100 of 3.3x for vector search and 5.7x for Llama-2-70B inference with input length 2048 and output length 128 for GH200 versus A100 and 2.5x for vector search and 3.9x for Llama-2-70B inference for H100 versus A100.\" class=\"wp-image-75722\" style=\"width:800px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG.png 722w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-300x238.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-625x495.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-145x115.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-645x511.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-379x300.png 379w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-114x90.png 114w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-362x287.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-RAG-139x110.png 139w\" sizes=\"(max-width: 722px) 100vw, 722px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Accelerated RAG inference performance for vector search (IVF-PQ) and Llama-2-70B inference (GH200 and H100 speedup over A100)</em></figcaption></figure></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>Preliminary measured performance per GPU, subject to change.&nbsp;<br>Vector Search Time: 1x GH200 (144 GB HBM3e GPU memory) | 1x H100 (80 GB HBM3 GPU Memory) | 1x A100 (80 GB HBM2e GPU memory) | Batch Size = 10,000 | Queries = 10,000 over 85M vectors&nbsp;<br>Llama-2-70B Inference: 1x GH200 (144 GB HBM3e GPU memory) | 2x H100 (80 GB HBM3 GPU Memory) | 4x A100 SXM (80 GB HBM2e GPU memory) | Batch Size = 64 (GH200), 96 (H100s), and 120 (A100s) | Precision = FP8 (GH200 and H100) and FP16 (A100s) | TensorRT-LLM<br>Throughput is measured by output tokens per second per GPU =&nbsp;<br>(Output Length * Batch Size) / (Total end-to-end Latency) / (# GPUs)</em></p>\n\n\n\n<p>After the preprocessing tasks of setting up the external knowledge base are complete, the GH200 further accelerates the RAG inference pipeline. For example, during vector search, the GH200 achieves a<strong> 3.3x speedup</strong> over A100 as shown in Figure 4.&nbsp;</p>\n\n\n\n<p>Given the need for handling extensive contextual data and producing concise responses, RAG applications often require long input lengths and short output lengths during LLM inference. Figure 4 illustrates results under such conditions, in which GH200 achieves a <strong>speedup of 5.7x</strong> over A100 for Llama-2-70B inference given an input length of 2048 and an output length of 128.\u00a0 Not only does GH200 deliver superior performance, but it also excels in power efficiency, offering favorable performance per watt. We expect these results to continue to improve with future TensorRT-LLM data offloading optimizations, further leveraging GH200 NVLink-C2C capability.\u00a0 </p>\n\n\n\n<p>When testing the GH200-powered RAG pipeline in over 200 real-world sample queries, it computed embeddings for the queries, ran vector search, and retrieved the necessary information from the external knowledge base all in 0.6 seconds.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>When deploying compute-intensive LLM applications using RAG, it&#8217;s essential to consider GPU memory and GPU-CPU bandwidth to unlock high-performance inference at scale. The GH200 GPU-CPU Superchip paired with software solutions like TensorRT-LLM play a pivotal role in addressing large-scale RAG deployment challenges. It enables efficient handling of new data, large batch sizes, and complex queries, for further performance gains in memory-constrained systems.&nbsp;</p>\n\n\n\n<p>Check out our cloud partners that announced NVIDIA GH200 instances like <a href=\"https://nvidianews.nvidia.com/news/aws-nvidia-strategic-collaboration-for-generative-ai\">AWS</a>, <a href=\"https://www.coreweave.com/blog/kicking-off-sc23-coreweave-collaborates-with-nvidia\">CoreWeave</a>, <a href=\"https://lambdalabs.com/blog/lambda-cloud-clusters-now-available-with-nvidia-gh200-grace-hopper-superchip?utm_medium=press-release&amp;utm_campaign=2023-11-NVIDIA-announcements\">Lambda</a>, <a href=\"https://blogs.oracle.com/cloud-infrastructure/post/oci-plans-offer-nvidia-grace-hopper-superchip\">OCI</a>, <a href=\"https://www.vultr.com/news/NVIDIA-GH200-Grace-Hopper-Superchip/\">Vultr</a>, and others, and familiarize yourself with the NVIDIA RAG example published on the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration\">NVIDIA generative AI GitHub</a>. </p>\n\n\n\n<p>Learn more about RAG by watching our on-demand content from <a href=\"https://www.nvidia.com/en-us/events/llm-developer-day/\">NVIDIA LLM Day</a>, which includes a session \u2018Tailoring LLMs to Your Use Case\u2019 where you can learn about strategies to build RAG-based systems.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language model (LLM) applications are essential in enhancing productivity across industries through natural language. However, their effectiveness is often limited by the extent of their training data, resulting in poor performance when dealing with real-time events and new knowledge the LLM isn\u2019t trained on. Retrieval-augmented generation (RAG) solves these problems. By augmenting models with &hellip; <a href=\"https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/\">Continued</a></p>\n", "protected": false}, "author": 1483, "featured_media": 72817, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1318674", "discourse_permalink": "https://forums.developer.nvidia.com/t/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/276482", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110, 1903], "tags": [46, 126, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-grace-hopper.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jpK", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74632"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1483"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74632"}], "version-history": [{"count": 42, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74632/revisions"}], "predecessor-version": [{"id": 75773, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74632/revisions/75773"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72817"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74632"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74632"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74632"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75553, "date": "2023-12-15T13:16:55", "date_gmt": "2023-12-15T21:16:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75553"}, "modified": "2024-01-22T13:35:40", "modified_gmt": "2024-01-22T21:35:40", "slug": "streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/", "title": {"rendered": "Streamline ETL Workflows with Nested Data Types in RAPIDS libcudf"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Nested data types are a convenient way to represent hierarchical relationships within columnar data. They are frequently used as part of <a href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\">extract, transform, load</a> (ETL) workloads in business intelligence, <a href=\"https://www.nvidia.com/en-us/glossary/data-science/recommendation-system/\">recommender systems</a>, cybersecurity, geospatial, and other applications.&nbsp;</p>\n\n\n\n<p>List types can be used to easily attach multiple transactions to a user without creating a new lookup table, for example. Struct types can be used to attach flexible metadata and many key-value pairs within the same column. In web and mobile applications, nested types represent raw JSON objects as elements in a column of data, enabling this data to feed into <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> (ML) training pipelines. And many data science applications rely on nested types to model, manage, and process complex data inputs.&nbsp;</p>\n\n\n\n<p>In the <a href=\"https://rapids.ai/\">RAPIDS</a> suite of accelerated data science libraries, libcudf is the CUDA C++ library for columnar data processing. <a href=\"https://github.com/rapidsai/cudf\">RAPIDS libcudf</a> is based on the <a href=\"https://arrow.apache.org/\">Apache Arrow</a> memory format and supports GPU-accelerated readers, writers, relational algebra functions and column transformations.&nbsp;</p>\n\n\n\n<p>In addition to flat data types like numbers and strings, libcudf also <a href=\"https://github.com/rapidsai/cudf/issues/11844\">supports nested data types</a> such as variable-length lists, structs, and arbitrarily nested combinations of list and struct types. In the releases from 23.02 to 23.12, RAPIDS libcudf has expanded support for nested data types in algorithms including aggregations, joins, and sorting.</p>\n\n\n\n<p>This post showcases data processing with nested data types, introduces the \u201crow operators\u201d that make nested data processing possible, and explores how nested data types impact performance.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Data processing with nested types</h2>\n\n\n\n<p>One common workflow in database management is monitoring and managing data duplicates. RAPIDS libcudf now includes a <a href=\"https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/nested_types/\"><code>C++ nested_types</code> example</a> that reads in JSON data as a libcudf table, computes the count of each distinct element from the first column, joins the count to the original table, and writes the data back as JSON. The libcudf public API enables data processing applications to work with flat types like numbers or strings and nested types like structs and lists, both with equal ease.</p>\n\n\n\n<p>The <code>C++ nested_types</code> example uses the libcudf JSON reader to ingest nested data in a columnar format as a table object. The accelerated JSON reader is also available to C++ developers. JSON provides a human-readable way to create and inspect nested columns. To learn about patterns for using the JSON reader in the Python layer, see <a href=\"https://developer.nvidia.com/blog/gpu-accelerated-json-data-processing-with-rapids/\">GPU-Accelerated JSON Data Processing with RAPIDS</a>.&nbsp;</p>\n\n\n\n<p>The <code>read_json</code> function in the <code>C++ nested_types</code> example accepts a <code>filepath</code> and returns a <code>table_with_metadata</code> object:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\ncudf::io::table_with_metadata read_json(std::string filepath)\n{\n  auto source_info = cudf::io::source_info(filepath);\n  auto builder     = cudf::io::json_reader_options::builder(source_info).lines(true);\n  auto options     = builder.build();\n  return cudf::io::read_json(options);\n}\n</pre></div>\n\n\n<p>Once the JSON data is read and parsed into a table object, the first processing step is a count aggregation to track the number of occurrences for each distinct element. The <code>count_aggregate</code> function in the example populates an aggregation request, executes the aggregate function, and then constructs an output table:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nstd::unique_ptr&lt;cudf::table&gt; count_aggregate(cudf::table_view tbl)\n{\n  // Get count for each key\n  auto keys = cudf::table_view{{tbl.column(0)}};\n  auto val  = cudf::make_numeric_column(cudf::data_type{cudf::type_id::INT32}, keys.num_rows());\n\n  cudf::groupby::groupby grpby_obj(keys);\n  std::vector&lt;cudf::groupby::aggregation_request&gt; requests;\n  requests.emplace_back(cudf::groupby::aggregation_request());\n  auto agg = cudf::make_count_aggregation&lt;cudf::groupby_aggregation&gt;();\n  requests&#91;0].aggregations.push_back(std::move(agg));\n  requests&#91;0].values = *val;\n  auto agg_results   = grpby_obj.aggregate(requests);\n  auto result_key    = std::move(agg_results.first);\n  auto result_val    = std::move(agg_results.second&#91;0].results&#91;0]);\n\n  auto left_cols = result_key-&gt;release();\n  left_cols.push_back(std::move(result_val));\n\n  // Join on keys to get\n  return std::make_unique&lt;cudf::table&gt;(std::move(left_cols));\n}\n</pre></div>\n\n\n<p>With the counts data in hand, the next processing step joins this data to the original table, adding this information to inform count-based filtering and root-cause investigations in downstream analysis. The <code>join_count</code> function in the <code>C++ nested_types</code> example accepts two <code>table_view</code> objects, joins them on their first columns, and then constructs an output table:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nstd::unique_ptr&lt;cudf::table&gt; join_count(cudf::table_view left, cudf::table_view right)\n{\n  auto &#91;left_indices, right_indices] =\n    cudf::inner_join(cudf::table_view{{left.column(0)}}, cudf::table_view{{right.column(0)}});\n  auto new_left  = cudf::gather(left, cudf::device_span&lt;int const&gt;{*left_indices});\n  auto new_right = cudf::gather(right, cudf::device_span&lt;int const&gt;{*right_indices});\n\n  auto left_cols  = new_left-&gt;release();\n  auto right_cols = new_right-&gt;release();\n  left_cols.push_back(std::move(right_cols&#91;1]));\n\n  return std::make_unique&lt;cudf::table&gt;(std::move(left_cols));\n}\n</pre></div>\n\n\n<p>The last data processing step sorts the table based on the elements in the first column. Sorting is useful for providing a deterministic ordering that facilitates downstream steps like partitioning and merging. The <code>sort_keys</code> function in the C++ nested_types example accepts a <code>table_view</code>, computes indices with <code>sorted_order</code>, and then gathers the table based on the ordering:</p>\n\n\n\n<p></p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nstd::unique_ptr&lt;cudf::table&gt; sort_keys(cudf::table_view tbl)\n{\n  auto sort_order = cudf::sorted_order(cudf::table_view{{tbl.column(0)}});\n  return cudf::gather(tbl, *sort_order);\n}\n</pre></div>\n\n\n<p>Finally, the processed data is serialized back to disk using a GPU-accelerated JSON writer, which uses the metadata from <code>read_json</code> to preserve the nested struct key names from the input data. The <code>write_json</code> function in the <code>C++ nested_types</code> example accepts a <code>table_view</code>, <code>table_metadata</code>, and a <code>filepath</code>:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nvoid write_json(cudf::table_view tbl, cudf::io::table_metadata metadata, std::string filepath)\n{\n  auto sink_info = cudf::io::sink_info(filepath);\n  auto builder   = cudf::io::json_writer_options::builder(sink_info, tbl).lines(true);\n  builder.metadata(metadata);\n  auto options = builder.build();\n  cudf::io::write_json(options);\n}\n</pre></div>\n\n\n<p>Taken together, the <code>C++ nested_types</code> example makes a count of each distinct element in the first column, joins those values to the original table, and then sorts the table on the first column. Note that no part of the code in this example is specific to nested types. In fact, this example is compatible with any supported data type in libcudf, flat or nested, demonstrating the power and flexibility of libcudf nested type support.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Introducing libcudf row operators</h2>\n\n\n\n<p>Under the hood, libcudf supports equality comparison, inequality comparison, and element hashing using a few key \u201crow operators.\u201d These row operators are reused in algorithms throughput libcudf and enable the separation of data type support from other algorithm details.&nbsp;</p>\n\n\n\n<p>Taking hash-based aggregations as an example, the hashing and equality operators are used when building and probing the hash tables. For sort-based aggregations, the lexicographic operator identifies one element as less than another element and is a key component of any sorting algorithm. The new row operators unlock support for nested types across the relational algebra functions in libcudf.</p>\n\n\n\n<p>For flat types such as numeric and strings, the row operators process the value and null state for each element. Strings types add more complexity with integer offsets associating a variable number of characters with each element. For struct types, the row operators process the null state for the struct parent as well as the values and null state for each child column.&nbsp;</p>\n\n\n\n<p>Variable-length list types add another layer of complexity, where the row operators account for the hierarchical structure, including null state, list depth, and list length at each nested level. If the hierarchical structure is matching, list operators then consider the value and null state for each leaf element. Of the row operators, hashing and equality are simpler because they can process the data from each element in any order. However, for types that include lists, lexicographic comparison must produce consistent ordering and so requires sequential parsing of null states, hierarchy, and values.</p>\n\n\n\n<p>The treatment for list types in the libcudf lexicographic operator is inspired by the <a href=\"https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper\">Dremel encoding algorithm used in the Parquet format</a>. In Dremel encoding, the list columns are represented using three data streams: the <em>definition</em> stream for recording null state and nesting depth, the <em>repetition</em> stream for recording list lengths, and the <em>value</em> stream for recording leaf values. The encoding gives a flat data structure that&#8217;s more efficient to process than the recursive variable-length list representation in Arrow.</p>\n\n\n\n<p>One limitation of Dremel encoding for lists is that the value stream only supports flat types. To extend support for lists containing structs, a preprocessing step replaces a nested struct column with an integer column corresponding to the rank of each struct element. This recursive preprocessing step extends the lexicographic operator type support to include any combination of lists and structs in the data type.</p>\n\n\n\n<h2 class=\"wp-block-heading\">How data types impact performance</h2>\n\n\n\n<p>The <code>C++ nested_types</code> example is compatible with any supported data type in libcudf. Comparing performance is easy using the command line interface in the example. The following performance data was collected based on timing implemented in the example and run on <a href=\"https://www.nvidia.com/en-us/data-center/dgx-h100/\">NVIDIA DGX H100</a> hardware.</p>\n\n\n\n<p>The data type of the column impacts the overall runtime of the example, with more complex data types increasing the runtime of sort-based processing steps (Figure 1). Across a range of data types, the results show 2-5 ms runtime for count aggregation step and 10-25 ms runtime for the inner join step. Both of these steps use hash-based implementations and rely on the hashing and equality row operators.&nbsp;</p>\n\n\n\n<p>However, the sorting step shows that runtimes increased to 60-90 ms for variable-sized types that include strings or lists. The sorting step relies on the more complex lexicographic row operator. While hash-based algorithms show relatively consistent runtimes as a function of data type, sort-based algorithms show longer runtimes for variable-sized types.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"501\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-625x501.png\" alt=\"Bar chart showing the runtime in ms of count_aggregate, join_count and sort_keys steps by data type, with 85% distinct elements and 20 million rows. \" class=\"wp-image-75612\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-625x501.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-768x615.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-645x517.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-137x110.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1-1024x820.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/runtime-chart-1.png 1100w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Runtime of <code>count_aggregate</code>, <code>join_count</code>, and <code>sort_keys</code> steps by data type, with 85% distinct elements and 20 million rows</em></figcaption></figure></div>\n\n\n\n<p>Row count and nesting depth also impact the performance of the example, with higher row counts and simpler data types showing the highest data processing throughput. Figure 2 shows <code>count_aggregate</code> performance from the <code>C++ nested_types</code> example, where throughput generally increases as the row count increases from 100K to 20 million rows. Data types marked with &#8216;8&#8217; have eight levels of nesting depth. <code>int</code> and <code>float</code> refer to 64-bit types.</p>\n\n\n\n<p>Note that the input data uses structs with one child and lists with length one. The performance data shows primitive types with about 45 GB/s peak throughput, singly nested types with about 30 GB/s peak throughput, and deeply nested types with 10-25 GB/s peak throughput. Struct levels incur less overhead than list levels, and mixed struct/list nesting incurs the largest overhead.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"427\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-625x427.png\" alt=\"Scatter plot showing data processing throughput in GB/s versus memory size of data in MB. \" class=\"wp-image-75615\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-625x427.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-300x205.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-168x115.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-768x524.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-645x440.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-439x300.png 439w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-132x90.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-362x247.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-161x110.png 161w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1-1024x699.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-versus-data-memory-size-1.png 1318w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Data processing throughput versus memory size of data for the <code>count_aggregate</code> example function. Each line shows the effect of sweeping from 100K rows to 20 million rows</em></figcaption></figure></div>\n\n\n\n<p>Finally, the length of list elements also impacts performance, with longer lengths showing higher throughput due to early exits in comparators. Figure 3 shows the impact of list length on data processing throughput using <code>list&lt;int&gt;</code> columns with list lengths from 1 to 16. As list length increases, the total integer leaf count and total memory size also increase, and the number of rows and total size of the offsets data are held constant.&nbsp;</p>\n\n\n\n<p>The data in Figure 3 uses randomly-ordered leaf values, so the comparators will often only need to examine the first element of each list. Performance data collected from lengths increasing from 1 to 16 shows a 7x increase in throughput for the <code>count_aggregate</code> step and a 4x increase in throughput for the <code>sort_keys</code> step. The data uses 10 million rows, 64-bit integer leaf elements, 85% distinct leaf values, and list length held constant within each table.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"563\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-625x563.png\" alt=\"Scatter plot showing data processing throughput in GB/s compared to list length from 1 to 16. As list length increases, the data shows data processing throughput also increases. \n\" class=\"wp-image-75596\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-625x563.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-300x270.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-768x691.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-645x581.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-333x300.png 333w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-100x90.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-362x326.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types-122x110.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-processing-throughput-singly-nested-list-types.png 1000w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Data processing throughput of the <code>count_aggregate</code>, <code>join_count</code>, and <code>sort_keys</code> steps for singly nested list types with varying list lengths</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>RAPIDS libcudf provides powerful, flexible, and accelerated tools for working with nested data types. Relational algebra algorithms such as aggregations, joins, and sorting are tuned and optimized for any supported nested data type, even deeply nested and mixed list and struct nested data types.&nbsp;</p>\n\n\n\n<p>Build and run a few <a href=\"https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/\">examples</a> to get started with RAPIDS libcudf. For more information about CUDA-accelerated dataframes, see the <a href=\"https://docs.rapids.ai/api/cudf/stable/\">cuDF documentation</a> and the <a href=\"https://github.com/rapidsai/cudf\">rapidsai/cudf</a> GitHub repo. For easier testing and deployment, <a href=\"https://hub.docker.com/r/rapidsai/rapidsai/\">RAPIDS Docker containers</a> are also available for releases and nightly builds. If you\u2019re already using cuDF, you can run the new <code>C++ nested_types</code> example by visiting <a href=\"https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/nested_types\">rapidsai/cudf/tree/HEAD/cpp/examples/nested_types</a> on GitHub.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Acknowledgments</h3>\n\n\n\n<p><em>Thank you Devavret Makkar, Jake Hemstad, and the rest of the RAPIDS team for contributing to this work.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Nested data types are a convenient way to represent hierarchical relationships within columnar data. They are frequently used as part of extract, transform, load (ETL) workloads in business intelligence, recommender systems, cybersecurity, geospatial, and other applications.&nbsp; List types can be used to easily attach multiple transactions to a user without creating a new lookup table, &hellip; <a href=\"https://developer.nvidia.com/blog/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/\">Continued</a></p>\n", "protected": false}, "author": 1438, "featured_media": 75566, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1317624", "discourse_permalink": "https://forums.developer.nvidia.com/t/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/276278", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [1433, 3273, 1953, 695], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-montage-watercolor.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jEB", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75553"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1438"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75553"}], "version-history": [{"count": 39, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75553/revisions"}], "predecessor-version": [{"id": 77061, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75553/revisions/77061"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75566"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75553"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75553"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75553"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75688, "date": "2023-12-15T09:52:22", "date_gmt": "2023-12-15T17:52:22", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75688"}, "modified": "2023-12-15T09:52:25", "modified_gmt": "2023-12-15T17:52:25", "slug": "new-video-connecting-any-data-source-to-openusd", "status": "publish", "type": "post", "link": "https://nvda.ws/3uTSBuc", "title": {"rendered": "New Video: Connecting Any Data Source to OpenUSD"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In the fourth installment of this series on the superpowers of OpenUSD, learn how any digital content creation tool can be connected to USD. OpenUSD\u2019s data source interoperability allows data from different tools to be used in the same scene or project.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the fourth installment of this series on the superpowers of OpenUSD, learn how any digital content creation tool can be connected to USD. OpenUSD\u2019s data source interoperability allows data from different tools to be used in the same scene or project.</p>\n", "protected": false}, "author": 338, "featured_media": 75690, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3uTSBuc", "_links_to_target": "_blank"}, "categories": [1235, 503], "tags": [3268, 2375, 1958, 3096], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nv-ov-thumbnail-openusd-video-episode_041.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jGM", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75688"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/338"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75688"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75688/revisions"}], "predecessor-version": [{"id": 75692, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75688/revisions/75692"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75690"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75688"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75688"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75688"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74280, "date": "2023-12-15T09:00:00", "date_gmt": "2023-12-15T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74280"}, "modified": "2023-12-11T12:20:45", "modified_gmt": "2023-12-11T20:20:45", "slug": "advanced-api-performance-swap-chains", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-api-performance-swap-chains/", "title": {"rendered": "Advanced API Performance: Swap Chains"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Swap chains are an integral part of how you get rendering data output to a screen. They usually consist of some group of output-ready buffers, each of which can be rendered to one at a time in rotation. In parallel with rendering to one of a swap chain&#8217;s buffers, some other buffer in the swap chain is generally read from for display output.</p>\n\n\n\n<p class=\"has-text-align-right\"><em>This post covers best practices when working with swap chains on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all </em><a href=\"https://developer.nvidia.com/blog/tag/advanced-api-performance\">Advanced API Performance tips</a>.</p>\n\n\n\n<p>It&#8217;s common to focus on the more frequently optimized parts of the rendering pipeline when looking to improve rendering performance. However, swap chains are often overlooked, leaving potential performance and latency on the table.&nbsp;</p>\n\n\n\n<p>The following suggestions and considerations should provide more insight into the best ways to ensure optimum swap chain performance.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Recommended</h2>\n\n\n\n<ul>\n<li>Use flip-mode swap chains. This is especially important for leveraging multiplane overlay support, which provides fullscreen-like performance and latency when running in windowed mode.&nbsp;</li>\n\n\n\n<li>Use <code>SetFullScreenState(TRUE)</code>, a (borderless) fullscreen window, and a non-windowed flip model swap chain to switch to true immediate independent flip mode.\n<ul>\n<li>This is the only mode that enables unlimited framerates with tearing for Direct 3D 12 when calling <code>Present(0,0)</code>.</li>\n\n\n\n<li>For proper unlimited frame rate support for displays that support variable refresh rates, you must also use the <code>DXGI_SWAP_CHAIN_FLAG_ALLOW_TEARING</code> swap chain flag, along with the <code>DXGI_PRESENT_ALLOW_TEARING</code> <code>Present</code> flag</li>\n</ul>\n</li>\n\n\n\n<li>Use the <code>DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH</code> flag consciously.\n<ul>\n<li>The flag is not necessary to achieve unlimited frame rates (see earlier note) if your window size matches the current screen resolution</li>\n\n\n\n<li>If this flag is set, trying to change the resolution using ResizeTarget before calling <code>SetFullScreenState(TRUE)</code> works fine and the framerate will be unlimited</li>\n\n\n\n<li>If this flag is not set, trying to change resolution using <code>ResizeTarget</code> before calling <code>SetFullScreenState(TRUE)</code> results in no change of display resolution. Your target is stretched to the current resolution and the frame rate is limited.</li>\n</ul>\n</li>\n\n\n\n<li>If not in the fullscreen state (true immediate independent flip mode), control your latency and buffer count in your swap chain carefully for the desired frame rate and latency.\n<ul>\n<li>Use <code>IDXGISwapChain2::SetMaximumFrameLatency(MaxLatency)</code> to set the desired latency, where <code>MaxLatency</code> is some number of frames (counted by queued <code>Present</code> calls).</li>\n\n\n\n<li>For this to work, you must create your swap chain with the <code>DXGI_SWAP_CHAIN_FLAG_FRAME_LATENCY_WAITABLE_OBJECT</code> flag set.</li>\n\n\n\n<li>DXGI starts to block in <code>Present</code> after you have the <code>MaxLatency</code> number of present calls queued.</li>\n\n\n\n<li>In this windowed state, a sync interval of 0 in a <code>Present</code> call ensures that the frame being presented is the most recent frame available for the next time desktop composition happens (combining the windowed rendered frame with the rest of the desktop), and all previously completed frames are discarded in favor of this latest one. No rendered frame is displayed until composition happens, which happens at <code>VSYNC</code> time. This latest finished frame is what is displayed</li>\n</ul>\n</li>\n\n\n\n<li>Use about 1-2 more swap chain buffers than the maximum number of frames that you intend to queue (in terms of command allocators, dynamic data, and the associated frame fences). Set the maximum frame latency to this number of swap chain buffers through <code>IDXGISwapChain2::SetMaximumFrameLatency(MaxLatency)</code>.\n<ul>\n<li>This ensures that you can limit queued frames and latency explicitly and optimally from within the application logic rather than relying on the OS to block or have it block at an unexpected time.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Not recommended</h2>\n\n\n\n<ul>\n<li>Forgetting that, by default, there&#8217;s a per\u2013swap chain limit of three queued frames before DXGI starts to block in <code>Present</code>. This means that it blocks on the fourth Present call if there are currently three <code>Present</code> calls queued.\n<ul>\n<li>Set the <code>DXGI_SWAP_CHAIN_FLAG_FRAME_LATENCY_WAITABLE_OBJECT</code> flag on swap chain creation and use <code>IDXGISwapChain2::SetMaximumFrameLatency</code> to modify this default value</li>\n</ul>\n</li>\n\n\n\n<li>Forgetting to call <code>ResizeBuffers</code> after you have switched to true immediate independent flip mode using <code>SetFullScreenState(TRUE)</code>.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Acknowledgments</h3>\n\n\n\n<p><em>Thanks to Cody Robson, Kumaresan Gnanasekaran, Adrian Muntianu, and Meenal Nachnani for their advice and assistance.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Swap chains are an integral part of how you get rendering data output to a screen. They usually consist of some group of output-ready buffers, each of which can be rendered to one at a time in rotation. In parallel with rendering to one of a swap chain&#8217;s buffers, some other buffer in the swap &hellip; <a href=\"https://developer.nvidia.com/blog/advanced-api-performance-swap-chains/\">Continued</a></p>\n", "protected": false}, "author": 1936, "featured_media": 66457, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1317535", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-api-performance-swap-chains/276249", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [97, 1235, 503], "tags": [2424], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/Advanced-API-series.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jk4", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74280"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1936"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74280"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74280/revisions"}], "predecessor-version": [{"id": 75133, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74280/revisions/75133"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/66457"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74280"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74280"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74280"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75194, "date": "2023-12-14T11:59:00", "date_gmt": "2023-12-14T19:59:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75194"}, "modified": "2023-12-14T15:33:04", "modified_gmt": "2023-12-14T23:33:04", "slug": "achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/", "title": {"rendered": "Achieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Best-in-class AI performance requires an efficient parallel computing architecture, a productive tool stack, and deeply optimized algorithms. NVIDIA released the open-source <a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">NVIDIA TensorRT-LLM,</a> which includes the latest kernel optimizations for the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper architecture</a> at the heart of the <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPU</a>. These optimizations enable models like Llama 2 70B to execute using accelerated FP8 operations on H100 GPUs while maintaining inference accuracy.</p>\n\n\n\n<p>At a recent launch event, AMD talked about the inference performance of the H100 GPU compared to that of its MI300X chip. The results shared did not use optimized software, and the H100, if benchmarked properly, is 2x faster.&nbsp; </p>\n\n\n\n<p>The following is the actual measured performance of a single NVIDIA DGX H100 server with eight NVIDIA H100 GPUs on the Llama 2 70B model. This includes results for both \u201cBatch-1\u201d where an inference request is processed one at a time, as well as results using fixed response-time processing.</p>\n\n\n\n<div class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"714\" height=\"1032\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100.jpg\" alt=\"Chart of Llama 2 70B server inference performance in queries.\" class=\"wp-image-75446\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100.jpg 714w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-208x300.jpg 208w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-625x903.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-80x115.jpg 80w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-645x932.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-62x90.jpg 62w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-362x523.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/LLM-Inference-H100-76x110.jpg 76w\" sizes=\"(max-width: 714px) 100vw, 714px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Llama 2 70B server inference performance in queries per second with 2,048 input tokens and 128 output tokens for &#8220;Batch 1&#8221; and various fixed response time settings</em></figcaption></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>AMD\u2019s implied claims for H100 are measured based on the configuration taken from AMD launch presentation footnote #MI300-38. Using vLLM v.02.2.2 inference software with NVIDIA DGX H100 system, Llama 2 70B query with an input sequence length of 2,048 and output sequence length of 128. They claimed relative performance compared to DGX H100 with 8x GPU MI300X system.</em></p>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>For NVIDIA measured data, DGX H100 with 8x NVIDIA H100 Tensor Core GPUs with 80 GB HBM3 with publicly available NVIDIA TensorRT-LLM, v0.5.0 for batch 1 and v0.6.1 for latency threshold measurements. Workload details same as footnote #MI300-38.</em></p>\n\n\n\n<p>DGX H100 can process a single inference in 1.7 seconds using a batch size of one\u2014in other words, one inference request at a time. A batch size of one results in the fastest possible response time for serving a model.&nbsp;To optimize both response time and data center throughput, cloud services set a fixed response time for a particular service. This enables them to combine multiple inference requests into larger \u201cbatches\u201d and increase the overall inferences per second of the server. Industry-standard benchmarks like MLPerf also measure performance with this fixed response time metric.</p>\n\n\n\n<p>Small tradeoffs in response time can yield x-factors in the number of inference requests that a server can process in real time.&nbsp;Using a fixed 2.5-second response time budget, an 8-GPU DGX H100 server can process over five Llama 2 70B inferences per second compared to less than one per second with batch one.</p>\n\n\n\n<p>AI is moving fast and the NVIDIA CUDA ecosystem enables us to optimize our stack quickly and continuously.&nbsp;We look forward to continuing to improve AI performance with every update of our software, so be sure to check out our <a href=\"https://developer.nvidia.com/deep-learning-performance-training-inference\">performance pages</a> and <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">GitHub sites</a> for the latest.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-wide\"/>\n\n\n\n<h2 class=\"wp-block-heading has-medium-font-size\"><em>How to reproduce these AI inference results</em></h2>\n\n\n\n<p class=\"has-small-font-size\"><em>DGX H100 AMD Footnote </em>was measured by NVIDIA in vLLM based on the configurations provided by AMD in their footnotes using <a href=\"https://github.com/vllm-project/vllm/tree/main\">vLLM</a> and its provided <a href=\"https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py\">benchmarking script</a> with the following command lines:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ python benchmarks/benchmark_latency.py --model &quot;meta-llama/Llama-2-70b-hf&quot; --input-len 2048 --output-len 128 --batch-size 1 -tp 8\n</pre></div>\n\n\n<p class=\"has-small-font-size\"><em>MI300X 8-Chip System </em>is the inferred data based on AMD\u2019s claimed speedup over <em>DGX H100 AMD Footnote </em>measured vLLM results.</p>\n\n\n\n<p class=\"has-small-font-size\"><em>DGX H100 Measured </em>was measured by NVIDIA using publicly available versions of TensorRT-LLM available on <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">GitHub</a> and using the command lines outlined in the TensorRT-LLM benchmarking <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance.md#llama2-70b\">guide for Llama 2</a>.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n// Build TensorRT optimized Llama-2-70b for H100 fp8 tensorcore\n$ python examples/llama/build.py --remove_input_padding --enable_context_fmha --parallel_build --output_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --dtype float16 --use_gpt_attention_plugin float16 --world_size 8 --tp_size 8 --pp_size 1 --max_batch_size 14 --max_input_len 2048 --max_output_len 128 --enable_fp8 --fp8_kv_cache --strongly_typed --n_head 64 --n_kv_head 8 --n_embd 8192 --inter_size 28672 --vocab_size 32000 --n_positions 4096 --hidden_act silu --ffn_dim_multiplier 1.3 --multiple_of 4096 --n_layer 80\n\n\n// Benchmark Llama-70B\n$ mpirun -n 8 --allow-run-as-root --oversubscribe ./cpp/build/benchmarks/gptSessionBenchmark --model llama_70b --engine_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --warm_up 1 --batch_size 14 --duration 0 --num_runs 5 --input_output_len 2048,1;2048,128\n</pre></div>", "protected": false}, "excerpt": {"rendered": "<p>Best-in-class AI performance requires an efficient parallel computing architecture, a productive tool stack, and deeply optimized algorithms. NVIDIA released the open-source NVIDIA TensorRT-LLM, which includes the latest kernel optimizations for the NVIDIA Hopper architecture at the heart of the NVIDIA H100 Tensor Core GPU. These optimizations enable models like Llama 2 70B to execute using &hellip; <a href=\"https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/\">Continued</a></p>\n", "protected": false}, "author": 465, "featured_media": 75195, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1316162", "discourse_permalink": "https://forums.developer.nvidia.com/t/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/276028", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [296, 453, 2932, 126], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Top-inference-performance-H100.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jyO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75194"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/465"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75194"}], "version-history": [{"count": 85, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75194/revisions"}], "predecessor-version": [{"id": 75509, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75194/revisions/75509"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75195"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75194"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75194"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75194"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74793, "date": "2023-12-14T11:58:00", "date_gmt": "2023-12-14T19:58:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74793"}, "modified": "2023-12-14T13:01:17", "modified_gmt": "2023-12-14T21:01:17", "slug": "generative-ai-research-spotlight-demystifying-diffusion-based-models", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/", "title": {"rendered": "Generative AI Research Spotlight: Demystifying Diffusion-Based Models"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>With Internet-scale data, the computational demands of AI-generated content have grown significantly, with data centers running full steam for weeks or months to train a single model\u2014not to mention the high inference costs in generation, often offered as a service. In this context, suboptimal algorithmic design that sacrifices performance is an expensive mistake.</p>\n\n\n\n<p>Much of the recent progress in AI-generated image, video, and audio content has been driven by <em>denoising diffusion</em>\u2014a technique that iteratively shapes random noise into novel samples of the data. A recent research paper published by our team, <a href=\"https://research.nvidia.com/publication/2022-11_elucidating-design-space-diffusion-based-generative-models\">Elucidating the Design Space of Diffusion Based Models</a>, recipient of the Outstanding Paper Award at NeurIPS 2022, identifies the simple core mechanisms underlying the seemingly complicated approaches in the literature. Starting with this clear view of the fundamentals, we then find the state-of-the-art practices for quality and computational efficiency.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Denoising diffusion&nbsp;</h2>\n\n\n\n<p>Denoising is the act of removing, for example, sensor noise from images or hiss from audio recordings. This post will use images as the running example, but the process applies to many other domains as well. This task is excellently suited for convolutional neural networks.&nbsp;</p>\n\n\n\n<p>What does this have to do with generating novel images? Imagine there is a large amount of noise on an image. Indeed, so much that the original image is lost. Could a denoiser be used to reveal some <em>random</em> image that could be hiding under all that noise? Surprisingly, the answer is yes.</p>\n\n\n\n<p>This is the simple essence of denoising diffusion: first draw a random image of pure white noise, and then chip away at the noise level\u2014say, 2% at a time\u2014by repeatedly feeding it to a neural denoiser. Gradually, a random clean image emerges from underneath the noise. The distribution of generated content (pictures of cats and dogs? audio waveforms of spoken English phrases? video clips of driving?) is determined by the dataset that the denoiser network was trained with.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1371\" height=\"412\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence.png\" alt=\"An image sequence where each consecutive image is produced by denoising the previous one. The first image is pure white noise, and the last image is the generation result.\n\" class=\"wp-image-74802\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence.png 1371w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-300x90.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-625x188.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-179x54.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-768x231.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-645x194.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-500x150.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-160x48.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-362x109.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-366x110.png 366w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-diffusion-image-sequence-1024x308.png 1024w\" sizes=\"(max-width: 1371px) 100vw, 1371px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Denoising diffusion reveals novel images from pure noise through repeated denoising</em></em></figcaption></figure>\n\n\n\n<p>The code below is a first guess of how to implement this idea, assuming a neural network function <code>denoise</code> is available.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# start with an image of pure large-magnitude noise\nsigma = 80\t\t# initial noise level\nx = sigma * torch.randn(img_shape)\n\nfor step in range(256):\n    # keep 98% of current noisy image, and mix in 2% of denoising\n\tx = 0.98 * x + 0.02 * denoise(x, sigma)\n\t\n    # keep track of current noise level\n    sigma *= 0.98\n</pre></div>\n\n\n<p>If you\u2019ve ever looked at code bases or scientific papers in the field, filled with pages of equations, you might be surprised to learn that this near-trivial piece of code is actually a theoretically valid implementation of something called a <em>probability flow ordinary differential equation solver</em>. While this snippet is hardly optimal, it embodies surprisingly many of the key good practices explained in the paper. The team&#8217;s top-of-the-line final sampler is essentially just a few more lines.&nbsp;</p>\n\n\n\n<p>What about that function <code>denoise</code>? At its core, it\u2019s surprisingly straightforward as well: the denoiser must output the blurry average of all possible clean images that could have been hiding under the noise. The desired output at various noise levels might look like the examples in Figure 2.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"376\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels.png\" alt=\" Three examples of denoising at different noise levels.\n\" class=\"wp-image-74809\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-300x56.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-625x118.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-768x144.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-1536x289.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-645x121.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-500x94.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-160x30.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-362x68.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-585x110.png 585w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/denoising-images-different-noise-levels-1024x193.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Examples of ideal denoiser outputs at different noise levels. At high noise levels, the image details are uncertain, and remain blurry in the output</em></em></figcaption></figure>\n\n\n\n<p>Training a denoiser network (typically a U-Net) with basic loss\u2014the mean square error between its output and the clean target\u2014achieves precisely this result. Fancier losses that aim for sharper output are in fact harmful, and violate the theory. Keep in mind that, even if the task is conceptually simple, most existing denoisers are not trained for it specifically.</p>\n\n\n\n<p>Much of the apparent mathematical complexity in the literature arises from justifying<em> </em>why this works. The theory can be built up from various formalisms, the most popular two being Markov chains and stochastic differential equations. While each approach boils down to a denoising loop that uses a trained denoiser, they open up a vast and confusing space of different practical implementations, and a minefield of opportunities to make poor choices.</p>\n\n\n\n<p>The paper peels back the layers of mathematical complexity, directly exposing the tangible design choices in a standardized framework where they are easy to analyze.&nbsp;</p>\n\n\n\n<p>This post presents the team&#8217;s key findings and intuitions through visualizations and code. We\u2019ll cover three topics:</p>\n\n\n\n<ul>\n<li>An intuitive overview of the <strong>theory</strong> behind denoising diffusion</li>\n\n\n\n<li>Design choices related to <strong>sampling</strong> (generating images when you already have a trained denoiser)</li>\n\n\n\n<li>Design choices when <strong>training </strong>that denoiser</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">What makes diffusion work?</h2>\n\n\n\n<p>To begin, this section takes a step back to the basics and builds the theory that justifies this straightforward piece of code. We find most insight in the differential equation framework, originally presented in <a href=\"https://arxiv.org/abs/2011.13456\">Score-Based Generative Modeling through Stochastic Differential Equations</a>. While the equations and mathematical concepts might look intimidating, they are not crucial for understanding the gist. It is useful to mention them occasionally to highlight that they are often just a different language for describing concrete things done in code.</p>\n\n\n\n<p>Imagine an RGB image <strong>x</strong> of shape, say, <code>[3, 64, 64]</code> from the dataset. Begin by considering the easy direction of destroying<em> </em>the image by gradually adding noise onto it. (Of course, this is the opposite of the end goal.)</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfor step in range(1000):\n\tx = x + 0.1 * torch.randn_like(x)\n</pre></div>\n\n\n<p>This is in fact (suitably squinting) a stochastic differential equation (SDE) solver corresponding to a simple SDE <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Ctext%7Bd%7D%5Cmathbf%7Bx%7D+%3D+%5Ctext%7Bd%7D%5Comega_t&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;text{d}&#92;mathbf{x} = &#92;text{d}&#92;omega_t\" class=\"latex\" />. It says that the change in image <strong>x</strong> over a short time step is random white noise. Here, <em>solving</em> simply means simulating a specific random numerical realization of the process described by the SDE.</p>\n\n\n\n<p>A nice thing about differential equations is that they have a fruitful geometric interpretation. You can visualize this process as the image taking a random walk (the famous Brownian motion, or Wiener process) in the pixel value space. If you consider <strong>x</strong> above to be just a single number (a \u201csingle pixel image\u201d), you can plot its evolution as the following graph. The real thing is exactly the same but in a much higher dimension, so it can\u2019t be visualized on a two-dimensional monitor.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gradual-image-noise-addition-over-time.gif\" alt=\"An animated diagram with pixel value on the vertical axis and time on the horizontal axis. A graph starting from the left edge draws a random path over time. An image with gradually increasing noise level is shown simultaneously.\" class=\"wp-image-74944\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Gradual noise addition is a random walk in pixel value space</em></em></figcaption></figure></div>\n\n\n<p>Studying this evolution using many different starting images and random paths, you begin to see some order within the chaos. Think of it like stacking lots of these squiggly paths on top of each other. On average, they create a changing shape over time.&nbsp;</p>\n\n\n\n<p>The complex pattern of data at the left edge (you could metaphorically imagine the two peaks corresponding to images of cats and dogs, respectively) gradually mixes and simplifies into a featureless blob at the right edge. This is the ubiquitous <em>normal distribution</em>, or pure white noise.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/image-denoising-random-paths-density-evolving.gif\" alt=\"A diagram with multiple paths and images making the random walk simultaneously. The overlapping random paths average into a smooth density.\" class=\"wp-image-74949\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 4. The random paths drawn by all dataset images establish a density that evolves with time</em></em></figcaption></figure></div>\n\n\n<p>The high-level goal (generative modeling) is to somehow find a trick to sample novel images from the true hidden data distribution on the left in Figure 4\u2014actual new images that <em>could have been in the dataset, but weren\u2019t</em>. You could easily sample from the pure-noise state on the right, using <code>randn</code>. Is it possible to then run the above noising process in the reverse direction, so as to end with random samples of clean images (Figure 5)?</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/image8.gif\" alt=\"The same diagram but with time direction reversed: the random path starts from the right edge and advances toward the left edge, and simultaneously an initial pure noise evolves into a novel generated image.\n\" class=\"wp-image-74951\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 5. A reversed random walk starting from noise, and ending at a randomly generated image</em></em></figcaption></figure>\n\n\n\n<p>Following a random path starting from the right edge, what guarantees a proper image at the left edge, rather than just more noise? Some kind of additional force is needed to gently pull the image towards the data on each step.</p>\n\n\n\n<p>The theory of SDE\u2019s provides a beautiful solution. Without diving too much into the technicalities, it indeed enables reversing the time direction, and doing so automatically introduces an extra term for the sought-after data-attraction force. The force pulls the noisy image towards its mean-square optimal denoising. This can be estimated with a trained neural network (here, sigma is the current noise level):</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Ctext%7Bd%7D%5Cmathbf%7Bx%7D+%3D+%5Cunderbrace%7B%5Ctext%7Bd%7D%5Comega_t%7D_%7B%5Ctext%7Badd+noise%7D%7D+-+%7E+%5Cunderbrace%7B%5B%5Ctext%7Bdenoise%7D%28%5Cmathbf%7Bx%7D%2C+%5Csigma%29+-+%5Cmathbf%7Bx%7D%5D%2F%5Csigma%5E2+%7E+%5Ctext%7Bd%7Dt%7D_%7B%5Ctext%7Bremove+noise%7D%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;text{d}&#92;mathbf{x} = &#92;underbrace{&#92;text{d}&#92;omega_t}_{&#92;text{add noise}} - ~ &#92;underbrace{[&#92;text{denoise}(&#92;mathbf{x}, &#92;sigma) - &#92;mathbf{x}]/&#92;sigma^2 ~ &#92;text{d}t}_{&#92;text{remove noise}}\" class=\"latex\" /></p>\n\n\n\n<p>You can even adjust the weight of the two terms, as long as you are careful to keep the total rate of noise reduction unchanged. Taking this idea to the extreme of removing noise exclusively leads to a fully deterministic <em>ordinary differential equation </em>(ODE) with no random component at all. The evolution then follows a smooth trajectory, and the image simply fades in from underneath the fixed noise (Figure 6).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/reverse-time-evolution-path-image.gif\" alt=\"Reverse time evolution of the path and the image. Instead of a squiggly random path, the line is now a smooth curve.\" class=\"wp-image-74956\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 6. The smooth evolution induced by the deterministic ordinary differential equation</em></em></figcaption></figure></div>\n\n\n<p>Notice how the curved trajectory in Figure 6 connects the initial random noise at the right edge to a unique generated image at the left edge. Indeed, the ODE establishes a different trajectory for each initial noise. Think of these curves as flow lines of a fluid pushing our image around. During the generation, the task is simply to follow the flow line from the start as accurately as possible. Start from a random spot on the right, and at each step, the formula (really, the denoiser network) shows where the flow line points for the current image. Inch a bit in its direction and repeat. That\u2019s the generation process in a nutshell.</p>\n\n\n\n<p>Figure 7 shows that each step of the solver advances the time backward by some chosen amount (dt), and consults the ODE formula (and consequently, the denoiser network) to determine how to change the image over the timestep.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1386\" height=\"1059\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion.png\" alt=\"A diagram showing the step direction arrow along a flow line as being constructed as a sum of a horizontal arrow corresponding to dt, and a vertical arrow corresponding to dx.\" class=\"wp-image-74960\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion.png 1386w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-300x229.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-625x478.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-151x115.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-768x587.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-645x493.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-393x300.png 393w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-118x90.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-362x277.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-144x110.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/step-direction-diagram-denoising-diffusion-1024x782.png 1024w\" sizes=\"(max-width: 1386px) 100vw, 1386px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 7. Each step of the solver advances the time backward by some chosen amount (dt) and consults the ODE formula to determine how to change the image over the timestep</em></em></figcaption></figure></div>\n\n\n<p>Subsequent sections analyze the deterministic version exclusively, as stochasticity obscures the geometric insight afforded by the deterministic picture. With appropriate tuning, stochasticity has beneficial error correction properties, but it is tedious to use and can be seen as something of a crutch. For more details, see <a href=\"https://research.nvidia.com/publication/2022-11_elucidating-design-space-diffusion-based-generative-models\">Elucidating the Design Space of Diffusion Based Models</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Design choices for sampling to generate images&nbsp;</h2>\n\n\n\n<p>As stated in the introduction, it\u2019s the details that make or break the performance. The key difficulty is that the step direction given by the network is valid <em>only in the immediate vicinity of the current noise level</em>. Trying to reduce too much noise at once without stopping to re-evaluate will result in adding something into the images that shouldn\u2019t be there. This shows up as variously reduced image quality: indescript blobbiness and graininess, color and intensity artifacts, distortions and lack of coherence in faces and other higher-level details, and so on.</p>\n\n\n\n<p>In the 1D visualization, this corresponds to taking a step that lands away from the starting flow line, as shown in Figure 8. Notice the gap that opens between the arrows (representing steps that might be taken) and the curve.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion.png\" alt=\"A diagram with a highlighted curved flow line, showing two straight arrows that attempt to follow it. The endpoints land away from the curve.\n\" class=\"wp-image-74964\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-768x614.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-113x90.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-138x110.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/diagram-highighted-curve-flow-line-denoising-diffusion-1024x819.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 8. The linear steps (straight arrows) can be a poor approximation of the true curved flow line</em></em></figcaption></figure>\n\n\n\n<p>The common brute-force solution is to simply take a large number of very short steps so as to avoid getting thrown off. However, this is expensive because every step incurs a full evaluation of the denoiser network. It\u2019s like crawling instead of running: safe but slow.&nbsp;</p>\n\n\n\n<p>Our sampler design drastically reduces the number of steps required without compromising quality. The strategy is three-fold:</p>\n\n\n\n<ul>\n<li>Design the ODE such that its flow lines are as straight as possible, and hence easy to follow (<em>noise schedules</em>)</li>\n\n\n\n<li>Identify what noise levels still need extra careful stepping (<em>time step discretization</em>)</li>\n\n\n\n<li>Take smarter steps to get the most bang for the buck from each (<em>higher-order solvers</em>)</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Straightening the flow for fewer steps</h3>\n\n\n\n<p>The key issue is the curvature of the flow lines. Had they been straight lines, they would be very easy to follow. It would be possible to take a single long straight step all the way to noise level zero, and never worry about falling off the curve. In reality, some curvature is unavoidably built into the setup. Can it be reduced?&nbsp;</p>\n\n\n\n<p>It turns out that the theory developed in the previous section enables some poor choices in this regard. For example, you can build different versions of the ODE by specifying different noise schedules. Recall that the 1D visualization was built by adding the same amount of noise at each step. Had it been added at some different time-varying rate, each noise level would be reached at some different time (a different schedule). This amounts to stretching and squeezing the time axis.&nbsp;</p>\n\n\n\n<p>Figure 9 shows a few different ODEs that are induced by different noise schedule choices.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/different-noise-schedules-flow-line-gif-1.gif\" alt=\"An animated diagram showing different choices of time schedule. The flow lines become warped in different ways.\n\" class=\"wp-image-74965\"/><figcaption class=\"wp-element-caption\"><em>Figure 9. Different noise schedules induce different flow line curvature. In some schedules, the linear steps are better approximations to the curve than in others</em></figcaption></figure>\n\n\n\n<p>Notice that this has the side effect of reshaping the flow lines. In fact, the lines are almost straight in one of these schedules. This is indeed the one that the team advocates. The arrows representing steps now almost perfectly align with the curves. Consequently, it&#8217;s possible to take many fewer steps compared to other choices (Figure 10).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time.png\" alt=\"A diagram showing almost straight flow lines, and a pair of linear arrows that now align with them well.\n\" class=\"wp-image-74966\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-768x614.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-113x90.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-138x110.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-schedule-with-noise-level-growing-linearly-over-time-1024x819.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 10. Our team\u2019s choice of noise schedule. While some curvature still exists at \u200clow noise levels (left edge), the flow lines are almost straight for much of the evolution</em></em></figcaption></figure>\n\n\n\n<p>Figure 10 shows a schedule with the noise level growing linearly as time progresses. Contrast the previous example of constant-rate addition, with the noise level growing fast at first but then slowing down. In other words, time becomes synonymous with noise level. Without diving into the technical details here, this particular choice gives the beautifully straightforward solver algorithm. This is Algorithm 1 in our paper, without the optional lines 6 to 8, using the proposed schedule and after some tidying up:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# a (poor) placeholder example time discretization\ntimesteps = np.linspace(80, 0, num_steps)\n\n# sample an image of random noise at first noise level\nx = torch.randn(img_shape) * timesteps&#91;0]\n\n# iterate through pairs of adjacent noise levels\nfor t_curr, t_next in zip(timesteps&#91;:-1], timesteps&#91;1:]):\n\n    # fraction of noise we keep in this iteration\n\tblend = t_next / t_curr\n\t\n    # mix in the denoised image\n\tx = blend * x + (1-blend) * denoise(x, t_curr)\n</pre></div>\n\n\n<p>The code is just a slight generalization of the listing in the introduction. It can\u2019t get much simpler than this. The algorithm is so straightforward that one wonders how it wasn\u2019t stumbled upon from heuristic grounds in 2015\u2014perhaps the idea seems too preposterous to work. Incidentally, denoising diffusion was discussed in the 2015 paper, <a href=\"https://arxiv.org/abs/1503.03585\">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a>, but under a complex mathematical guise. Its potential went unnoticed for years.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Careful stepping at low noise levels</h3>\n\n\n\n<p>This clearly highlights another design choice, which in most treatments is obscured and entangled with the noise schedule: the choice of time steps. The linear spacing used in the previous code snippet is in fact a poor choice. Empirically (and reasoning from natural image statistics), it&#8217;s clear that detail is revealed more rapidly near low noise levels. In the 1D visualization, little is happening for most of the right side of the plot, but then the flow lines take a sudden turn into one of the two basins on the left. This means that long steps are possible at high noise levels, but it is necessary to slow down when approaching low noise levels (Figure 11).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"960\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels.png\" alt=\"A diagram where a chain of arrows follows a flow line, such that the arrows are longer at the right edge of the image and become progressively shorter towards the left edge.\n\" class=\"wp-image-74968\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-768x614.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-113x90.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-138x110.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/time-steps-long-at-high-noise-levels-1024x819.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 11. Time steps are long at high noise levels and short at low noise levels</em></em></figcaption></figure>\n\n\n\n<p>Our paper empirically studies what the relative length of steps should be at low versus high noise levels. The following code snippet arrives at a simple but robust modification for timesteps. Roughly, raise the numbers in it to the power of seven (careful to keep them scaled to the original range of 0 to 80). This strongly biases the steps towards the low noise levels:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nsigma_max = 80\nsigma_min = 0.002\t# leave a microscopic bit of noise for stability\nrho = 7\n\nstep_indices = torch.arange(num_steps)\ntimesteps = (sigma_max ** (1 / rho) \\\n          + step_indices / (num_steps - 1) \\\n            * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Higher-order solvers for more accurate steps</h3>\n\n\n\n<p>The ODE viewpoint enables the use of fancier higher-order solvers, which essentially take curved instead of linear steps. This is an advantage when trying to follow \u200ccurved flow lines. The benefits are not clear-cut, as estimating \u200clocal curvature requires additional neural network evaluations. The team tested a range of approaches and consistently found the so-called second-order Heun scheme to be the best (Figure 12). This adds a couple of lines to the code (see Algorithm 1 in <a href=\"https://research.nvidia.com/publication/2022-11_elucidating-design-space-diffusion-based-generative-models\">Elucidating the Design Space of Diffusion Based Models</a>) and doubles the expense per iteration, but cuts the number of required iterations to a fraction.&nbsp;</p>\n\n\n\n<p>The Heun step has a nice geometric interpretation and a straightforward implementation in code. Take a tentative step as before, then take a second one, and retrace back halfway from the landing point. Notice how the final corrected step lands much closer to the actual flow line than the original one (Figure 12).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"929\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration.png\" alt=\"A diagram showing a curve and arrows that attempt to follow it. The first arrow lands away from the curve, and is followed by a second arrow that continues from there. A third arrow shows a half-way point between the base of the first arrow, and the tip of the last arrow. This indicates the corrected step.\n\" class=\"wp-image-74969\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-300x139.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-625x290.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-179x83.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-768x357.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-1536x714.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-645x300.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-500x232.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-362x168.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-237x110.png 237w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/heun-step-geometric-illustration-1024x476.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 12. The Heun step illustrated geometrically</em></em></figcaption></figure>\n\n\n\n<p>With all of these improvements combined, it now suffices to evaluate the denoiser only 30 to 80 times, as opposed to 250 to 1,000 times as in most previous work.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Design choices for training the denoiser</h2>\n\n\n\n<p>This is now a sleek and efficient chain of denoising steps. Thus far, it\u2019s been assumed that each step can call a readily-trained denoiser <code>denoise (x, sigma)</code> taking in the noisy image and a number indicating its noise level. But how to\u200c parametrize and train it for best results? </p>\n\n\n\n<p>The most basic form of theoretically valid training for such a network (here, a PyTorch module instantiated as <code>denoise</code>) would look something like the following:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# WARNING: this code illustrates poor choices across the board!\n\nfor clean_image in training_data:\t# we\u2019ll ignore minibatching for brevity\n\n\t# pick a random noise level to train at\n\tsigma = np.random.uniform(0, 80)\n\n    # add noise with this level\n\tnoisy_image = clean_image + sigma * torch.randn_like(clean_image)\n\n    # feed to network under training\n    denoised_image = denoise(noisy_image, sigma)\n\n    # compute mean square loss\n\tloss = (denoised_image - clean_image).square().sum()\n\n\t# ... plus the usual backpropagation and parameter updates\n</pre></div>\n\n\n<p>The theory requires using white noise and mean-square loss, and to touch all the noise levels that are intended for use in sampling. Within these constraints, there is a lot of freedom to rearrange the computations. The following subsections identify and address each of the serious practical problems in this code.</p>\n\n\n\n<p>Note that the network architecture itself will not be addressed. This discussion is largely orthogonal and agnostic to layer counts, shapes and sizes, use of attention or transformers, and so on. For all the results in the paper, network architectures from previous work were adopted.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Network-friendly numerical magnitudes</h3>\n\n\n\n<p>The maximum noise level of 80 in these examples has been empirically chosen as a large enough number to completely drown out the image. Consequently, the denoiser is sometimes fed with an image whose pixel values are roughly in the range of -1 to 1 (when the noise level is very low), and sometimes with images in a range beyond -100 to 100. This raises a red flag, as neural networks are known to suffer from unstable training and poor final performance if their inputs vary vastly in scale between examples. It is necessary<em> </em>to standardize the scales.</p>\n\n\n\n<p>Some works combat this by modifying the ODE itself, such that the sampling process keeps the noisy image in a constant-magnitude range rather than allowing it to expand over time (a so-called <em>variance preserving</em> <em>scale schedule</em>). Unfortunately, this distorts the flow lines again, defeating the benefits of straightening uncovered in the previous section.</p>\n\n\n\n<p>A straightforward solution that does not suffer from such numerical drawbacks follows. The noise level is known, so simply scale the noisy image to a standard magnitude before feeding it to the network. It will automatically adapt to the different scale convention through training, but the problematic range variation is eliminated.&nbsp;</p>\n\n\n\n<p>The clean way to achieve this is to keep <code>denoise</code> unchanged from the viewpoint of external callers (the ODE solver and the training loop), but change the way it utilizes the network internally. Isolate the actual raw network layers into their own black box module <code>net</code>, and wrap it with magnitude management code (\u201cpreconditioning\u201d) within <code>denoise</code>:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nsigma_data = 0.5\t# approximate standard deviation of ImageNet pixels\ndef denoise(noisy_image, sigma):\n    noisy_image_variance = sigma**2 + sigma_data**2\n\tscaled_noisy_image = noisy_image / noisy_image_variance ** 0.5\n\treturn net(scaled_noisy_image, sigma)\n</pre></div>\n\n\n<p>Here, the noisy image is divided by its expected standard deviation to bring it roughly to unit variance.</p>\n\n\n\n<p>As a minor detail (not shown here), similarly also warp the noise level label input to <code>net</code> with a logarithmic function to make it more evenly spread around the range of -1 to 1.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Predicting the image versus the noise</h3>\n\n\n\n<p>If you\u2019re familiar with existing diffusion methods, you may have noticed that most of them train the network to predict noise (in unit variance) instead of the clean image, explicitly scale it to the known noise level <code>sigma</code>, and then recover the denoised image by subtraction from input.&nbsp;</p>\n\n\n\n<p>It turns out that this is a good idea specifically at low noise levels, but a bad one at high noise levels. Because most image detail is suddenly revealed at relatively low noise levels, the benefits outweigh the drawbacks.</p>\n\n\n\n<p>Why is this a good idea at low noise levels? This approach recycles the almost-clean image from the input, and only uses the network to add a small noise correction to it. Importantly, the network output is explicitly scaled down (by <code>sigma</code>) to match the noise level. Consequently, if the network makes some error (as it always does), that error becomes downscaled as well, and has less of an opportunity to mess up the image. This minimizes the contribution from the unreliable learned network, and maximizes the re-use of what is already known in the input.</p>\n\n\n\n<p>Why is this a bad idea at high noise levels? It ends up boosting the network output according to the large noise magnitude. Consequently, any small error the network makes now becomes a big error in the denoiser output.</p>\n\n\n\n<p>A better option is a continuous transition, where the network\u200c predicts a noise level dependent mixture of the (negative) noise and clean image. Then blend this with the noisy input in appropriate quantities to cancel the noise out.</p>\n\n\n\n<p>The paper presents a principled way to calculate the blending weights as a function of the noise level. The exact statistical argument is somewhat involved, so this post won&#8217;t attempt to replicate it in full. Basically, it is asking for the blend coefficients that result in minimal amplification of the network output. The implementation is quite straightforward. The last return line is replaced with the code below, where <code>c_skip</code> and <code>c_out</code> are those blend factors controlling how much of the input is recycled, and how much the network contributes, respectively.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nreturn c_skip * noisy_image + c_out * net(scaled_noisy_image, sigma)\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Equalizing the gradient feedback magnitudes across noise levels</h3>\n\n\n\n<p>With the denoiser internals done, this section addresses the noise level issues in our straw-man training code snippet. It was a (poor) implicit choice not to apply any noise-level-dependent scaling on the loss. It\u2019s as though the following was written:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nweight = 1\nloss = weight * (denoised_image - clean_image).square().sum()\n</pre></div>\n\n\n<p>The problem is that the value of this loss is large for some noise levels and small for others, due to the various scalings inside the denoiser. Consequently, the magnitude of the updates (gradient feedback) made to the network weights will also depend on the noise level. It&#8217;s like a different learning rate was used for different noise levels, for no good reason.&nbsp;</p>\n\n\n\n<p>This is yet another situation where unifying the magnitudes leads to a more stable and successful training. Fortunately, a simple data-independent statistical formula gives the expected loss magnitude for each noise level. <code>weight</code> is set accordingly to scale this magnitude back to 1.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Allocating \u200ctraining efforts</h3>\n\n\n\n<p>A tempting misuse of <code>weight</code> could be also weighing the noise levels according to their relative importance, so as to direct more network capacity where it counts. However, the same goal can be achieved without impacting the magnitudes by training more often at those important noise levels. The division of labor the team advocates is conceptually illustrated in Figure 13.&nbsp;</p>\n\n\n\n<p>Each noise level contributes gradient updates (the arrows) to the network weights throughout the training. Separately, we took control of the magnitudes and counts of these updates using the two respective mechanisms. By default, both the magnitude (length of the arrows) and the frequency (their number) depends on the noise level in an uncontrolled manner. The team advocates a division of labor where the loss scaling standardizes the lengths, and noise level distribution decides how often to train at each level.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"769\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram.png\" alt=\"A diagram showing a panel with \u201cbefore\u201d state, and an \u201cafter\u201d state corresponding to our recommendations. In the \u201cbefore\u201d state, an uncontrolled bunch of arrows of various lengths and number emanate from different noise levels. In the \u201cafter\u201d state, the loss scaling makes them equally long, and noise level distribution sets their number.\" class=\"wp-image-74974\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-300x115.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-625x240.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-179x69.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-768x295.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-1536x591.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-645x248.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-500x192.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-362x139.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-286x110.png 286w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/noise-level-distribution-before-after-diagram-1024x394.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 13. By default, both the magnitude (length of the arrows) and the frequency (their number) depends on the noise level in an uncontrolled manner</em></em></figcaption></figure></div>\n\n\n<p>Unsurprisingly, the code example that selects the training noise level from a uniform distribution is a poor choice. The theory offers quite little guidance in this choice, as it depends on the characteristics of the dataset. At very low noise levels, progress is minimal because predicting the noise from a noise-free image is effectively impossible (but also irrelevant). Conversely, at very high noise levels, the optimal denoising (blurry average of dataset images) is rather easy to predict. The middle provides a broad range of levels where progress can be made.&nbsp;</p>\n\n\n\n<p>In practice, we chose the random training noise levels from the formula <code>sigma = torch.exp(P_mean + P_std * torch.randn([]))</code>, where <code>P_mean</code> and <code>P_std</code> specify the average noise level for training, and the breadth of randomization around that value, respectively. This specific formula was chosen simply because it\u2019s a straightforward heuristic for drawing non-negative random values spanning multiple orders of magnitude. The values for these parameters are tuned empirically, but turn out to be fairly robust across regular image datasets.</p>\n\n\n\n<p>To summarize, below is a minimal piece that brings together all the discussed changes to our original training code, including any omitted formulas:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nP_mean = -1.2\t\t# average noise level (logarithmic)\nP_std = 1.2\t\t# spread of random noise levels\nsigma_data = 0.5\t# ImageNet standard deviation\n\ndef denoise(noisy_image, sigma):\n\t    # Input, output and skip scale\n    \tc_in = 1 / torch.sqrt(sigma_data**2 + sigma**2)\n    \tc_out = sigma * sigma_data / torch.sqrt(sigma**2 + sigma_data**2)\n    \tc_skip = sigma_data**2 / (sigma**2 + sigma_data**2)\n    \tc_noise = torch.log(sigma) / 4\t\t# noise label warp\n\n\t    # mix the input and network output to extract the clean image\n        return c_skip * noisy_image + \\\n                   c_out  * net(c_in * noisy_image, c_noise)\n\nfor clean_image in training_data:\t# we\u2019ll ignore minibatching for brevity\n       # random noise level\n        sigma = torch.exp(P_mean + P_std * torch.randn(&#91;]))\n\n       noisy_image = clean_image \\\n                            + sigma * torch.randn_like(clean_image)\n       denoised_image = denoise(noisy_image, sigma)\n\t\n\t   # weighted least squares loss\n       weight = (sigma**2 + sigma_data**2) / (sigma * sigma_data)**2\n\t   loss = weight * (denoised_image - clean_image).square().sum()\n\n\t   # ... plus backpropagation and optimizer update\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\">Results and conclusions</h2>\n\n\n\n<p>All findings presented in this post are shown to be beneficial by thorough numerical experiments, as detailed in <a href=\"https://research.nvidia.com/publication/2022-11_elucidating-design-space-diffusion-based-generative-models\">Elucidating the Design Space of Diffusion Based Models</a>. The net effect of incorporating all the improvements is a significant advancement of previous work. In particular, we held the world record FID metric in the highly competitive Imagenet 64&#215;64 category for some while. Moreover, we achieved this record with a greatly reduced number of denoiser evaluations at generation time.</p>\n\n\n\n<p>We believe these findings remain relevant into the future with other data modalities, improved network architectures, or higher-resolution images. Of course, one should still be mindful of the underlying reasoning when applying the model in a different context. For example, many constants (such as that maximum noise level of 80, or position and width of the training noise level distribution) will surely need to be adjusted when, for example, adopting latent diffusion or raising the resolution.</p>\n\n\n\n<p>To see our official implementation along with pretrained networks, visit <a href=\"https://github.com/NVlabs/edm\">NVlabs/edm</a> on GitHub. The code is a clean and minimal implementation that follows the paper notation and conventions, and could serve as an excellent starting point for experimenting and building on these ideas. Note that we include several functions and classes that reproduce previous methods for comparison, but these are not required to use or study our method. For the particularly relevant code, see:</p>\n\n\n\n<ul>\n<li><code>generate.py</code>\n<ul>\n<li><code>edm_sampler</code> implements the full sampler, including optional stochasticity</li>\n</ul>\n</li>\n\n\n\n<li><code>training/</code>\n<ul>\n<li><code>loss.EDMLoss</code> for the loss function and weightings</li>\n\n\n\n<li><code>networks.EDMPrecond</code> for the scale management and mixture prediction</li>\n\n\n\n<li><code>networks.DhariwalUNet</code> for our reimplementation of the commonly used ADM network architecture</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>The team has also just recently published a follow-up research paper, <a href=\"https://arxiv.org/abs/2312.02696\">Analyzing and Improving the Training Dynamics of Diffusion Models</a> that picks up where this post ends. In this work, they achieve record-breaking generation quality through a deep dive into the design and training of the denoiser network.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>With Internet-scale data, the computational demands of AI-generated content have grown significantly, with data centers running full steam for weeks or months to train a single model\u2014not to mention the high inference costs in generation, often offered as a service. In this context, suboptimal algorithmic design that sacrifices performance is an expensive mistake. Much of &hellip; <a href=\"https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/\">Continued</a></p>\n", "protected": false}, "author": 1951, "featured_media": 74794, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1316832", "discourse_permalink": "https://forums.developer.nvidia.com/t/generative-ai-research-spotlight-demystifying-diffusion-based-models/276135", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 696, 3110, 1903], "tags": [296, 453, 2932, 1953, 1962, 3596], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/generative-ai-image-graphic-e1702582621268.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jsl", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74793"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1951"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74793"}], "version-history": [{"count": 51, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74793/revisions"}], "predecessor-version": [{"id": 75558, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74793/revisions/75558"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74794"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74793"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74793"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74793"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75136, "date": "2023-12-14T11:57:06", "date_gmt": "2023-12-14T19:57:06", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75136"}, "modified": "2023-12-14T12:02:20", "modified_gmt": "2023-12-14T20:02:20", "slug": "fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/", "title": {"rendered": "Fast-Track Computer Vision Deployments with NVIDIA DeepStream and Edge Impulse"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>AI-based computer vision (CV) applications are increasing, and are particularly important for extracting real-time insights from video feeds. This revolutionary technology empowers you to unlock valuable information that was once impossible to obtain without significant operator intervention, and provides new opportunities for innovation and problem-solving.&nbsp;&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/deepstream-sdk\">NVIDIA DeepStream SDK</a> targets Intelligent Video Analytics (IVA) use cases that leverage machine learning (ML) to extract insights from video streams. It uses GPU acceleration for ML and accelerated hardware for maximum preprocessing performance when running on NVIDIA hardware.</p>\n\n\n\n<p>This post explores the potential of combining <a href=\"https://edgeimpulse.com/\">Edge Impulse</a> for model development with the NVIDIA DeepStream SDK for deployment so that you can rapidly create end-to-end applications. Edge Impulse is a member of the <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception</a> program.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Computer vision applications</h2>\n\n\n\n<p>The ability to build complex, scalable CV applications rapidly is critical in today\u2019s environment. Typical CV applications include diverse use cases such as vehicle identification, traffic measurement, inspection systems, quality control on production lines, safety and security enhancement through surveillance, smart checkout system implementation, and process measurement.</p>\n\n\n\n<p>Integrating machine intelligence to analyze multimedia streams in business processes can add immense value. Thanks to unparalleled accuracy and reliability, machine intelligence can help streamline operations, resulting in increased efficiency.</p>\n\n\n\n<p>Prebuilt AI models aren\u2019t always the right solution and often require fine-tuning for a specific problem that prebuilt models don&#8217;t account for.</p>\n\n\n\n<p>Building AI-based CV applications generally requires expertise in three skill sets: MLOps, CV application development, and deployment (DevOps). Without these specialized skills, the project ROI and delivery timeline could be at risk.&nbsp;</p>\n\n\n\n<p>In the past, sophisticated CV applications required highly specialized developers. This translated to long learning curves and expensive resources.&nbsp;</p>\n\n\n\n<p>The combination of Edge Impulse and the NVIDIA DeepStream SDK offers a user-friendly, complementary solution stack that helps developers quickly create IVA solutions. You can easily customize applications for a specific use case, integrating NVIDIA hardware directly into your solution.&nbsp;</p>\n\n\n\n<p>DeepStream is free to use and Edge Impulse offers a free tier that suits many ML model-building use cases.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1837\" height=\"1165\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack.jpg\" alt=\"Edge Impulse ML tools are at the top of the stack and used for training. The bottom half of the stack is for building models and consists of Python and C/C++ at the top, followed by Deepstream SDK, CUDA-X, and the NVIDIA computing platform as the foundation. \n\" class=\"wp-image-75144\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack.jpg 1837w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-300x190.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-625x396.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-179x115.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-768x487.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-1536x974.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-645x409.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-473x300.jpg 473w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-142x90.jpg 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-362x230.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-173x110.jpg 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-nvidia-deepstream-solution-stack-1024x649.jpg 1024w\" sizes=\"(max-width: 1837px) 100vw, 1837px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Edge Impulse and NVIDIA DeepStream SDK solution stack</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Building CV applications with NVIDIA DeepStream&nbsp;</h2>\n\n\n\n<p>Deepstream SDK is a component of <a href=\"https://developer.nvidia.com/metropolis\">NVIDIA Metropolis</a>, which is designed to support video analytics at scale. You can quickly and easily create production-ready CV pipelines that can be deployed directly on NVIDIA hardware appliances.&nbsp;</p>\n\n\n\n<p>DeepStream apps are built using the following approaches:</p>\n\n\n\n<ul>\n<li>From the command line</li>\n\n\n\n<li>Visually using Graph Composer</li>\n\n\n\n<li>Without code using the DeepStream reference application and config files</li>\n\n\n\n<li>With C++ or Python code for more customization&nbsp;</li>\n</ul>\n\n\n\n<p>If you aren\u2019t a developer, you can have a pipeline up and running using one of the first three options together with your trained ML model in less than an hour. If you need more customization, you can build a custom-coded solution from existing templates as a starting point.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Deploying CV applications</h2>\n\n\n\n<p>Once you have created your pipeline, it can be deployed directly on NVIDIA hardware appliances. These range from edge devices, like the <a href=\"https://developer.nvidia.com/embedded/jetson-nano\">NVIDIA Jetson Nano</a>, to high-performance computing (HPC) and cloud deployments, and a hybrid approach.</p>\n\n\n\n<p>You can deploy your application to run locally on NVIDIA edge hardware with your video source directly connected for minimal latency. If you need to handle complex pipelines or accommodate multiple video sources that exceed the capability of an NVIDIA edge appliance, you can deploy the same pipeline to an NVIDIA-based cloud instance on your preferred IaaS provider.&nbsp;&nbsp;</p>\n\n\n\n<p>A hybrid approach is also possible, where the pipeline can be deployed to an NVIDIA edge appliance and inference can be performed remotely using <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a>.</p>\n\n\n\n<p>Triton enables remote execution of models, receiving input frames from the client and serving back the results. Triton leverages NVIDIA GPUs when present and can also perform inference on x86 with support for concurrency and dynamic batching. Triton also has native support for most popular frameworks, including TensorFlow and PyTorch.</p>\n\n\n\n<p>DeepStream supports Triton through an alternative to the Gst-nvinfer inference plugin called Gst-nvinferserver. This plugin enables you to use a Triton instance in a DeepStream application.</p>\n\n\n\n<p>IVA applications are only as good as the ML models they are built around. While many pre built models are available, use cases often require customized models and MLOps workflows. This is where having an easy-to-use MLOps platform enables speedy deployments, especially when combined with DeepStream rapid application development.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Edge Impulse for machine learning</h2>\n\n\n\n<p>Edge Impulse offers a powerful suite of tools to build ML models that can be deployed directly onto NVIDIA targets and dropped into DeepStream applications. Seamlessly integrating with NVIDIA hardware acceleration and the DeepStream SDK, Edge Impulse helps you scale your projects quickly.&nbsp;</p>\n\n\n\n<p>Edge Impulse guides developers at all levels throughout the process. Experienced ML professionals will appreciate the ease and convenience of bringing in data from different sources, as well as the end-to-end model-buildinging process. You can also integrate custom models with the custom learning blocks feature, which takes the heavy lifting out of MLOps.</p>\n\n\n\n<p>For those new to machine learning, the Edge Impulse process guides you in building basic models as you use the environment. The basic model types you can use with DeepStream are YOLO object detection and classification.&nbsp;</p>\n\n\n\n<p>You can also repurpose models built for tinyML targets so they work with edge use cases and the more powerful NVIDIA hardware. Many edge AI use cases involve complex applications that demand more powerful compute resources. NVIDIA hardware can help solve challenges associated with the limitations of constrained devices.&nbsp;</p>\n\n\n\n<p>While you can create your own models from scratch with Edge Impulse, it also integrates with <a href=\"https://developer.nvidia.com/tao-toolkit\">NVIDIA TAO Toolkit</a> so you can leverage over 100 pretrained models in the <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/index.html\">Computer Vision Model Zoo</a>. Edge Impulse complements TAO and can be used to adapt these models to custom applications. It is a great starting point for enterprise users.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"334\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-625x334.png\" alt=\"Diagram showing models from NVIDIA TAO can be used by Edge Impulse Enterprise users\" class=\"wp-image-75152\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-625x334.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-768x410.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-645x345.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-960x515.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-500x267.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-362x193.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability-206x110.png 206w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-tao-edge-impulse-interoperability.png 964w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA TAO combined with Edge Impulse Enterprise</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Building models for DeepStream with Edge Impulse</h2>\n\n\n\n<p>Once you are done building your model, deploy it into DeepStream. Export your model files from Edge Impulse and drop them into your DeepStream project. Then follow the configuration steps to ensure your Edge Impulse model works with DeepStream. The process generally involves four steps (Figure 3).\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1922\" height=\"347\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream.png\" alt=\"Diagram of four steps for deploying model files from Edge Impulse into NVIDIA DeepStream. Step 1: Build model in Edge Impulse. Step 2: Export model from Edge Impulse. Step 3: Convert model to DeepStream compatible ONNX. Step 4: Create inference plugin configuration file.\n\" class=\"wp-image-75156\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream.png 1922w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-300x54.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-625x113.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-179x32.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-768x139.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-1536x277.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-645x116.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-500x90.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-160x29.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-362x65.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-609x110.png 609w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/deploy-model-files-from-edge-impulse-into-deepstream-1024x185.png 1024w\" sizes=\"(max-width: 1922px) 100vw, 1922px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Four steps for deploying model files from Edge Impulse into NVIDIA DeepStream</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Step 1: Build model in Edge Impulse</h3>\n\n\n\n<p>Start by building either a YOLO or Image Classification model in Edge Impulse Studio. The DeepStream inference <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html\">Gst-nvinfer plugin</a> requires tensors to be in NCHW format for the input layer. Be sure to select Jetson Nano as the target and use FP32 weights.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 2: Export model from Edge Impulse</h3>\n\n\n\n<p>Edge Impulse can export models from the Dashboard page in Edge Impulse Studio. YOLOv5 can be exported as an ONNX with an NCHW input layer ready for use with DeepStream.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1327\" height=\"496\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export.png\" alt=\"Screenshot of Edge Impulse Studio Dashboard, indicating where to export as an ONNX.\n\" class=\"wp-image-75157\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export.png 1327w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-300x112.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-625x234.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-179x67.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-768x287.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-645x241.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-500x187.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-160x60.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-362x135.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-294x110.png 294w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-onnx-export-1024x383.png 1024w\" sizes=\"(max-width: 1327px) 100vw, 1327px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Edge Impulse Studio Dashboard showing how to export as an ONNX model</em></figcaption></figure>\n\n\n\n<p>An IVA pipeline in DeepStream typically consists of a primary inference (PGIE) step that performs object detection with the bounding box coordinates. Associated object classes are passed to a secondary inference step (SGIE) that classifies each object. Each is implemented as an instance of the Gst-nvinfer plugin.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Step 3: Convert model to DeepStream compatible ONNX</h3>\n\n\n\n<p>When using YOLO with DeepStream, a custom output layer parser is required to extract the bounding boxes and object classes from the output layers that are then passed to the next plugin. For more details about the custom YOLO output parser, see <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_custom_YOLO.html\">How to Use the Custom YOLO Model</a>.</p>\n\n\n\n<p>Edge Impulse uses YOLOv5, which is a more recent, higher performance model, and has a slightly different output tensor format than YOLOv3. YOLOv3 has three output layers, each responsible for detecting objects at different scales, whereas YOLOv5 has a single output layer that uses anchor boxes to handle objects of various sizes.</p>\n\n\n\n<p>DeepStream is based on GStreamer, which was designed for multimedia use cases. NVIDIA has added features to support deep learning within a GStreamer pipeline, including additional ML-related metadata which is passed down the pipeline with Gst-Buffer and encapsulated in the NvDsBatchMeta structures with Gst-Buffer.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"720\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy.png\" alt=\"DeepStream NvDsBatchMeta hierarchy diagram.\" class=\"wp-image-75158\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy.png 720w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-300x200.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-625x417.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-173x115.png 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-645x430.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-450x300.png 450w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-135x90.png 135w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-362x241.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvidia-deepstream-metadata-hierarchy-165x110.png 165w\" sizes=\"(max-width: 720px) 100vw, 720px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. DeepStream metadata hierarchy</em></figcaption></figure></div>\n\n\n<p>The output tensor from YOLO is different from the bounding box data required by DeepStream which is held in NvDsObjectMeta. To use YOLO with DeepStream, a custom output parser is needed to transform YOLO output to meet NvDsObjectMeta\u2019s requirements at run-time. NVIDIA provides <a href=\"https://docs.nvidia.com/metropolis/deepstream/dev-guide/_images/DS_plugin_metadata.png\">a sample plugin</a> that works through YOLOv3.&nbsp;</p>\n\n\n\n<p>Edge Impulse uses YOLOv5. The differences between the output layers of YOLOv3 and YOLOv5 make YOLOv3 plugin unsuitable for use with YOLOv5 (Figure 6).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"531\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure.png\" alt=\"YOLOv3 and YOLOv5 output layer visualized in Netron.\" class=\"wp-image-75159\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-300x80.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-625x166.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-179x48.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-768x204.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-1536x408.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-645x171.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-500x133.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-160x43.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-362x96.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-414x110.png 414w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/comparison-yolov3-yolov5-output-tensor-structure-1024x272.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. A comparison of the YOLOv3 and YOLOv5 output tensor structure</em></figcaption></figure></div>\n\n\n<p>To use the YOLOv5 model trained in Edge Impulse, a custom YOLOv5 output parser must be created to process the single output tensor. One implementation that can be used is a <a href=\"https://github.com/marcoslucianops/DeepStream-Yolo\">third-party output parser</a> that works with the Edge Impulse ONNX exports.</p>\n\n\n\n<p>For Image Classification models, the default TFLite Float32 provided by Edge Impulse in NHWC format and its input layer need to be converted to NCHW.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1373\" height=\"497\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32.png\" alt=\"Screenshot of Edge Impulse Studio Dashboard, indicating where to find the TFLight Float32.\" class=\"wp-image-75160\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32.png 1373w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-625x226.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-768x278.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-645x233.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-500x181.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-362x131.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-304x110.png 304w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-studio-dashboard-tflight-float32-1024x371.png 1024w\" sizes=\"(max-width: 1373px) 100vw, 1373px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Edge Impulse Studio Dashboard showing where to find the TFLight Float32</em></figcaption></figure></div>\n\n\n<p>This is easily achieved using the following <code>tf2onnx</code> command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: bash; title: ; notranslate\" title=\"\">\npython -m tf2onnx.convert  --inputs-as-nchw serving_default_x:0  --opset 13 --tflite MODELFILE  --output OUTPUT.ONNX\n</pre></div>\n\n\n<p><code>MODELFILE</code> is the input file and <code>OUTPUT.ONNX</code> is the output file that specifies the input layer name generated by Edge Impulse as <code>serving_default_x:0</code>. As a result, the input layer is transformed to meet DeepStream requirements.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"490\" height=\"123\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer.png\" alt=\"Edge Impulse\u2019s default input layer is NHWC where as Gst-nvinfer requires NCHW\n\" class=\"wp-image-75161\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer.png 490w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer-300x75.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer-179x45.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer-160x40.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer-362x91.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/edge-impulse-default-input-layer-438x110.png 438w\" sizes=\"(max-width: 490px) 100vw, 490px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. Edge Impulse default input layer shape compared to the DeepStream Gst-nvinfer plugin</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Step 4: Create inference plugin configuration file</h3>\n\n\n\n<p>DeepStream requires the creation of plain text configuration files for each instance of the Gst-nvinfer plugin to specify runtime requirements. This includes the ONNX model file or generated TRT Engine file and the text file containing the label names.\u00a0 Figure 9 shows the minimum set of parameters required to use the Edge Impulse YOLOv5 and classification models.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"862\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration.png\" alt=\"Screenshot of Gst-nvinfer configuration parameter for Edge Impulse.\n\" class=\"wp-image-75164\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-625x270.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-768x331.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-1536x662.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-645x278.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-500x216.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-362x156.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-255x110.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gst-nvinfer-plugin-configuration-1024x442.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. Gst-nvinfer plugin configuration parameters for models built using Edge Impulse</em></figcaption></figure>\n\n\n\n<p>Note that, although comments are shown inline with parameters for illustrative purposes, all configuration parameters should be separated into new lines.</p>\n\n\n\n<p>The <code>process-mode</code> parameter can be used to specify whether the plugin is a primary or secondary stage. Note that the ONNX file is specified and DeepStream uses <code>trtexec</code> to generate the TensorRT Engine that <a href=\"https://github.com/NVIDIA/TensorRT\">NVIDIA TensorRT</a> executes on NVIDIA GPUs.\u00a0</p>\n\n\n\n<p>After creating the engine, specify it using the <code>model-engine-file</code> parameter. The <code>model-file</code> parameter can be commented out to prevent the engine from being recreated on each run, thereby saving on startup time.\u00a0</p>\n\n\n\n<p>Depending on the <code>model-color-mode</code> (whether the model is RGB or grayscale), the parameter must be set to 0 or 2, respectively. This will correspond to the color depth set in Edge Impulse Studio.</p>\n\n\n\n<p>The preceding example shows how the model used as the primary inference plugin. The model can also be used as the second-stage classifier by setting the <code>process-mode</code> property as follows:\u00a0</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nprocess-mode=2 #SGIE\n</pre></div>\n\n\n<p>The example in Figure 9 also shows the minimal configuration files needed for a two-stage pipeline where the YOLO model first detects objects, then individually classifies them in the second stage classifier. For the YOLO model, the default YOLO label file can be edited and the labels replaced with labels from the custom model with each label on a new line, per the YOLO standard format.&nbsp;</p>\n\n\n\n<p>In the case of the classification model, labels are separated by semicolons. During run time, the models will be indexed accordingly from these files and the text you specify will be displayed.</p>\n\n\n\n<p>DeepStream can be used by referencing the configuration files in your pipeline that have these settings embedded.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>This post has explained how to leverage Edge Impulse and NVIDIA DeepStream SDK to quickly create HPC vision applications. These applications include vehicle identification, traffic measurement, inspection systems, quality control on production lines, safety and security enhancement through surveillance, smart checkout system implementation, and process measurement. New use cases for AI and IVA are continually emerging.&nbsp;</p>\n\n\n\n<p>To learn more about DeepStream, see <a href=\"https://developer.nvidia.com/deepstream-getting-started\">Get Started With the NVIDIA DeepStream SDK</a>. To get started with Edge Impulse and DeepStream, see <a href=\"https://docs.edgeimpulse.com/experts/featured-machine-learning-projects/nvidia-deepstream-community-guide\">Using Edge Impulse with NVIDIA DeepStream</a>. This guide includes a link to a repository with a preconfigured DeepStream pipeline that you can use to validate performance or develop your own pipeline. A precompiled custom parser for the Jetson Nano architecture is also included to help you get started.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI-based computer vision (CV) applications are increasing, and are particularly important for extracting real-time insights from video feeds. This revolutionary technology empowers you to unlock valuable information that was once impossible to obtain without significant operator intervention, and provides new opportunities for innovation and problem-solving.&nbsp;&nbsp; NVIDIA DeepStream SDK targets Intelligent Video Analytics (IVA) use cases &hellip; <a href=\"https://developer.nvidia.com/blog/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/\">Continued</a></p>\n", "protected": false}, "author": 1962, "featured_media": 75142, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1316852", "discourse_permalink": "https://forums.developer.nvidia.com/t/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/276142", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 852, 2758], "tags": [422, 453, 1472, 1961, 2056, 1177], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/boxes-conveyor-belt-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jxS", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75136"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1962"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75136"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75136/revisions"}], "predecessor-version": [{"id": 75562, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75136/revisions/75562"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75142"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75136"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75136"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75136"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75396, "date": "2023-12-14T11:44:18", "date_gmt": "2023-12-14T19:44:18", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75396"}, "modified": "2023-12-14T11:50:12", "modified_gmt": "2023-12-14T19:50:12", "slug": "simulate-and-localize-a-husky-robot-with-nvidia-isaac", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simulate-and-localize-a-husky-robot-with-nvidia-isaac/", "title": {"rendered": "Simulate and Localize a Husky Robot with NVIDIA Isaac"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The <a href=\"https://clearpathrobotics.com/husky-unmanned-ground-vehicle-robot/\">Husky robot</a>, developed by Clearpath Robotics, is a versatile four-wheeled platform made for indoor and outdoor research use. It is simple to modify by adding other sensors and changing the high-level board. This post explains how to use the official ROS 2 Husky packages to import the robot into <a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim</a> and create a simulation. </p>\n\n\n\n<p>For this demo, the Husky robot is equipped with an <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">NVIDIA Jetson Orin Nano</a> and a ZED 2 camera mounted on top. Driving the Husky uses the latest version of Isaac ROS 2, which includes Isaac ROS packages for robot localization (<a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_visual_slam/index.html\">NVIDIA Isaac ROS VSLAM</a>), map building (<a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_nvblox/index.html\">NVIDIA Isaac ROS NvBlox</a>), and Apriltag detection (<a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_apriltag/index.html\">NVIDIA Isaac ROS AprilTag</a>).&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">How the Isaac ROS demo works</h2>\n\n\n\n<p>The Husky unified robotics description format (URDF) is dynamically loaded from a ROS 2 topic. It is used to visualize the robot in ROS visualization (<code>rviz</code>) and load it into NVIDIA Isaac Sim.</p>\n\n\n\n<p>When the robot is loaded, the main Isaac Sim script creates a graph to drive the robot, converting the velocity and steering commands into velocity for each wheel, and publishes the wheel status in a <code>tf</code> (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"825\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph.png\" alt=\"Husky motor control graph screenshot.\" class=\"wp-image-75402\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-300x124.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-625x258.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-768x317.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-1536x634.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-645x266.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-500x206.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-362x149.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-267x110.png 267w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-motor-control-action-graph-1024x423.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Motor control action graph, automatically generated when Husky is loaded from the ROS 2 topic</em></figcaption></figure></div>\n\n\n<p>The script generates a graph for each camera to publish the output in a ROS 2 topic. Each camera has a ROS 2 image message and camera information.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"825\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1.png\" alt=\"Husky camera control graph screenshot.\" class=\"wp-image-75406\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-300x124.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-625x258.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-768x317.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-1536x634.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-645x266.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-500x206.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-362x149.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-267x110.png 267w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-camera-control-graph-1-1024x423.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Camera graph with resolution configuration and publish ROS 2 topic&nbsp;</em></figcaption></figure></div>\n\n\n<p>After loading all graphs and successfully importing the URDF, a new Husky robot appears in Isaac Sim, as shown in Figure 3.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1460\" height=\"955\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim.png\" alt=\"Husky robot on NVIDIA Isaac Sim screenshot.\" class=\"wp-image-75407\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim.png 1460w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-300x196.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-625x409.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-176x115.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-768x502.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-645x422.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-459x300.png 459w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-138x90.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-362x237.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-168x110.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-on-nvidia-isaac-sim-1024x670.png 1024w\" sizes=\"(max-width: 1460px) 100vw, 1460px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. NVIDIA Isaac Sim 2023.1.0 with the Husky and all cameras output</em></figcaption></figure></div>\n\n\n<p>When the Husky has been successfully loaded, different options for ROS 2 will be available, including camera streams, the motor controller, and other auxiliary outputs. You can quickly check what is running in another terminal using the following command:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>ros2 topic list\n</code></pre>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1572\" height=\"1066\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal.png\" alt=\"Husky ROS 2 topics on terminal screenshot.\" class=\"wp-image-75410\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal.png 1572w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-300x203.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-625x424.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-170x115.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-768x521.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-1536x1042.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-645x437.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-442x300.png 442w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-133x90.png 133w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-362x245.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-162x110.png 162w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-ros-2-topics-on-terminal-1024x694.png 1024w\" sizes=\"(max-width: 1572px) 100vw, 1572px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. List of all Husky ROS 2 topics</em></figcaption></figure></div>\n\n\n<p>When the ROS 2 launcher script starts, all required ROS 2 nodes and Isaac ROS nodes run. The robot will build a connected pipeline where the husky URDF coming from the \u2018Husky description\u2019 will be used for Isaac Sim and the robot itself. Isaac ROS VSLAM and Isaac ROS NvBlox will also be used to localize and build a real-time map.</p>\n\n\n\n<p>With all the necessary packages now running, the robot can be fully localized and is capable of building a 3D map. You can now drive it using Nav2, a keyboard, or a joystick.&nbsp;</p>\n\n\n\n<p>The next step is setting up your workstation and Jetson Orin (optional) to test Husky on Isaac Sim 2023 and Isaac ROS 2.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1574\" height=\"1115\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map.png\" alt=\"3D-generated map on rviz2 with Husky robot.\" class=\"wp-image-75411\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map.png 1574w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-300x213.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-625x443.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-162x115.png 162w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-768x544.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-1536x1088.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-645x457.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-423x300.png 423w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-127x90.png 127w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-362x256.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-155x110.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-3d-generated-map-1024x725.png 1024w\" sizes=\"(max-width: 1574px) 100vw, 1574px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Husky on the move and generating a 3D map</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Set up your demo</h2>\n\n\n\n<p>There are two ways to run this demo: directly from your workstation or using hardware-in-the-loop (HIL) configuration. This section covers the setup requirements for each of these options.&nbsp;</p>\n\n\n\n<p>For either option, first install the latest version of Isaac Sim. To see detailed instructions, visit <a href=\"https://www.nvidia.com/en-us/omniverse/download/\">Get Started With NVIDIA Omniverse</a>.&nbsp;</p>\n\n\n\n<p>Note: Do not start Isaac Sim when the download is complete.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Husky demo workstation version</h3>\n\n\n\n<p>To run the Husky demo, you need a workstation with an NVIDIA RTX GPU and the latest versions of both <a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim</a> and <a href=\"https://docs.ros.org/en/humble/Installation.html\">ROS 2 Humble</a> installed. Remember to install the desktop versions.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>sudo apt install ros-humble-desktop\n</code></pre>\n\n\n\n<h4 class=\"wp-block-heading\">Install and run</h4>\n\n\n\n<p>When your system is ready, you can clone the demo and try it on your workstation using the following script:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>git clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\n</code></pre>\n\n\n\n<p>After running it, use the following command to start Isaac Sim and open a new terminal with Docker built to use Isaac ROS and all packages needed for the demo:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>./husky_demo.sh</code></pre>\n\n\n\n<p>This command will:</p>\n\n\n\n<ul>\n<li>Clone all repositories required</li>\n\n\n\n<li>Clone the Isaac ROS Docker image</li>\n\n\n\n<li>Build a new image</li>\n</ul>\n\n\n\n<p>When the build is complete, the terminal will be waiting for a new command (Figure 6). Figure 7 shows Isaac Sim running and the environment fully loaded.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"786\" height=\"533\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt.png\" alt=\"Screenshot of terminal with Isaac ROS prompt.\n\" class=\"wp-image-75417\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt.png 786w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-300x203.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-625x424.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-170x115.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-768x521.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-645x437.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-442x300.png 442w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-133x90.png 133w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-362x245.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-terminal-isaac-ros-prompt-162x110.png 162w\" sizes=\"(max-width: 786px) 100vw, 786px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Terminal with a Docker container with Isaac ROS prompt waiting for a command</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1460\" height=\"955\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment.png\" alt=\"Screenshot of Isaac Sim with empty environment loaded\n\" class=\"wp-image-75419\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment.png 1460w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-300x196.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-625x409.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-176x115.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-768x502.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-645x422.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-459x300.png 459w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-138x90.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-362x237.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-168x110.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-sim-warehouse-environment-1024x670.png 1024w\" sizes=\"(max-width: 1460px) 100vw, 1460px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Isaac Sim running with the warehouse environment loaded&nbsp;</em></figcaption></figure>\n\n\n\n<p>Now you can run this script on your second terminal:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>bash src/husky_isaac_sim/scripts/run_in_docker.sh</code></pre>\n\n\n\n<p>The script <code>run_in_docker.sh</code> will execute some steps before running the Isaac ROS launch file. The first stage will update and build all required ROS 2 and Isaac ROS packages. Then it will execute the launch file:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>ros2 launch husky_isaac_sim allinone.launch.py</code></pre>\n\n\n\n<p>This ROS 2 script launches all Isaac ROS packages to localize the robot and start mapping and <code>rviz</code> to visualize the husky on map.</p>\n\n\n\n<p>The script will also load a Husky on the environment and automatically set up cameras and controllers.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Husky demo HIL version</h3>\n\n\n\n<p>For the HIL version, you&#8217;ll need a workstation with an NVIDIA RTX GPU and the latest version of NVIDIA Isaac Sim 2023.1.0 installed, as well as a router and an NVIDIA Jetson Orin Nano. (Note that other Jetson Orin models will also do the job.)&nbsp;</p>\n\n\n\n<p>It&#8217;s important to establish a wired connection between your workstation and your Jetson, and using a good router between them is highly recommended. Keep in mind that there will be a lot of data shared between your workstation and the Jetson, so a Wi-Fi connection won&#8217;t be sufficient.</p>\n\n\n\n<p>Your NVIDIA Jetson Orin Nano must use the latest Jetpack 5.1.2 and be fully installed. Keep the IP address. If you hostname, you&#8217;ll need to connect remotely from your workstation.</p>\n\n\n\n<p>The <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/installation/requirements.html\">required components</a> to run with this configuration are listed below:</p>\n\n\n\n<ul>\n<li>x86/64 machine with Ubuntu 22.04</li>\n\n\n\n<li>NVIDIA RTX GPU</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/embedded/learn/get-started-jetson-orin-nano-devkit\">NVIDIA Jetson Orin Nano Developer Kit</a> with Jetpack 5.1.2</li>\n\n\n\n<li>Router and cables</li>\n</ul>\n\n\n\n<p>Figure 8 shows how to configure your hardware environment.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"451\" height=\"201\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup.png\" alt=\"Diagram of hardware setup. From the left is a desktop with an NVIDIA RTX graphic card, router (center), and an NVIDIA Jetson Orin Nano Developer Kit (right).\n\" class=\"wp-image-75424\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup.png 451w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup-300x134.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup-179x80.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup-160x71.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup-362x161.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-hardware-setup-247x110.png 247w\" sizes=\"(max-width: 451px) 100vw, 451px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. Hardware setup: desktop within an NVIDIA RTX graphics card (left), a router (center), and an NVIDIA Jetson Orin Nano Developer Kit (right)</em></figcaption></figure>\n\n\n\n<h4 class=\"wp-block-heading\">Install and run</h4>\n\n\n\n<p>When your system is ready, clone the demo and try it on your workstation using the following command:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>git clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\n</code></pre>\n\n\n\n<p>After running it, use the following command:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>./husky_demo.sh --HIL\n</code></pre>\n\n\n\n<p>This script will automatically:</p>\n\n\n\n<ul>\n<li>Clone all repositories required for this demo</li>\n\n\n\n<li>Build all ROS 2 packages</li>\n\n\n\n<li>Start Isaac Sim&nbsp;</li>\n</ul>\n\n\n\n<p>When the build is complete, the terminal will be waiting for a new command. Run the following script:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>ros2 launch husky_isaac_sim robot_display.launch.py</code></pre>\n\n\n\n<p>You will see an image of a Husky on Isaac Sim displayed on your workstation.</p>\n\n\n\n<p>Next, open a new terminal to remotely connect to the NVIDIA Jetson Orin series.</p>\n\n\n\n<p>When you are logged in to the Jetsterminal, clone the Husky demo repository:&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>git clone https://github.com/NVIDIA-AI-IOT/husky_demo.git\ncd husky_demo\n</code></pre>\n\n\n\n<p>After running it, use the following command:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>./husky_demo.sh\n</code></pre>\n\n\n\n<p>This script will automatically:</p>\n\n\n\n<ul>\n<li>Clone all repositories required for this demo</li>\n\n\n\n<li>Clone the Isaac ROS docker image</li>\n\n\n\n<li>Build a new image</li>\n</ul>\n\n\n\n<p>Next, run the following script from the Docker container:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>bash src/husky_isaac_sim/scripts/run_in_docker.sh\n</code></pre>\n\n\n\n<p>This script will automatically:</p>\n\n\n\n<ul>\n<li>Build all ROS 2 packages</li>\n\n\n\n<li>Start Isaac ROS&nbsp;</li>\n</ul>\n\n\n\n<p>You will then see all Isaac ROS packages running on the terminal (Figure 9).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1437\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros.png\" alt=\"Remote terminal to Jetson Orin Nano with Isaac ROS running.\" class=\"wp-image-75432\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-300x216.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-625x449.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-160x115.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-768x552.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-1536x1104.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-645x464.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-417x300.png 417w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-125x90.png 125w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-362x260.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-153x110.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/screenshot-remote-terminal-jetson-orin-nano-isaac-ros-1024x736.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. Isaac ROS running on Jetson Orin Nano and transmitting data</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Drive your Husky</h2>\n\n\n\n<p>Assuming that both Isaas Sim and Isaac ROS are up and running\u2014either on your workstation or Jetson, according to your preference\u2014and there are no issues, you should be able to see the robot localized successfully on <code>rviz</code>. Once localized, it should start building a 3D map (Figure 10).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1574\" height=\"1115\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz.png\" alt=\"Husky localized on rviz.\" class=\"wp-image-75436\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz.png 1574w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-300x213.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-625x443.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-162x115.png 162w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-768x544.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-1536x1088.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-645x457.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-423x300.png 423w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-127x90.png 127w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-362x256.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-155x110.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/husky-robot-localized-rviz-1024x725.png 1024w\" sizes=\"(max-width: 1574px) 100vw, 1574px\" /><figcaption class=\"wp-element-caption\"><em>Figure 10. Husky on <code>rviz</code> localized and plotting a first slice of a 3D map with NvBlox</em></figcaption></figure></div>\n\n\n<p>Now, you can drive your Husky using a keyboard, a joystick, or with Nav2. In just a few quick steps, you can control the movement of the Husky within the environment.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>This post has explained how to operate the Husky robot using NVIDIA Isaac ROS, including how to load a URDF derived from a robot description, and how to drive it on <code>rviz</code>. This demo can be run in various ways, either directly from your workstation with an NVIDIA RTX graphics card or in the Hardware In the Loop configuration, where Isaac ROS runs on your NVIDIA Jetson.</p>\n\n\n\n<p>To learn more about Isaac Sim and Isaac ROS, check out our new <a href=\"https://www.youtube.com/playlist?list=PL3jK4xNnlCVePQFJ9zVOvMofngfK49xki\">Isaac ROS Office Hours</a> on YouTube. And join the conversation on the <a href=\"https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67\">NVIDIA Developer Robotics Forum</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The Husky robot, developed by Clearpath Robotics, is a versatile four-wheeled platform made for indoor and outdoor research use. It is simple to modify by adding other sensors and changing the high-level board. This post explains how to use the official ROS 2 Husky packages to import the robot into NVIDIA Isaac Sim and create &hellip; <a href=\"https://developer.nvidia.com/blog/simulate-and-localize-a-husky-robot-with-nvidia-isaac/\">Continued</a></p>\n", "protected": false}, "author": 1255, "featured_media": 75536, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1316846", "discourse_permalink": "https://forums.developer.nvidia.com/t/simulate-and-localize-a-husky-robot-with-nvidia-isaac/276140", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [453, 3130, 1305, 2792, 1410, 570], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/isaac-robotics-clearpath-husky-robot.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jC4", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75396"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1255"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75396"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75396/revisions"}], "predecessor-version": [{"id": 75560, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75396/revisions/75560"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75536"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75396"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75396"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75396"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75292, "date": "2023-12-12T15:33:55", "date_gmt": "2023-12-12T23:33:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75292"}, "modified": "2023-12-14T11:27:26", "modified_gmt": "2023-12-14T19:27:26", "slug": "oracle-cloud-infrastructure-sets-quantitative-financial-hpc-calculations-record-with-nvidia-gpus", "status": "publish", "type": "post", "link": "https://blogs.oracle.com/cloud-infrastructure/post/oci-nvidia-a100-tensor-core-gpus-hpc-ai-fsi", "title": {"rendered": "Oracle Cloud Infrastructure Sets Quantitative Financial HPC Calculations Record with NVIDIA GPUs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA A100 Tensor Core GPUs were featured in a stack that set several records in a recent STAC-A2\u2122 benchmark standard based on financial market risk analysis.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA A100 Tensor Core GPUs were featured in a stack that set several records in a recent STAC-A2\u2122 benchmark standard based on financial market risk analysis.</p>\n", "protected": false}, "author": 338, "featured_media": 75296, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1315494", "discourse_permalink": "https://forums.developer.nvidia.com/t/oracle-cloud-infrastructure-sets-quantitative-financial-hpc-calculations-record-with-nvidia-gpus/275903", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://blogs.oracle.com/cloud-infrastructure/post/oci-nvidia-a100-tensor-core-gpus-hpc-ai-fsi", "_links_to_target": "_blank"}, "categories": [852], "tags": [453, 1914], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Oracle-NV-STAC-A2-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jAo", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75292"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/338"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75292"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75292/revisions"}], "predecessor-version": [{"id": 75298, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75292/revisions/75298"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75296"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75292"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75292"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75292"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75113, "date": "2023-12-12T10:00:00", "date_gmt": "2023-12-12T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75113"}, "modified": "2023-12-14T11:27:27", "modified_gmt": "2023-12-14T19:27:27", "slug": "next-generation-seismic-monitoring-with-neural-operators", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/next-generation-seismic-monitoring-with-neural-operators/", "title": {"rendered": "Next-Generation Seismic Monitoring with Neural Operators"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Virtual seismology has only been around for a few years, and it has already had a significant impact on earthquake monitoring.\u00a0</p>\n\n\n\n<p>Historically, seismic phase picking\u2014the task of annotating seismograms with seismic wave arrival times that underpins earthquake monitoring operations globally\u2014was a manual process. As such, it was labor-intensive, fraught with subjectivity, and prone to errors. </p>\n\n\n\n<p>Deep learning techniques powered by NVIDIA GPUs have addressed these challenges. Our paper, <a href=\"https://arxiv.org/abs/2305.03269\">Phase Neural Operator for Multi-Station Picking of Seismic Arrivals</a>, introduces a general-purpose network-wide phase-picking algorithm based on a recently developed machine learning paradigm called neural operator. Our model, called PhaseNO, leverages the spatio-temporal contextual information to pick phases simultaneously for any seismic network geometry.\u00a0</p>\n\n\n\n<p>Accelerated with NVIDIA DGX GPUs, PhaseNO is rooted in the realm of earthquake seismology and stands as a testament to the transformative potential of neural operators in revolutionizing seismic phase-picking methodologies, opening new chapters in scientific computing.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Seismic phase detection</h2>\n\n\n\n<p>Earthquake detection and phase picking are foundational tasks in earthquake seismology, where the aim is to identify earthquakes in the continuous data and measure the arrival times of seismic waves. These arrival time measurements, or phase picks<em>,</em> are crucial for constructing accurate earthquake catalogs, which are databases of earthquake attributes, including the occurrence time, source location, and magnitude.&nbsp;</p>\n\n\n\n<p>When navigating the vast global expanse of continuous seismic data, the imperative to <a href=\"https://www.science.org/doi/10.1126/sciadv.abi8368\">refine earthquake catalogs</a> becomes more pronounced. The enriched catalogs provide profound insights into previously unnoticed seismic events, illuminating fault complexities, earthquake behaviors, and subsurface dynamics.</p>\n\n\n\n<p>State-of-the-art approaches for phase picking use deep neural networks (DNNs) to annotate seismograms at each station independently. Trained with abundant datasets manually labeled by human analysts over the past few decades, DNNs predict the probabilities of P and S arrival times from input seismograms. This enables the determination of arrivals by setting a predetermined probability threshold.&nbsp;</p>\n\n\n\n<p>PhaseNO exemplifies a paradigm shift towards automated and precise phase picking. In this context, deep neural operators are trained to recognize complex patterns in the seismic data and extract useful features for earthquake phase picking without requiring any prior information about the dataset.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Breaking the single-station barrier</h2>\n\n\n\n<p>Current single-station deep learning models operate significantly differently from human analysts in analyzing seismic waveforms. Experienced analysts examine waveforms from multiple stations, enabling them to recognize weak but coherent seismic signals and distinguish noisy spikes.&nbsp;</p>\n\n\n\n<p>In contrast, due to the lack of contextual information from multiple stations, single-station-based models are inherently limited by their design, making them easily fail to detect events buried in a high level of noise or mistakenly detect local noise signals with emergence pulses.</p>\n\n\n\n<p>With traditional approaches centered around single-station algorithms nearing their performance zenith, PhaseNO introduces a paradigmatic shift. This algorithm employs neural operators to transcend the limitations of single-station methodologies, offering a network-wide perspective. By assimilating data from diverse stations with arbitrary geometries, PhaseNO achieves unprecedented seismic monitoring efficacy.&nbsp;</p>\n\n\n\n<p>At the neural operator lab at California Institute of Technology (CalTech), directed by Zachary E. Ross, and partnering with NVIDIA Research, we trained PhaseNO on an earthquake dataset from the Northern California Earthquake Data Center spanning the period 1984-2019. We evaluated our approach on real-world seismic datasets and compared its performance with state-of-the-art phase-picking methods <a href=\"https://arxiv.org/abs/1803.03211\">PhaseNet</a>, <a href=\"https://www.nature.com/articles/s41467-020-17591-w\">EQTransformer</a>, and <a href=\"https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022GC010453\">EdgePhase</a>.&nbsp;</p>\n\n\n\n<p>With the highest F1 scores for P- and S-waves being 0.99 and 0.98, respectively, PhaseNO detected more true positives, fewer false negatives, and fewer false positive picks than the other deep learning models.</p>\n\n\n\n<h2 class=\"wp-block-heading\">The architectural ingenuity of PhaseNO</h2>\n\n\n\n<p>The efficacy of PhaseNO stems from its architectural innovation. It combines two types of neural operators to handle the mathematical structure of seismic network data. Integrating Fourier neural operator (FNO) layers for temporal information and graph neural operators (GNO) for spatial insights, PhaseNO adeptly navigates irregular sensor placements (Figure 2).&nbsp;</p>\n\n\n\n<p>FNO and GNO layers are sequentially connected and repeated several times, enabling sufficient communications and exchange of spatiotemporal information between all stations in a seismic network. This synergistic combination facilitates efficient and accurate phase picking across seismic networks of varying complexities.&nbsp;</p>\n\n\n\n<p>In addition, effective graph-type data augmentation strategies (by adding virtual stations with only noise and stacking multiple events within a time window) further exploit the potential of PhaseNO and amplify its power dealing with complex real-world datasets.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1689\" height=\"885\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic.png\" alt=\"A graphic of the PhaseNO model. The model consists of multiple FNO and GNO layers. This model takes input data and predicts the probabilites of arrival times.\n\" class=\"wp-image-75120\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic.png 1689w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-625x327.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-768x402.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-1536x805.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-645x338.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-500x262.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-210x110.png 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/phaseno-architecture-graphic-1024x537.png 1024w\" sizes=\"(max-width: 1689px) 100vw, 1689px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. PhaseNO architecture. The model consists of multiple FNO and GNO layers that are sequentially connected and repeat. The model uses seismograms from a seismic network containing multiple stations with an arbitrary geometry as the input and predicts the probabilities of P-phase and S-phase arrival times for all input stations</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Real-world applications</h2>\n\n\n\n<p>Over the past year, PhaseNO has undergone rigorous evaluation on real-world seismic datasets, benchmarked against state-of-the-art phase-picking methods. We applied the PhaseNO trained on the Northern California Seismic Network to the Southern California Seismic Network. PhaseNO detects an additional 4,428 events than PhaseNet for the 2019 Ridgecrest earthquake sequence, a challenging dataset due to the overlap of numerous events.&nbsp;</p>\n\n\n\n<p>Figure 2 shows an example of one additional event detected by PhaseNO, which is hidden in the waveforms of the larger event. The results underscore its superior performance in detecting a greater number of earthquakes, picking numerous phase arrivals, and substantially improving measurement accuracy.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1313\" height=\"926\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example.png\" alt=\"Example of continuous seismic data captured during the 2019 Ridgecrest earthquake sequence. PhaseNO detects an additional event compared with PhaseNet in a 35-s time window.\n\" class=\"wp-image-75124\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example.png 1313w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-300x212.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-625x441.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-163x115.png 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-768x542.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-645x455.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-425x300.png 425w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-362x255.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-156x110.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/continuous-seismic-data-example-1024x722.png 1024w\" sizes=\"(max-width: 1313px) 100vw, 1313px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Example of continuous data during the 2019 Ridgecrest earthquake sequence. PhaseNO detects an additional event compared with PhaseNet in this 35-second time window. The first arrivals of the newly detected event overlap with the waveforms of the larger event and thus make them challenging for the single-station detector</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">The future of seismic monitoring</h2>\n\n\n\n<p>PhaseNO has emerged as a beacon of progress in seismic monitoring. The success of the algorithm exemplifies the intersection of advanced machine learning techniques and seismic monitoring, laying the groundwork for refined earthquake monitoring systems.<strong> </strong>Its nuanced approach, blending temporal and spatial considerations through neural operators, offers a glimpse into the future of earthquake seismology. We look forward to the continued evolution of PhaseNO and its enduring positive impact on seismic monitoring systems.</p>\n\n\n\n<p>To try the pretrained PhaseNO model, visit <a href=\"https://github.com/sun-hongyu/PhaseNO\">PhaseNO</a> on GitHub. And join us for our upcoming <a href=\"https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1331827\">presentation</a> at the <a href=\"https://www.agu.org/fall-meeting\">American Geophysical Union Annual Meeting 2023</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Virtual seismology has only been around for a few years, and it has already had a significant impact on earthquake monitoring.\u00a0 Historically, seismic phase picking\u2014the task of annotating seismograms with seismic wave arrival times that underpins earthquake monitoring operations globally\u2014was a manual process. As such, it was labor-intensive, fraught with subjectivity, and prone to errors. &hellip; <a href=\"https://developer.nvidia.com/blog/next-generation-seismic-monitoring-with-neural-operators/\">Continued</a></p>\n", "protected": false}, "author": 1957, "featured_media": 75178, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1315412", "discourse_permalink": "https://forums.developer.nvidia.com/t/next-generation-seismic-monitoring-with-neural-operators/275887", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [1913, 453, 1877], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/earth-from-space-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jxv", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75113"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1957"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75113"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75113/revisions"}], "predecessor-version": [{"id": 75220, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75113/revisions/75220"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75178"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75113"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75113"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75113"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74886, "date": "2023-12-12T09:00:00", "date_gmt": "2023-12-12T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74886"}, "modified": "2023-12-14T11:27:27", "modified_gmt": "2023-12-14T19:27:27", "slug": "benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/", "title": {"rendered": "Benchmarking Quantum Computing Applications with BMW Group and NVIDIA cuQuantum"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Quantum computing has the potential to revolutionize various aspects of industry, ranging from numerical simulations and optimization of complex systems to machine learning (ML). Many computational challenges within the automotive industry are well-suited for quantum computing, including the development of novel materials, efficient design of parts, optimal manufacturing processes, and accurate risk forecasting.&nbsp;</p>\n\n\n\n<p>The advantages of the <a href=\"https://developer.nvidia.com/cuquantum-sdk\">NVIDIA cuQuantum</a> SDK for accelerating quantum circuit simulations were showcased by the <a href=\"https://www.bmwgroup.com/en.html\">BMW Group</a> in their recent publication on <a href=\"https://arxiv.org/abs/2308.04082\">application-oriented benchmarking of quantum machine learning</a>.</p>\n\n\n\n<p>Currently, the group is investigating how quantum computing can enhance algorithms in generative modeling. They have observed a 300x increase in simulation performance with cuQuantum, from 8 hours to a few minutes per iteration.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">Benchmarking quantum computing systems&nbsp;</h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">Generative AI</a> is a type of ML where algorithms learn to create new data samples that resemble the training data. It&#8217;s commonly used for tasks like image generation, text-to-speech, and other applications where producing new content is the goal. Quantum methods in ML can help generate designs that are optimized for specific parameters, enabling faster and more efficient design exploration.&nbsp;</p>\n\n\n\n<p>Generative AI can also be applied along the entire automotive value chain. For example, battery technology development can benefit from quantum ML by aiding in exploring novel materials for fuel cells, optimizing charging and discharging methods, and predicting battery lifespan.</p>\n\n\n\n<p>However, few systematic assessments of the potential of quantum computers for practical applications have been carried out, leaving the precise benefits and applications of quantum computing in industry indistinct. Therefore, the need for benchmarking quantum computing systems, from hardware to simulators, becomes critical to evaluate the scaling performance of quantum algorithms and improve the development of quantum computers.</p>\n\n\n\n<h3 class=\"wp-block-heading\">QUARK</h3>\n\n\n\n<p><a href=\"https://github.com/QUARK-framework/QUARK\">QUARK</a> is a benchmarking framework designed to accommodate applications from different domains of quantum computing, such as quantum machine learning, optimization, and numerical simulations. It provides a standardized and extensible platform for evaluating and comparing quantum algorithms and hardware implementations. QUARK assesses the efficiency of the BMW Group&#8217;s quantum computing algorithms while also enabling comparison of the performance obtained from both quantum simulators and quantum hardware.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Simulating quantum circuits</h2>\n\n\n\n<p>Current <a href=\"https://blogs.nvidia.com/blog/what-is-a-qpu/\">quantum processors (QPUs)</a> suffer from noise and are only available on a small scale, hindering the advancement of algorithmic research. Additionally, simulators offer direct access to the information representing the quantum system, unlike real quantum hardware. Therefore, circuit simulation techniques are a critical tool to advance the field of quantum computing.</p>\n\n\n\n<p>Especially in quantum ML, it&#8217;s essential to verify the output of algorithms empirically. While many quantum algorithms have been proposed and implemented for small system sizes, evaluating the robustness of these algorithms as the system size scales up remains a critical challenge.</p>\n\n\n\n<p>This has created a pressing need for accelerated large-scale quantum simulations. Numerous quantum circuit simulators are available. The QUARK benchmarking framework helps identify the optimal choice of a simulator for a specific quantum workload or research objective.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Enabling quantum applications across industries</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/cuquantum-sdk\">NVIDIA cuQuantum</a> is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use cuQuantum to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"582\" height=\"423\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1.png\" alt=\"A plot of the execution time of quantum circuits as a function of qubit count comparing the CPU execution time to GPU execution time and showing the relative speed-up on GPU.\n\" class=\"wp-image-74898\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1.png 582w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1-300x218.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1-158x115.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1-413x300.png 413w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1-362x263.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gpu-cpu-execution-time-quantum-circuits-1-151x110.png 151w\" sizes=\"(max-width: 582px) 100vw, 582px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA cuQuantum accelerates the simulation of quantum circuits by 300x, enabling the training of a single iteration of a quantum generative model to be performed in a few minutes instead of 8 hours</em></figcaption></figure>\n\n\n\n<p>With only minor changes in their code base, scientists at the BMW Group unlocked the power of the cuQuantum SDK for simulations with up to 30 qubits. By leveraging the power of the <a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/custatevec/index.html\">cuStateVec</a>, they achieved a 300x speedup in the simulation of quantum circuits for their quantum ML workload using <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core</a> 40 GB GPUs, compared to CPU implementations on dual AMD EPYC 7742.&nbsp;</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"512\" height=\"512\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/evolution-quantum-model-training-gif.gif\" alt=\"An animated GIF showing the evolution of a quantum model as it is trained.\" class=\"wp-image-75054\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. During \u200ctraining, the quantum model learns to replicate a target distribution</em></figcaption></figure></div>\n\n\n\n<h2 class=\"wp-block-heading\">Quantum applications for automotive use cases</h2>\n\n\n\n<p>The developments showcased by the BMW Group and accelerated by NVIDIA demonstrate a valuable and high-performance framework for benchmarking quantum applications. With NVIDIA cuQuantum, the BMW Group was able to significantly improve the training time and benchmarking of quantum generative models using QUARK, unblocking a considerable computational bottleneck towards useful quantum ML workloads. Together, this work enables an important step in transitioning quantum applications from theoretical explorations to practical, impactful solutions, enabling broader quantum applications across industries.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Quantum computing has the potential to revolutionize various aspects of industry, ranging from numerical simulations and optimization of complex systems to machine learning (ML). Many computational challenges within the automotive industry are well-suited for quantum computing, including the development of novel materials, efficient design of parts, optimal manufacturing processes, and accurate risk forecasting.&nbsp; The advantages &hellip; <a href=\"https://developer.nvidia.com/blog/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/\">Continued</a></p>\n", "protected": false}, "author": 1952, "featured_media": 74892, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1315346", "discourse_permalink": "https://forums.developer.nvidia.com/t/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/275856", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [2734, 453, 2735], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/bmw-cars-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jtQ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74886"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1952"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74886"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74886/revisions"}], "predecessor-version": [{"id": 75111, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74886/revisions/75111"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74892"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74886"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74886"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74886"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75093, "date": "2023-12-11T10:30:00", "date_gmt": "2023-12-11T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75093"}, "modified": "2023-12-14T11:27:28", "modified_gmt": "2023-12-14T19:27:28", "slug": "automating-data-center-networks-with-nvidia-nvue-and-ansible", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-nvue-and-ansible/", "title": {"rendered": "Automating Data Center Networks with NVIDIA NVUE and Ansible"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Data center automation dates to the early days of the mainframe, with operational efficiency topping the list of its benefits. Over the years, technologies have changed both inside and outside the data center. As a result, tools and approaches have evolved as well.&nbsp;</p>\n\n\n\n<p><a href=\"https://galaxy.ansible.com/nvidia/nvue\">The NVIDIA NVUE Collection</a> and <a href=\"https://www.ansible.com/\">Ansible</a> aim to simplify your network automation journey by providing a comprehensive list of modules and roles for getting started with reference topologies in your environment.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA NVUE</h2>\n\n\n\n<p>NVDIA User Experience (NVUE) is an object-oriented, schema-driven model of a complete hardware and software system. It provides a robust API that enables multiple interfaces to both view (show) and configure (set and unset) any element within a system running the NVUE software. <a href=\"https://www.nvidia.com/en-us/networking/ethernet-switching/cumulus-linux/\">NVIDIA Cumulus Linux</a> 5.x includes the NVUE model.&nbsp;</p>\n\n\n\n<p>NVUE is an API-first structured object model that simplifies operations. It provides a declarative command line interface (CLI) and a single configuration file. The CLI and the REST API are equivalent in functionality. You can run all management operations from either the REST API or the CLI.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Ansible</h2>\n\n\n\n<p><a href=\"https://www.ansible.com/\">Ansible</a> is an open-source agentless IT automation tool that automates provisioning, configuration management, application deployment, orchestration, and many other manual IT processes. It works by connecting to your automation target and pushing programs that execute instructions.</p>\n\n\n\n<p>Ansible modules are included in the NVIDIA NVUE Collection to help you interact with NVIDIA devices managed by NVUE. These modules provide granularity in the configuration options and can be used to build Day 0 and Day 1 through Day N configurations.</p>\n\n\n\n<p>Red Hat Ansible Certified Collections can help jump-start the process, with prebuilt roles ready to download and run. Workflows and templates can further simplify manual steps while making the process repeatable. See a list of <a href=\"https://access.redhat.com/support/articles/ansible-automation-platform-certified-content?extIdCarryOver=true&amp;intcmp=701f2000001OEGmAAO&amp;sc_cid=701f2000001Css0AAC\">certified partners</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA NVUE Collection</h2>\n\n\n\n<p>The various NVIDIA NVUE Collection modules currently available include high-level modules and object-specific modules. Visit <a href=\"https://gitlab.com/nvidia-networking/systems-engineering/nvue\">/nvue</a> on GitHub to download the modules and <a href=\"https://gitlab.com/nvidia-networking/systems-engineering/nvue/-/blob/main/README.md?ref_type=heads#installing-this-collection\">read the instructions</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">High-level modules</h3>\n\n\n\n<p>The high-level modules provide a wrapper around the NVUE utilities. You can leverage these to interact with the switches using the CLI or REST API commands.</p>\n\n\n\n<ul>\n<li><strong><code>nvidia.nvue.command</code></strong> is a wrapper around nv command-line tool with added templating and automated dialog prompting.</li>\n\n\n\n<li><strong><code>nvidia.nvue.api</code></strong> is a wrapper around the NVUE REST API to send and retrieve NVUE configuration.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Object-specific modules&nbsp;</h3>\n\n\n\n<p>Object-specific modules are designed to work with the individual network objects and support various parameters that enable you to interact with them as required. The various modules supported include acl, bridge, router, interface, evpn, mlag, system, vrf, and VXLAN.&nbsp;</p>\n\n\n\n<p>For REST API endpoints that aren&#8217;t covered by the object-specific modules or for subpaths within the object-specific modules (for example, <code>/interface/&lt;id&gt;/qos/roce/counters</code>), you can leverage the <code>nvidia.nvue.api</code> high-level module and specify the endpoint in the path parameter.</p>\n\n\n\n<p>All modules other than <code>nvidia.nvue.cli</code> leverage the REST API to connect with the NVIDIA Cumulus Linux switch, as shown in Figure 1.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"782\" height=\"515\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture.png\" alt=\"Diagram displaying the communication between modules and the NVIDIA Cumulus Linux switch.\" class=\"wp-image-75099\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture.png 782w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-300x198.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-625x412.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-175x115.png 175w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-768x506.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-645x425.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-456x300.png 456w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-137x90.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-362x238.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvue-modules-architecture-167x110.png 167w\" sizes=\"(max-width: 782px) 100vw, 782px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVUE modules architecture</em></em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">NVUE Collection roles</h3>\n\n\n\n<p>In general, <a href=\"https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_reuse_roles.html\">Ansible roles</a> provide a well-defined framework and structure for setting your tasks, variables, handlers, metadata, templates, and other files.</p>\n\n\n\n<p>The <a href=\"https://galaxy.ansible.com/ui/repo/published/nvidia/nvue/content/?showing=role\">roles in this collection</a> provide several examples of a fully operationalized, automated data center in the form of playbooks. They provide a standard reference topology for various examples, such as MLAG and BGP configurations. <a href=\"https://gitlab.com/nvidia-networking/systems-engineering/nvue/-/tree/main/examples/playbooks/roles?ref_type=heads\">See examples of role use</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started</h2>\n\n\n\n<p>Automation enables enterprises to improve operational efficiency, ensure compliance, and effectively manage workloads. The NVIDIA NVUE Collection provides a comprehensive list of modules and roles to simply your network automation journey and work with reference topologies in your environment.</p>\n\n\n\n<p>To get started with Ansible and the NVIDIA NVUE Collection, see <a href=\"https://docs.nvidia.com/networking-ethernet-software/guides/Data-Center-Network-Automation-Ansible-Deployment-Guide/#\">Data Center Network Automation with Ansible</a>. You can also try the <a href=\"https://air.nvidia.com/marketplace?demo_id=e1e59db5-f59d-49b5-b9b7-2aa21d0d7f57\">hands-on-lab on NVIDIA Air</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Data center automation dates to the early days of the mainframe, with operational efficiency topping the list of its benefits. Over the years, technologies have changed both inside and outside the data center. As a result, tools and approaches have evolved as well.&nbsp; The NVIDIA NVUE Collection and Ansible aim to simplify your network automation &hellip; <a href=\"https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-nvue-and-ansible/\">Continued</a></p>\n", "protected": false}, "author": 1738, "featured_media": 75095, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1314504", "discourse_permalink": "https://forums.developer.nvidia.com/t/automating-data-center-networks-with-nvidia-nvue-and-ansible/275728", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205], "tags": [2429, 453, 2428], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/man-with-three-computer-monitors-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jxb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75093"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1738"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75093"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75093/revisions"}], "predecessor-version": [{"id": 75112, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75093/revisions/75112"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75095"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75093"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75093"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75093"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74887, "date": "2023-12-11T09:00:00", "date_gmt": "2023-12-11T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74887"}, "modified": "2023-12-14T11:39:03", "modified_gmt": "2023-12-14T19:39:03", "slug": "nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/", "title": {"rendered": "NVIDIA Research Shows Interactive Texture Painting with Gen AI at SIGGRAPH Asia Real-Time Live"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA researchers took the stage at <a href=\"https://asia.siggraph.org/2023/attend/real-time-live/\">SIGGRAPH Asia Real-Time Live</a> event in Sydney to showcase generative AI integrated into an interactive texture painting workflow, enabling artists to paint complex, non-repeating textures directly on the surface of 3D objects.\u00a0</p>\n\n\n\n<p>Rather than generating complete results with only high-level user guidance, this prototype shows how AI can function as a brush in the hands of an artist. It enables the interactive addition of local details with infinite texture variations and realistic transitions. If you missed the live show, see the <a rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=kE8SS25DKt4\" target=\"_blank\">prerecorded version</a> of this demo.</p>\n\n\n\n<p>This is one in a series of NVIDIA research projects seeking to harness the power of AI to support creativity by developing new iterative workflows with real-time AI inference and direct control. The same group showcased <a href=\"https://blogs.nvidia.com/blog/siggraph-research-generative-ai-materials-3d-scenes/\">Gen AI Materials at SIGGRAPH</a> in August 2023, winning the Real-Time Live show.&nbsp;</p>\n\n\n\n<p>AI texture painting takes AI one step further into the interactive loop. Rather than enabling you to generate and iterate on square tiling physically based rendering (PBR) materials that can then be applied to UV-mapped 3D objects, this project enables you to directly control the placement, scale, and direction of textures by interactive painting. Every patch of the 3D paint stroke is generated by AI in real time.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Tailoring AI for creativity</h3>\n\n\n\n<p>Among all the aspects of designing tools for creativity, direct iterative control over the outcome is one of the most important. One of the challenges in integrating modern foundational image AI models into interactive workflows, such as painting, is that AI is simply too good at imagining things that may not necessarily be the artist\u2019s intent. In some cases, this can lead to the need for careful prompt engineering and unpredictable results that appear difficult to control.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"978\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture.png\" alt=\"NVIDIA Omniverse interface showing a scene with a grass meadow and a house. A windy stone path goes through the meadow and includes swirls with varying stone sizes.\" class=\"wp-image-74907\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-625x306.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-768x376.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-1536x751.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-645x316.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-500x245.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-225x110.png 225w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/winding-path-ai-brush-texture-1024x501.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Adding winding paths with varying brush sizes, using an AI brush seeded with the example rock texture</em></figcaption></figure></div>\n\n\n<p>In the case of this interface, researchers opted not to include a text-based interface for either placement or identity of the texture. Following the proverb, \u201cAn image is worth a thousand words,\u201d the AI brush is conditioned on an example image of the target texture.&nbsp;</p>\n\n\n\n<p>Inspiration images are a common concept in 3D design. These images typically serve only as a reference and must be heavily processed before they can be integrated into the 3D scene.&nbsp;</p>\n\n\n\n<p>The AI Material presentation at SIGGRAPH showed how an imperfect inspiration image can be converted to a tileable PBR material, making it much easier to bring inspiration from the real world into 3D workflows. In this new demo, inspirational images of any real-world textures can be turned into AI brushes that artists can use for painting in 3D. You control not just the stroke shape, but brush size and texture direction.&nbsp;</p>\n\n\n\n<p>The AI in the prototype is designed to ensure that the brushstroke includes variations of the reference, without deviating too much from its identity. The backbone foundational AI model also provides realistic transitions between regions of different textures, without any reference of such transitions. For example, AI can fill in a realistic transition between the original grass texture and the rocky path interactively painted using the AI texture brush.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"976\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai.png\" alt=\"NVIDIA Omniverse interface showing a scene with a grass meadow and a pagoda. A windy stone path goes through the meadow and transitions to the pink daisies path seamlessly.\" class=\"wp-image-74908\" style=\"aspect-ratio:2.048155737704918;width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-625x305.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-768x375.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-1536x750.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-645x315.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-225x110.png 225w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/pink-daisies-texture-ai-1024x500.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Painting with pink daisies generated from text queries</em></figcaption></figure>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"982\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai.png\" alt=\"In this Omniverse screenshot, a stone base is painted for the pagoda with a runes texture.\" class=\"wp-image-74909\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-768x377.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-1536x755.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-645x317.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-500x246.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-224x110.png 224w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ancient-runes-texture-ai-1024x503.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Painting with ancient rune textures generated from text queries</em></figcaption></figure></div>\n\n\n<p>What if there is no inspirational image available to seed the brush?&nbsp;</p>\n\n\n\n<p>Text-to-image AI can be used to generate several versions. You pick the exact brush you would like to use, opening up a wide array of creative possibilities with direct artist control in the interactive loop.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Empowered by NVIDIA technologies</h3>\n\n\n\n<p>Several NVIDIA technologies come together to enable this prototype. One of the requirements of interactive interfaces, as well as the Real-Time Live program, is speed. This prototype achieves an inference speed of 0.23-0.15s per brush stamp, enabled by accelerated inference on Tensor Cores in NVIDIA GPUs.&nbsp;</p>\n\n\n\n<p>This prototype was developed as an <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> extension. Omniverse is a <a href=\"https://developer.nvidia.com/omniverse\">modular development platform</a> of APIs and microservices for building applications and services powered by <a href=\"https://www.nvidia.com/en-us/omniverse/usd/\">OpenUSD</a> and<a href=\"https://developer.nvidia.com/rtx/ray-tracing\"> NVIDIA RTX</a>, empowering developers to build complex 3D tools incorporating AI.&nbsp;</p>\n\n\n\n<p>In this case, efficient raycasting from the integrated <a href=\"https://developer.nvidia.com/warp-python\">NVIDIA Warp Library</a> and efficient dynamic texture support allowed AI to deliver fast updates directly to the rendered object.\u00a0</p>\n\n\n\n<p>Under the hood, the method relies on the <a href=\"https://github.com/NVIDIAGameWorks/kaolin\">NVIDIA Kaolin Library</a> for 3D deep learning for efficient offscreen rasterization and texture back-projection directly on the GPU.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Acknowledgments</h3>\n\n\n\n<p><em>This demo is the result of a cross-team effort by Anita Hu, Nishkrit Desai, Hassan Abu Alhaija, Alexander Zook, Seung Wook Kim, Ashley Goldstein, Carsten Klove, Daniela Hasenbring, Rajeev Rao, and Masha Shugrina. Anita Hu and Alexander Zook delivered the live presentation.&nbsp;</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA researchers took the stage at SIGGRAPH Asia Real-Time Live event in Sydney to showcase generative AI integrated into an interactive texture painting workflow, enabling artists to paint complex, non-repeating textures directly on the surface of 3D objects.\u00a0 Rather than generating complete results with only high-level user guidance, this prototype shows how AI can function &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/\">Continued</a></p>\n", "protected": false}, "author": 1956, "featured_media": 75089, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1314459", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/275712", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1903], "tags": [453, 1962, 604], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/AI_Texture_Painting.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jtR", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74887"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1956"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74887"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74887/revisions"}], "predecessor-version": [{"id": 75337, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74887/revisions/75337"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75089"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74887"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74887"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74887"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74698, "date": "2023-12-08T14:07:12", "date_gmt": "2023-12-08T22:07:12", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74698"}, "modified": "2023-12-14T11:27:28", "modified_gmt": "2023-12-14T19:27:28", "slug": "available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/", "title": {"rendered": "Available Now: NVIDIA AI Accelerated DGL and PyG Containers for GNNs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>From credit card transactions, social networks, and <a href=\"https://www.nvidia.com/en-us/glossary/data-science/recommendation-system/\">recommendation systems</a> to transportation networks and protein-protein interactions in biology, graphs are the go-to data structure for modeling and analyzing intricate connections. Graph neural networks (GNNs), with their ability to learn and reason over graph-structured data, have emerged as a game-changer across various domains. </p>\n\n\n\n<p>However, uncovering the hidden patterns and valuable insights within these graphs can be challenging, especially in data sampling and end-to-end training of GNNs.</p>\n\n\n\n<p>To address this gap, NVIDIA has released accelerated <a href=\"https://developer.nvidia.com/gnn-frameworks\">GNN framework</a> containers for DGL and PyG with features such as:</p>\n\n\n\n<ul>\n<li>GPU acceleration for data sampling</li>\n\n\n\n<li>GNN Training and Deployment Tool (GNN Tool)</li>\n</ul>\n\n\n\n<p>This post provides an overview of the benefits of NVIDIA accelerated DGL and PyG containers, showcases how customers are using them in production, and provides metrics on the performance.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Introducing the DGL container in the NGC catalog</h2>\n\n\n\n<p><a href=\"https://www.dgl.ai/\">Deep Graph Library (DGL)</a> is one of the popular open-source libraries available for implementing and training GNNs on top of existing DL frameworks such as PyTorch.</p>\n\n\n\n<p>We are excited to announce that DGL is now accelerated with other NVIDIA libraries and is publicly available as a <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/dgl\">container</a> through the <a href=\"https://ngc.nvidia.com/\">NGC Catalog</a>\u2014the hub for GPU-accelerated AI/ML, HPC applications, SDKs, and tools. The catalog provides faster access to performance-optimized software and simplifies building and deploying <a href=\"https://www.nvidia.com/en-us/glossary/data-science/artificial-intelligence/\">AI</a> solutions bringing your solutions to market faster. For more information, see <a href=\"https://youtu.be/kilD5r7bCkI\">100s of Pretrained Models for AI, Digital Twins, and HPC in the NGC Catalog</a> (video).</p>\n\n\n\n<p>The 23.09 release of the DGL container improves data sampling and training performance for \u200cDGL users. The following are the top features of this release.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GPU acceleration for data loader sampling</h3>\n\n\n\n<p><a href=\"https://github.com/rapidsai/cugraph\">RAPIDS cuGraph\u2019s</a> sampler can process hundreds of billions of edges in a matter of seconds, and compute samples for thousands of batches at one time for even the world\u2019s largest GNN datasets. The DGL container comes with cuGraph-DGL, an accelerated extension to DGL, which enables users to take advantage of this incredible performance.</p>\n\n\n\n<p>Even on midsize datasets (~1B edges), cuGraph data loading performance is at least 2\u20133x faster than native DGL, based on benchmarks run with eight V100 GPUs. cuGraph-DGL sampling offers better than linear scaling for up to 100B edges, by distributing the graph across multiple nodes and multiple GPUs, also saving memory in the process.&nbsp;</p>\n\n\n\n<p>cuGraph can sample 100B edges in only 16 seconds! cuGraph-ops, the proprietary NVIDIA library, has accelerated GNN operators and models, such as cuGraphSAGE, cuGraphGAT, and cuGraphRGCN, cutting model forward time in half.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GNN Training and Deployment Tool</h3>\n\n\n\n<p>GNN Tool is a flexible platform for training and deploying GNN models with minimal effort. This tool, built on top of the popular Deep Graph Library (DGL) and PyTorch Geometric (PyG) frameworks, enables you to build end-to-end workflows for rapid GNN experimentation.&nbsp;</p>\n\n\n\n<p>It provides a fully modular and configurable workflow that enables fast iteration and experimentation for custom GNN use cases. NVIDIA includes example notebooks in our containers for easy experimentation.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Multi-arch support</h3>\n\n\n\n<p>The DGL containers published in NGC have both x86 and ARM64 versions to support the new NVIDIA Grace Hopper GPU. Both versions use the same container tag. When you pull the container from an Arm-based Linux system, you pull the ARM64 container.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Example of training GNN on Grace Hopper with an ARM64-based DGL container</h4>\n\n\n\n<p>The Unified Virtual Addressing (UVA) mode in GNN training benefits tremendously from the connection between the NVIDIA Grace CPU and NVIDIA Hopper GPU in Grace Hopper.&nbsp; On Grace Hopper, training the same GraphSAGE model with the ogbn-papers100M dataset takes 1.9 seconds/epoch, which is about 9x faster compared to training on the H100 + Intel CPUs with PCIe connections (Table 1).</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>System</strong></td><td><strong>GH200</strong></td><td><strong>H100 + Intel CPU</strong></td><td><strong>A100 + AMD<br>(DGX A100)</strong></td><td><strong>AMD Genoa<br>(CPU only)</strong></td></tr><tr><td>(seconds / epoch)</td><td>1.9</td><td>16.92</td><td>24.8</td><td>107.11</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Comparison of training time per epoch for GraphSage model with the </em>ogbn-papers100M<em> dataset on Grace Hopper with UVA mode</em></figcaption></figure>\n\n\n\n<p>These numbers take advantage of the huge pages on Grace Hopper for the graph and its features. They were benchmarked on a 512-GB Grace Hopper node. The model is run with batch size 4096 and a (30,30) fanout (looking at up to 30 neighbors of each node in a two-layer GraphSAGE model). This is running on DGL version 1.1 on CUDA 12.1.</p>\n\n\n\n<h3 class=\"wp-block-heading\">DGL training performance</h3>\n\n\n\n<p>One of the challenges in training GNNs is the data loading process. In some cases, such as node classification using GraphSAGE for the ogb-papers100M dataset, the data loading process takes more than 90% of the end-to-end training time.&nbsp; DGL 0.8v enabled the UVA mode for efficient GPU loading of graph features, which has improved the performance since then.&nbsp;</p>\n\n\n\n<p>Consider the GraphSAGE model with the <a href=\"https://ogb.stanford.edu/docs/nodeprop/#ogbn-products\">ogbn-products</a> dataset for a node classification task. It has 2.4M nodes and 61.9M edges. On a DGX-1 V100 GPU, with the UVA mode, it can give up to a 20x speed-up compared to CPU-only training (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1590\" height=\"984\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products.png\" alt=\"A bar graph compares the training times in sec/epoch for the obgn-products dataset with UVA mode on CPU (46 seconds), one GPU (10.28 seconds), and eight GPUs (2.35 seconds).\" class=\"wp-image-74716\" title=\"Points scored\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products.png 1590w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-1536x951.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-obgn-products-1024x634.png 1024w\" sizes=\"(max-width: 1590px) 100vw, 1590px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Node classification training time with the UVA mode for the obgn-products dataset</em></figcaption></figure></div>\n\n\n<p>As the ogbn-product dataset can be loaded into GPU memory, it can be even faster with an up to 115x speed-up (Figure 2).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1490\" height=\"920\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory.png\" alt=\"A bar graph compares the training times in sec/epoch for the obgn-products dataset on CPU (46 seconds) and with GPU memory on one GPU (2.96 seconds), and eight GPUs (0.4 seconds).\" class=\"wp-image-74715\" title=\"Points scored\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory.png 1490w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-300x185.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-768x474.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-645x398.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-486x300.png 486w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-gpu-memory-1024x632.png 1024w\" sizes=\"(max-width: 1490px) 100vw, 1490px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Node classification training time using GPU memory for the obgn-products dataset</em></figcaption></figure></div>\n\n\n<p>For large datasets, such as ogbn-papers100M, UVA mode must be turned on.&nbsp;</p>\n\n\n\n<p>Figure 3 shows the per epoch training time in seconds for ogbn-papers100M for the node classification task. It has 111M nodes and 3.2B edges.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1448\" height=\"896\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode.png\" alt=\"A bar graph compares the training times in sec/epoch for obgn-papers dataset with UVA mode on CPU (107 seconds), one GPU (20.29 seconds), and eight GPUs (5.15 seconds).\" class=\"wp-image-74713\" title=\"Points scored\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode.png 1448w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/node-classification-uva-mode-1024x634.png 1024w\" sizes=\"(max-width: 1448px) 100vw, 1448px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Node classification training time with the UVA mode for the obgn-papers dataset</em></figcaption></figure></div>\n\n\n<p>You can find the node classification script in the NGC DGL 23.09 container under the <code>/workspace/examples/</code>multigpu directory.</p>\n\n\n\n<h2 class=\"wp-block-heading\">PyTorch Geometric container</h2>\n\n\n\n<p><a href=\"https://github.com/pyg-team/pytorch_geometric\">PyTorch Geometric (PyG)</a> is another popular open-source library for writing and training GNNs for a wide range of applications. We are launching the PyG container accelerated with NVIDIA libraries such as cuGraph. </p>\n\n\n\n<p>On midsize datasets (~1B edges), cuGraph data loading performance is at least 4x faster than native PyG, based on benchmarks run with eight A100 GPUs.</p>\n\n\n\n<p>We are already observing several customers getting benefits from the PyG container, and we plan on leveraging PyG acceleration for use with NVIDIA BioNeMo models as well.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Customer success stories</h2>\n\n\n\n<p>Here\u2019s how different companies have been using the NVIDIA Accelerated DGL and PyG containers to accelerate their workflows.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GNNs for physics-based ML</h3>\n\n\n\n<p><a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a> is an open-source framework for building, training, and fine-tuning physics-based <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> (ML) models in Python.&nbsp;</p>\n\n\n\n<p>With the growing interest and application of GNNs across disciplines that include computational fluid dynamics, molecular dynamics simulations, and material science, NVIDIA Modulus started supporting GNNs by leveraging the DGL and cuGraphOps libraries.&nbsp;</p>\n\n\n\n<p>NVIDIA Modulus currently supports GNNs that include the <a href=\"https://arxiv.org/abs/2010.03409\">MeshGraphNet</a>, AeroGraphNet, and <a href=\"https://arxiv.org/abs/2212.12794\">GraphCast</a> models for mesh-based simulations and global weather forecasting. In addition to the network architectures, Modulus includes training recipes for developing models for <a href=\"https://github.com/NVIDIA/modulus-launch/tree/main/examples/weather\">weather forecasting</a>, <a href=\"https://github.com/NVIDIA/modulus-launch/tree/main/examples/cfd/ahmed_body_mgn\">aerodynamic simulation</a>, and <a href=\"https://github.com/NVIDIA/modulus-launch/tree/main/examples/cfd/vortex_shedding_mgn\">vortex shedding</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GNNs for fraud detection</h3>\n\n\n\n<p>Drawing upon decades of experience, <a href=\"https://www.americanexpress.com/\">American Express</a> has a significant track record of using AI-powered tools and models to monitor and mitigate fraud risks. They also effectively identify individuals engaged in fraudulent activities within the credit card industry.&nbsp;</p>\n\n\n\n<p>At American Express AI Labs, research is continuously being conducted to gain a deeper understanding of fraudster networks through the implementation of graph-based machine learning solutions.&nbsp;</p>\n\n\n\n<p>The DGL containers published on NGC enabled AmEx to experiment with a variety of GNN architectures and exploit node and edge information at scale. This results in computational efficiency when dealing with millions of nodes and billions of edges using NVIDIA libraries in a multi-node, multi-GPU environment. Moreover, the user-friendly and adaptable libraries enabled them to easily customize the components such as the loss functions, sampling techniques, and more.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GNNs for drug discovery</h3>\n\n\n\n<p><a href=\"https://www.astellas.com/en/\">Astellas</a>, a leading pharmaceutical company known for its forward-thinking drug development approaches, is harnessing the capabilities of GNNs for a range of pivotal tasks in drug discovery. These tasks encompass the application of generative probabilistic <a href=\"https://www.nvidia.com/en-us/glossary/data-science/deep-learning/\">deep learning</a> models diffusion-based models for 3D molecular conformation generation, feature extraction, and predictive ML models, particularly in <em>de novo</em> protein design and engineering.\u00a0</p>\n\n\n\n<p>As such, Astellas taps into the computational efficiency of the NVIDIA PyG and DGL containers to facilitate and amplify GNN-based AI/ML solutions in drug discovery research activities. The integration of AI and ML-based pipelines powered by PyG and DGL enabled Astellas to boost its internal capabilities, granting researchers and AI practitioners with the expertise needed for developing and implementing AI-powered cutting technologies.&nbsp;</p>\n\n\n\n<p>As one use case, an Astellas scientist can achieve acceleration rates of at least 50x compared to traditional simulation-based methods for 3D molecular conformations.</p>\n\n\n\n<p><a href=\"https://www.gene.com/\">Genentech</a>, a biotechnology company and member of the Roche Group, is using GNNs with the NVIDIA PyG container to accelerate their small molecule prediction training. The PyG container provided a reliable base for the PyG framework, which enabled the development team to focus more on development rather than setting up the dev environments.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Next steps</h2>\n\n\n\n<p>With NVIDIA accelerated DGL and PyG containers, you can also significantly improve the data sampling and training performance of GNNs.&nbsp;</p>\n\n\n\n<p>To get started, download the following resources:</p>\n\n\n\n<ul>\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/dgl\">DGL container</a>: The overview section on the catalog page provides detailed steps for pulling and running the DGL container.&nbsp;</li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pyg\" data-type=\"link\" data-id=\"https://developer.nvidia.com/pyg-container-early-accessserved=0\">PyG container</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>From credit card transactions, social networks, and recommendation systems to transportation networks and protein-protein interactions in biology, graphs are the go-to data structure for modeling and analyzing intricate connections. Graph neural networks (GNNs), with their ability to learn and reason over graph-structured data, have emerged as a game-changer across various domains. However, uncovering the hidden &hellip; <a href=\"https://developer.nvidia.com/blog/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/\">Continued</a></p>\n", "protected": false}, "author": 1921, "featured_media": 74711, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1313432", "discourse_permalink": "https://forums.developer.nvidia.com/t/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/275490", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 3110, 1968], "tags": [453, 369], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gnn-dgl-pyg-integration.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jqO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74698"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74698"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74698/revisions"}], "predecessor-version": [{"id": 75287, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74698/revisions/75287"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74711"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74698"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74698"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74698"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74846, "date": "2023-12-06T06:00:00", "date_gmt": "2023-12-06T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74846"}, "modified": "2023-12-14T11:40:12", "modified_gmt": "2023-12-14T19:40:12", "slug": "develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/", "title": {"rendered": "Develop and Optimize Vision AI Models for Trillions of Devices with NVIDIA TAO"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>With <a href=\"https://developer.nvidia.com/tao-toolkit\">NVIDIA TAO Toolkit</a>, developers around the world are building AI-powered visual perception and computer vision applications. Now the process is faster and easier than ever, thanks to significant platform enhancements and strong ecosystem adoption.</p>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/resources/tao-getting-started\">NVIDIA TAO Toolkit</a> supports more than 10 computer vision and vision AI modalities, including image classification, object detection, three types of segmentation, optical character recognition (OCR), action recognition, key point estimation, body pose estimation, embedding models, Siamese networks, and more.\u00a0</p>\n\n\n\n<p>Getting started with TAO Toolkit is faster than ever, with support for over <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/containers/tao-toolkit\">40 pretrained models on NVIDIA NGC</a>. Recipes for leveraging TAO continue to expand with workflows for tuning models for various industries. To learn more, see <a href=\"https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models\">Transforming Industrial Defect Detection with NVIDIA TAO and Vision AI Models</a> and <a href=\"https://developer.nvidia.com/blog/create-custom-character-detection-and-recognition-models-with-nvidia-tao-part-1/\">Customizing AI Models: Train Character Detection and Recognition Models with NVIDIA TAO</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Massive adoption</h2>\n\n\n\n<p>TAO has been downloaded over\u202f100,000 times with nearly 1 million downloads of TAO pretrained models. And now that <a href=\"https://github.com/nvidia?q=tao&amp;type=all&amp;language=&amp;sort=\">TAO is open source</a>, solution providers can access more custom integration of TAO into their services and explore the inner workings of the platform with finer granularity.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>Leading MLOps and cloud services are leveraging TAO to improve their services. Integration with MLOps platforms such as <a href=\"https://wandb.ai/site\">Weights &amp; Biases</a> and <a href=\"https://clear.ml/\">ClearML</a> helps streamline machine learning (ML) workflows, enabling better experiment tracking for model training. TAO can also work with various cloud ML and Kubernetes services such as Azure ML, GCP Vertex AI, Azure AKS, AWS EKS, and GCP GKE.&nbsp;</p>\n\n\n\n<p>Major enterprises are also building TAO into their own industry-specific AI development workflows, including <a href=\"https://blogs.nvidia.com/blog/pepsi-koivision/\">PepsiCo</a> in consumer supply chain, <a href=\"https://blogs.nvidia.com/blog/electronics-giants-industrial-automation-nvidia-metropolis-for-factories/\">Pegatron, Siemens, and others</a> in manufacturing, and<a href=\"https://blogs.nvidia.com/blog/two-i-worker-safety-metropolis/\"> ExxonMobil</a> in energy. Large cities and airports around the world also use TAO.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Built for AI workflows&nbsp;</h2>\n\n\n\n<p>Enterprises large and small are building their own vision AI factories and development workflows using TAO.\u202fThese AI workflows are increasingly requiring the following three capabilities now provided by TAO.&nbsp;</p>\n\n\n\n<p>First, TAO can handle the latest AI models and algorithms, including foundation model tuning, generative AI, and vision transformers. Second, TAO can seamlessly connect with synthetically generated datasets from simulation approaches such as<a href=\"https://developer.nvidia.com/omniverse/replicator\"> NVIDIA Omniverse Replicator</a> and Stable Diffusion.\u202fThird, TAO includes enterprise-level support as an essential tool in the development workflow through<a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\"> NVIDIA AI Enterprise</a>.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Advanced AI on trillions of devices &nbsp;</h2>\n\n\n\n<p>With TAO, NVIDIA offers the flexibility to deploy the latest AI models on trillions of devices\u202fat the far edge through ONNX and TFLite model export, along with strong ecosystem adoption from the world\u2019s leading providers and edge AI software platforms.</p>\n\n\n\n<p><a href=\"https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/optimizing-ai-models-for-arm-ethos-u-npus-using-the-nvidia-tao-toolkit\">ARM is leveraging TAO</a> to optimize AI runtime on Ethos NPU devices. <a href=\"https://blog.st.com/tao-toolkit/\">STMicroelectronics uses TAO</a> to run complex vision AI for the first time on STM32 microcontrollers. And partners like<a href=\"https://edgeimpulse.com/nvidia-tao\"> Edge Impulse</a> and<a href=\"https://www.nota.ai/community/integrating-launchx-with-nvidia-tao-toolkit-for-running-on-various-edge-devices\"> Nota</a> are integrating TAO into their edge AI platforms to bring edge-optimized solutions to their customers.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/tao-toolkit\">Download NVIDIA TAO Toolkit</a> to get started building AI-powered visual perception and computer vision applications. Access expert help on the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17\">NVIDIA Developer TAO Forum</a>.\u00a0</p>\n\n\n\n<p>Join the <a href=\"https://www.st.com/content/st_com/ja/st-edge-ai-summit.html\">ST Edge AI Summit</a> on December 6 to hear NVIDIA VP and GM of Embedded and Edge Computing Deepu Talla speak about building the next generation of edge AI platforms, and how NVIDIA TAO continues to evolve.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>With NVIDIA TAO Toolkit, developers around the world are building AI-powered visual perception and computer vision applications. Now the process is faster and easier than ever, thanks to significant platform enhancements and strong ecosystem adoption. NVIDIA TAO Toolkit supports more than 10 computer vision and vision AI modalities, including image classification, object detection, three types &hellip; <a href=\"https://developer.nvidia.com/blog/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/\">Continued</a></p>\n", "protected": false}, "author": 1701, "featured_media": 74849, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1311733", "discourse_permalink": "https://forums.developer.nvidia.com/t/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/275226", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63, 503], "tags": [3284, 453, 2056], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/data-train-deploy-nvidia-tao-graphic-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jtc", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74846"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1701"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74846"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74846/revisions"}], "predecessor-version": [{"id": 74882, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74846/revisions/74882"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74849"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74846"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74846"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74846"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74577, "date": "2023-12-05T10:30:00", "date_gmt": "2023-12-05T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74577"}, "modified": "2024-01-10T10:05:00", "modified_gmt": "2024-01-10T18:05:00", "slug": "reconstructing-dynamic-driving-scenarios-using-self-supervised-learning", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/", "title": {"rendered": "Reconstructing Dynamic Driving Scenarios Using Self-Supervised Learning"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>From monotonous highways to routine neighborhood trips, driving is often uneventful. As a result, much of the training data for autonomous vehicle (AV) development collected in the real world is heavily skewed toward simple scenarios.&nbsp;</p>\n\n\n\n<p>This poses a challenge to deploying robust perception models. AVs must be thoroughly trained, tested, and validated to handle complex situations, which requires an immense amount of data covering such scenarios.</p>\n\n\n\n<p>Simulation offers an alternative to finding and collecting such data in the real world\u2014which would be incredibly time- and cost-intensive. And yet generating complicated, dynamic scenarios at scale is still a significant hurdle.</p>\n\n\n\n<p>In a recently released paper, NVIDIA Research shows how a new neural radiance field (NeRF)-based method, known as EmerNeRF, uses self-supervised learning to accurately generate dynamic scenarios. By training through self-supervision, EmerNeRF not only outperforms other NeRF-based methods for dynamic objects, but also for static scenes. For more details, see <a href=\"https://arxiv.org/abs/2311.02077\">EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"267\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-dynamic-driving-nighttime.gif\" alt=\"Example of EmerNeRF reconstructing dynamic driving scene: nighttime.\" class=\"wp-image-74648\"/></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"267\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-dynamic-driving-intersection-cars.gif\" alt=\"Example of EmerNeRF reconstructing dynamic driving scene: busy intersection with cars.\" class=\"wp-image-74652\"/></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"267\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-dynamic-driving-intersection-pedestrians.gif\" alt=\"Example of EmerNeRF reconstructing dynamic driving scene: busy intersection with pedestrians.\" class=\"wp-image-74654\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Examples of EmerNeRF reconstructing dynamic driving scenes</em></em></figcaption></figure></div>\n\n\n<p>When running EmerNeRF alongside similar NeRFs, it increases dynamic scene reconstruction accuracy by 15% and static scene by 11%, additionally achieving a 12% improvement for novel view synthesis.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Addressing limitations in NeRF-based methods</h2>\n\n\n\n<p>NeRFs take a set of static images and reconstruct them into a realistic 3D scene. They make it possible to create high-fidelity simulations from driving logs for closed-loop deep neural network (DNN) training, testing, and validation.&nbsp;</p>\n\n\n\n<p>However, current NeRF-based reconstruction methods struggle with dynamic objects and have proven difficult to scale. For example, while some approaches can generate both static and dynamic scenes, they require ground truth (GT) labels to do so. This means that each object in the driving logs must be accurately outlined and defined using autolabeling techniques or human annotators.</p>\n\n\n\n<p>Other NeRF methods rely on additional models to achieve complete information about a scene, such as optical flow.&nbsp;</p>\n\n\n\n<p>To address these limitations, EmerNeRF uses self-supervised learning to decompose a scene into static, dynamic, and flow fields. The model learns associations and structure from raw data rather than relying on human-labeled GT annotations. It then renders both the temporal and spatial aspects of a scene simultaneously, eliminating the need for an external model to fill in the gaps while improving accuracy.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"800\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposed-nighttime-driving-scenes.gif\" alt=\"A decomposed version of the nighttime driving scene, broken into rendered depth, decomposed dynamic RGB, decomposed dynamic depth, emerged forward flow, decomposed static RGB, and decomposed static depth.\" class=\"wp-image-74663\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. EmerNeRF breaks down the scene shown in the first video in Figure 1 into dynamic, static, and flow fields</em></em></figcaption></figure></div>\n\n\n\n<p>As a result, while other models tend to produce over-smoothed renderings and dynamic objects with lower accuracy, EmerNeRF reconstructs high-fidelity background scenery as well as dynamic objects, all while preserving the fine details of a scene.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"9\">Dynamic-32 Split</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"4\">Scene Reconstruction</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"4\">Novel View Synthesis</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Methods</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">Full Image</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">Dynamic Only</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">Full Image</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">Dynamic Only</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">PSNR\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">SSIM\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">PSNR\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">SSIM\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">PSNR\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">SSIM\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">DPSNR\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">SSIM\u2191</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">D<sup>2</sup> NeRF</td><td class=\"has-text-align-center\" data-align=\"center\">24.35</td><td class=\"has-text-align-center\" data-align=\"center\">0.645</td><td class=\"has-text-align-center\" data-align=\"center\">21.78</td><td class=\"has-text-align-center\" data-align=\"center\">0.504</td><td class=\"has-text-align-center\" data-align=\"center\">24.17</td><td class=\"has-text-align-center\" data-align=\"center\">0.642</td><td class=\"has-text-align-center\" data-align=\"center\">21.44</td><td class=\"has-text-align-center\" data-align=\"center\">0.494</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">HyperNeRF</td><td class=\"has-text-align-center\" data-align=\"center\">25.17</td><td class=\"has-text-align-center\" data-align=\"center\">0.688</td><td class=\"has-text-align-center\" data-align=\"center\">22.93</td><td class=\"has-text-align-center\" data-align=\"center\">0.569</td><td class=\"has-text-align-center\" data-align=\"center\">24.71</td><td class=\"has-text-align-center\" data-align=\"center\">0.682</td><td class=\"has-text-align-center\" data-align=\"center\">22.43</td><td class=\"has-text-align-center\" data-align=\"center\">0.554</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>EmerNeRF</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>28.87</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>0.814</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>26.19</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>0.736</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>27.62</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>0.792</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>24.18</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>0.67</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Evaluation results comparing EmerNeRF with other NeRF-based reconstruction methods for dynamic scenes, categorized into performance for scene reconstruction and novel view synthesis</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\">Static-32 Split</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Methods</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">Static Scene Reconstruction</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">PSNR\u2191</td><td class=\"has-text-align-center\" data-align=\"center\">SSIM\u2191</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">iNGP</td><td class=\"has-text-align-center\" data-align=\"center\">24.46</td><td class=\"has-text-align-center\" data-align=\"center\">0.694</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">StreetSurf</td><td class=\"has-text-align-center\" data-align=\"center\">26.15</td><td class=\"has-text-align-center\" data-align=\"center\">0.753</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>EmerNeRF</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>29.08</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>0.803</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 2. Evaluation results comparing EmerNeRF with other NeRF-based reconstruction for static scenes</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">The EmerNeRF approach</h2>\n\n\n\n<p>Using self-supervised learning, rather than human annotation or external models, enables EmerNeRF to bypass challenges previous methods have encountered.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1174\" height=\"297\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline.png\" alt=\"A diagram showing how EmerNeRF breaks a scene into static, dynamic, and flow fields, then reconstructs the final scene rendering simultaneously, rather than use an external model.\n\" class=\"wp-image-74675\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline.png 1174w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-300x76.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-625x158.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-179x45.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-768x194.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-645x163.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-500x126.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-160x40.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-362x92.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-435x110.png 435w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-decomposition-reconstruction-pipeline-1024x259.png 1024w\" sizes=\"(max-width: 1174px) 100vw, 1174px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. EmerNeRF decomposition and reconstruction pipeline</em></em></figcaption></figure>\n\n\n\n<p>EmerNeRF is designed to break down a scene into dynamic and static elements. As it decomposes a scene, EmerNeRF also estimates a flow field from dynamic objects, such as cars and pedestrians, and uses this field to further improve reconstruction quality by aggregating features across time. Other approaches use external models to provide such optical flow data, which can often lead to inaccuracies.</p>\n\n\n\n<p>By combining the static, dynamic, and flow fields all at once, EmerNeRF can represent highly dynamic scenes self-sufficiently, which improves accuracy and enables scaling to general data sources.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Adding semantic understanding with foundation models</h2>\n\n\n\n<p>EmerNeRF&#8217;s semantic understanding of a scene is further strengthened using foundation models for additional supervision. Foundation models have a broad knowledge of objects (specific types of vehicles or animals, for example). EmerNeRF leverages vision transformer (ViT) models such as DINO and DINOv2 to incorporate semantic features into its scene reconstruction.</p>\n\n\n\n<p>This enables EmerNeRF to better predict objects in a scene, as well as perform downstream tasks such as autolabeling.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"133\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-foundation-models.gif\" alt=\"Four views of the same driving scene, clockwise: the original ground truth recording, the EmerNeRF reconstruction, the DINO semantic rendering and the DINOv2 semantic rendering.\n\" class=\"wp-image-74681\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 4. EmerNeRF uses foundation models such as DINO and DINOv2 to strengthen its semantic understanding of a scene</em></em></figcaption></figure>\n\n\n\n<p>However, transformer-based foundation models pose a new challenge: semantic features can exhibit position-dependent noise, which can significantly limit downstream task performance.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"130\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/emernerf-positional-embedding.gif\" alt=\"Four views of the same driving scene, clockwise: the original ground truth recording, the EmerNeRF reconstruction, the decomposed noise-free DINO semantic rendering and the decomposed noise-free DINOv2 semantic rendering.\n\" class=\"wp-image-74682\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 5. EmerNeRF uses positional embedding to eliminate noise caused by transformer-based foundation models</em></em></figcaption></figure>\n\n\n\n<p>To solve the noise issue, EmerNeRF uses positional embedding decomposition to recover a noise-free feature map. This unlocks the full, accurate representation of foundation model semantic features, as shown in Figure 5.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Evaluating EmerNeRF</h2>\n\n\n\n<p>As detailed in <a href=\"https://arxiv.org/abs/2311.02077\">EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</a>, we evaluated the performance of EmerNeRF by curating a dataset of 120 unique scenarios, divided into 32 static, 32 dynamic, and 56 diverse scenes across challenging conditions such as high-speed and low-light conditions.&nbsp;</p>\n\n\n\n<p>Each NeRF model was then evaluated on its ability to reconstruct scenes and synthesize novel views based on different subsets of the dataset.</p>\n\n\n\n<p>Accordingly, we found EmerNeRF consistently and significantly outperformed other methods in both scene reconstruction and novel view synthesis, as shown in Table 1.&nbsp;</p>\n\n\n\n<p>EmerNeRF also outperformed methods specifically designed for static scenes, suggesting that self-supervised decomposing of a scene into static and dynamic elements improves static reconstruction as well as dynamic.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>AV simulation is only effective if it can accurately reproduce the real world. The need for fidelity increases\u2014and becomes more challenging to achieve\u2014as scenarios become more dynamic and complex.</p>\n\n\n\n<p>EmerNeRF represents and reconstructs dynamic scenarios more accurately than previous methods, without requiring human supervision or external models. This enables reconstructing and modifying complicated driving data at scale, addressing current imbalances in AV training datasets.</p>\n\n\n\n<p>We&#8217;re eager to investigate new capabilities that EmerNeRF unlocks, including end-to-end driving, autolabeling, and simulation.&nbsp;</p>\n\n\n\n<p>To learn more, visit the <a href=\"https://emernerf.github.io/\">EmerNeRF project page</a> and read the paper, <a href=\"https://arxiv.org/abs/2311.02077\">EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>From monotonous highways to routine neighborhood trips, driving is often uneventful. As a result, much of the training data for autonomous vehicle (AV) development collected in the real world is heavily skewed toward simple scenarios.&nbsp; This poses a challenge to deploying robust perception models. AVs must be thoroughly trained, tested, and validated to handle complex &hellip; <a href=\"https://developer.nvidia.com/blog/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/\">Continued</a></p>\n", "protected": false}, "author": 1932, "featured_media": 74589, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1310756", "discourse_permalink": "https://forums.developer.nvidia.com/t/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/275129", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [2892, 3366, 453, 1962], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cars-driving-city.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-joR", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74577"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1932"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74577"}], "version-history": [{"count": 38, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74577/revisions"}], "predecessor-version": [{"id": 76468, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74577/revisions/76468"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74589"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74577"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74577"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74577"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74771, "date": "2023-12-04T17:11:43", "date_gmt": "2023-12-05T01:11:43", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74771"}, "modified": "2023-12-14T11:27:30", "modified_gmt": "2023-12-14T19:27:30", "slug": "nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/", "title": {"rendered": "NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Large language models (LLMs) have seen dramatic growth over the last year, and the challenge of delivering great user experiences depends on both high-compute throughput as well as large amounts of high-bandwidth memory. <a href=\"https://github.com/NVIDIA/TensorRT-LLM/\">NVIDIA TensorRT-LLM</a> provides optimizations for both peak throughput and memory optimization, delivering massive improvements in LLM inference performance.&nbsp;</p>\n\n\n\n<p>The latest TensorRT-LLM enhancements on NVIDIA H200 GPUs deliver a 6.7x speedup on the Llama 2 70B LLM, and enable huge models, like Falcon-180B, to run on a single GPU. Llama 2 70B acceleration stems from optimizing a technique called Grouped Query Attention (GQA)\u2014an extension of multi-head attention techniques\u2014which is the key layer in Llama 2 70B.&nbsp;</p>\n\n\n\n<p>Falcon-180B is one of the largest and most accurate open-source large language models available, and previously required a minimum of eight NVIDIA A100 Tensor Core GPUs to run it.\u00a0</p>\n\n\n\n<p>TensorRT-LLM advancements in a custom INT4 AWQ make it possible to run entirely on a single H200 Tensor Core GPU, featuring 141 GB of the latest HBM3e memory with nearly 5 TB/s of memory bandwidth.\u00a0</p>\n\n\n\n<p>In this post, we share the latest TensorRT-LLM innovations and the performance they\u2019re bringing to two popular LLMs, Llama 2 70B and Falcon-180B.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Llama 2 70B on H200 delivers a 6.7x performance boost </h2>\n\n\n\n<p>The latest version of TensorRT-LLM features improved group query attention (GQA) kernels in the generation phase, providing up to a 6.7x performance boost with H200 compared to the same network running on an NVIDIA A100 GPU.</p>\n\n\n\n<p>Used in Llama 2 70B, GQA is a variant of multi-head attention (MHA) that groups key-value (KV) heads together, resulting in fewer KV heads than query (Q) heads. TensorRT-LLM has a custom implementation of MHA that supports GQA, multi-query attention (MQA), and standard MHA.&nbsp;</p>\n\n\n\n<p>It leverages <a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\">NVIDIA Tensor Cores</a>, in the generation and context phases, and delivers great performance on NVIDIA GPUs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1202\" height=\"908\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200.png\" alt=\"A chart showing the Llama 2 70B inference throughputs and speedups baselined to A100 across various input/output sequence lengths; H200 with the latest release of TensorRT-LLM achieves up to 6.7x more throughput compared to A100 using the same TensorRT-LLM version.\" class=\"wp-image-74839\" style=\"aspect-ratio:1.440046565774156;width:662px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200.png 1202w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-300x227.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-625x472.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-152x115.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-768x580.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-645x487.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-397x300.png 397w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-119x90.png 119w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-362x273.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-146x110.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Llama70B-Inference-H200-1024x774.png 1024w\" sizes=\"(max-width: 1202px) 100vw, 1202px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Llama 2 70B inference throughput on H200 across various input/output sequence length sizes\u00a0</em></figcaption></figure></div>\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td colspan=\"3\"><strong>H200 Llama 2 70B Inference Performance</strong></td></tr><tr><td>Input Sequence Length</td><td>Output Sequence Length</td><td>Throughput&nbsp;(Tokens/s per GPU)</td></tr><tr><td>128</td><td>128</td><td>3,803</td></tr><tr><td>128</td><td>2048</td><td>3,163</td></tr><tr><td>128</td><td>4096</td><td>2,263</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Llama 2 70B H200 inference throughput per GPU at different input sequence lengths</em></figcaption></figure>\n\n\n\n<p>When evaluating LLM performance, it\u2019s important to consider different input and output sequence lengths, which vary depending on the specific application where the LLM is being deployed. As we increase the output sequence length, raw throughput decreases as expected, however, the performance speedup compared to A100 increases significantly.&nbsp;</p>\n\n\n\n<p>Improvements in TensorRT-LLM software alone are bringing a 2.4x improvement compared to the previous version running on H200.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Falcon-180B performance examined</h2>\n\n\n\n<p>LLMs place both significant compute and memory demands on data center systems, and with the ongoing growth of these models, this problem will persist for some time to come. There are many techniques that developers are evolving to help address this challenge.&nbsp;</p>\n\n\n\n<p>One of these is INT4 Activation-aware Weight Quantization (AWQ) (<a href=\"https://arxiv.org/pdf/2306.00978.pdf\">Lin et al., 2023</a>). This quantization technique compresses the weights of an LLM down to just four bits based on their relative importance and then performs the computation in FP16.\u00a0</p>\n\n\n\n<p>This approach enables AWQ to maintain higher accuracy than other 4-bit methods while also reducing memory usage. To achieve this, special kernels capable of handling the change in precision at high performance are required.&nbsp;</p>\n\n\n\n<p>The latest release of TensorRT-LLM implements custom kernels for AWQ. It takes the technique a step further, performing the computations in FP8 precision on NVIDIA Hopper GPUs instead of FP16, using the latest Hopper Tensor Core technology.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"626\" height=\"639\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference.png\" alt=\"A chart showing the achieved inference throughput in tokens/second when running Falcon-180B on a single H200 GPU for two different batch size/sequence length combinations. The first shows throughput of 798 tokens/second using a batch size of 256 and input/output sequence lengths of 128. The second shows throughput of 664 tokens/second using a batch size of 128, input sequence length of 128 and output length of 2048. \" class=\"wp-image-74781\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference.png 626w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-294x300.png 294w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-113x115.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-88x90.png 88w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-362x370.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-inference-108x110.png 108w\" sizes=\"(max-width: 626px) 100vw, 626px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Falcon-180B inference throughput on a single H200 GPU</em></figcaption></figure></div>\n\n\n<p>These are the results seen running INT4 AWQ with FP8 on a single H200. In addition to being able to fit the entire Falcon-180B model, H200 also runs the model with excellent inference throughput of up to 800 tokens/second.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Stay on target</h2>\n\n\n\n<p>Quantization can often hurt model accuracy. However, TensorRT-LLM AWQ achieves a nearly 4x reduction in memory footprint and excellent inference throughput all while maintaining exceptional accuracy.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1613\" height=\"1019\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy.png\" alt=\"A chart showing the accuracy scores on Falcon-180B at FP16, FP8, and INT4 AWQ, across the following accuracy metrics: Rogue1, Rogue2, RogueL, RogueLsum, and MMLU.\u00a0\" class=\"wp-image-74786\" style=\"aspect-ratio:1.5822784810126582;width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy.png 1613w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-625x395.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-768x485.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-1536x970.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-645x407.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-475x300.png 475w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Falcon-180B-accuracy-1024x647.png 1024w\" sizes=\"(max-width: 1613px) 100vw, 1613px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Accuracy scores at FP16, FP8, and INT4 AWQ</em><br><br><em>TensorRT-LLM v0.7a | Falcon-180B | 1xH200 TP1 | INT4 AWQ</em><br></figcaption></figure></div>\n\n\n<p>Accuracy stays at or above 95% compared to running at higher precision, while delivering higher performance, and making the best use of GPU compute resources by fitting the entire model onto a single GPU. Making efficient use of the GPUs on deployed applications makes optimal use of compute resources, and helps reduce operational costs as well.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Ongoing work</h2>\n\n\n\n<p>These improvements will be available soon in TensorRT-LLM, and will be included in the v0.7 and v0.8 releases. Similar examples running Llama 2 70B in TensorRT-LLM are available on the <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">TensorRT-LLM GitHub page</a>.&nbsp;</p>\n\n\n\n<p>For more information, visit the <a href=\"https://www.nvidia.com/en-us/data-center/h200/\">NVIDIA H200 Tensor Core GPU</a> product page.</p>\n\n\n\n<p>This blog post has been adapted from a technical post on the TensorRT-LLM GitHub: <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md\">Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100</a></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) have seen dramatic growth over the last year, and the challenge of delivering great user experiences depends on both high-compute throughput as well as large amounts of high-bandwidth memory. NVIDIA TensorRT-LLM provides optimizations for both peak throughput and memory optimization, delivering massive improvements in LLM inference performance.&nbsp; The latest TensorRT-LLM enhancements &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/\">Continued</a></p>\n", "protected": false}, "author": 1355, "featured_media": 74774, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1309596", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/275005", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [296, 453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/TensorRT-LLM-Enhancements-.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jrZ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74771"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1355"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74771"}], "version-history": [{"count": 18, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74771/revisions"}], "predecessor-version": [{"id": 74842, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74771/revisions/74842"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74774"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74771"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74771"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74771"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74159, "date": "2023-12-04T14:00:00", "date_gmt": "2023-12-04T22:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74159"}, "modified": "2023-12-14T11:41:01", "modified_gmt": "2023-12-14T19:41:01", "slug": "create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/", "title": {"rendered": "Create Lifelike Avatars with AI Animation and Speech Features in NVIDIA ACE"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA today unveiled major upgrades to the <a href=\"https://developer.nvidia.com/omniverse/ace\">NVIDIA Avatar Cloud Engine (ACE)</a> suite of technologies, bringing enhanced realism and accessibility to AI-powered avatars and digital humans. These latest animation and speech capabilities enable more natural conversations and emotional expressions.&nbsp;</p>\n\n\n\n<p>Developers can now easily implement and scale intelligent avatars across applications using new cloud APIs for <a href=\"https://developer.nvidia.com/blog/essential-guide-to-automatic-speech-recognition-technology/\">automatic speech recognition (ASR)</a>, <a href=\"https://www.nvidia.com/en-us/glossary/text-to-speech/\">text-to-speech (TTS)</a>, neural machine translation (NMT), and Audio2Face (A2F).&nbsp;</p>\n\n\n\n<p>With these advanced features, available through the <a href=\"https://developer.nvidia.com/ace/early-access-form\">early access program</a>, creators can leverage NVIDIA technologies to rapidly build next-generation avatar experiences. It is now easier than ever to build and deploy digital humans anywhere and at scale, using some of the most popular rendering tools such as Unreal Engine 5.</p>\n\n\n\n<h2 class=\"wp-block-heading\">AI-powered animations with emotion&nbsp;</h2>\n\n\n\n<p>Build more expressive digital humans with the latest ACE AI animation features and microservices, including newly added A2F emotional support. An Animation Graph microservice for body, head, and eye movements is also now available.&nbsp;</p>\n\n\n\n<p>For developers handling rendering production through the cloud or looking to do real-time inference, there is now an easy-to-use microservice. And A2F quality improvements include lip sync, bringing even more realism to digital humans.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"630\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline.jpg\" alt=\"Diagram of an NVIDIA ACE end-to-end avatar development pipeline.\" class=\"wp-image-74555\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline.jpg 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-300x95.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-625x197.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-179x56.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-768x242.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-1536x484.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-645x203.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-500x158.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-160x50.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-362x114.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-349x110.jpg 349w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-development-pipeline-1024x323.jpg 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA ACE end-to-end development suite</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Enhanced AI speech capabilities&nbsp;</h2>\n\n\n\n<p>Languages supported now include Italian, EU Spanish, German, and Mandarin. The overall accuracy of the ASR technology has also been improved. And cloud APIs for ASR, TTS, and NMT simplify access to the latest Speech AI features.</p>\n\n\n\n<p>A new Voice Font microservice enables you to customize TTS output, whether you want to apply a custom voice to an intelligent NPC using your own voice or randomize a user\u2019s voice in a video conferencing call. This technology converts a speaker\u2019s distinct pitch and volume and converts it into a reference audio while maintaining the same patterns of rhythm and sound.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">New tooling and frameworks&nbsp;</h2>\n\n\n\n<p>ACE Agent is a streamlined dialog management and system integrator that provides a more seamless end-to-end experience, efficiently orchestrating connections between microservices. Developers also have more control over accurate, adjustable responses through integrations with <a href=\"https://github.com/NVIDIA/NeMo-Guardrails\">NVIDIA NeMo Guardrails</a>, <a href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\">NVIDIA SteerLM</a>, and LangChain.&nbsp;</p>\n\n\n\n<p>It is now easier to get these tools up and running in your renderer or coding environment of choice. New features include:</p>\n\n\n\n<ul>\n<li>Support for blendshapes within the Avatar configurator to easily integrate popular renderers, including Unreal Engine.&nbsp;</li>\n\n\n\n<li>A new A2F application for Python users.</li>\n\n\n\n<li>A reference application for developers interested in building virtual assistants for customer service.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>These newly introduced NVIDIA ACE features raise the quality bar for digital human experiences. With enhancements that make building and deployment easier, developers now have simplified configurations necessary to build next-generation digital human applications.&nbsp;</p>\n\n\n\n<p>Interested in exploring cutting-edge digital human technologies? <a href=\"https://developer.nvidia.com/ace/early-access-form\">Apply for early access</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA today unveiled major upgrades to the NVIDIA Avatar Cloud Engine (ACE) suite of technologies, bringing enhanced realism and accessibility to AI-powered avatars and digital humans. These latest animation and speech capabilities enable more natural conversations and emotional expressions.&nbsp; Developers can now easily implement and scale intelligent avatars across applications using new cloud APIs for &hellip; <a href=\"https://developer.nvidia.com/blog/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/\">Continued</a></p>\n", "protected": false}, "author": 1046, "featured_media": 74164, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1309531", "discourse_permalink": "https://forums.developer.nvidia.com/t/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/274995", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 1050, 3110, 503], "tags": [3063, 453, 2932, 2379, 570, 3166, 3633], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-ace-avatar.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ji7", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74159"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1046"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74159"}], "version-history": [{"count": 39, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74159/revisions"}], "predecessor-version": [{"id": 75106, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74159/revisions/75106"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74164"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74159"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74159"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74159"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74615, "date": "2023-12-04T10:00:00", "date_gmt": "2023-12-04T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74615"}, "modified": "2023-12-14T11:27:31", "modified_gmt": "2023-12-14T19:27:31", "slug": "new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/", "title": {"rendered": "New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The rapid growth in the size, complexity, and diversity of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language models</a> (LLMs) continues to drive an insatiable need for AI training performance. Delivering top performance requires the ability to train models at the scale of an entire data center efficiently. This is achieved through exceptional craftsmanship at every layer of the technology stack, spanning chips, systems, and software.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo framework</a> is an end-to-end, cloud-native framework for building, customizing, and deploying generative AI models. It incorporates a full array of advanced parallelism techniques to enable efficient training of LLMs at scale.&nbsp;</p>\n\n\n\n<p>In fact, NeMo powered the exceptional GPT-3 175B performance submissions by NVIDIA in the latest <a href=\"https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/\">MLPerf Training</a> industry-standard benchmarks, achieving up to 797 TFLOPS per H100 GPU. And, in the largest scale submitted by NVIDIA, record performance and near-linear performance scaling were achieved using an unprecedented 10,752 H100 Tensor Core GPUs.</p>\n\n\n\n<blockquote class=\"wp-block-quote\">\n<p>Today, NVIDIA is announcing that the upcoming January release of the NeMo framework incorporates a host of optimizations and new features. These dramatically improve performance on <a href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\">NVIDIA AI Foundation </a>Models, including <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/collections/llama2/entities\">Llama 2</a>, <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nemo-8b-steerlm\">Nemotron-3</a>, and other LLMs, and expand NeMo model architecture support. It also provides a much-requested parallelism technique, making it even easier to train various models on the NVIDIA AI platform.&nbsp;</p>\n</blockquote>\n\n\n\n<h2 class=\"wp-block-heading\">Up to 4.2x faster Llama 2 70B pre-training and supervised fine-tuning</h2>\n\n\n\n<p>Llama 2 is a popular, open-source large language model originally developed by Meta. The latest release of NeMo includes many improvements that increase Llama 2 performance. Compared to the prior NeMo release running on A100 GPUs, the latest NeMo release running on H200 GPUs delivers up to 4.2x faster Llama 2 pre-training and supervised fine-tuning performance.&nbsp;</p>\n\n\n\n<p>The first improvement is the addition of mixed-precision implementations of the model optimizer\u2019s state. This reduces model capacity requirements and improves the effective memory bandwidth for operations that interact with the model state by 1.8x.&nbsp;</p>\n\n\n\n<p>The performance of rotary positional embedding (RoPE) operations\u2014state-of-the-art algorithms employed by many recent LLM architectures\u2014has also increased. Additionally, the performance of Swish-Gated Linear Unit (SwiGLU) activation functions is optimized, which commonly substitutes Gaussian Error Linear Unit (GELU) in modern LLMs.&nbsp;</p>\n\n\n\n<p>Finally, the communication efficiency for tensor parallelism has been greatly improved, and communication chunk sizes for pipeline parallelism have been tuned.&nbsp;</p>\n\n\n\n<p>Collectively, these improvements dramatically increase Tensor Core usage on GPUs based on the <a href=\"https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\">NVIDIA Hopper architecture</a>, achieving up to 836 TFLOPS per H200 GPU for Llama 2 70B pre-training and supervised fine-tuning.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1575\" height=\"679\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements.png\" alt=\"A chart showing that H200 with the latest release of NeMo delivers 779 TFLOPS, 822 TFLOPS, and 836 TFLOPs on Llama 2 7B, 13B, and 70B, respectively. This compares to 211 TFLOPS, 206 TFLOPS, and 201 TFLOPS on these models, respectively, on A100 and the prior NeMo release.\" class=\"wp-image-75187\" style=\"aspect-ratio:2.3195876288659796;width:780px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements.png 1575w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-625x269.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-768x331.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-1536x662.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-645x278.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-500x216.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-362x156.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-255x110.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/H200-and-NeMo-Perf-Improvements-1024x441.png 1024w\" sizes=\"(max-width: 1575px) 100vw, 1575px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Training performance, in model TFLOPS per GPU, on the Llama 2 family of models (7B, 13B, and 70B) on H200 using the latest NeMo release compared to performance on A100 using the prior NeMo release&nbsp;</em><br><br><em>Measured performance per GPU. Global Batch Size = 128.<br>Llama 2 7B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha<br>Llama 2 13B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha<br>Llama 2 70B: Sequence Length 4096 | A100 32x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha</em></figcaption></figure></div>\n\n\n<p>H200 GPUs, coupled with the latest version of NeMo, can achieve exceptional Llama 2 training throughput, delivering up to a 4.2x uplift compared to A100 GPUs running the prior NeMo release.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><thead><tr><th><strong>Training Tokens/Sec/GPU</strong></th><th><strong>Llama 2 7B</strong></th><th><strong>Llama 2 13B</strong></th><th><strong>Llama 2 70B</strong></th></tr></thead><tbody><tr><td>H200 (Latest NeMo)</td><td>16,913</td><td>9,432</td><td>1,880</td></tr><tr><td>A100 (Prior NeMo)</td><td>4,583</td><td>2,357</td><td>451</td></tr><tr><td>Speedup</td><td><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-vivid-green-cyan-color\">3.7X</mark></strong></td><td><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-vivid-green-cyan-color\">4.0X</mark></strong></td><td><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-vivid-green-cyan-color\">4.2X</mark></strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Training performance, in tokens per second per GPU&nbsp;</em> <br><br><em>Measured performance per GPU. Global Batch Size = 128.</em><br><em>Llama 2 7B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha<br>Llama 2 13B: Sequence Length 4096 | A100 8x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha<br>Llama 2 70B: Sequence Length 4096 | A100 32x GPU, NeMo 23.08 | H200 8x GPU, NeMo 24.01-alpha</em></figcaption></figure>\n\n\n\n<p>Putting this performance into context, a single system based on the eight-way NVIDIA HGX H200 can fine-tune Llama 2 with 70B parameters on sequences of length 4096 at a rate of over 15,000 tokens/second. This means that it can complete a supervised fine-tuning task consisting of 1B tokens in just over 18 hours.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Fully Sharded Data Parallelism</h2>\n\n\n\n<p>Fully Sharded Data Parallelism (FSDP) is a well-known and popular feature within the deep learning community. It is used by deep learning practitioners across major frameworks, including PyTorch, DeepSpeed, and JAX, and is applied to a wide variety of models.&nbsp;</p>\n\n\n\n<p>FSDP can be applied to a wide variety of models, and is particularly useful for LLMs, as the compute and memory requirements of modern LLMs are well beyond the scope of even a single, advanced GPU.&nbsp;</p>\n\n\n\n<p>Pipelining a model is an effective performance optimization. But it requires that a model has a very regular structure (for example, the same layer repeated 128 times) because different layers are distributed to different GPUs and data flows between them in a pipelined manner.&nbsp;</p>\n\n\n\n<p>FSDP provides developers with improved usability and minimal performance loss across various situations. This is because the data and memory of a model are distributed on a per-layer basis, which makes it easier to manage regular or irregular neural network structures.&nbsp;</p>\n\n\n\n<p>A natural extension of data parallelism, FSDP can often be used through simple model wrappers, without needing to consider how a model is partitioned (as is the case in pipeline parallelism). This also makes it easier to extend FSDP to new and emerging model architectures, such as multi-modal LLMs.&nbsp;</p>\n\n\n\n<p>FSDP can also achieve performance competitive with traditional combinations of tensor parallelism and pipeline parallelism methods when there is sufficient parallelism at scales smaller than the global batch size.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td colspan=\"6\"><strong>GPT-20B Training Performance (BF16)</strong></td></tr><tr><td># of H100 GPUs</td><td><strong>8</strong></td><td><strong>16</strong></td><td><strong>32</strong></td><td><strong>64</strong></td><td><strong>128</strong></td></tr><tr><td>FSDP</td><td>0.78x</td><td>0.88x</td><td>0.89x</td><td>0.86x</td><td>0.96x</td></tr><tr><td>3D Parallelism</td><td>1.0x</td><td>1.0x</td><td>1.0x</td><td>1.0x</td><td>1.0x</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2: Measured relative training performance on H100 GPUs when using FSDP compared to using 3D Parallelism</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-center\"><em>Measured performance. Global Batch Size = 256, Sequence Length = 2048</em></p>\n\n\n\n<h2 class=\"wp-block-heading\">Mixture of Experts</h2>\n\n\n\n<p>One proven method of improving the information absorption and generalization capabilities of generative AI models is to increase the number of parameters in the model. However, a challenge that emerges with larger models is that as their capacity increases, the compute required to perform inference also grows, increasing the cost to run the models in production.&nbsp;</p>\n\n\n\n<p>Recently, a mechanism called Mixture of Experts (MoE) has gained significant attention. It enables model capacity to be increased without a proportional increase in both the training and inference compute requirements. MoE architectures achieve this through a conditional computation approach where each input token is routed to only one or a few expert neural network layers instead of being routed through all of them. This decouples model capacity from required compute.</p>\n\n\n\n<p>The latest release of NeMo introduces official support for MoE-based LLM architectures with expert parallelism. This implementation uses an architecture similar to that of <a href=\"https://arxiv.org/pdf/2103.16716.pdf\">Balanced Assignment of Experts (BASE)</a> where each MoE layer routes each token to exactly one expert and uses algorithmic load balancing. NeMo uses <a href=\"https://arxiv.org/pdf/2202.01169.pdf\">Sinkhorn-based</a> routing to balance the token load across the various experts.</p>\n\n\n\n<p>MoE models based on NeMo support expert parallelism, which can be used in combination with data parallelism to distribute MoE experts across data parallel ranks. NeMo also provides the ability to configure expert parallelism arbitrarily. Users can map experts to different GPUs in various ways without restricting the number of experts on a single device (all devices, however, must contain the same number of experts). NeMo also supports cases where the expert parallel size is less than the data parallel size.</p>\n\n\n\n<p>Developers can use the NeMo expert parallelism method in combination with the many other parallelism dimensions offered by NeMo including tensor, pipeline, and sequence parallelism. This facilitates efficient training of models with more than a trillion parameters on clusters with many NVIDIA GPUs.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">RLHF with TensorRT-LLM</h2>\n\n\n\n<p>NeMo support for reinforcement learning from human feedback (RLHF) has now been enhanced with the ability to use TensorRT-LLM for inference inside of the RLHF loop.&nbsp;</p>\n\n\n\n<p>TensorRT-LLM accelerates the inference stage of the actor model, which currently takes most of the end-to-end compute time. The actor model is the model of interest that is being aligned and will be the ultimate output of the RLHF process.&nbsp;</p>\n\n\n\n<p>The upcoming NeMo release enables pipeline parallelism for RLHF through TensorRT-LLM, enabling it to achieve better performance with fewer nodes, all while also supporting larger models. </p>\n\n\n\n<p>In fact, for the Llama 2 70B parameter model, using TensorRT-LLM in the RLHF loop with H100 GPUs enables up to a 5.6x performance increase compared to RLHF without TensorRT-LLM in the loop on the same H100 GPUs.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1644\" height=\"715\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements.png\" alt=\"A chart showing the relative performance of H100 using the latest NeMo release for RLHF across four different models\u2013GPT-2B, Llama 2 7B, Llama 2 13B, and Llama 2 70B\u2013with respective H100 GPU counts of 16, 16, 32, and 128 compared to the prior NeMo release. Performance increases are 5x, 2.2x, 3.1x, and 5.6x, respectively.\" class=\"wp-image-75190\" style=\"aspect-ratio:2.310991957104558;object-fit:cover;width:780px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements.png 1644w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-625x272.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-768x334.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-1536x668.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-645x281.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-500x217.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-253x110.png 253w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/RLHF-on-H100-Performance-Improvements-1024x445.png 1024w\" sizes=\"(max-width: 1644px) 100vw, 1644px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The latest version of NeMo brings dramatic improvements compared to the previous version, accelerating the inference part of the RLHF training operation</em><br><em><br>Measured performance. Global Batch Size =&nbsp; 64, Rollout Size&nbsp; = 512, Maximum Generation Length = 1024.&nbsp;<br>Half of the nodes for each result run the actor and the other half run the critic in the RLHF algorithm implemented.</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Pushing the boundaries of generative AI</h2>\n\n\n\n<p>AI training requires a full-stack approach. NVIDIA NeMo is regularly updated to provide optimal performance for training advanced generative AI models. It incorporates the most recent training methods to improve performance and provide more flexibility for NVIDIA platform users.&nbsp;</p>\n\n\n\n<p>The NVIDIA platform is also incredibly versatile and accelerates the entire AI workflow end-to-end, from data prep to model training to deploying inference. Following the introduction of TensorRT-LLM in October, NVIDIA recently demonstrated the ability to run the latest Falcon-180B model on a single H200 GPU, leveraging TensorRT-LLM\u2019s advanced 4-bit quantization feature, while maintaining 99% accuracy. Read more about this implementation in the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md\">latest post</a> about TensorRT-LLM.</p>\n\n\n\n<p>The NVIDIA AI platform continues to advance performance, versatility, and features at the speed of light. That\u2019s why it is the platform of choice for developing and deploying today\u2019s generative AI applications and inventing the models and techniques that are powering what comes next.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started with NeMo framework</h2>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/get-started/\">NVIDIA NeMo framework</a> is available as an open-source library on GitHub, a container on NGC, and as part of NVIDIA AI Enterprise, an enterprise-grade AI software platform with security, stability, manageability, and support.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The rapid growth in the size, complexity, and diversity of large language models (LLMs) continues to drive an insatiable need for AI training performance. Delivering top performance requires the ability to train models at the scale of an entire data center efficiently. This is achieved through exceptional craftsmanship at every layer of the technology stack, &hellip; <a href=\"https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/\">Continued</a></p>\n", "protected": false}, "author": 1355, "featured_media": 74743, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1309395", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/274912", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [453, 1950, 2932, 3613], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/NeMo-framework-features.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jpt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74615"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1355"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74615"}], "version-history": [{"count": 25, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74615/revisions"}], "predecessor-version": [{"id": 76239, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74615/revisions/76239"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74743"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74615"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74615"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74615"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74475, "date": "2023-12-01T13:00:00", "date_gmt": "2023-12-01T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74475"}, "modified": "2023-12-14T11:29:46", "modified_gmt": "2023-12-14T19:29:46", "slug": "webinar-analysis-of-openacc-validation-and-verification-testsuite", "status": "publish", "type": "post", "link": "https://bit.ly/openacc_vvdn", "title": {"rendered": "Webinar: Analysis of OpenACC Validation and Verification Testsuite"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>On December 7, learn how to verify OpenACC implementations across compilers and system architectures with the validation testsuite.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>On December 7, learn how to verify OpenACC implementations across compilers and system architectures with the validation testsuite.</p>\n", "protected": false}, "author": 1466, "featured_media": 74480, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://bit.ly/openacc_vvdn", "_links_to_target": "_blank"}, "categories": [503], "tags": [1258, 52, 53, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/man-laptop-webinar.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jnd", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74475"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74475"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74475/revisions"}], "predecessor-version": [{"id": 74494, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74475/revisions/74494"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74480"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74475"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74475"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74475"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74303, "date": "2023-12-01T09:00:00", "date_gmt": "2023-12-01T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74303"}, "modified": "2023-12-14T11:27:32", "modified_gmt": "2023-12-14T19:27:32", "slug": "what-is-a-supernic", "status": "publish", "type": "post", "link": "https://blogs.nvidia.com/blog/what-is-a-supernic/", "title": {"rendered": "Explainer: What Is a SuperNIC?"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>A SuperNIC is a type of network accelerator for AI cloud data centers that delivers robust and seamless connectivity between GPU servers.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>A SuperNIC is a type of network accelerator for AI cloud data centers that delivers robust and seamless connectivity between GPU servers.</p>\n", "protected": false}, "author": 746, "featured_media": 74307, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1308351", "discourse_permalink": "https://forums.developer.nvidia.com/t/explainer-what-is-a-supernic/274695", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://blogs.nvidia.com/blog/what-is-a-supernic/", "_links_to_target": "_blank"}, "categories": [852, 1205], "tags": [3563, 2973, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/bluefield-supernic-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jkr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74303"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/746"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74303"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74303/revisions"}], "predecessor-version": [{"id": 74312, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74303/revisions/74312"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74307"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74303"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74303"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74303"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74179, "date": "2023-11-30T11:12:44", "date_gmt": "2023-11-30T19:12:44", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74179"}, "modified": "2023-12-29T16:47:31", "modified_gmt": "2023-12-30T00:47:31", "slug": "building-your-first-llm-agent-application", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/building-your-first-llm-agent-application/", "title": {"rendered": "Building Your First LLM Agent Application"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>When building a <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language model</a> (LLM) agent application, there are four key components you need: an agent core, a memory module, agent tools, and a planning module. Whether you are designing a question-answering agent, multi-modal agent, or swarm of agents, you can consider many implementation frameworks\u2014from open-source to production-ready. For more information, see <a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/\">Introduction to LLM Agents</a>.</p>\n\n\n\n<p>For those experimenting with developing an LLM agent for the first time, this post provides the following:</p>\n\n\n\n<ul>\n<li>An overview of the developer ecosystem, including available frameworks and recommended readings to get up-to-speed on LLM agents</li>\n\n\n\n<li>A beginner-level tutorial for building your first LLM-powered agent</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Developer ecosystem overview for agents</h2>\n\n\n\n<p>Most of you have probably read articles about LangChain or LLaMa-Index agents. Here are a few of the implementation frameworks available today:</p>\n\n\n\n<ul>\n<li><a href=\"https://python.langchain.com/docs/modules/agents/\">LangChain Agents</a></li>\n\n\n\n<li><a href=\"https://gpt-index.readthedocs.io/en/stable/module_guides/deploying/agents/root.html\">LLaMaIndex Agents</a></li>\n\n\n\n<li><a href=\"https://docs.haystack.deepset.ai/docs/agent\">HayStack Agents</a></li>\n\n\n\n<li><a href=\"https://github.com/microsoft/autogen\">AutoGen</a></li>\n\n\n\n<li><a href=\"https://github.com/OpenBMB/AgentVerse\">AgentVerse</a></li>\n\n\n\n<li><a href=\"https://github.com/OpenBMB/ChatDev\">ChatDev</a></li>\n\n\n\n<li><a href=\"https://github.com/joonspk-research/generative_agents\">Generative Agents</a></li>\n</ul>\n\n\n\n<p>So, which one do I recommend? The answer is, \u201cIt depends.\u201d</p>\n\n\n\n<h4 class=\"wp-block-heading\">Single-agent frameworks</h4>\n\n\n\n<p>There are several frameworks built by the community to further the LLM application development ecosystem, offering you an easy path to develop agents. Some examples of popular frameworks include LangChain, LlamaIndex, and Haystack. These frameworks provide a generic agent class, connectors, and features for memory modules, access to third-party tools, as well as data retrieval and ingestion mechanisms.</p>\n\n\n\n<p>A choice of which framework to choose largely comes down to the specifics of your pipeline and your requirements. In cases where you must build complex agents that have a directed acyclic graph (DAG), like logical flow, or which have unique properties, these frameworks offer a good reference point for prompts and general architecture for your own custom implementation.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Multi-agent frameworks</h4>\n\n\n\n<p>You might ask, \u201cWhat\u2019s different in a multi-agent framework?\u201d The short answer is a &#8220;world\u201d class. To manage multiple agents, you must architect the world, or rather the environment in which they interact with each other, the user, and the tools in the environment.&nbsp;</p>\n\n\n\n<p>The challenge here is that for every application, the world will be different. What you need is a toolkit custom-made to build simulation environments and one that can manage world states and has generic classes for agents. You also need a communication protocol established for managing traffic amongst the agents.<strong> </strong>The choice of OSS frameworks depends on the type of application that you are building and the level of customization required.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Recommended reading list for building agents</h3>\n\n\n\n<p>There are plenty of resources and materials that you can use to stimulate your thinking around what is possible with agents, but the following resources are an excellent starting point to cover the overall ethos of agents:</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/Significant-Gravitas/AutoGPT\">AutoGPT</a><strong>: </strong>This GitHub project was one of the first true agents that was built to showcase the capabilities that agents can provide. Looking at the general architecture and the prompting techniques used in the project can be quite helpful.</li>\n\n\n\n<li><a href=\"https://voyager.minedojo.org/\">Voyager</a>: This project from <a href=\"https://www.nvidia.com/en-us/research/\">NVIDIA Research</a> touches upon the concept of self-improving agents that learn to use new tools or build tools without any external intervention.</li>\n\n\n\n<li><a href=\"https://arxiv.org/pdf/2305.16334.pdf\">OlaGPT</a>: Conceptual frameworks for agents, like OlaGPT, are a great starting point to stimulate ideas on how to go beyond simple agents, which have the basic four modules.</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2205.00445\">MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</a>: This paper first suggested the core mechanism for using tools with language models to execute complex tasks.</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2304.03442\">Generative Agents: Interactive Simulacra of Human Behavior</a>:<strong> </strong>This was one of the first projects to build a true swarm of agents: a solution made up of multiple agents interacting with each other in a decentralized manner.</li>\n</ul>\n\n\n\n<p>If you are looking for more reading material, I find the <a href=\"https://github.com/hyp1231/awesome-llm-powered-agent#awesome-llm-powered-agent\">Awesome LLM-Powered Agent</a> list to be useful. If you have specific queries, drop a comment on this post.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Tutorial: Build a question-answering agent</h2>\n\n\n\n<p>For this tutorial, you build a question-answering (QA) agent that can help you talk to your data.</p>\n\n\n\n<p>To show that a fairly simple agent can tackle fairly hard challenges, you build an agent that can mine information from earnings calls. You can view the <a href=\"https://www.fool.com/earnings/call-transcripts/2023/08/23/nvidia-nvda-q2-2024-earnings-call-transcript/\">earnings call transcripts</a>. Figure 1 shows the general structure of the earnings call so that you can understand the files used for this tutorial.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"479\" height=\"298\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown.png\" alt=\"The earnings call transcript is largely divided into three distinct sections: metadata, attendees, introductory remarks; overall remarks and revenue, general trends, and remarks for the next quarter; and a Q&amp;A session. Not every section contains complete context.\" class=\"wp-image-74250\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown.png 479w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown-300x187.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown-362x225.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/earnings-call-breakdown-177x110.png 177w\" sizes=\"(max-width: 479px) 100vw, 479px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Conceptual breakdown of an earnings call</em></figcaption></figure></div>\n\n\n<p>By the end of this post, the agent you build will answer complex and layered questions like the following:</p>\n\n\n\n<ul>\n<li>How much did revenue grow between Q1 of 2024 and Q2 of 2024?</li>\n\n\n\n<li>What were the key takeaways from Q2 of FY24?</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"849\" height=\"483\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa.png\" alt=\"Screenshot of an agent called Clone providing answers to a complex question related to earnings.\" class=\"wp-image-74251\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa.png 849w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-300x171.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-625x356.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-768x437.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-645x367.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-500x284.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-158x90.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-362x206.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/example-qa-193x110.png 193w\" sizes=\"(max-width: 849px) 100vw, 849px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Example question and answer for the agent you are building</em></figcaption></figure></div>\n\n\n<p>&nbsp;As described in <a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/\">part 1 of this series</a>, there are four agent components:</p>\n\n\n\n<ul>\n<li>Tools</li>\n\n\n\n<li>Planning module</li>\n\n\n\n<li>Memory</li>\n\n\n\n<li>Agent core</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Tools</h3>\n\n\n\n<p>To build an LLM agent, you need the following tools:</p>\n\n\n\n<ul>\n<li><strong>Retrieval-augmented generation (RAG) pipeline: </strong>You can\u2019t solve the talk-to-your-data problem without <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\" data-type=\"link\" data-id=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\" target=\"_blank\" rel=\"noreferrer noopener\">RAG</a>. So, one of the tools that you need is a RAG pipeline. For more information about how to build a production-grade RAG pipeline, refer to the <a href=\"https://nvda.ws/47OvlMU\" data-type=\"link\" data-id=\"https://nvda.ws/47OvlMU\" target=\"_blank\" rel=\"noreferrer noopener\">GitHub repo</a>.</li>\n\n\n\n<li><strong>Mathematical tool: </strong>You also require a mathematical tool for performing any type of analysis. To keep it simple for this post, I use an LLM to answer math questions, but tools like <a href=\"https://www.wolframalpha.com/\">WolframAlpha</a> are the ones that I recommend for production applications.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Planning module</h3>\n\n\n\n<p>With this LLM agent, you will be able to answer questions such as: \u201cHow much did the revenue grow between Q1 of 2024 and Q2 of 2024?\u201d Fundamentally, these are three questions rolled into one:</p>\n\n\n\n<ul>\n<li>What was the revenue in Q1?</li>\n\n\n\n<li>What was the revenue in Q2?</li>\n\n\n\n<li>And, what\u2019s the difference between the two?</li>\n</ul>\n\n\n\n<p>The answer is that you must build a question decomposition module:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndecomp_template = &quot;&quot;&quot;GENERAL INSTRUCTIONS\nYou are a domain expert. Your task is to break down a complex question into simpler sub-parts.\n\nUSER QUESTION\n{{user_question}}\n\nANSWER FORMAT\n{&quot;sub-questions&quot;:&#91;&quot;&lt;FILL&gt;&quot;]}&quot;&quot;\n</pre></div>\n\n\n<p>As you can see, the decomposition module is prompting the LLM to break the question down into less complex parts. Figure 3 shows what an answer looks like.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"818\" height=\"355\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example.png\" alt=\"Screenshot of an LLM response following the prompt of breaking a question into subparts: What was the revenue in Q1, the revenue in Q2, and the difference between the two?\" class=\"wp-image-74252\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example.png 818w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-625x271.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-768x333.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-645x280.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-500x217.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/planning-module-example-253x110.png 253w\" sizes=\"(max-width: 818px) 100vw, 818px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Planning module and prototyping decomposition</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Memory</h3>\n\n\n\n<p>Next, you must build a memory module to keep track of all the questions being asked or just to keep a list of all the sub-questions and the answers for said questions.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nclass Ledger:\n    def __init__(self):\n        self.question_trace = &#91;]\n        self.answer_trace = &#91;]\n</pre></div>\n\n\n<p>You do this with a simple ledger made up of two lists: one to keep track of all the questions and one to keep track of all the answers. This helps the agent remember the questions it has answered and has yet to answer.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Evaluate the mental model</h4>\n\n\n\n<p>Before you build an agent core, evaluate what you have right now:</p>\n\n\n\n<ul>\n<li>Tools to search and do mathematical calculations</li>\n\n\n\n<li>A planner to break down the question</li>\n\n\n\n<li>A memory module to keep track of questions asked.</li>\n</ul>\n\n\n\n<p>At this point, you can tie these together to see if it works as a mental model (Figure 4).&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ntemplate = &quot;&quot;&quot;GENERAL INSTRUCTIONS\nYour task is to answer questions. If you cannot answer the question, request a helper or use a tool. Fill with Nil where no tool or helper is required.\n\nAVAILABLE TOOLS\n- Search Tool\n- Math Tool\n\nAVAILABLE HELPERS\n- Decomposition: Breaks Complex Questions down into simpler subparts\n\nCONTEXTUAL INFORMATION\n&lt;No previous questions asked&gt;\n\nQUESTION\nHow much did the revenue grow between Q1 of 2024 and Q2 of 2024?\n\nANSWER FORMAT\n{&quot;Tool_Request&quot;: &quot;&lt;Fill&gt;&quot;, &quot;Helper_Request &quot;&lt;Fill&gt;&quot;}&quot;&quot;&quot;\n</pre></div>\n\n\n<p>Figure 4 shows the answer received for the LLM.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"813\" height=\"570\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request.png\" alt=\"Screenshot shows that the LLM lists a search tool as a tool request and that the helper request is nil.\u00a0\" class=\"wp-image-74253\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request.png 813w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-300x210.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-625x438.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-164x115.png 164w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-768x538.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-645x452.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-428x300.png 428w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-362x254.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/search-tool-request-157x110.png 157w\" sizes=\"(max-width: 813px) 100vw, 813px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Putting all the modules together</em></figcaption></figure></div>\n\n\n<p>You can see that the LLM requested the use of a search tool, which is a logical step as the answer may well be in the corpus. That said, you know that none of the transcripts contain the answer. In the next step (Figure 5), you provide the input from the RAG pipeline that the answer wasn\u2019t available, so the agent then decides to decompose the question into simpler sub-parts.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"857\" height=\"613\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer.png\" alt=\"Screenshot shows that, after adding the sub-answer &quot;The tool cannot answer this question,&quot; the tool request is now nil but that the helper request is Decomposition.\" class=\"wp-image-74254\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer.png 857w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-300x215.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-625x447.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-161x115.png 161w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-768x549.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-645x461.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-419x300.png 419w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-126x90.png 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-750x535.png 750w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-362x259.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/adding-contextual-answer-154x110.png 154w\" sizes=\"(max-width: 857px) 100vw, 857px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Adding an answer to the sub-contextual question</em></figcaption></figure></div>\n\n\n<p>With this exercise, you validated that the core mechanism of logic is sound. The LLM is selecting tools and helpers as and when required.&nbsp;</p>\n\n\n\n<p>Now, all that is left is to neatly wrap this in a Python function, which would look something like the following code example:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndef agent_core(question):\n    answer_dict = prompt_core_llm(question, memory)\n    update_memory()\n    if answer_dict&#91;tools]:\n        execute_tool()\n        update_memory()\n    if answer_dict&#91;planner]:\n        questions = execute_planner()\n        update_memory()\n    if no_new_questions and no tool request:\n        return generate_final_answer(memory)\n</pre></div>\n\n\n<h3 class=\"wp-block-heading\">Agent core</h3>\n\n\n\n<p>You just saw the example of an agent core, so what&#8217;s left? Well, there is a bit more to an agent core than just stitching all the pieces together. You must define the mechanism by which the agent is supposed to execute its flow. There are essentially three major choices:&nbsp;</p>\n\n\n\n<ul>\n<li>Linear solver</li>\n\n\n\n<li>Single-thread recursive solver</li>\n\n\n\n<li>Multi-thread recursive solver</li>\n</ul>\n\n\n\n<h4 class=\"wp-block-heading\">Linear solver</h4>\n\n\n\n<p>This is the type of execution that I discussed earlier. There is a single linear chain of solutions where the agent can use tools and do one level of planning. While this is a simple setup, true complex and nuanced questions often require layered thinking.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Single-thread recursive solver</h4>\n\n\n\n<p>You can also build a recursive solver that constructs a tree of questions and answers till the original question is answered. This tree is solved in a depth-first traversal. The following code example shows the logic:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndef Agent_Core(Question, Context):\n    Action = LLM(Context + Question)\n\n    if Action == &quot;Decomposition&quot;:\n        Sub Questions = LLM(Question)\n        Agent_Core(Sub Question, Context)\n\n    if Action == &quot;Search Tool&quot;:\n        Answer = RAG_Pipeline(Question)\n        Context = Context + Answer\n        Agent_Core(Question, Context)\n\n    if Action == &quot;Gen Final Answer\u201d:\n        return LLM(Context)\n\n    if Action == &quot;&lt;Another Tool&gt;&quot;:\n        &lt;Execute Another Tool&gt;\n</pre></div>\n\n\n<h4 class=\"wp-block-heading\">Multi-thread recursive solver</h4>\n\n\n\n<p>Instead of iteratively solving the tree, you can spin off parallel execution threads for each node on the tree. This method adds execution complexity but yields massive latency benefits as the LLM calls can be processed in parallel.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What\u2019s next?</h2>\n\n\n\n<p>Congratulations! You are now armed with the knowledge you for building fairly complex agents! One next step is adapting the principles discussed earlier to your problems.&nbsp;</p>\n\n\n\n<p>To develop more tools for LLM agents, see the <a href=\"https://nvdam.widen.net/s/rvsgdxpfkz/dli-generative-ai-llm-learning-path-2740963\">NVIDIA Deep Learning Institute</a> page. To build, customize, and deploy an LLM for your use case, see the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo Framework</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>When building a large language model (LLM) agent application, there are four key components you need: an agent core, a memory module, agent tools, and a planning module. Whether you are designing a question-answering agent, multi-modal agent, or swarm of agents, you can consider many implementation frameworks\u2014from open-source to production-ready. For more information, see Introduction &hellip; <a href=\"https://developer.nvidia.com/blog/building-your-first-llm-agent-application/\">Continued</a></p>\n", "protected": false}, "author": 969, "featured_media": 75842, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1307690", "discourse_permalink": "https://forums.developer.nvidia.com/t/building-your-first-llm-agent-application/274594", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-agents-part-2.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jir", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74179"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/969"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74179"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74179/revisions"}], "predecessor-version": [{"id": 76104, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74179/revisions/76104"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75842"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74179"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74179"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74179"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74535, "date": "2023-11-30T09:30:00", "date_gmt": "2023-11-30T17:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74535"}, "modified": "2023-12-14T11:27:33", "modified_gmt": "2023-12-14T19:27:33", "slug": "webinar-explore-nvidia-rtx-workflows-with-jsfilmz", "status": "publish", "type": "post", "link": "https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia", "title": {"rendered": "Webinar: Explore NVIDIA RTX Workflows with JSFILMZ"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In this webinar, see how YouTube creator JSFILMZ uses NVIDIA RTX and how it enables him to iterate on creative ideas.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In this webinar, see how YouTube creator JSFILMZ uses NVIDIA RTX and how it enables him to iterate on creative ideas.</p>\n", "protected": false}, "author": 1115, "featured_media": 74536, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia", "_links_to_target": "_blank"}, "categories": [1235], "tags": [453, 483, 582, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/jsfilmz-webinar-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-job", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74535"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74535"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74535/revisions"}], "predecessor-version": [{"id": 74541, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74535/revisions/74541"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74536"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74535"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74535"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74535"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74178, "date": "2023-11-30T09:00:00", "date_gmt": "2023-11-30T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74178"}, "modified": "2023-12-20T09:34:56", "modified_gmt": "2023-12-20T17:34:56", "slug": "introduction-to-llm-agents", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/introduction-to-llm-agents/", "title": {"rendered": "Introduction to LLM Agents"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Consider a <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language model</a> (LLM) application that is designed to help financial analysts answer questions about the performance of a company. With a well-designed retrieval augmented generation (RAG) pipeline, analysts can answer questions like, \u201cWhat was X corporation\u2019s total revenue for FY 2022?\u201d This information can be easily extracted from financial statements by a seasoned analyst.</p>\n\n\n\n<p>Now consider a question like, \u201cWhat were the three takeaways from the Q2 earnings call from FY 23? Focus on the technological moats that the company is building\u201d. This is the type of question a financial analyst would want answered to include in their reports but would need to invest time to answer.</p>\n\n\n\n<p>How do we develop a solution to answer a question like above? It is immediately apparent that this information requires more than a simple lookup from an earnings call. This inquiry requires planning, tailored focus, memory, using different tools, and breaking down a complex question into simpler sub-parts<s>.</s>. These concepts assembled together are essentially what we have come to refer to as an LLM Agent.&nbsp;<sup>&nbsp;</sup></p>\n\n\n\n<p>In this post, I introduce LLM-powered agents and discuss what an agent is and some use cases for enterprise applications. For more information, see <a href=\"https://developer.nvidia.com/blog/building-your-first-llm-agent-application/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/llm-powered-agents-building-your-first-agent-application/\" target=\"_blank\" rel=\"noreferrer noopener\">Building Your First Agent Application</a>. In that post, I offer an ecosystem walkthrough, covering the available frameworks for building AI agents and a getting started guide for anyone experimenting with question-and-answer (Q&amp;A) agents.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">What is an AI agent?</h2>\n\n\n\n<p>While there isn\u2019t a widely accepted definition for LLM-powered agents, they can be described as a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools.&nbsp;</p>\n\n\n\n<p>In short, agents are a system with complex reasoning capabilities, memory, and the means to execute tasks.</p>\n\n\n\n<p>This capability was first observed in projects like <a href=\"https://github.com/Significant-Gravitas/AutoGPT\">AutoGPT</a> or <a href=\"https://github.com/yoheinakajima/babyagi\">BabyAGI</a>, where complex problems were solved without much intervention. To describe agents a bit more, here\u2019s the general architecture of an LLM-powered agent application (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"379\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-625x379.png\" alt=\"Architecture diagram of an LLM-powered agent. The agent core is at the center with bi-directional arrow connecting user requests, memory module, planning module and tools. \" class=\"wp-image-74194\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-625x379.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-300x182.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-768x466.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-645x391.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-494x300.png 494w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-362x220.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-181x110.png 181w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components-1024x621.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components.png 1424w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. General components of an agent</em></figcaption></figure></div>\n\n\n<p>An agent is made up of the following key components (more details on these shortly):</p>\n\n\n\n<ul>\n<li>Agent core</li>\n\n\n\n<li>Memory module</li>\n\n\n\n<li>Tools</li>\n\n\n\n<li>Planning module</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Agent core</h3>\n\n\n\n<p>The agent core is the central coordination module that manages the core logic and behavioral characteristics of an Agent. Think of it as the \u201ckey decision making module\u201d of the agent. It is also where we define:</p>\n\n\n\n<ul>\n<li><strong>General goals of the agent</strong>: Contains overall goals and objectives for the agent.</li>\n\n\n\n<li><strong>Tools for execution</strong>: Essentially a short list or a&nbsp; \u201cuser manual\u201d for all the tools to which the agent has access</li>\n\n\n\n<li><strong>Explanation for how to make use of different planning modules</strong>: Details about the utility of different planning modules and which to use in what situation.</li>\n\n\n\n<li><strong>Relevant Memory</strong>: This is a dynamic section which fills the most relevant memory items from past conversations with the user at inference time. The \u201crelevance\u201d is determined using the question user asks.</li>\n\n\n\n<li><strong>Persona of the Agent (optional)</strong>: This persona description is typically used to either bias the model to prefer using certain types of tools or to imbue typical idiosyncrasies in the agent\u2019s final response.&nbsp;</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"428\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core.png\" alt=\"The Agent core has access to Goals, Tools, planning helpers and a general format for the answer. \" class=\"wp-image-74723\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core-300x206.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core-168x115.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core-437x300.png 437w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core-131x90.png 131w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core-362x248.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Figure-2-Basic-template-of-how-the-different-modules-of-an-agent-are-assembled-in-its-core-160x110.png 160w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Basic template of how the different modules of an agent are assembled in its core.</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Memory module</h3>\n\n\n\n<p>Memory modules play a critical role in AI agents. A memory module can essentially be thought of as a store of the agent\u2019s internal logs as well as interactions with a user.&nbsp;</p>\n\n\n\n<p>There are two types of memory modules:</p>\n\n\n\n<ul>\n<li><strong>Short-term memory:</strong> A ledger of actions and thoughts that an agent goes through to attempt to answer a single question from a user: the agent\u2019s \u201ctrain of thought.\u201d</li>\n\n\n\n<li><strong>Long-term memory:</strong> A ledger of actions and thoughts about events that happen between the user and agent. It is a log book that contains a conversation history stretching across weeks or months<strong>.</strong></li>\n</ul>\n\n\n\n<p>Memory requires more than semantic similarity-based retrieval. Typically, a composite score is made up of semantic similarity, importance, recency, and other application-specific metrics. It is used for retrieving specific information.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Tools</h3>\n\n\n\n<p>Tools are well-defined executable workflows that agents can use to execute tasks. Oftentimes, they can be thought of as specialized third-party APIs.</p>\n\n\n\n<p>For instance, agents can use a RAG pipeline to generate context aware answers, a code interpreter to solve complex programmatically tasks, an API to search information over the internet, or even any simple API service like a weather API or an API for an Instant messaging application.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Planning module</h3>\n\n\n\n<p>Complex problems, such as analyzing a set of financial reports to answer a layered business question, often require nuanced approaches. With an LLM\u2013powered agent, this complexity can be dealt with by using a combination of two techniques:</p>\n\n\n\n<ul>\n<li>Task and question decomposition</li>\n\n\n\n<li>Reflection or critic</li>\n</ul>\n\n\n\n<h4 class=\"wp-block-heading\">Task and question decomposition</h4>\n\n\n\n<p>Compound questions or inferred information require some form of decomposition. Take, for instance, the question, \u201cWhat were the three takeaways from NVIDIA&#8217;s last earnings call?\u201d&nbsp;</p>\n\n\n\n<p>The information required to answer this question is not directly extractable from the transcript of an hour long meeting. However, the problem can be broken down into multiple question topics:</p>\n\n\n\n<ul>\n<li>\u201cWhich technological shifts were discussed the most?\u201d</li>\n\n\n\n<li>\u201cAre there any business headwinds?\u201d</li>\n\n\n\n<li>\u201cWhat were the financial results?\u201d</li>\n</ul>\n\n\n\n<p>Each of these questions can be further broken into subparts. That said, a specialized AI agent must guide this decomposition.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Reflection or critic</h4>\n\n\n\n<p>Techniques like ReAct, Reflexion, Chain of Thought, and Graph of thought have served as critic\u2013 or evidence-based prompting frameworks. They have been widely used to improve the reasoning capabilities and responses of LLMs. These techniques can also be used to refine the execution plan generated by the agent.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Agents for enterprise applications</h2>\n\n\n\n<p>While the applications of agents are practically boundless, the following are a few interesting cases that may have an outsized impact for many businesses:</p>\n\n\n\n<ul>\n<li>\u201cTalk to your data\u201d agent</li>\n\n\n\n<li>Swarm of agents</li>\n\n\n\n<li>Recommendation and experience design agents</li>\n\n\n\n<li>Customized AI author agents</li>\n\n\n\n<li>Multi-modal agents</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">&#8220;Talk to your data&#8221; agent</h3>\n\n\n\n<p>\u201cTalk to your data\u201d isn\u2019t a simple problem. There are a lot of challenges that a straightforward RAG pipeline can\u2019t solve:</p>\n\n\n\n<ul>\n<li>Semantic similarity of source documents</li>\n\n\n\n<li>Complex data structures, like tables</li>\n\n\n\n<li>Lack of apparent context (not every chunk contains markers for its source)</li>\n\n\n\n<li>The complexity of the questions that users ask</li>\n\n\n\n<li>\u2026and more</li>\n</ul>\n\n\n\n<p>For instance, go back to the prior earning\u2019s call transcript example (<a href=\"https://www.fool.com/earnings/call-transcripts/2022/11/16/nvidia-nvda-q3-2023-earnings-call-transcript/#:~:text=Q3%20revenue%20was%20%245.93%20billion,down%2017%25%20year%20on%20year.\">Q3, 2023</a> | <a href=\"https://www.fool.com/earnings/call-transcripts/2023/05/24/nvidia-nvda-q1-2024-earnings-call-transcript/\">Q1 2024</a>). How do you answer the question, \u201cHow much did the data center revenue increase between Q3 of 2023 and Q1 of 2024?\u201d To answer this question, you essentially must answer three questions individually (i.e., we need a planning module):&nbsp;</p>\n\n\n\n<ul>\n<li>What was the data center revenue in Q3 of 2023?</li>\n\n\n\n<li>What was the data center revenue in Q1 of 2024?</li>\n\n\n\n<li>What was the difference between the two?</li>\n</ul>\n\n\n\n<p>In this case, you would need an agent that has access to a &nbsp;Planning Module that does question-decomposition (generates sub-questions and searches for answers till the larger problem is solved), a RAG pipeline (used as a tool) to retrieve specific information, and memory modules to accurately handle the subquestions. In the <a href=\"https://developer.nvidia.com/blog/building-your-first-llm-agent-application/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/llm-powered-agents-building-your-first-agent-application/\" target=\"_blank\" rel=\"noreferrer noopener\">LLM-Powered Agents: Building Your First Agent Application</a> post, I go over this type of case in detail.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Swarm of agents</h3>\n\n\n\n<p>A swarm of agents can be understood as a collection of agents working together towards co-existing in a single environment which can collaborate with each other to solve problems. A decentralized ecosystem of agents is very much akin to multiple \u201csmart\u201d microservices used in tandem to solve problems.</p>\n\n\n\n<p>Multi-agent environments like <a href=\"https://github.com/joonspk-research/generative_agents\">Generative Agents</a> and <a href=\"https://github.com/OpenBMB/ChatDev\">ChatDev</a> have been extremely popular with the community (Figure 3). Why? Frameworks like ChatDev enable you to build a team of engineers, designers, product management, CEO, and agents to build basic software at low costs. Popular games like Brick Breaker or Flappy Bird can be prototyped for as low as 50 cents!&nbsp;</p>\n\n\n\n<p>With a swarm of agents, you can populate a digital company, neighborhood, or even a whole town for applications like behavioral simulations for economic studies, enterprise marketing campaigns, UX elements of physical infrastructure, and more. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1784\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev.png\" alt=\"Graphic showing a swarm of comic digital agents for application development: designing, coding, testing, and documenting.\" class=\"wp-image-74192\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-300x268.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-625x558.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-129x115.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-768x685.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-1536x1371.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-645x576.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-336x300.png 336w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-101x90.png 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-362x323.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-123x110.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/multiple-agents-chatdev-1024x914.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Example of multiple agents in a ChatDev environment</em></figcaption></figure></div>\n\n\n<p>These applications are currently not possible to simulate without LLMs and are extremely expensive to run in the real world.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Agents for recommendation and experience design</h3>\n\n\n\n<p>The internet works off recommendations. Conversational <a href=\"https://www.nvidia.com/en-us/glossary/data-science/recommendation-system/\">recommendation systems</a> powered by agents can be used to craft personalized experiences.&nbsp;</p>\n\n\n\n<p>For example, consider an AI agent on an e-commerce website that helps you compare products and provides recommendations based on your general requests and selections. A full concierge-like experience can also be built, with multiple agents assisting an end user to navigate a digital store. Experiences like selecting which movie to watch or which hotel room to book can be crafted as conversations\u2014and not just a series of decision-tree-style conversations!</p>\n\n\n\n<h3 class=\"wp-block-heading\">Customized AI author agents</h3>\n\n\n\n<p>Another powerful tool is having a personal AI author that can help you with tasks such as co-authoring emails or preparing you for time-sensitive meetings and presentations. The problem with regular authoring tools is that different types of material must be tailored according to various audiences. For instance, an investor pitch must be worded differently than a team presentation.&nbsp;</p>\n\n\n\n<p>Agents can harness your previous work. Then, you have the agent mold an agent-generated pitch to your personal style and customize the work according to your specific use case and needs. This process is often too nuanced for general LLM fine-tuning.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Multi-modal agents</h3>\n\n\n\n<p>With only text as an input, you cannot really \u201ctalk to your data.\u201d All the mentioned use cases can be augmented by building multi-modal agents that can digest a variety of inputs, such as images and audio files.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"955\" height=\"570\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response.png\" alt=\"Screenshot shows a Schrodinger query and an answer that is based on information in files with both text and bar chart.\" class=\"wp-image-74191\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response.png 955w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-300x179.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-625x373.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-768x458.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-645x385.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-500x298.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-151x90.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-362x216.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/schrodinger-non-text-response-184x110.png 184w\" sizes=\"(max-width: 955px) 100vw, 955px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Example multi-modal agent answering questions based on graphs</em></figcaption></figure></div>\n\n\n<p>That was just a few examples of directions that can be pursued to solve enterprise challenges. Agents for data curation, social graphs, and domain expertise are all active areas being pursued by the development community for enterprise applications.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What\u2019s next?</h2>\n\n\n\n<p>LLM-powered agents differ from typical chatbot applications in that they have complex reasoning skills. Made up of an agent core, memory module, set of tools, and planning module, agents can generate highly personalized answers and content in a variety of enterprise settings\u2014from data curation to advanced e-commerce recommendation systems.</p>\n\n\n\n<p>For an overview of the technical ecosystem around agents, such as implementation frameworks, must-read papers, posts, and related topics, see <a href=\"https://developer.nvidia.com/blog/building-your-first-llm-agent-application/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/building-your-first-agent-application/\" target=\"_blank\" rel=\"noreferrer noopener\">Building Your First Agent Application</a>. The walkthrough of a no-framework implementation of a Q&amp;A agent helps you better talk to your data.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Consider a large language model (LLM) application that is designed to help financial analysts answer questions about the performance of a company. With a well-designed retrieval augmented generation (RAG) pipeline, analysts can answer questions like, \u201cWhat was X corporation\u2019s total revenue for FY 2022?\u201d This information can be easily extracted from financial statements by a &hellip; <a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/\">Continued</a></p>\n", "protected": false}, "author": 969, "featured_media": 75839, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1307636", "discourse_permalink": "https://forums.developer.nvidia.com/t/introduction-to-llm-agents/274582", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/llm-agents-part-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jiq", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74178"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/969"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74178"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74178/revisions"}], "predecessor-version": [{"id": 75841, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74178/revisions/75841"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75839"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74178"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74178"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74178"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]