[{"id": 72986, "date": "2023-11-15T09:30:00", "date_gmt": "2023-11-15T17:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72986"}, "modified": "2023-11-30T11:43:32", "modified_gmt": "2023-11-30T19:43:32", "slug": "deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/", "title": {"rendered": "Deploy Large Language Models at the Edge with NVIDIA IGX Orin Developer Kit"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>As <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language models (LLMs)</a> become more powerful and techniques for reducing their computational requirements mature, two compelling questions emerge. First, what is the most advanced LLM that can be run and deployed at the edge? And second, how can real-world applications leverage these advancements?&nbsp;</p>\n\n\n\n<p>Running a state-of-the-art open-source LLM like Llama 2 70B, even at reduced FP16 precision, requires more than 140 GB of GPU VRAM (70 billion parameters x 2 bytes = 140 GB in FP16, plus more for KV Cache). For most developers and smaller companies, this amount of VRAM is not easily accessible. Additionally, whether due to cost, bandwidth, latency, or data privacy issues, application-specific requirements may preclude the option of hosting an LLM using cloud computing resources.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\">NVIDIA IGX Orin Developer Kit</a> and <a href=\"https://developer.nvidia.com/holoscan-sdk\">NVIDIA Holoscan SDK</a> address these challenges, bringing the power of LLMs to the edge. The NVIDIA IGX Orin Developer Kit coupled with a discrete <a href=\"https://www.nvidia.com/en-us/design-visualization/rtx-a6000/\">NVIDIA RTX A6000</a> GPU delivers an industrial-grade edge AI platform tailored to the demands of industrial and medical environments. This includes NVIDIA Holoscan, an SDK that harmonizes data movement, accelerated computing, real-time visualization, and AI inferencing.&nbsp;</p>\n\n\n\n<p>This platform enables developers to add open-source LLMs into edge AI streaming workflows and products, offering new possibilities in real-time AI-enabled sensor processing while ensuring that sensitive data remains within the secure boundaries of the IGX hardware.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Open-source LLMs for real-time streaming</h2>\n\n\n\n<p>The recent rapid development of open-source LLMs has transformed the perception of what is possible in a real-time streaming application. The prevailing perception was that any application requiring human-like abilities was exclusive to closed-source LLMs powered by enterprise-grade GPUs at data center scale. However, thanks to the recent surge in the performance of new open-source LLMs, models such as Falcon, MPT, and Llama 2 can now provide a viable alternative to closed-source black-box LLMs.&nbsp;</p>\n\n\n\n<p>There are many possible applications that can make use of these open-source models on the edge, and many of them involve the distillation of streaming sensor data into natural language summaries. Such possibilities include real-time monitoring of surgical videos to keep families informed of progress, summarizing recent radar contacts for air traffic controllers, and even converting the play-by-play commentary of a soccer match into another language.</p>\n\n\n\n<p>Access to powerful, open-source LLMs has also inspired a community devoted to refining the accuracy of these models, as well as reducing the computation required to run them. This vibrant community is active on the <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\">Hugging Face Open LLM Leaderboard</a>, which is updated often with the latest top-performing models.</p>\n\n\n\n<h2 class=\"wp-block-heading\">AI capabilities at the edge&nbsp;&nbsp;</h2>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/edge-computing/products/igx/\">NVIDIA IGX Orin platform</a> is uniquely positioned to leverage the surge in available open-source LLMs and supporting software. The NVIDIA RTX A6000 GPU provides an ample 48 GB of VRAM, enabling it to run some of the largest open-source models.&nbsp; For example, a version of Llama 2 70B whose model weights have been quantized to 4 bits of precision, rather than the standard 32 bits, can run entirely on the GPU at 14 tokens per second.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>Diverse problems and use cases can be addressed by the robust Llama 2 model, bolstered by the security measures of the NVIDIA IGX Orin platform, and seamlessly incorporated into low-latency Holoscan SDK pipelines. This convergence not only marks a significant advancement in AI capabilities at the edge, but also unlocks the potential for transformative solutions across various domains.</p>\n\n\n\n<p>One notable application could capitalize on the newly introduced <a href=\"https://arxiv.org/abs/2305.12031\">Clinical Camel</a>, a fine-tuned Llama 2 70B variant specialized in medical knowledge. By creating a localized medical chatbot based on this model, it ensures that sensitive patient data remains confined within the secure boundaries of the IGX hardware. Applications where privacy, bandwidth concerns, or real-time feedback are critical are where the IGX platform really shines.</p>\n\n\n\n<p>Imagine inputting a patient&#8217;s medical records and querying the bot to discover similar cases, gaining fresh insights into hard-to-diagnose patients, or even equipping medical professionals with a short list of potential medications that circumvents interactions with existing prescriptions. All of this could\u200c be automated through a Holoscan application that converts real-time audio from physician-patient interactions into text and seamlessly feeds it into the Clinical Camel model.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"783\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue.png\" alt=\"Clinical notes between a doctor and patient, generated by the Clinical Camel model.\" class=\"wp-image-73098\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-300x118.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-625x245.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-768x301.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-1536x602.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-645x253.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-500x196.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-160x63.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-362x142.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-281x110.png 281w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/clinical-camel-doctor-patient-dialogue-1024x401.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Clinical note generated by the Clinical Camel model from the example dialogue</em></em></figcaption></figure></div>\n\n\n<p>Expanding on its potential, the NVIDIA IGX platform extends the capabilities of LLMs beyond text-only applications, thanks to its exceptional optimization for low-latency sensor input. While the medical chatbot presents a compelling use case, the power of the IGX Orin Developer Kit is its capacity to seamlessly integrate real-time data from various sensors.&nbsp;</p>\n\n\n\n<p>Designed for edge environments, the IGX Orin can process streaming information from cameras, lidar sensors, radio antennas, accelerometers, ultrasound probes, and more. This versatility empowers cutting-edge applications that seamlessly merge LLM prowess with real-time data streams.&nbsp;</p>\n\n\n\n<p>Integrated into Holoscan operators, these LLMs have the potential to significantly enhance the capabilities and functionalities of AI-enabled sensor processing pipelines. Specific examples are detailed below.</p>\n\n\n\n<p><strong>Multimodal medical assistants:</strong> Enhance LLMs with the capability to interpret not only text but also medical imaging, as demonstrated by projects like <a href=\"https://arxiv.org/abs/2307.15189\">Med-Flamingo</a>, which interprets MRIs, x-rays, and histology images.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"592\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-625x592.png\" alt=\"Rationale and answer generated by Med-Flamingo in response to image and question about lesions in aorta. \n\" class=\"wp-image-73100\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-625x592.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-300x284.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-121x115.png 121w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-768x728.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-1536x1455.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-645x611.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-317x300.png 317w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-95x90.png 95w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-362x343.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-116x110.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer-1024x970.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/med-flamingo-answer.png 1557w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. LLMs can interpret text and extract relevant insights from medical images</em></em></figcaption></figure>\n\n\n\n<p><strong>Signals Intelligence (SIGINT):</strong> Derive natural language summaries from real-time electronic signals captured by communication systems and radars, providing insights that bridge technical data and human understanding.</p>\n\n\n\n<p><strong>Surgical case note generation:</strong> Channel endoscopy video, audio, system data, and patient records to multimodal LLMs to generate comprehensive surgical case notes that are automatically uploaded to a patient\u2019s electronic medical records.</p>\n\n\n\n<p><strong>Smart agriculture:</strong> Tap into soil sensors monitoring pH, moisture, and nutrient levels, enabling LLMs to offer actionable insights for optimized planting, irrigation, and pest control strategies.</p>\n\n\n\n<p>Software development copilots for education, troubleshooting, or productivity enhancement agents are another novel use case of LLMs. These models help developers to develop more efficient code and thorough documentation.&nbsp;</p>\n\n\n\n<p>The Holoscan team recently released HoloChat, an AI-driven chatbot serving as a developer&#8217;s copilot in Holoscan development. It generates human-like responses to questions about Holoscan and writing code. For details, visit <a href=\"https://github.com/nvidia-holoscan/holohub/tree/main/applications/holochat_local\">nvidia-holoscan/holohub</a> on GitHub.&nbsp;</p>\n\n\n\n<p>The HoloChat local hosting approach is designed to provide developers with the same benefits as popular closed-source chatbots while eliminating the privacy and security concerns associated with sending data to third-party remote servers for processing.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Model quantization for optimal accuracy and memory usage</h2>\n\n\n\n<p>With the influx of open-source models being released under Apache 2, MIT, and commercially viable licenses, anyone can download and use these model weights. However, just because this is possible, does not mean that it is feasible for the vast majority of developers.&nbsp;</p>\n\n\n\n<p>Model quantization provides one solution. Model quantization reduces the computational and memory costs of running inference by representing the weights and activations as low-precision data types (int8 and int4) instead of higher-precision data types (FP16 and FP32).&nbsp;</p>\n\n\n\n<p>However, removing this precision from the model does result in some degradation in the accuracy of the model. Yet research indicates that, given a memory budget, the best LLM performance is achieved by using the largest possible model that will fit into memory when the parameters are stored in 4-bit precision. For more details, see <a href=\"https://arxiv.org/abs/2212.09720\">The Case for 4-bit Precision: k-Bit Inference Scaling Laws</a>.</p>\n\n\n\n<p>As such, the Llama 2 70B model achieves its optimal balance of accuracy and memory usage when it is implemented in a 4-bit quantization, which reduces the RAM required to about 35 GB only. This memory requirement is within reach for smaller development teams or even individuals. It is easily met with the single NVIDIA RTX A6000 48 GB GPU that is optionally included with the IGX Orin.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Open-source LLMs open new development opportunities</h2>\n\n\n\n<p>With the ability to run state-of-the-art LLMs on commodity hardware, the open-source community has exploded with new libraries that support running locally and offer tools that extend the capabilities of these models beyond just predicting a sentence\u2019s next word.&nbsp;</p>\n\n\n\n<p>Libraries such as Llama.cpp, ExLlama, and AutoGPTQ enable you to quantize your own models and run blazing-fast inference on your local GPUs. Quantizing your own models is an entirely optional step, however, as the <a href=\"https://huggingface.co/models\">HuggingFace.co model repository</a> is full of quantized models ready for use. This is thanks in large part to power users like <a href=\"https://huggingface.co/TheBloke\">/TheBloke</a>, who upload newly quantized models daily.<br><br>While these models on their own offer exciting development opportunities, augmenting them with additional tools from a host of newly created libraries makes them all the more powerful. Examples include:&nbsp;</p>\n\n\n\n<ul>\n<li><a href=\"https://www.langchain.com/\">LangChain</a>, a library with 58,000 GitHub stars that provides everything from vector database integration to enable document Q&amp;A, to multi-step agent frameworks that enable LLMs to browse the web.&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/deepset-ai/haystack\">Haystack</a>, which enables scalable semantic search.</li>\n\n\n\n<li><a href=\"https://github.com/jackmpcollins/magentic\">Magentic</a>, offering easy integration of LLMs into your Python code.&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/oobabooga/text-generation-webui\">Oobabooga</a>, a web UI for running quantized LLMs locally.&nbsp;</li>\n</ul>\n\n\n\n<p>If you have an LLM use case, an open-source library is likely available that will provide most of what you need.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started deploying LLMs at the edge</h2>\n\n\n\n<p>Deploying cutting-edge LLMs at the edge with NVIDIA IGX Orin Developer Kit opens untapped development opportunities. To get started, check out the comprehensive tutorial detailing the creation of a simple chatbot application on IGX Orin, <a href=\"https://github.com/nvidia-holoscan/holohub/tree/main/tutorials/local-llama\">Deploying Llama 2 70B Model on the Edge with IGX Orin</a>.\u00a0</p>\n\n\n\n<p>This tutorial illustrates the seamless integration of Llama 2 on IGX Orin, and guides you through developing a Python application using Gradio. This is the initial step toward harnessing any of the exceptional LLM libraries mentioned in this post. IGX Orin delivers resilience, unparalleled performance, and end-to-end security, empowering developers to forge innovative Holoscan-optimized applications around state-of-the-art LLMs operating at the edge.&nbsp;</p>\n\n\n\n<p>On Friday, November 17, join the NVIDIA Deep Learning Institute for <a href=\"https://www.nvidia.com/en-us/events/llm-developer-day/\">LLM Developer Day</a>, a free virtual event delving into cutting-edge techniques in LLM application development. <a href=\"https://www.nvidia.com/en-us/events/llm-developer-day/\">Register to access the event live or on demand</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As large language models (LLMs) become more powerful and techniques for reducing their computational requirements mature, two compelling questions emerge. First, what is the most advanced LLM that can be run and deployed at the edge? And second, how can real-world applications leverage these advancements?&nbsp; Running a state-of-the-art open-source LLM like Llama 2 70B, even &hellip; <a href=\"https://developer.nvidia.com/blog/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/\">Continued</a></p>\n", "protected": false}, "author": 1901, "featured_media": 72992, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298059", "discourse_permalink": "https://forums.developer.nvidia.com/t/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/272984", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 2758, 3110, 63], "tags": [453, 1948, 2903, 2932, 570], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/large-language-models-edge.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iZc", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72986"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1901"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72986"}], "version-history": [{"count": 17, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72986/revisions"}], "predecessor-version": [{"id": 73579, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72986/revisions/73579"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72992"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72986"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72986"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72986"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73688, "date": "2023-11-15T08:00:00", "date_gmt": "2023-11-15T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73688"}, "modified": "2024-01-02T10:37:01", "modified_gmt": "2024-01-02T18:37:01", "slug": "build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/", "title": {"rendered": "Build Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\u00a0"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In the realm of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a>, building enterprise-grade <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language models</a> (LLMs) requires expertise collecting high-quality data, setting up the accelerated infrastructure, and optimizing the models.&nbsp;</p>\n\n\n\n<p>Developers can begin with pretrained models and fine-tune them for their use case, saving time and getting their solutions faster to market. Developers need an easy way to try out models and evaluate their capabilities by integrating them through APIs. This helps them decide which model is best for their application.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA AI Foundation Models&nbsp;</h2>\n\n\n\n<p><a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\" target=\"_blank\">NVIDIA AI Foundation Models</a> are a curated set of community and NVIDIA-built models, optimized for peak performance. Developers can quickly use them directly from their browser through APIs or \u200cgraphical user interface, without any setup. The models \u200care optimized with <a rel=\"noreferrer noopener\" href=\"https://github.com/NVIDIA/TensorRT-LLM\" target=\"_blank\">NVIDIA TensorRT-LLM</a> and Activation-aware Weight Quantization (AWQ) to identify the configuration for the highest throughput and lowest latency and run at scale on the NVIDIA accelerated computing stack.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Introducing the NVIDIA Nemotron-3 8B family of LLMs</h3>\n\n\n\n<p>The <a href=\"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Nemotron-3 8B family of models</a> offers a foundation for customers looking to build production-ready generative AI applications. These models are constructed on responsibly sourced datasets and operate at comparable performance to much larger models, making them ideal for enterprise deployments.&nbsp;</p>\n\n\n\n<p>A key differentiator of the NVIDIA Nemotron-3 8B family of models is its multilingual capabilities, which make it ideal for global enterprise. Out of the box, these models are proficient in 53 languages, including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch.</p>\n\n\n\n<p>The family of models also features a range of alignment techniques, including supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), as well as the new NVIDIA <a href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\" target=\"_blank\" rel=\"noreferrer noopener\">SteerLM</a> customization technique, where customers can tune models at inference. These variants provide a variety of starting points for supporting different use cases, whether customizing or running the models from scratch.</p>\n\n\n\n<p>The Nemotron-3 8B family of models includes:</p>\n\n\n\n<ul>\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nemo-8b-steerlm\" target=\"_blank\" rel=\"noreferrer noopener\">Nemotron-3-8B-Chat-SteerLM</a>, a generative language model based on the NV-Nemotron-3-8B base model that is customized for user control of model outputs during inference using the SteerLM technique</li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nemo-8b-qa\" target=\"_blank\" rel=\"noreferrer noopener\">Nemotron-3-8B-QA</a>, a generative language model based on the NV-Nemotron-3-8B base model that further fine-tunes for instructions for Question Answering.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">NVIDIA-optimized community models</h3>\n\n\n\n<p>Additionally, NVIDIA offers leading community models that \u200care optimized using NVIDIA TensorRT-LLM to deliver the highest performance per dollar that organizations can customize for their enterprise applications. These include:</p>\n\n\n\n<ul>\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/llama2-70b\" target=\"_blank\" rel=\"noreferrer noopener\">Llama 2</a>, one of the most popular LLMs capable of generating text from prompts.&nbsp;</li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/sdxl\" target=\"_blank\" rel=\"noreferrer noopener\">Stable Diffusion XL</a>, a popular Generative AI model that can create expressive images with text.</li>\n\n\n\n<li><a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/codellama-34b\" target=\"_blank\">Code Llama</a>, a fine-tuned version of Llama 2 model that can generate code in several popular languages such as Java, C++, Python, and more.&nbsp;</li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mistral-7b-instruct\" target=\"_blank\" rel=\"noreferrer noopener\">Mistral 7B</a>, an LLM that can follow instructions, complete requests, and generate creative text formats.</li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/clip\" target=\"_blank\" rel=\"noreferrer noopener\">Contrastive Language-Image Pre-Training (CLIP)</a>, a popular open-source model that understands images and text together, enabling tasks like image classification and object detection.</li>\n</ul>\n\n\n\n<p>As developers identify the right foundation models, there&#8217;s an easy path for them to fine-tune and deploy these models either on their own or on NVIDIA-maintained infrastructure through <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\" target=\"_blank\">NVIDIA DGX Cloud</a>.&nbsp;</p>\n\n\n\n<p>Let\u2019s walk through the steps to experience, customize, and deploy the fine-tuned Llama 2 model.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Experience Llama 2</h2>\n\n\n\n<p>NVIDIA offers an easy-to-use interface to interact with the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/llama2-70b\" target=\"_blank\" rel=\"noreferrer noopener\">Llama 2 </a>model directly from your browser. Simply enter the text in the prompt field, click generate, and the model will begin generating informative responses instantly.&nbsp;</p>\n\n\n\n<p>In Figure 1, a user asked the model for an SQL query to retrieve the list of customers who spent at least $50,000 in the first quarter of 2021. The model interpreted the user\u2019s query correctly and provided the answer with a detailed explanation. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"447\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1.png\" alt=\"An image showing the output of the Llama 2 model for an SQL query to get the list of customers who spent at least $50,000 in the first quarter of 2021.\" class=\"wp-image-73419\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1-300x215.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1-161x115.png 161w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1-419x300.png 419w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1-126x90.png 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1-362x259.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Image-of-the-Llama-2-model-response-for-an-SQL-query-to-get-the-list-of-customers-who-spent-at-least-50000-in-the-first-quarter-of-2021-1-154x110.png 154w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Image of the Llama 2 model response for an SQL query to get the list of customers who spent at least $50,000 in the first quarter of 2021</em></figcaption></figure></div>\n\n\n<p>Developers are often more interested in working with code. NVIDIA also offers an API widget directly in the browser, making it seamless to experience the models through an API.&nbsp;</p>\n\n\n\n<p>To try the in-browser API, click on API mode, and select the language you prefer from the dropdown. Figure 2 shows the API instructions for calling the API through cURL.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/1usspVe6cADzfEI2L9wkFnRjPSJ1M_q6Ai3t6guYsFY9GtWvasEN0HVq-8WPP-mMyL69nxVQZTHVxtDkDw4FpQnRrSLvBpH-PSRUfiHXRtDaMNRUyitaU_bZUdCY9Jjsx_ijMXsPferw_mO6bJa821s\" alt=\"Image showing the in-browser API widget for Llama 2 model for cURL. \"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Picture of the in-browser API Widget for the Llama 2 model for cURL</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Customize the model</h2>\n\n\n\n<p>It&#8217;s often the case that the models don\u2019t meet the developer&#8217;s \u200cneeds as-is and must be fine-tuned with proprietary data. NVIDIA offers various paths for customizing the available models.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/nemo\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NeMo</a> is an end-to-end, enterprise-grade cloud-native framework for developers to build, customize, and deploy generative AI models with billions of parameters. NeMo also provides APIs to fine-tune LLMs like Llama 2.&nbsp;</p>\n\n\n\n<p>To get started quickly, we are offering an NVIDIA LaunchPad lab\u2014a universal proving ground, offering comprehensive testing of the latest NVIDIA enterprise hardware and software.&nbsp;</p>\n\n\n\n<p>The following example in the LaunchPad lab experience, fine-tunes a Llama 2 7B text-to-text model using a custom dataset to better perform a Question-Answering task.&nbsp;</p>\n\n\n\n<ol>\n<li>To get started, click on the<strong> </strong><a href=\"https://www.nvidia.com/en-us/launchpad/ai/customize-llama-2-with-enterprise-data/\" target=\"_blank\" rel=\"noreferrer noopener\">Llama 2 fine-tuning lab</a><strong> </strong>and request access. When working on Launchpad, the Llama 2 model files are pre-downloaded as a .nemo checkpoints, which enable fine-tuning compatibility with NVIDIA NeMo Framework.&nbsp;</li>\n\n\n\n<li>After the model is prepared, we load the <a href=\"https://huggingface.co/datasets/databricks/databricks-dolly-15k\" target=\"_blank\" rel=\"noreferrer noopener\">Dolly dataset</a> from Hugging Face and preprocess it by getting rid of unnecessary fields, renaming certain fields to better fit the p-tuning task, and splitting the dataset into train and test files.</li>\n</ol>\n\n\n\n<pre class=\"wp-block-code\"><code>dataset = load_dataset(\"aisquared/databricks-dolly-15k\")</code></pre>\n\n\n\n<p>A sample of the data is shown. The datasets can be swapped to fit specific use cases.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n{\n&quot;question&quot;: &quot;When did Virgin Australia start operating?&quot;, \n&quot;context&quot;: &quot;Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.&#91;3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.&#91;4]&quot;, \n&quot;answer&quot;: &quot;Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.&quot;, \n&quot;taskname&quot;: &quot;genqa&quot;\n}\n</pre></div>\n\n\n<ol start=\"3\">\n<li>The fine-tuning job is then configured by updating certain fields of a default NeMo config file to fit the training task at hand. The job is then launched using a NeMo training script, which runs the fine-tuning and generates model checkpoints along the way.&nbsp;</li>\n\n\n\n<li>Once the fine-tuning task is completed, we&#8217;ll be able to run in-notebook inference to generate a few example outputs and evaluate the performance of the fine-tuned model.&nbsp;</li>\n\n\n\n<li>Let\u2019s look at one such example output. We provided the context with descriptions of the two products: a lawn mower and a kitchen robot, and asked the model whether the lawn mower was solar-powered.&nbsp;</li>\n</ol>\n\n\n\n<p>The model accurately grasped the context and responded with a &#8220;Yes.&#8221; Fine-tuning the model enabled it to respond accurately to our questions from the provided context.&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>{\n\"input\": \"Context: The Auto Chef Master is a personal kitchen robot that effortlessly turns raw ingredients into gourmet meals with the precision of a Michelin-star chef. The Eco Lawn Mower is a solar powered high-tech lawn mower that provides an eco-friendly and efficient way to maintain your lawn. Question: Is the lawn mower product solar powered? \nAnswer:\", \n\"pred\": \"Yes\", \n\"label\": \"Yes, the Eco Lawn Mower is solar powered.\", \n\"taskname\": \"genqa\"\n}</code></pre>\n\n\n\n<h2 class=\"wp-block-heading\">Deploy the model</h2>\n\n\n\n<p>NVIDIA AI Foundation Endpoints provide fully serverless and scalable APIs that can be deployed on either your own or NVIDIA DGX Cloud. <a href=\"https://developer.nvidia.com/ai-foundation-endpoints\" target=\"_blank\" rel=\"noreferrer noopener\">Complete this form to get started</a> with AI Foundation Endpoints.&nbsp;</p>\n\n\n\n<p>You can also deploy on your own cloud or data center infrastructure with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI Enterprise</a>. This end-to-end, cloud-native software platform accelerates the development and deployment of production-grade generative AI with enterprise-grade security, stability, manageability, and support. When you are ready to move from experimentation to production, you can leverage NVIDIA AI Enterprise for enterprise-grade runtimes to fine-tune and deploy these models.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Learn more</h2>\n\n\n\n<p>In this post, we learned how NVIDIA AI Foundation models enable enterprise developers to find the right model for various use cases by providing an easy-to-use interface to experience the models and simplified paths to fine-tune and deploy them.&nbsp;</p>\n\n\n\n<p>Explore the different <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\" target=\"_blank\">AI Foundation models</a> available on the NVIDIA NGC Catalog and find the right model for you.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the realm of generative AI, building enterprise-grade large language models (LLMs) requires expertise collecting high-quality data, setting up the accelerated infrastructure, and optimizing the models.&nbsp; Developers can begin with pretrained models and fine-tune them for their use case, saving time and getting their solutions faster to market. Developers need an easy way to try &hellip; <a href=\"https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/\">Continued</a></p>\n", "protected": false}, "author": 1921, "featured_media": 73717, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298016", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/272974", "wpdc_publishing_response": "", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110, 1903], "tags": [453, 3257, 2932, 3270, 3267], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ngc-ai-summit-blog-2973793-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jaw", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73688"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73688"}], "version-history": [{"count": 24, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73688/revisions"}], "predecessor-version": [{"id": 76137, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73688/revisions/76137"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73717"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73688"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73688"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73688"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73312, "date": "2023-11-15T08:00:00", "date_gmt": "2023-11-15T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73312"}, "modified": "2023-12-29T16:41:50", "modified_gmt": "2023-12-30T00:41:50", "slug": "elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/", "title": {"rendered": "Elevate Enterprise Generative AI App Development with NVIDIA AI on Azure Machine Learning"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Generative AI is revolutionizing how organizations across all industries are leveraging data to increase productivity, advance personalized customer engagement, and foster innovation.&nbsp;Given its tremendous value, enterprises are looking for tools and expertise that help them integrate this new technology into their business operations and strategies effectively and reliably.&nbsp;&nbsp;</p>\n\n\n\n<p>NVIDIA and Microsoft are working together to provide enterprises with a comprehensive solution for building, optimizing, and deploying AI applications, including generative AI, using NVIDIA AI on Azure Machine Learning (Azure ML).&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>This week at Microsoft Ignite, NVIDIA and Microsoft announced two additional milestones, bringing new capabilities to Azure ML for managing production AI and developing generative AI applications.&nbsp;</p>\n\n\n\n<ul>\n<li><a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\" target=\"_blank\">NVIDIA NeMo</a>, a framework for building and customizing generative AI models, and <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\" target=\"_blank\">NVIDIA AI Foundation Models</a>, including the new <a href=\"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\">NVIDIA Nemotron-3 8B family</a> of models, are available in the Azure Machine Learning Model Catalog.</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Triton Inference Server</a>, which scales AI in production, is generally available with Azure ML-managed endpoints.</li>\n</ul>\n\n\n\n<p>In June, we published a post explaining the <a href=\"https://developer.nvidia.com/blog/harnessing-the-power-of-nvidia-ai-enterprise-on-azure-machine-learning/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI Enterprise software integration with Azure Machine Learning</a>, and how to get started. This post provides updates to the progress made by the NVIDIA and Azure teams, explains the benefits of the two new integrations, and steps for accessing them.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NeMo Framework integration in Azure Machine Learning Model Catalog</h2>\n\n\n\n<p>LLMs are gaining significant attention due to their ability to perform a variety of tasks, such as text summarization, language translation, and text generation. Open-source and proprietary LLMs available as model weights or APIs are pretrained on a large corpus of data using different generative AI frameworks.&nbsp;</p>\n\n\n\n<p>Custom LLMs, tailored for domain-specific insights using generative AI frameworks, are also finding increased traction in the enterprise domain.&nbsp;&nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/%20%20https://developer.nvidia.com/nemo\" target=\"_blank\" rel=\"noreferrer noopener\">NeMo</a> is an end-to-end, cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. It includes training and inferencing frameworks, guard-railing toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI.</p>\n\n\n\n<p>For a secure, optimized full-stack solution designed to accelerate enterprises with support, security, and API stability, NeMo is available as part of NVIDIA AI Enterprise. Azure ML customers can now customize, optimize, and deploy through the user interface in a no-code flow. Customers can also get support directly from NVIDIA on their generative AI projects, including best practices for performance and accuracy.&nbsp;&nbsp;</p>\n\n\n\n<p>With the availability of the NVIDIA Nemotron-3 8B family of foundational models and NeMo Framework within the Azure Machine Learning Model Catalog, users can now access, customize, and deploy these models out of the box. The framework offers a choice of several customization techniques and is optimized for at-scale inference of models for language and image applications.</p>\n\n\n\n<figure class=\"wp-block-video\"><video controls src=\"https://developer.download.nvidia.com/devblogs/TME-Azure-ML.mov\"></video></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Triton Inference Server Integration in Azure ML-managed endpoint</h2>\n\n\n\n<p>Triton Inference Server is a multi-framework, open-source software that optimizes inference for multiple query types, such as real-time, batch, and streaming. It also supports model ensembles and is included with NVIDIA AI Enterprise. Triton is compatible with various machine learning frameworks, such as TensorFlow, ONNX Runtime, PyTorch, and NVIDIA TensorRT. It can be used for CPU and GPU workloads.&nbsp;</p>\n\n\n\n<p>Triton is available on Azure ML, which provides dynamic batching, concurrent execution, and optimal model configuration. It also offers enterprise-grade security, manageability, and API stability through NVIDIA AI Enterprise within Azure ML.&nbsp;</p>\n\n\n\n<p>Azure ML-managed endpoints make it easy for enterprises to monitor, deploy, and scale AI models, reducing the complexity of setting up and managing your own AI infrastructure.</p>\n\n\n\n<p>The GA release is based on production branches, exclusively available with NVIDIA AI Enterprise. Production branches provide stability and security for applications built on NVIDIA AI, offering nine months of support, API stability, and monthly fixes for software vulnerabilities. Learn more about <a rel=\"noreferrer noopener\" href=\"https://docs.nvidia.com/ai-enterprise/planning-resources-release-branches/latest/release-branches.html\" target=\"_blank\">production branches</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Get started with Triton Inference Server</h3>\n\n\n\n<p>Deploying your model in Triton Inference Server on an Azure ML-managed endpoint is simple. Watch the following video and follow the teps listed for guidance.  </p>\n\n\n\n<figure class=\"wp-block-video\"><video controls src=\"https://developer.download.nvidia.com/devblogs/Triton-Azure.mov\"></video></figure>\n\n\n\n<ol>\n<li>In Azure Machine Learning, go to <strong>Models</strong> and register your model in Triton format. Confirm that the type is <strong>Triton</strong>.</li>\n</ol>\n\n\n\n<ol start=\"2\">\n<li>Under <strong>Endpoints</strong>, choose <strong>Create</strong> to create a real-time online endpoint. Select the <strong>Triton</strong> server for deployment.</li>\n</ol>\n\n\n\n<ol start=\"3\">\n<li>Configure your deployment parameters and choose <strong>Next</strong>. In the <strong>Environment</strong> section, the environment and scoring scripts are preselected.&nbsp;Choose <strong>Next</strong>.</li>\n</ol>\n\n\n\n<ol start=\"4\">\n<li>Confirm the model and environment and choose <strong>Create</strong> to deploy for model inference.</li>\n</ol>\n\n\n\n<ol start=\"5\">\n<li>Review the test page.</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"437\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning.png\" alt=\"Screenshot of a test endpoint page on Azure Machine Learning endpoints.\" class=\"wp-image-73325\" style=\"aspect-ratio:1.4279176201372998;width:624px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning-300x210.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning-164x115.png 164w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning-428x300.png 428w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning-129x90.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning-362x254.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Test-endpoint-on-Azure-Machine-Learning-157x110.png 157w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Test endpoint on Azure Machine Learning</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Get started with NVIDIA on Azure Machine Learning</h2>\n\n\n\n<p>Combining NVIDIA AI Enterprise and Azure Machine Learning creates powerful GPU-accelerated computing and a comprehensive cloud-based machine learning platform, enabling businesses to develop and deploy AI models more efficiently. Enterprises can take advantage of cloud resources and the performance benefits of NVIDIA GPUs and software with this synergy.&nbsp;</p>\n\n\n\n<p>Visit NVIDIA at <a href=\"https://ignite.microsoft.com/en-US/partners/3c7c8276-fad8-4276-8977-5a7d4d383a45?source=partners\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft Ignite</a> to learn more about the latest innovations and collaborations propelling AI and cloud computing into the future.</p>\n\n\n\n<p>Gain insights into groundbreaking NVIDIA solutions by participating in our sponsor sessions or stop by our EMU demo space #311. Check out the<a rel=\"noreferrer noopener\" href=\"https://ignite.microsoft.com/en-US/partners/3c7c8276-fad8-4276-8977-5a7d4d383a45?source=partners\" target=\"_blank\"> NVIDIA showcase page</a> for more information.<br>Ready to get started now? Check out <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\" target=\"_blank\">NVIDIA AI Foundation Models</a> and NeMo Framework in the Azure Machine Learning Model Catalog and NVIDIA Triton Inference Server on <a rel=\"noreferrer noopener\" href=\"https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-with-triton?view=azureml-api-2&amp;tabs=azure-cli%2Cendpoint\" target=\"_blank\">Azure ML endpoints</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI is revolutionizing how organizations across all industries are leveraging data to increase productivity, advance personalized customer engagement, and foster innovation.&nbsp;Given its tremendous value, enterprises are looking for tools and expertise that help them integrate this new technology into their business operations and strategies effectively and reliably.&nbsp;&nbsp; NVIDIA and Microsoft are working together to &hellip; <a href=\"https://developer.nvidia.com/blog/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/\">Continued</a></p>\n", "protected": false}, "author": 777, "featured_media": 73328, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298015", "discourse_permalink": "https://forums.developer.nvidia.com/t/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/272973", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110, 1903], "tags": [453, 2932, 3270, 3267], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/blog-mlog-1920x1080-2778350.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-j4s", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73312"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/777"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73312"}], "version-history": [{"count": 27, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73312/revisions"}], "predecessor-version": [{"id": 76099, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73312/revisions/76099"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73328"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73312"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73312"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73312"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73296, "date": "2023-11-15T08:00:00", "date_gmt": "2023-11-15T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73296"}, "modified": "2023-11-28T15:57:22", "modified_gmt": "2023-11-28T23:57:22", "slug": "nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/", "title": {"rendered": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\" target=\"_blank\" rel=\"noreferrer noopener\">Large language models</a> (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, <a href=\"https://www.nvidia.com/en-us/glossary/data-science/artificial-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\">AI</a>, and <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\" target=\"_blank\" rel=\"noreferrer noopener\">machine learning</a>. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.</p>\n\n\n\n<p>The NVIDIA Nemotron-3 8B family of <a href=\"https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/\" target=\"_blank\" rel=\"noreferrer noopener\">foundation models</a> is a powerful new tool for building production-ready <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\" target=\"_blank\" rel=\"noreferrer noopener\">generative AI</a> applications for the enterprise\u2013fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.</p>\n\n\n\n<p>These new foundation models join <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NeMo</a>, an end-to-end framework for building, customizing, and deploying LLMs tailored for enterprise use. Businesses can now use these tools for developing AI applications quickly, cost-effectively, and on a large scale. These applications can run in the cloud, data center, and on Windows desktop and laptop machines.</p>\n\n\n\n<p>The Nemotron-3 8B family is available in the Azure AI Model Catalog, HuggingFace, and the NVIDIA AI Foundation Model hub on the <a rel=\"noreferrer noopener\" href=\"https://catalog.ngc.nvidia.com/\" target=\"_blank\">NVIDIA NGC </a>Catalog. It includes base, chat, and question-and-answer (Q&amp;A) models that are designed to solve a variety of downstream tasks. Table 1 shows the full family of foundation models.</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td><strong>Model</strong></td><td><strong>Variant</strong></td><td><strong>Key Benefit</strong></td></tr><tr><td><br><strong>Base</strong></td><td>Nemotron-3-8B-Base</td><td>Enables customization, including parameter-efficient fine-tuning and continuous pretraining for domain-adapted LLMs</td></tr><tr><td rowspan=\"3\"><br><strong>Chat</strong></td><td>Nemotron-3-8B-Chat-SFT</td><td>A building block for instruction tuning custom models or user-defined alignment, such as RLHF or SteerLM models</td></tr><tr><td>Nemotron-3-8B-Chat-RLHF</td><td>Best out-of-the-box chat model performance</td></tr><tr><td>Nemotron-3-8B-Chat-SteerLM</td><td>Best out-of-the-box chat model with flexible alignment at inference time</td></tr><tr><td><strong>Question-and- Answer</strong></td><td>Nemotron-3-8B-QA</td><td>Q&amp;A LLMs customized on knowledge bases</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. The Nemotron-3 8B family of foundation models supports a wide range of LLM use cases</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Designing foundation models for production use</h2>\n\n\n\n<p>Foundation models are a powerful building block that reduces the time and resources required to build useful, custom applications. However, organizations must make sure that these models meet their enterprise requirements.&nbsp;</p>\n\n\n\n<p>NVIDIA AI Foundation Models are trained on responsibly sourced datasets, capturing myriad voices and experiences. Rigorous monitoring provides data fidelity and compliance with evolving legal stipulations. Any arising data issues are swiftly addressed, making sure that businesses are armed with AI applications that comply with both legal norms and user privacy. These models can assimilate both publicly available and proprietary datasets.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Nemotron-3-8B base</h2>\n\n\n\n<p>The Nemotron-3-8B base model is a compact, high-performance model for generating human-like text or code. The model has an MMLU 5-shot average of 54.4. The base model also caters to the needs of global enterprises with multilingual capabilities, as it is proficient in 53 languages including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch. This base model is also trained on 37 different coding languages.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Nemotron-3-8B chat</h2>\n\n\n\n<p>Adding to this suite are the Nemotron-3-8B chat models, which target LLM-powered chatbot interactions. There are three chat model versions, each designed for unique user-specific adjustments:&nbsp;</p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\" target=\"_blank\" rel=\"noreferrer noopener\">Supervised fine-tuning (SFT)</a></li>\n\n\n\n<li>Reinforcement learning from human feedback (RLHF)</li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/2023/10/11/customize-ai-models-steerlm/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA SteerLM</a></li>\n</ul>\n\n\n\n<p>The Nemotron-3-8B-SFT model is the first step in instruct-tuning, from which we build our RLHF model that has the highest MT-Bench score within the 8B category, the most cited metric for chat quality. We suggest the user begin with 8B-chat-RLHF for the best immediate chat interaction, but for enterprises interested in uniquely aligning with their end users\u2019 preferences, we recommend the SFT model while applying their own RLHF.</p>\n\n\n\n<p>Finally, the latest alignment method, SteerLM, offers a new level of flexibility for training and customizing LLMs at inference. With SteerLM, users define all the attributes they want and embed them in a single model. Then, they can choose the combination they need for a given use case while the model is running.&nbsp;</p>\n\n\n\n<p>This method enables a continuous improvement cycle. Responses from a custom model can serve as data for a future training run that dials the model into new levels of usefulness.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Nemotron-3-8B question-and-answer</h2>\n\n\n\n<p>The Nemotron-3-8B-QA model is a question-and-answer (QA) model fine-tuned on a large amount of data focused on the target use case.</p>\n\n\n\n<p>The Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the<strong> </strong><a href=\"https://ai.google.com/research/NaturalQuestions/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Natural Questions dataset</strong></a><strong>. </strong>This metric measures how closely the generated answer resembles the truth in \u200cQA.&nbsp;</p>\n\n\n\n<p>The Nemotron-3-8B-QA model has been tested against other state-of-the-art language models that have a larger parameter size. This testing was conducted on datasets created by NVIDIA, as well as on Natural Questions and Doc2Dial datasets. Results show that this model performs well.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Building custom LLMs with NVIDIA NeMo framework</h2>\n\n\n\n<p>NVIDIA NeMo simplifies the path to building customized, enterprise generative AI models by providing end-to-end capabilities and containerized recipes for several model architectures. With the Nemotron-3-8B family of models, developers have access to pretrained models from NVIDIA that can be easily adapted for their specific use cases.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Fast model deployment</h3>\n\n\n\n<p>When using the NeMo framework, there isn\u2019t a need for collecting data or setting up infrastructure. NeMo streamlines the process. Developers can customize existing models and deploy them in production quickly.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Optimal model performance</h3>\n\n\n\n<p>Furthermore, it integrates seamlessly with the NVIDIA TensorRT-LLM open-source library, which optimizes model performance, along with <a href=\"https://developer.nvidia.com/triton-inference-server\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Triton Inference Server</a>, which accelerates the inference serving process. This combination of tools enables cutting-edge accuracy, low latency, and high throughput.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Data privacy and security</h3>\n\n\n\n<p>NeMo enables secure, efficient large-scale deployments that comply with safety and security regulations. For example, if data privacy is a key concern for your business, you can use <a href=\"https://github.com/NVIDIA/NeMo-Guardrails\" data-type=\"link\" data-id=\"https://github.com/NVIDIA/NeMo-Guardrails\" target=\"_blank\" rel=\"noreferrer noopener\">NeMo Guardrails</a> to store customer data securely without compromising on performance or reliability.</p>\n\n\n\n<p>Overall, building custom LLMs with the NeMo framework is an effective way to create enterprise AI applications quickly without sacrificing quality or security standards. It offers developers flexibility in terms of customization while providing the robust tools needed for rapid deployment at scale.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Getting started with Nemotron-3-8B</h2>\n\n\n\n<p>You can easily run inference on the Nemotron-3-8B model with the <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\" target=\"_blank\">NeMo framework</a>, which leverages <a rel=\"noreferrer noopener\" href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main\" target=\"_blank\">TensorRT-LLM</a>, an open source library that provides advanced optimizations for efficient and easy LLM inference on NVIDIA GPUs. It has built-in support for various optimization techniques including:</p>\n\n\n\n<ul>\n<li>KV caching</li>\n\n\n\n<li>Efficient Attention modules (including MQA, GQA, and Paged Attention)</li>\n\n\n\n<li>In-flight (or continuous) batching</li>\n\n\n\n<li>Support for low-precision (INT8/FP8) quantization, among <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main#key-features\" target=\"_blank\" rel=\"noreferrer noopener\">other optimizations</a>.</li>\n</ul>\n\n\n\n<p>The NeMo framework inference container includes all the necessary scripts and dependencies to apply TensorRT-LLM optimizations on NeMo models such as the Nemotron-3-8B family, and host them with Triton Inference Server. With deployment, it exposes an endpoint you can send your inference queries to.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Prerequisites</h3>\n\n\n\n<p>To follow the instructions to deploy and infer, you&#8217;ll need access to:</p>\n\n\n\n<ul>\n<li>NVIDIA Data Center GPUs: at least (1) A100 &#8211; 40 GB / 80 GB, (2) H100 &#8211; 80 GB, or (3)&nbsp; L40S.</li>\n\n\n\n<li>NVIDIA <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/nemo-framework\" target=\"_blank\">NeMo framework</a>: provides you with both the training and inference containers to customize \u200cor deploy the Nemotron-3-8B family of models.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Steps to deploy on Azure ML</h3>\n\n\n\n<p>The models in the Nemotron-3-8B family are available in the Azure ML Model Catalog for deploying in Azure ML-managed endpoints. AzureML provides an easy to use \u2018no-code deployment\u2019 flow hat makes deploying Nemotron-3-8B family models very easy. The underlying plumbing that is the NeMo framework inference container is integrated within the platform.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"352\" height=\"259\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Select-real-time-endpoint-in-Azure-ML.png\" alt=\"Screenshot showing real-time endpoint selection for model deployment within Azure ML.\" class=\"wp-image-73339\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Select-real-time-endpoint-in-Azure-ML.png 352w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Select-real-time-endpoint-in-Azure-ML-300x221.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Select-real-time-endpoint-in-Azure-ML-156x115.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Select-real-time-endpoint-in-Azure-ML-122x90.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Select-real-time-endpoint-in-Azure-ML-149x110.png 149w\" sizes=\"(max-width: 352px) 100vw, 352px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Select real-time endpoint in Azure ML</em></figcaption></figure></div>\n\n\n<p>To deploy the NVIDIA foundation model on Azure ML for inference, follow the following steps:</p>\n\n\n\n<ol>\n<li>Log in to your Azure account: <a href=\"https://portal.azure.com/#home\" target=\"_blank\" rel=\"noreferrer noopener\">https://portal.azure.com/#home</a>&nbsp;</li>\n\n\n\n<li>Navigate to Azure ML Machine Learning Studio&nbsp;</li>\n\n\n\n<li>Select your workspace &amp; navigate to the model catalog</li>\n</ol>\n\n\n\n<p>NVIDIA AI Foundation models are available for fine-tuning, evaluation, and deployment on Azure. Customization of the models can be done within Azure ML using the NeMo training framework. The NeMo framework, consisting of training and inference containers, is already integrated within AzureML.&nbsp;</p>\n\n\n\n<p>To fine-tune the base model, select your favorite model variant, click \u2018fine-tune\u2019, fill in parameters like task type, custom training data, train and validation split, and the compute cluster.</p>\n\n\n\n<p>To deploy the model, select your favorite model variant, click \u2018Real-time endpoint\u2019, select instance, endpoint, and other parameters for customizing deployment. Click deploy to deploy the model for inference to an endpoint.</p>\n\n\n\n<p>Azure CLI and SDK support are also available for running fine-tuning jobs and deployments on Azure ML. For more information, please refer to <a href=\"https://learn.microsoft.com/en-us/azure/machine-learning/concept-foundation-models?view=azureml-api-2\" target=\"_blank\" rel=\"noreferrer noopener\">Foundation Models in Azure ML</a> documentation.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Steps for deploying on-prem or on other clouds</h3>\n\n\n\n<p>The models in the Nemotron-3-8B family have distinct prompt templates for inference requests that are recommended as best practice, however, the instructions to deploy them are similar, as they share the same base architecture.</p>\n\n\n\n<p>For the latest deployment instructions using the NeMo framework inference container, see <a rel=\"noreferrer noopener\" href=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/beta-inf-prerelease/containers/infer\" target=\"_blank\"><a href=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference\">https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference</a></a>.<br>To demonstrate, let\u2019s deploy Nemotron-3-8B-Base-4k.</p>\n\n\n\n<ol>\n<li>Log in to the <a href=\"https://catalog.ngc.nvidia.com/\">NGC Catalog</a>, and fetch the Inference container.</li>\n</ol>\n\n\n\n<pre class=\"wp-block-code\"><code># log in to your NGC organization\ndocker login nvcr.io\n\n# Fetch the NeMo framework inference container\ndocker pull nvcr.io/ea-bignlp/ga-participants/nemofw-inference:23.10</code></pre>\n\n\n\n<ol start=\"2\">\n<li>Download the Nemotron-3-8B-Base-4k model. The 8B family of models is available on <a href=\"https://developer.nvidia.com/nemotron-3-8b\" target=\"_blank\" rel=\"noreferrer noopener\">NGC Catalog</a>, as well as on Hugging Face. You may choose to download the model from either one.</li>\n</ol>\n\n\n\n<p><strong>NVIDIA NGC</strong></p>\n\n\n\n<p>The easiest way to download the model from NGC is to use the CLI. If you don\u2019t have the NGC CLI installed, follow the <a href=\"https://docs.ngc.nvidia.com/cli/cmd.html#getting-started-with-the-ngc-cli\">Getting Started instructions</a> to install and configure it.</p>\n\n\n\n<pre class=\"wp-block-code\"><code># Downloading using CLI. The model path can be obtained from it\u2019s page on NGC\nngc registry model download-version \"dztrnjtldi02/nemotron-3-8b-base-4k:1.0\"</code></pre>\n\n\n\n<p><strong>Hugging Face Hub</strong></p>\n\n\n\n<p>The following command uses git-lfs, but you may use any of the <a href=\"https://huggingface.co/docs/hub/models-downloading\" target=\"_blank\" rel=\"noreferrer noopener\">methods supported</a> by Hugging Face to download models.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>git lfs install\ngit clone https://huggingface.co/nvidia/nemotron-3-8b-base-4k nemotron-3-8b-base-4k_v1.0</code></pre>\n\n\n\n<ol start=\"3\">\n<li>Run the NeMo inference container in interactive mode, mounting the relevant paths</li>\n</ol>\n\n\n\n<pre class=\"wp-block-code\"><code># Create a folder to cache the built TRT engines. This is recommended so they don\u2019t have to be built on every deployment call. \nmkdir -p trt-cache\n\n# Run the container, mounting the checkpoint and the cache directory\ndocker run --rm --net=host \\\n                     --gpus=all \\\n                     -v $(pwd)/nemotron-3-8b-base-4k_v1.0:/opt/checkpoints/  \\\n                     -v $(pwd)/trt-cache:/trt-cache  \\\n                     -w /opt/NeMo \\\n                     -it nvcr.io/ea-bignlp/ga-participants/nemofw-inference:23.10 bash</code></pre>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;4. Convert and Deploy the model on Triton Inference Server with TensorRT-LLM backend.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>python scripts/deploy/deploy_triton.py \\\n                     --nemo_checkpoint /opt/checkpoints/Nemotron-3-8B-Base-4k.nemo \\\n                     --model_type=\"gptnext\" \\\n                     --triton_model_name Nemotron-3-8B-4K \\\n                     --triton_model_repository /trt-cache/8b-base-4k \\\n                     --max_input_len 3000 \\\n                     --max_output_len 1000 \\\n                     --max_batch_size 2</code></pre>\n\n\n\n<p>Note that this script will export the built TensorRT-LLM engine files at the path specified in \u201c&#8211;triton_model_repository\u201d. To load the exported model without rebuilding engines in subsequent deployments, you can skip the \u201c&#8211;nemo_checkpoint\u201d, \u201c&#8211;max_input_len\u201d, \u201c&#8211;max_output_len\u201d, and \u201cmax_batch_size\u201d arguments. More information about using this script can be found in the <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/deployingthenemoframeworkmodel.html#serve-in-framework-or-tensorrt-llm-model-on-triton\" target=\"_blank\" rel=\"noreferrer noopener\">documentation</a>.</p>\n\n\n\n<p>When this command is successfully completed, it exposes an endpoint you can query. Let\u2019s look at how you can do that.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Steps to run inference</h3>\n\n\n\n<p>There are several options available to run inference depending on how you want to integrate the service:</p>\n\n\n\n<ol>\n<li>Using NeMo client APIs available in the <a rel=\"noreferrer noopener\" href=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference\" data-type=\"link\" data-id=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference\" target=\"_blank\">NeMo framework inference container</a></li>\n\n\n\n<li>Using PyTriton to create a client app in your environment</li>\n\n\n\n<li>Using any library/tool that can send an HTTP request, as the deployed service exposes an HTTP endpoint.</li>\n</ol>\n\n\n\n<p>An example of option 1, using NeMo client APIs is as follows. You can use this from the NeMo framework inference container on the same machine or a different machine with access to the service IP and ports.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nemo.deploy import NemoQuery\n\n# In this case, we run inference on the same machine\nnq = NemoQuery(url=\"localhost:8000\", model_name=\"Nemotron-3-8B-4K\")\n\noutput = nq.query_llm(prompts=&#91;\"The meaning of life is\"], max_output_token=200, top_k=1, top_p=0.0, temperature=0.1)\nprint(output)</code></pre>\n\n\n\n<p>Examples of the other options are available in the <a rel=\"noreferrer noopener\" href=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference\" data-type=\"link\" data-id=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-inference\" target=\"_blank\">README</a> of the inference container.</p>\n\n\n\n<p>NOTE: The chat models (SFT, RLHF and SteerLM) require post-processing the output as these three models have been trained to end their response with \u201c&lt;extra_id_1&gt;\u201d, but the `NemoQuery` API does not yet support stopping the generation automatically when this special token is generated. This can be achieved by modifying `output` as follows:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>output = nq.query_llm(...)\noutput = &#91;&#91;s.split(\"&lt;extra_id_1&gt;\", 1)&#91;0].strip() for s in out] for out in output]</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Prompting the 8B family of models&nbsp;</h3>\n\n\n\n<p>The models in the NVIDIA Nemotron-3-8B family share a common pretrained foundation. However, the datasets used to tune the chat (SFT, RLHF, SteerLM), and QA models are customized for their specific purposes. Also, building these models employs different training techniques. Consequently, these models are most effective with tailored prompts that follow a template similar to how they were trained.&nbsp;</p>\n\n\n\n<p>The recommended prompt templates for these models can be found on their respective model cards.</p>\n\n\n\n<p>As an example, here are the single-turn and multi-turn formats applicable to Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF models:</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td colspan=\"2\"><strong>Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF</strong></td></tr><tr><td><strong>Single-turn prompting</strong></td><td><strong>Multi-turn or Few-shot</strong></td></tr><tr><td><code>&lt;extra_id_0&gt;System<br>&nbsp;<br>&lt;extra_id_1&gt;User<br>{prompt}<br>&lt;extra_id_1&gt;Assistant</code></td><td><code>&lt;extra_id_0&gt;System<br>&nbsp;<br>&lt;extra_id_1&gt;User<br>{<strong>prompt 1</strong>}<br>&lt;extra_id_1&gt;Assistant<br>{<strong>response 1</strong>}<br>&lt;extra_id_1&gt;User<br>{<strong>prompt 2</strong>}<br>&lt;extra_id_1&gt;Assistant<br>{<strong>response 2</strong>}<br>...<br>&lt;extra_id_1&gt;User<br>{<strong>prompt N</strong>}<br>&lt;extra_id_1&gt;Assistant</code></td></tr></tbody></table></figure>\n\n\n\n<p>The prompt and response fields correspond to where your input will go. Here&#8217;s an example of formatting your input using the single-turn template.&nbsp;&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>PROMPT_TEMPLATE = \"\"\"&lt;extra_id_0&gt;System\n{system}\n&lt;extra_id_1&gt;User\n{prompt}\n&lt;extra_id_1&gt;Assistant\n\"\"\"\nsystem = \"\"\nprompt = \"Write a poem on NVIDIA in the style of Shakespeare\"\n\nprompt = PROMPT_TEMPLATE.format(prompt=prompt, system=system)\nprint(prompt)</code></pre>\n\n\n\n<p>NOTE: For the Nemotron-3-8B-Chat-SFT and Nemotron-3-8B-Chat-RLHF models, we recommend keeping the system prompt empty.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Further training and customization</h2>\n\n\n\n<p>The NVIDIA Nemotron-3-8B family of models is suitable for further customization for domain-specific datasets. There are several options for this, such as continuing pretraining from checkpoints, SFT or parameter-efficient fine-tuning, alignment on human demonstrations with RLHF, or using the new <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\" target=\"_blank\">SteerLM</a> technique from NVIDIA.</p>\n\n\n\n<p>Easy-to-use scripts for the mentioned techniques exist in the NeMo framework training container. We also provide tools for data curation, identifying optimal hyperparameters for training and inference, and running the NeMo framework on your choice of hardware that is on-prem <a rel=\"noreferrer noopener\" href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/dgxcloud.html\" target=\"_blank\">DGX cloud</a>, <a rel=\"noreferrer noopener\" href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/kubernetes.html\" target=\"_blank\">Kubernetes-enabled</a> platform, or a <a rel=\"noreferrer noopener\" href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/cloudserviceproviders.html\" target=\"_blank\">Cloud Service Provider</a>.&nbsp;</p>\n\n\n\n<p>For more information, check out the <a rel=\"noreferrer noopener\" href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/index.html\" target=\"_blank\">NeMo Framework User Guide</a> or container <a rel=\"noreferrer noopener\" href=\"https://registry.ngc.nvidia.com/orgs/ea-bignlp/containers/nemofw-training\" target=\"_blank\">README</a>.</p>\n\n\n\n<p>The Nemotron-3-8B family of models is designed for diverse use cases, that not only perform competitively on various benchmarks but are also capable of multiple languages.&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/nemotron-3-8b\" data-type=\"link\" data-id=\"https://developer.nvidia.com/nemotron-3-8b\" target=\"_blank\" rel=\"noreferrer noopener\">Take them for a spin</a>, and let us know what you think in the comments.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications. The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise\u2013fostering innovations &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\">Continued</a></p>\n", "protected": false}, "author": 1920, "featured_media": 73659, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1298017", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/272975", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110, 1903], "tags": [453, 2932, 559, 3270], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b.png", "jetpack_shortlink": "https://wp.me/pcCQAL-j4c", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73296"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1920"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73296"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73296/revisions"}], "predecessor-version": [{"id": 74526, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73296/revisions/74526"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73659"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73296"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73296"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73296"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72819, "date": "2023-11-14T09:00:00", "date_gmt": "2023-11-14T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72819"}, "modified": "2023-11-16T11:16:36", "modified_gmt": "2023-11-16T19:16:36", "slug": "accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/", "title": {"rendered": "Accelerating Ptychography Workflows with NVIDIA Holoscan at Diamond Light Source"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.diamond.ac.uk/Home.html;jsessionid=4BAA95953BF40E0E903A2AA86C8A17B7\">Diamond Light Source</a> is a world-renowned synchrotron facility in the UK that provides scientists with access to intense beams of x-rays, infrared, and other forms of light to study materials and biological structures. The facility boasts over 30 experimental stations or beamlines, and is home to some of the most advanced and complex scientific research projects in the world.&nbsp;</p>\n\n\n\n<p>I08-1, a soft x-ray beamline at Diamond Light Source, offers an advanced high-resolution imaging technique called ptychography to provide nanoscale-resolution images. Ptychography uses a computational imaging approach which, from measurements or diffraction patterns resulting from the interaction of an x-ray beam with a sample, can reconstruct an image of the sample to nanometer-scale resolution.&nbsp;</p>\n\n\n\n<p>This is critical for imaging nanometer-scale features in many biological structures, such as mitochondria and organelles in cells, and also the interior structure and defects in material science samples. This process of reconstructing an image is powerful but can result in a significant gap between measuring the data and seeing an image.&nbsp;</p>\n\n\n\n<p>The I08-1 detector operates in the range of 25 image frames per second, but detectors that can operate at the thousands of image frames per second range are on the horizon.&nbsp;Accelerated computing at the edge is needed for these sensor instruments.</p>\n\n\n\n<p>Faster scans enable the study of more dynamic working processes. They increase the throughput of experiments, and live processing offers users real-time feedback to adjust the experiment samples, detector settings, and explore the sample to discover interesting scientific results.&nbsp;</p>\n\n\n\n<p>This post discusses our work with I08-1 to speed up the live processing of beamline experiment data by refactoring the data analysis workflows. It also addresses key challenges, such as the generic ptychography workflow that currently operates in a serial fashion where images are written to disk at a frame rate frequency of 25 Hz.&nbsp;</p>\n\n\n\n<p>The live-processing pipeline is launched after the scan is finished, and the processing application (<a href=\"https://ptycho.github.io/ptypy/\">PtyPy</a>) can work on the complete dataset. The PtyPy application is optimized using GPU acceleration, but I/O communications are a major bottleneck to achieving higher throughputs.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"845\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline.png\" alt=\"Ptychographic pipeline showing sensor, preprocessing, data loading and 2D reconstruction, and Image display pipeline steps with each step handing off to the next through a written image file.  The delay and idle time grows with each step and is shown on the vertical axis.\n\" class=\"wp-image-72843\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-625x264.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-768x325.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-1536x649.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-645x273.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-500x211.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-362x153.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-260x110.png 260w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/serial-ptychographic-pipline-1024x433.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Serial file-based ptychographic pipeline before introduction of NVIDIA Holoscan</em></em></figcaption></figure>\n\n\n\n<p>To accelerate the ptychography workflow, we incorporated <a href=\"https://developer.nvidia.com/holoscan-sdk\">NVIDIA Holoscan</a>, an SDK for sensor processing that makes it easier for scientists, researchers, and developers to optimize and scale sensor processing workflows (Figure 2). For example, the JAX library was used within a Holoscan operator to speed up the image preprocessing.&nbsp;&nbsp;</p>\n\n\n\n<p>Holoscan enables researchers and developers to develop high-performance, low-latency sensor processing applications that can scale more easily using reference examples in familiar languages.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"520\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-625x520.png\" alt=\"The Holoscan SDK diagram shows a block at the top for Holohub containing reference applications and a block for Model Zoo containing NGC and Monai.  The Holoscan SDK block contains blocks for Python, JAX, CuPy, RAPIDS, C++, or Graph Composer.  The Holoscan SDK block also contains operator blocks with re-usable code segments with APIs for IO, AI Inference, Visualization, and Customer functions. The Holoscan software stack sits on top of blocks representing  I/O libraries with DPDK and Rivermax, AI libraries such as TensorRT and Triton, and Visualization libraries such as Vulcan.  NVIDIA Acceleration Libraries is shown as foundation block underlying everything comprising the Holoscan SDK to deliver accelerated computing.  Icons at the bottom of the figure denote it is available to run on appliances, workstations, servers, or the cloud.\n\" class=\"wp-image-72847\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-625x520.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-300x250.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-138x115.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-768x639.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-1536x1279.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-645x537.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-360x300.png 360w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-108x90.png 108w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-362x301.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-132x110.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram-1024x853.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-holoscan-sdk-diagram.png 1744w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. NVIDIA Holoscan SDK is designed for sensor applications, from surgery to satellites</em></em></figcaption></figure>\n\n\n\n<p>The soft x-ray ptychography instrument at I08-1 uses a sCMOS camera for collecting diffraction data for the ptychography experiments. The raw data comes as frames of shape (<code>2048, 2048</code>) and type <code>uint16</code>. Before feeding this data into the iterative ptychography solver application (PtyPy), the following routine preprocessing tasks are performed per raw frame:</p>\n\n\n\n<ul>\n<li>Subtraction of a background dark current image&nbsp;</li>\n\n\n\n<li>Cropping around the center and re-binning to reduce reconstruction times&nbsp;</li>\n</ul>\n\n\n\n<p>While the first task is necessary to obtain clean diffraction images, the other two provide a level of loss-less compression depending on \u200cexperimental circumstances. Ideally, all of these steps are performed as close as possible to the source (on-chip or using FPGAs, for example). Unfortunately, none of these are available options for the particular camera used in this scenario.</p>\n\n\n\n<p><a href=\"https://jax.readthedocs.io/\">JAX</a> was used to significantly speed up a single-threaded Python script performing the tasks listed above, and with minimal changes to the code. Because the original frame processing code was written in NumPy, the JAX JIT was able to fuse the processing routine into a single GPU kernel, which yielded a speedup of more than 2,000x for a single image over the original NumPy version (ignoring the required data transfers from host to device). Even with data transfers, the speedup is more than 40x over the original CPU-based NumPy implementation.</p>\n\n\n\n<p>While acquiring the data can be relatively quick, it can easily take minutes or tens of minutes to reconstruct the data into an image that the investigator or researcher can interpret. This deadtime between a scan and an image is both inefficient and hampers the ability of the investigator to determine if the instrument has been set up properly or if the sample area being scanned is interesting until a scan result is shown.&nbsp;&nbsp;</p>\n\n\n\n<p>By architecting the ptychography application as a collection of application and Holoscan operator fragments, developers can leverage the PtyPy ptychography code, Holoscan AI inference, and Holoscan network operators, to relatively quickly prototype and test a new GPU-accelerated version of the live-processing ptychography application.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"691\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators.png\" alt=\"The figure shows an application made of two fragments, each composed of chained operators that are serially connected. One example shows two operators connected in parallel.\n\" class=\"wp-image-72848\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-300x104.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-625x216.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-768x265.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-1536x531.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-645x223.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-500x173.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-362x125.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-318x110.png 318w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-application-directed-acyclic-graph-operators-1024x354.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Applications in Holoscan are a directed acyclic graph of operators</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"305\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-625x305.png\" alt=\"An operator consists of input ports and output ports and contact re-usable algorithm logic inside.\n\" class=\"wp-image-72851\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-625x305.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-768x375.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-645x315.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-225x110.png 225w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram-1024x500.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/dag-operator-diagram.png 1152w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Operators ingest input data, then process and publish on the output ports</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"418\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-625x418.png\" alt=\"The table of Core Holoscan Operators are listed under the categories of I/O, AI Inference, and Visualization.  I/O operators are V4L2Source, AJASource, EmergentSource, BasicNetworkRx/Tx, and VideoStreamReplayer. AI inference operators are TensorRTInference and MultiAIInference. Visualization operators are Holoviz, OpenGLRenderer, and SegmentationVisualizer.\n\" class=\"wp-image-72852\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-625x418.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-300x201.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-172x115.png 172w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-768x514.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-1536x1028.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-645x432.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-448x300.png 448w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-134x90.png 134w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-362x242.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-164x110.png 164w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators-1024x685.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/holoscan-sdk-applications-core-operators.png 1769w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. Holoscan SDK reference applications and core operators facilitate streaming edge HPC development</em></em></figcaption></figure>\n\n\n\n<p>The next part of the challenge was how to speed up the live-processing frame rate for the beamline I-08 ptychography workflow to cope with the current and future sCMOS frames rates. By overlapping the serial workflow steps and using Holoscan, this beamline should be able to provide live processing that matches the frame rate of the sensor. This would enable beamline users to observe ptychographically reconstructed images of a sample in real time.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"793\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan.png\" alt=\"The ptychography workflow has been redone replacing all of the file-based IO stages between steps with streaming IO.  The steps are Sensor processing with EPICS,  preprocessing which is a Python script, data loading and 2D reconstruction which are embodied in the ptychography software, and Image display which provides the user interface.  The total time to run is much lower that the original file-based workflow and is shown on the vertical axis with smaller and fewer idle time periods.\n\" class=\"wp-image-72854\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-300x119.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-625x248.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-179x71.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-768x305.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-1536x609.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-645x256.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-500x198.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-160x63.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-362x144.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-277x110.png 277w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/socket-based-ptychographic-reconstruction-pipeline-holoscan-1024x406.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. On-the-fly socket-based ptychographic reconstruction pipeline after using Holoscan </em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1000\" height=\"493\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing.png\" alt=\"On the left  are 16 out of 1257 diffraction patterns, which are ptychographically processed to reconstruct a 2D image of a butterfly wing. On the right is an image of a butterfly wing at the one-micrometer level of resolution.\n\" class=\"wp-image-72855\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-300x148.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-625x308.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-768x379.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-645x318.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-500x247.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/2d-reconstruction-butterfly-wing-223x110.png 223w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 7. 2D reconstruction from 1,257 diffraction patterns to create a high-resolution image of a butterfly wing</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-left\" data-align=\"left\"></td><td>Before&nbsp;</td><td>After&nbsp;</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">Data collection</td><td>57 sec</td><td>57 sec&nbsp;</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">Preprocessing</td><td>94 sec</td><td>58 sec</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">PtyPy data loaded</td><td>119 sec</td><td>61 sec</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">PtyPy data reconstructed</td><td>128 sec</td><td>72 sec</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\"><strong>User wait time</strong></td><td><strong>~ 71 sec</strong></td><td><strong>15 sec</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. All times are relative to the start of scan except the user wait time, which is relative to the end of scan</em></figcaption></figure>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1006\" height=\"540\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_.png\" alt=\"e figure shows the before and after times for the ptychography reconstruction pipeline.  The before pipeline does not use Holoscan and has file I/O stage between steps.  The times match the numbers in Table 1 above.  The after pipeline uses Holoscan for streaming network IO between steps and reduced user waiting time to only 15 seconds.\" class=\"wp-image-72868\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_.png 1006w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-300x161.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-625x335.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-768x412.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-645x346.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-500x268.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-362x194.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-file-based-streaming-ptychography-pipelines-holoscan_-205x110.png 205w\" sizes=\"(max-width: 1006px) 100vw, 1006px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 8. Comparison of file-based and streaming ptychography pipelines using Holoscan</em></em></figcaption></figure></div>\n\n\n<p>3D reconstruction creates a larger problem that has the same requirements for live-processing. Scaling up multi-GPU and multi-node processing to provide overlapping, parallel processing of many scans may be a means to keep up with the processing and I/O requirements for live processing.&nbsp;</p>\n\n\n\n<p>Our collaboration aims to test various workflow configurations on a local edge server with two NVIDIA A2 GPUs, where preprocessing runs on one GPU, and image reconstruction runs on the second GPU. This approach enables focusing on bespoke ptychography code while leveraging an edge network I/O operator and AI acceleration libraries that can readily be reused and scaled up to multi-node if necessary for production usage.&nbsp;&nbsp;</p>\n\n\n\n<p>Holoscan enables the creation of an end-to-end data streaming pipeline that enables live ptychographic image processing at the I08-1 beam line, which considerably enriches the overall user interaction.<em> </em>As mentioned, other Diamond Light Source beamlines operate in the kilohertz detection range, but none can perform live processing at that rate.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"623\" height=\"502\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1.png\" alt=\"On the left, the x-ray sensor scans all positions and produces diffractions data.  Ptychography software runs 1 to N times to process scans to construct the sample image.\n\" class=\"wp-image-72874\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1.png 623w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1-300x242.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1-372x300.png 372w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1-362x292.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/tomography-spectroscopy-scans-1-137x110.png 137w\" sizes=\"(max-width: 623px) 100vw, 623px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 9. Tomography, spectroscopy, or other multi-dimensional scans consist of 1 to N runs of conventional ptychography software</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1234\" height=\"506\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy.png\" alt=\"Figure 10 shows the x-ray beam-line instrument on the left. It is shown to generate diffraction patterns and the raw data is shown transmitted to a GPU-accelerated  edge inference server running a trained AI surrogate model with many parts of the algorithm running in parallel to compute phase and amplitude information.  A line shows training data and image outputs are sent to the local supercomputing facility cluster that trains the ptychography reconstruction model. The Edge inference server fine tunes this AI surrogate model using the image estimates.\n\" class=\"wp-image-72861\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy.png 1234w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-300x123.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-625x256.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-179x73.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-768x315.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-645x264.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-500x205.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-362x148.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-268x110.png 268w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/train-ai-models-tomography-spectroscopy-1024x420.png 1024w\" sizes=\"(max-width: 1234px) 100vw, 1234px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 10. Train AI models for tomography, spectroscopy, or other multi-dimensional scans at the supercomputer facility</em></em></figcaption></figure>\n\n\n\n<p>Training AI models on scan data and then using the model to run inference on GPUs at the beamline is a promising method to achieve live processing at kilohertz speeds.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Sensor processing pipelines, such as the ptychography pipeline described in this post, combine significant processing and I/O requirements into a single application. As \u200csensor resolution and refresh rates increase, a file-based approach is no longer tenable, pushing processing redesigns to use a real-time streaming workflow.&nbsp;</p>\n\n\n\n<p>This requires \u200cproper consideration of end-to-end performance, which immediately highlights I/O bottlenecks across the pipeline. The acceleration of the preprocessing and reconstruction operations by NVIDIA GPU edge systems using a combination of JAX, CuPy, and CUDA yields the necessary performance. However, that only amplifies the effect of the I/O bottlenecks, according to our end-to-end analysis.</p>\n\n\n\n<p>Holoscan provides the tools to build streaming processing software pipelines that also leverage the capabilities of the hardware. This includes operators to ingest data directly into the GPU from the network (and the reverse) to better feed the GPUs and increase their utilization. This utilization can be particularly low during pure streaming processing like the preprocessing steps previously highlighted. Because Holoscan is designed to be easily modular, it also enables additional features like the deep learning or live-visualization features discussed.</p>\n\n\n\n<p>By using GPU-accelerated computing with Holoscan, I08-1 can significantly reduce the time it takes to process x-ray microscopy data and accelerate the frame rates for image processing. The edge nodes are equipped with <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">high-performance computing (HPC)</a> hardware, including NVIDIA GPUs installed on CPU servers. These edge appliances or servers are designed to accelerate image processing and machine learning.</p>\n\n\n\n<p>To enable real-time processing, Diamond Light Source has a distributed computing architecture that includes multiple edge nodes and a central data processing facility.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1269\" height=\"394\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows.png\" alt=\"The chart depicts a canonical architecture for edge processing.  On the left starting with data preparation, the data center supercomputer is employed to train the AI model which can be deployed at scale to edge sites next to sensor instruments such as light sheet microscopes, x-ray beam lines, radio telescopes, etc.  The streaming data from the sensor instrument on the right is analyzed in edge servers that run AI at the edge.  The edge server sends data back to the data center to assist with retraining the model.\n\" class=\"wp-image-72862\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows.png 1269w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-300x93.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-625x194.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-179x56.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-768x238.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-645x200.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-500x155.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-160x50.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-362x112.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-354x110.png 354w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/data-center-to-edge-computing-workflows-1024x318.png 1024w\" sizes=\"(max-width: 1269px) 100vw, 1269px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 11. Data center-to-edge computing workflows run AI at the edge</em></em></figcaption></figure>\n\n\n\n<p>The edge nodes are located near the x-ray beamlines and are responsible for processing the data as it is generated. The processed data is then sent to the central data processing facility, where it&#8217;s further analyzed and stored.</p>\n\n\n\n<p>Edge HPC with AI processing on NVIDIA GPUs can accelerate x-ray microscopy data processing in several ways. GPUs are highly efficient at processing large amounts of image data in parallel. In x-ray microscopy, NVIDIA GPUs can be used to accelerate tasks such as noise reduction, image registration, and image segmentation.&nbsp;</p>\n\n\n\n<p>Machine learning algorithms, such as deep learning neural networks, can be used to analyze x-ray microscopy data and extract meaningful information. NVIDIA GPUs are well-suited for accelerating the training and inference stages of these algorithms.</p>\n\n\n\n<p>By using NVIDIA Holoscan in a streaming AI framework for image processing, machine learning, tomographic reconstruction, and data compression, edge AI processing can significantly reduce the time and resources required to process x-ray microscopy data and enable faster scientific breakthroughs.&nbsp;</p>\n\n\n\n<p>With HPC edge processing, Diamond Light Source is taking a significant step towards the democratization of science by providing scientists with the tools they need to make real-time decisions and accelerate research.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Diamond Light Source is a world-renowned synchrotron facility in the UK that provides scientists with access to intense beams of x-rays, infrared, and other forms of light to study materials and biological structures. The facility boasts over 30 experimental stations or beamlines, and is home to some of the most advanced and complex scientific research &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/\">Continued</a></p>\n", "protected": false}, "author": 1279, "featured_media": 72839, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1297191", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/272842", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 2758, 503], "tags": [3284, 453, 2903, 608], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/diamond-light-source.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iWv", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72819"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1279"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72819"}], "version-history": [{"count": 31, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72819/revisions"}], "predecessor-version": [{"id": 73413, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72819/revisions/73413"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72839"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72819"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72819"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72819"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73103, "date": "2023-11-14T08:00:00", "date_gmt": "2023-11-14T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73103"}, "modified": "2023-12-07T11:19:52", "modified_gmt": "2023-12-07T19:19:52", "slug": "energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/", "title": {"rendered": "Energy Efficiency in High-Performance Computing: Balancing Speed and Sustainability"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The world of computing is on the precipice of a seismic shift.&nbsp;</p>\n\n\n\n<p>The demand for computing power, particularly in <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">high-performance computing</a> (HPC), is growing year over year, which in turn means so too is energy consumption. However, the underlying issue is, of course, that energy is a resource with limitations. So, the world is faced with the question of how we can best shift our computational focus from performance to <a href=\"https://www.nvidia.com/en-us/glossary/energy-efficiency/\">energy efficiency</a>.&nbsp;</p>\n\n\n\n<p>When considering this question, it is important to take into account the correlation between task completion rate and energy consumption. This relationship is often overlooked, but it can be an essential factor.&nbsp;&nbsp;</p>\n\n\n\n<p>This post explores the relationship between speed and energy efficiency, and the implications should the shift to completing tasks more quickly \u200coccur.</p>\n\n\n\n<p>Take transportation for example. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"2383\" height=\"1409\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1.png\" alt=\"A horizontal bar chart showing modes of transportation from walking (0) to taking a commercial air flight (1277) showing the energy used per meter transported per person.\" class=\"wp-image-73400\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1.png 2383w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-300x177.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-625x370.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-768x454.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-1536x908.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-2048x1211.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-645x381.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-500x296.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-152x90.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-362x214.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-186x110.png 186w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-1.-Energy-Efficiency-in-Transportation-1-1024x605.png 1024w\" sizes=\"(max-width: 2383px) 100vw, 2383px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Transportation energy-efficiency comparison</em></figcaption></figure></div>\n\n\n<p>In the case of object motion, in anything other than a vacuum, drag is proportional to the square of the speed of travel. Meaning it takes four times the force and energy to go twice as fast for a given distance. Movement of people and goods around our planet means traveling through either air or water (both are \u201cfluids\u201d in physics) and this concept helps explain why going faster requires much more energy.</p>\n\n\n\n<p>Most transportation technologies operate on fossil fuels because, even now, the energy density of fossil fuels and the weight of the engines that run on them are difficult to rival. For example, nuclear energy technology presents challenges including waste, and specialized personnel required to operate safely, and weight, meaning nuclear-powered cars, buses, or airplanes are not happening anytime soon.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"2269\" height=\"1454\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited.png\" alt=\"A line chart of Power compared to Integer speed showing that as speed increases, the power goes up, linearly at first, then exponentially.\" class=\"wp-image-73473\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited.png 2269w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-300x192.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-625x401.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-768x492.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-1536x984.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-2048x1312.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-645x413.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-468x300.png 468w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-362x232.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-172x110.png 172w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-2.-CPU-Power-vs-Benchmark-Score-1-edited-1024x656.png 1024w\" sizes=\"(max-width: 2269px) 100vw, 2269px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. CPU power compared to a normalized benchmark score</em></figcaption></figure></div>\n\n\n<p>The silicon processor has the same kind of relationship with speed. The preceding chart shows that increasing integer processing speed is achieved with an increase in power (energy per unit time). So similar to transportation, the faster the processor goes, the more energy it uses.</p>\n\n\n\n<p>You might be asking, &#8220;<em>Why don\u2019t computers run on tens of kilowatts today? They have been getting faster for 50 years.&#8221; </em></p>\n\n\n\n<p>The answer is that as processors have gotten faster, they have also gotten smaller. Shrinking silicon features make them more efficient from an energy perspective, and therefore at constant power, can run faster at higher clock speeds. Loosely stated, this is known as <a href=\"https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/\">Dennard scaling</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Multi-node parallel computing</h2>\n\n\n\n<p>A pivotal concept in HPC is the effect that parallel computing reduces wall-clock runtime at the expense of total aggregate compute time. For HPC applications containing small portions of serial operations (for the uninitiated, almost all do), the runtime asymptotically approaches the runtime of the serial operations as more computational resources are added.&nbsp;</p>\n\n\n\n<p>This is known as <a href=\"https://www.geeksforgeeks.org/computer-organization-amdahls-law-and-its-proof/\">Amdahl\u2019s law</a>.</p>\n\n\n\n<p>This is important when considering the energy consumed by parallel computing. Assuming a compute unit draws the same amount of power while it performs a computation, means that a computational task will consume more power as it is parallelized among larger and larger sets of compute resources.</p>\n\n\n\n<p>It means that a computational task will consume more power as it is parallelized among larger and larger sets of compute resources. Simultaneously, the task runs for less time. The hypothesis during this investigation is that the reduction in runtime by adding resources does not keep pace with the additional power required by incremental compute resources, and in the end uses more energy.</p>\n\n\n\n<p>Or, as was discussed in the introduction, the faster you want to go, the more energy you will use.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Measuring energy usage&nbsp;</h2>\n\n\n\n<p>Using the Selene system, based on the <a href=\"https://www.nvidia.com/en-us/data-center/dgx-a100/\">NVIDIA DGX A100</a>, engineers can collect various metrics, which are aggregated using <a href=\"https://grafana.com/\">Grafana</a>. With its API, users can query power use over time for individual CPUs, GPUs, and power supply units, either for the whole job or a given time frame.&nbsp;</p>\n\n\n\n<p>Using this data with timestamps from simulation output quantifies the energy used. The reported numbers for energy consumption do not take into account some factors:</p>\n\n\n\n<ul>\n<li>Network switching infrastructure (network cards internal to servers are included)</li>\n\n\n\n<li>Shared filesystem usage for data distribution and results collection</li>\n\n\n\n<li>Conversion from AC to DC at the power supply&nbsp;</li>\n\n\n\n<li>Cooling energy consumption for data center <a href=\"https://dataspan.com/blog/crac-vs-crah-cooling-units-whats-the-difference/\">CRAC</a> units</li>\n</ul>\n\n\n\n<p>The largest of the errors is the item for an air-cooled data center and is closely aligned with the power usage effectiveness (PUE) for the data center (~1.4 for air-cooled and ~1.15 for liquid-cooled). Further, the AC to DC conversion could be approximated by increasing measured energy consumption by 12%. However, this discussion focuses on comparisons of energy, and both of these errors are ignored.</p>\n\n\n\n<p>For the rest of this post, we use Watts (Joules per second) for power and kilowatt-hours (kWh) for energy.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Experiment setup</h3>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-left\" data-align=\"left\" colspan=\"2\" rowspan=\"2\"></td><td colspan=\"3\">Active NVIDIA Infiniband connections per node</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">1</td><td>2</td><td>4</td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\" rowspan=\"3\">GPUs per node</td><td>1</td><td>\u2714</td><td></td><td></td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">2</td><td></td><td>\u2714</td><td></td></tr><tr><td class=\"has-text-align-left\" data-align=\"left\">4</td><td>\u2714</td><td>\u2714</td><td>\u2714</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Execution geometry of simulations</em></figcaption></figure>\n\n\n\n<p>Due to various configurations of GPU-accelerated HPC systems worldwide, requirements, and stages of adoption for accelerated technologies across HPC simulation applications, it is useful to execute parallel simulations using different configurations of compute nodes. This optimizes performance and efficiency further.</p>\n\n\n\n<p>The preceding table shows the five different ways that each application was scaled. The following plots use a consistent visual representation for ease of reading. Single <a href=\"https://www.nvidia.com/en-us/networking/products/infiniband/\">NVIDIA InfiniBand</a> connections use a dotted gray or black line. Dual Infiniband connections use a solid green line, and quadruple Infiniband connections use an orange-dashed line.</p>\n\n\n\n<h2 class=\"wp-block-heading\">HPC application performance and energy</h2>\n\n\n\n<p>This section provides insights into some key HPC applications representing disciplines such as computational fluid dynamics, molecular dynamics, weather simulation, and quantum chromodynamics.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Applications and datasets</h3>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><thead><tr><th><strong>Application</strong></th><th><strong>Version</strong></th><th><strong>Dataset</strong></th></tr></thead><tbody><tr><td>FUN3D</td><td>14.0-d03712b</td><td>WB.C-30M</td></tr><tr><td>GROMACS</td><td>2023</td><td>STMV (h-bond)</td></tr><tr><td>ICON</td><td>2.6.5</td><td>QUBICC 10 km resolution</td></tr><tr><td>LAMMPS</td><td>develop_7ac70ce</td><td>Tersoff (85M atoms)</td></tr><tr><td>MILC</td><td>gauge-action-quda_16a2d47119</td><td>NERSC Large</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. HPC simulation applications used for this study</em></figcaption></figure>\n\n\n\n<h4 class=\"wp-block-heading\">FUN3D</h4>\n\n\n\n<p>FUN3D was first written in the 1980s to study algorithms and develop new methods for unstructured-grid fluid dynamic simulations for incompressible flow up to transonic flow regimes. Over the last 40 years, the project has blossomed into a suite of tools that cover not only analysis, but adjoint-based error estimation, mesh adaptation, and design optimization. It also handles flow regimes up to the hypersonic.</p>\n\n\n\n<p>Currently, a US citizen-only code, research, academia, and industry users all take advantage of FUN3D today. For example, Boeing, Lockheed, Cessna, New Piper, and others have used the tools for applications such as high-lift, cruise performance, and studies of revolutionary concepts.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">GROMACS</h4>\n\n\n\n<p>GROMACS is a molecular dynamics package that uses Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have many complicated bonded interactions. GROMACS is extremely fast at calculating the non-bonded interactions (that usually dominate simulations) and many groups are also using it for research on non-biological systems (for example, polymers).</p>\n\n\n\n<p>GROMACS supports all the usual algorithms from a modern molecular dynamics implementation.</p>\n\n\n\n<h4 class=\"wp-block-heading\">ICON modeling framework</h4>\n\n\n\n<p>ICON modeling framework is a joint project between the Deutscher Wetterdienst (DWD) and the Max-Planck-Institute for Meteorology for developing a unified next-generation global numerical weather prediction (NWP) and climate modeling system. The ICON modeling framework became operational in DWD\u2019s forecast system in January 2015.</p>\n\n\n\n<p>ICON sets itself apart from other NWP tools through better conservation properties, with the obligatory requirement of exact local mass conservation and mass-consistent transport.</p>\n\n\n\n<p>It recognizes that climate is a global issue, and therefore strives for better scalability on future massively parallel HPC architectures.</p>\n\n\n\n<h4 class=\"wp-block-heading\">LAMMPS</h4>\n\n\n\n<p>LAMMPS is a classical molecular dynamics code. As the name implies, it is designed to run well on parallel machines.&nbsp;&nbsp;</p>\n\n\n\n<p>Its focus is on materials modeling. As such, it contains potential models for solid-state materials (metals, semiconductors) soft matter (biomolecules, polymers), and coarse-grained or mesoscopic systems.&nbsp;LAMMPS achieves parallelism using message-passing techniques and a spatial decomposition of the simulation domain. Many of its models have versions that provide accelerated performance on both CPUs and GPUs.</p>\n\n\n\n<h4 class=\"wp-block-heading\">MILC</h4>\n\n\n\n<p><a href=\"https://web.physics.utah.edu/~detar/milc/\">MILC</a> is a collaborative project used and maintained by scientists studying the theory of strong interactions of subatomic physics, otherwise called quantum chromodynamics (QCD). MILC performs simulations of four-dimensional SU(3) lattice gauge theory on MIMD parallel machines. MILC is publicly available for research purposes, and publications of work using MILC or derivatives of this code should acknowledge this use.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Parallel scalability</h4>\n\n\n\n<p>In the following plots, we show a dimensionless quantity <code>SPEEDUP</code>, or strong scaling, as calculated in the standard fashion:</p>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=SPEEDUP+%3D+%5Cfrac%7Bruntime_%7B1%5C+GPU+%5C+and%5C+1%5C+IB%7D%7D%7Bruntime_%7Bn%5C+GPUs%5C+and%5C+m%5C+IB%7D%7D%3D0&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"SPEEDUP = &#92;frac{runtime_{1&#92; GPU &#92; and&#92; 1&#92; IB}}{runtime_{n&#92; GPUs&#92; and&#92; m&#92; IB}}=0\" class=\"latex\" /></p>\n\n\n\n<p>Parallel <code>SPEEDUP</code> is often plotted against an \u201cideal\u201d <code>SPEEDUP</code>, where employing one computational resource gives a <code>SPEEDUP</code> value of one, employing two resource units gives a <code>SPEEDUP</code> value of two, and so on. This is not done here because multiple scaling traces are being shown, and the plots get very crowded with data when each has its own ideal <code>SPEEDUP</code> reference.</p>\n\n\n\n<p>The number of GPUs per node and the number of enabled CX6 EDR Infiniband connections for each node sweep are shown in the legend with {# GPUs / node} &#8211; {# IB / node}.</p>\n\n\n\n<div class=\"wp-block-image aligncenter\"><figure class=\"aligncenter size-full-page-width is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1024\" height=\"744\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-1024x744.png\" alt=\"Left side: FUN3D Speedup showing GPU per node and IB connections per node perform the same starting with one GPU and diverge slightly when scaled out to 128 GPUs with the 2-2 configuration being the most performant, and the max performance point at 64 GPUs.\" class=\"wp-image-73375\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-1024x744.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-300x218.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-625x454.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-158x115.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-768x558.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-1536x1115.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-2048x1487.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-645x468.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-413x300.png 413w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-362x263.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-3.-FUN3D-Speedup-151x110.png 151w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. FUN3D Speedup with FUN3D plotted relative to four GPUs each with one active GPU and InfiniBand connection&nbsp;</em></figcaption></figure></div>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"438\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-625x438.png\" alt=\"GROMACS speedup showing the 4-1 configuration being a poor choice, and both the 2-2 and 4-4 configurations scaling very well.\n\" class=\"wp-image-73507\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-625x438.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-300x210.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-164x115.png 164w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-768x539.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-1536x1077.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-2048x1436.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-645x452.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-428x300.png 428w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-362x254.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-157x110.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig4.-GROMACS-Speedup-1024x718.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. GROMACS speedup plotted relative to one GPU with and a single node with one InfiniBand connection</em></figcaption></figure>\n\n\n\n<div class=\"wp-block-image aligncenter\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"385\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-625x385.png\" alt=\"All ICON configurations scaling from 64, almost identical except 4-1  is 10% worse.\" class=\"wp-image-73475\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-625x385.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-300x185.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-768x473.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-1536x947.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-2048x1262.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-645x398.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-487x300.png 487w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-362x223.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-5.-ICON-Speedup-edited-1024x631.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. ICON <em>Speedup</em> with ICON plotted relative to 64 GPUs with one GPU and one active InfiniBand connection</em></figcaption></figure></div>\n\n\n\n<div class=\"wp-block-image aligncenter\"><figure class=\"aligncenter size-full-page-width is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"2719\" height=\"1686\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited.png\" alt=\"All LAMMPS configurations scaling almost identically except 4-1 is 20% worse.\" class=\"wp-image-73477\" style=\"aspect-ratio:1.5732984293193717;object-fit:contain;width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited.png 2719w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-625x388.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-768x476.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-1536x952.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-2048x1270.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-645x400.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-484x300.png 484w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-177x110.png 177w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure-6.-LAMMPS-Speedup-edited-1024x635.png 1024w\" sizes=\"(max-width: 2719px) 100vw, 2719px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. LAMMPS <em><em><em>Speedup</em></em></em> with LAMMPS plotted relative to eight GPUs with one GPU and one active InfiniBand connection</em></figcaption></figure></div>\n\n\n\n<div class=\"wp-block-image aligncenter\"><figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"438\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-625x438.png\" alt=\"MILC speedup scaled from 32 to 256 GPUs where 4-2 and 4-4 configurations scale the best and reach the highest performance.\" class=\"wp-image-73509\" style=\"aspect-ratio:1.3601036269430051;object-fit:contain;width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-625x438.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-300x210.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-164x115.png 164w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-768x538.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-1536x1077.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-2048x1436.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-645x452.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-428x300.png 428w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-362x254.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-157x110.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Fig7.-MILC-Speedup-1024x718.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. MILC Speedup plotted relative to 32 nodes with one GPU and one active InfiniBand connection</em></figcaption></figure></div>\n\n\n\n<p>Figures 3 through 7 show the scalability of each of the simulations. Generally, most of the data follows the trend expected based on Amdahl\u2019s law. A notable exception is the GROMACS configuration using four GPUs and one InfiniBand connection per node (labeled 4-1). This configuration exhibits negative scaling at first, meaning it slows down as resources are added before it starts to scale.&nbsp;</p>\n\n\n\n<p>Given the network configuration, it seems that \u200cscaling is bound by the bandwidth of the network and the data each parallel thread is trying to communicate. As the number of resources grows between 16 and 32 GPUs, the task reaches a point where it is no longer bandwidth-bound and scales to a small degree.</p>\n\n\n\n<p>Similar outlier behavior for the 4-1 configuration is seen for ICON and LAMMPS, though both scale much more readily than GROMACS.</p>\n\n\n\n<p>It is also interesting to note that the 4-4 configuration is not always the best for performance. For FUN3D (2-2) and a portion of the MILC (4-2), scaling other configurations shows superior performance by small margins. However, in all cases, the 4-4 configuration is among the best configurations.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Energy Usage</h4>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"556\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-625x556.png\" alt=\"ICON uses the most energy, ranging from 17\u201337 kWh, linearly growing as it scales. MILC's energy consumption has the same shape but ranges between 10 and 30 kWh. FUN3D, LAMMPS, and GROMACS all use less than 5 kWh, and slope up to the right, with LAMMPS being the flattest, ranging between 2.5 and 4 kWh on 16-256 GPUs.\" class=\"wp-image-73523\" style=\"object-fit:cover\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-625x556.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-300x267.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-129x115.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-768x683.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-1536x1367.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-2048x1823.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-645x574.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-337x300.png 337w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-101x90.png 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-362x322.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-124x110.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/HPC-Energy-Consumption-App-1024x911.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. Energy usage for HPC applications running with four GPUs and four Infiniband connections per node</em></figcaption></figure></div>\n\n\n<p>For brevity, Figure 8 shows only the four GPU and four IB connections (4-4) energy consumption in a single plot. The amount of energy used by each simulation is arbitrary and can be adjusted up or down by changing the problem size or analysis type. There are two relevant characteristics to note:</p>\n\n\n\n<ul>\n<li>The slope for each application is positive, meaning that they use more energy as they scale.</li>\n\n\n\n<li>Some applications like FUN3D increase used energy quickly, while others like LAMMPS increase more gradually.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Time to solution compared to energy to solution</h2>\n\n\n\n<p>Some may conclude from Figure 8 that the best configuration to run these simulations is by minimizing the number of resources used per simulation. While the conclusion from an energy standpoint is correct, there are more objectives to consider beyond energy.</p>\n\n\n\n<p>For example, in the case of a researcher working against a project deadline, time to solution is crucially important. Or, in the case of a commercial enterprise, final data is often required to start the manufacturing process and get a product ready for release to market. Here, too, the time to solution value may outweigh the extra energy required to produce the simulation output sooner.</p>\n\n\n\n<p>Therefore, this is a multi-objective optimization problem with several solutions depending on the weights of each defined objective.</p>\n\n\n\n<h3 class=\"wp-block-heading\">The ideal case</h3>\n\n\n\n<p>Before exploring the objectives, consider the following ideal case:</p>\n\n\n\n<p>A perfectly parallel (meaning no serialized operations) accelerated HPC application, which scales linearly as additional processors are added. Graphically, the<em><em> </em></em><code>SPEEDUP</code> curve is simply a line starting from (1,1) and going up and to the right at a slope of 1.0.</p>\n\n\n\n<p>For such an application, now consider energy. If every processor added consumes the same amount of power, then power consumption during the run is the following equation:</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Power+%3D+n+%5Ctimes+P_1&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"Power = n &#92;times P_1\" class=\"latex\" /></p>\n\n\n\n<p>In this case, <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"n\" class=\"latex\" /> is the number of processors used and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=P_1&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"P_1\" class=\"latex\" /> is the power used by one GPU.</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Energy+%3D+Power+%5Ctimes+time&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"Energy = Power &#92;times time\" class=\"latex\" /> </p>\n\n\n\n<p>But time is an inverse function of <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"n\" class=\"latex\" />, so you can rewrite it as follows: </p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Energy+%3D+P_1+%5Ctimes+time_1&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"Energy = P_1 &#92;times time_1\" class=\"latex\" />  </p>\n\n\n\n<p class=\"has-text-align-left\">The <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"n\" class=\"latex\" /> values cancel, and we see that <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Energy&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"Energy\" class=\"latex\" />, in the ideal case, is not a function of the number of GPUs and is in fact constant.</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=Energy%3D+n+%5Ctimes+P_1+%5Ctimes+%5Cfrac%7Btime_1%7D%7Bn%7D&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"Energy= n &#92;times P_1 &#92;times &#92;frac{time_1}{n}\" class=\"latex\" /></p>\n\n\n\n<p>In several cases, HPC applications are not ideal in either parallel speedup or energy. Because speedup is a case of diminishing returns as resources are added, and energy grows roughly linearly as resources are added, there should be a number of GPUs where the ratio of speedup to energy is a maximum.&nbsp;</p>\n\n\n\n<p>More formally, assuming equal weights for energy and time:</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=SPEEDUP_%7BEnergy%7D+%3D+%5Cfrac%7BSPEEDUP%7D%7BEnergyRatio%7D+%3D+%5Cfrac%7B%5Cleft+%5B+%5Cfrac%7Bruntime_%7B1%5C+GPU+%5C+and%5C+1%5C+IB%7D%7D%7Bruntime_%7Bn%5C+GPUs%5C+and%5C+m%5C+IB%7D%7D+%5Cright+%5D%7D%7B%5Cleft+%5B+%5Cfrac%7Benergy_%7Bn%5C+GPUs%5C+and%5C+m%5C+IB%7D%7D%7Benergy_%7B1%5C+GPUs%5C+and%5C+1%5C+IB%7D%7D+%5Cright+%5D%7D%5C%5C+%3D+%5Cleft+%5B+%5Cfrac%7Bruntime_%7B1%5C+GPU+%5C+and%5C+1%5C+IB%7D%7D%7Bruntime_%7Bn%5C+GPUs%5C+and%5C+m%5C+IB%7D%7D+%5Cright+%5D+%5Ctimes+%5Cleft+%5B+%5Cfrac%7Benergy_%7Bn%5C+GPUs%5C+and%5C+m%5C+IB%7D%7D%7Benergy_%7B1%5C+GPUs%5C+and%5C+1%5C+IB%7D%7D+%5Cright+%5D&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"SPEEDUP_{Energy} = &#92;frac{SPEEDUP}{EnergyRatio} = &#92;frac{&#92;left [ &#92;frac{runtime_{1&#92; GPU &#92; and&#92; 1&#92; IB}}{runtime_{n&#92; GPUs&#92; and&#92; m&#92; IB}} &#92;right ]}{&#92;left [ &#92;frac{energy_{n&#92; GPUs&#92; and&#92; m&#92; IB}}{energy_{1&#92; GPUs&#92; and&#92; 1&#92; IB}} &#92;right ]}&#92;&#92; = &#92;left [ &#92;frac{runtime_{1&#92; GPU &#92; and&#92; 1&#92; IB}}{runtime_{n&#92; GPUs&#92; and&#92; m&#92; IB}} &#92;right ] &#92;times &#92;left [ &#92;frac{energy_{n&#92; GPUs&#92; and&#92; m&#92; IB}}{energy_{1&#92; GPUs&#92; and&#92; 1&#92; IB}} &#92;right ]\" class=\"latex\" /></p>\n\n\n\n<p>In this formula, the following is constant:</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cleft+%5B+runtime_%7B1%5C+GPU+%5C+and%5C+1%5C+IB%7D+%5Ctimes+energy_%7B1%5C+GPU+%5C+and%5C+1%5C+IB%7D%5Cright+%5D&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"&#92;left [ runtime_{1&#92; GPU &#92; and&#92; 1&#92; IB} &#92;times energy_{1&#92; GPU &#92; and&#92; 1&#92; IB}&#92;right ]\" class=\"latex\" /></p>\n\n\n\n<p>You should maximize the following: </p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Cleft+%5B+runtime_%7Bn%5C+GPU+%5C+and%5C+m%5C+IB%7D+%5Ctimes+energy_%7Bn%5C+GPU+%5C+and%5C+m%5C+IB%7D%5Cright+%5D%7D&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"&#92;frac{1}{&#92;left [ runtime_{n&#92; GPU &#92; and&#92; m&#92; IB} &#92;times energy_{n&#92; GPU &#92; and&#92; m&#92; IB}&#92;right ]}\" class=\"latex\" /></p>\n\n\n\n<p>Or, you should minimize the following:&nbsp;</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=runtime_%7Bn%5C+GPU+%5C+and%5C+m%5C+IB%7D+%5Ctimes+energy_%7Bn%5C+GPU+%5C+and%5C+m%5C+IB%7D&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"runtime_{n&#92; GPU &#92; and&#92; m&#92; IB} &#92;times energy_{n&#92; GPU &#92; and&#92; m&#92; IB}\" class=\"latex\" /></p>\n\n\n\n<p>If one plots runtime on one axis and energy on the opposite axis, then the quantity to minimize is simply the area defined by runtime multiplied by energy. </p>\n\n\n\n<p>Defining the problem this way also enables the exploration of multiple independent variables, including but not limited to the number of resources used in a parallel run, GPU clock frequency, network latency, bandwidth, and any other variable that influences runtime and energy usage.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Balanced speed and energy</h3>\n\n\n\n<p>The following plots are the product of runtime and energy consumption, where the independent variable is the number of GPUs in the parallel run. These plots provide one method to determine one solution among the Pareto front of solutions for optimization of runtime and energy.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"393\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-625x393.png\" alt=\"A curved plot showing decreasing then increasing with a minimum at 16 GPUs. The 4-2 and 4-4 configurations are best {right side}.\" class=\"wp-image-73528\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-625x393.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-300x189.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-768x483.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-1536x967.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-2048x1289.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-645x406.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-477x300.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-362x228.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-175x110.png 175w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/FUN3D-GPUs-Node-1024x645.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. Runtime * Energy for FUN3D showing a minimum of 16-32 GPUs&nbsp;</em><br></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"374\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-625x374.png\" alt=\"A less obvious optimum for GROMACS with 2-2 being almost flat between four and 32 GPUs and 4-2 and 4-4 configurations best with the peak at eight GPUs.\" class=\"wp-image-73546\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-625x374.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-300x179.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-768x459.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-1536x919.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-2048x1225.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-645x386.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-362x217.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-184x110.png 184w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPus-GROMACS-1024x613.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 10. Runtime * Energy for GROMACS</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"394\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-625x394.png\" alt=\"ICON Energy * Runtime decreases with GPUs in all cases, slope reaching very close to zero, so 512 GPUs may be optimal. 4-4 and 4-2 configurations are best.\" class=\"wp-image-73531\" style=\"aspect-ratio:1.5768115942028986;width:600px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-625x394.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-300x189.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-768x484.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-1536x969.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-2048x1291.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-645x407.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-476x300.png 476w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-362x228.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ICON-GPUs-Node-1024x646.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 11. Runtime * Energy for ICON&nbsp;</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"395\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-625x395.png\" alt=\"LAMMPS decreases Runtime * Energy throughout the 8-256 GPU range. 4-4 and 4-2 configurations are best.\" class=\"wp-image-73535\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-625x395.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-768x485.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-1536x970.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-2048x1294.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-645x407.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-475x300.png 475w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LAMMPS-GPUs-nodepng-1024x647.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 12. Runtime * Energy for LAMMPS</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"403\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-625x403.png\" alt=\"MILC shows a clear minimum at 32 GPUs using 4-4 configuration.\" class=\"wp-image-73548\" style=\"width:600px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-768x496.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-1536x991.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-2048x1321.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-465x300.png 465w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/GPUs-MILC-Node-1024x661.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 13. Runtime * Energy for MILC</em></figcaption></figure></div>\n\n\n<p>The preceding figures show changes in Runtime * Energy as each of the five applications and five GPU/network configurations are scaled with GPU count. Different from the scalability plots of performance, we see that in all cases the 4-4 configuration (reminder, \u201c4-4\u201d refers to {# GPUs / node} &#8211; {# IB / node}) is the best with the 4-2 configuration occasionally a close second. Aligned with the scalability plots, the 1-1 configuration is consistently the worst configuration, likely because of overhead on each server being mostly idle (that is each node had eight A100 GPUs, and eight CX6 Infiniband adaptors).&nbsp;</p>\n\n\n\n<p>Perhaps more interesting is the fact that both LAMMPS and ICON show they have not reached a minimum in Runtime * Energy for the data we collected. Larger runs should be performed for LAMMPS and ICON to show minima.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1468\" height=\"1278\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency.png\" alt=\"A graph showing energy performance and energy drop.\" class=\"wp-image-75016\" style=\"aspect-ratio:1.1486697965571204;width:536px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency.png 1468w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-300x261.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-625x544.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-132x115.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-768x669.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-645x562.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-345x300.png 345w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-103x90.png 103w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-362x315.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-126x110.png 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Cost_Benefit_Optimization_HPC_Energy_Efficiency-1024x891.png 1024w\" sizes=\"(max-width: 1468px) 100vw, 1468px\" /><figcaption class=\"wp-element-caption\"><em>Figure 14. Cost and benefits of optimizing time to solution and energy to solution compared to maximum performance</em></figcaption></figure>\n\n\n\n<p>The desired outcome of the optimization is to run simulations at increased efficiency. Comparing the performance drop to the energy drop per simulation (normalized by the maximum performance point) is a simple way to check that the optimization output achieved efficiency increase. If the performance drop is less than the energy drop, the efficiency is higher, which is seen for FUN3D, GROMACS, and MILC in Figure 14.</p>\n\n\n\n<p>There is an opportunity for saving energy and the performance cost compared to the maximum performance point. This is shown in Figure 14. ICON and LAMMPS were not plotted because they did not display a minimum point in the optimization. The desired outcome is that performance drops less than the energy used per simulation, and that is precisely what is seen for FUN3D, GROMACS, and MILC.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Navigating the intersection of multi-node scalability and energy&nbsp;</h2>\n\n\n\n<p>Data center growth and the expanding exploration and use of AI will drive increased demand for data centers, data center space, cooling, and electrical energy. Given the fraction of electricity generated from fossil fuels, greenhouse gasses driven by data center energy demands are a problem that must be managed.&nbsp;</p>\n\n\n\n<p>NVIDIA continues to focus on making internal changes and influencing the design of future products, maximizing the positive impact of the AI revolution on society. By using the NVIDIA accelerated computing platform, researchers get more science done per unit of time and <a href=\"https://resources.nvidia.com/en-us-energy-efficiency/supercomputing\">use less energy for each scientific result</a>. Such energy savings can be equated to a proportional amount of carbon dioxide emissions avoided, which benefits everyone on the planet.&nbsp;</p>\n\n\n\n<p>Read the recent post, <a href=\"https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/\">Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO</a> for a deeper dive focused on VASP and energy efficiency.</p>\n\n\n\n<p>This short discussion and set of results are offered to jump-start a conversation about shifting the way HPC centers allocate and track resources provided to the research community. It may also influence users to consider the impact of their choices when running large parallel simulations, and the downstream effects.&nbsp;</p>\n\n\n\n<p>It is time to start a conversation regarding energy to solution for HPC and AI simulations and the multi-objective optimization balance with time to solution discussed here. Someday, the HPC community may measure scientific advancements per megawatt-hour, or perhaps even per mTon CO2eq, but that won\u2019t be achieved until conversations begin.</p>\n\n\n\n<p>For more information, visit <a href=\"https://www.nvidia.com/en-us/data-center/sustainable-computing/\">NVIDIA sustainable computing</a>. Read about <a href=\"https://blogs.nvidia.com/blog/2023/05/21/gpu-energy-efficiency-nersc/\">NERSC</a> and their impressions of the accelerated platform. Or, listen to <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52087/\">Alan Gray\u2019s NVIDIA 2023 GTC session</a> about tuning the accelerated platform for maximum efficiency.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The world of computing is on the precipice of a seismic shift.&nbsp; The demand for computing power, particularly in high-performance computing (HPC), is growing year over year, which in turn means so too is energy consumption. However, the underlying issue is, of course, that energy is a resource with limitations. So, the world is faced &hellip; <a href=\"https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/\">Continued</a></p>\n", "protected": false}, "author": 1442, "featured_media": 73260, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1297162", "discourse_permalink": "https://forums.developer.nvidia.com/t/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/272832", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [1916, 1937, 453, 608], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Energy-efficiency-HPC.png", "jetpack_shortlink": "https://wp.me/pcCQAL-j15", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73103"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1442"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73103"}], "version-history": [{"count": 144, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73103/revisions"}], "predecessor-version": [{"id": 75328, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73103/revisions/75328"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73260"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73103"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73103"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73103"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73443, "date": "2023-11-13T11:14:42", "date_gmt": "2023-11-13T19:14:42", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73443"}, "modified": "2023-11-16T11:16:37", "modified_gmt": "2023-11-16T19:16:37", "slug": "upcoming-webinar-series-how-to-get-started-with-ai-inference", "status": "publish", "type": "post", "link": "https://nvda.ws/3QEAyiQ", "title": {"rendered": "Upcoming Webinar Series: How to Get Started With AI Inference"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Join us for a series of expert-led talks where we&#8217;ll explore a full-stack approach to AI inference and how to optimize the AI-inferencing workflow to lower cloud expenses and boost user adoption.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Join us for a series of expert-led talks where we&#8217;ll explore a full-stack approach to AI inference and how to optimize the AI-inferencing workflow to lower cloud expenses and boost user adoption.</p>\n", "protected": false}, "author": 338, "featured_media": 73444, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3QEAyiQ", "_links_to_target": "_blank"}, "categories": [1050, 852, 3110], "tags": [296, 453, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/inference-tech-blog-webinar-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-j6z", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73443"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/338"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73443"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73443/revisions"}], "predecessor-version": [{"id": 73446, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73443/revisions/73446"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73444"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73443"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73443"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73443"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72365, "date": "2023-11-13T09:30:00", "date_gmt": "2023-11-13T17:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72365"}, "modified": "2023-11-16T11:16:38", "modified_gmt": "2023-11-16T19:16:38", "slug": "nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/", "title": {"rendered": "NVIDIA Deep Learning Institute Launches Science and Engineering Teaching Kit"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>AI is quickly becoming an integral part of diverse industries, from transportation and healthcare to manufacturing and finance. AI powers chatbots, recommender systems, computer vision applications, fraud prevention, and autonomous vehicles. It also has broad applications in engineering and science.&nbsp;</p>\n\n\n\n<p>Physics-informed machine learning (physics-ML) leverages knowledge of the physical world to train AI models. It is well suited for modeling real-world systems &#8212; some applications include: predicting&nbsp;<a rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=FUUT6IrQjo4\" target=\"_blank\">extreme weather</a>, data center cooling, turbulent flow over a car, and protein modeling.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/surrogate-earth-model-gif.gif\" alt=\"A gif showing spinning surrogate model of the Earth with bright colors indicates extreme weather events across the planet\u2019s surface.\n\" class=\"wp-image-72387\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Physics-ML is used to model physical systems such as weather, ocean currents, and tidal activity</em></figcaption></figure>\n\n\n\n<p>Academic institutions play a pivotal role in nurturing emerging technologies and driving the innovation needed for their widespread adoption. There is no denying that today&#8217;s students who want to succeed in tomorrow\u2019s workplace need to understand how AI can enable solutions.</p>\n\n\n\n<p>To support this work, NVIDIA is collaborating with pioneers at the intersection of science, engineering, and AI to create the first Deep Learning for Science and Engineering Teaching Kit for educators in academia worldwide.&nbsp;</p>\n\n\n\n<p>This new teaching kit will enable the next generation of engineers and scientists to leverage AI for innovation in the field of engineering and science. It was created with leading academics including George Karniadakis, professor of Applied Mathematics and Engineering at Brown University, and his team.&nbsp;</p>\n\n\n\n<p>&#8220;We designed this course with my collaborator, Dr. Raj Shukla, to address the urgent need&nbsp;for specific material for scientists and engineers,\u201d said Karniadakis. \u201cWe focused on regression and mimicking the approximation theory and algorithms required in classical numerical analysis courses in the engineering curriculum.&#8221;&nbsp;</p>\n\n\n\n<p>Educators can get full, free access to the Deep Learning for Science and Engineering Teaching Kit (and many more) by joining the <a href=\"https://developer.nvidia.com/teaching-kits\">NVIDIA DLI Teaching Kit Program</a>. Kits include lecture materials, labs, and sample problem sets to facilitate incorporating advanced technology into course curriculums. The entire lecture portion of the course is also available on <a href=\"https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/\">NVIDIA On-Demand</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Overview of the Science and Engineering Teaching Kit&nbsp;</h2>\n\n\n\n<p>Educators in various fields will find this teaching kit useful, from mechanical, structural, and electrical engineering to atmospheric science, computational science, and more. Materials in the kit include the fundamentals of deep learning as well as advanced topics and hands-on exercises. It contains 15 lectures totaling about 30-35 hours, homework, and 20 projects across different fields.</p>\n\n\n\n<p>The Deep Learning for Science and Engineering Teaching Kit contains focused modules that combine theory, algorithms, programming, and examples. Highlights include:</p>\n\n\n\n<ul>\n<li>A primer on Python plus scientific and deep learning libraries</li>\n\n\n\n<li>Deep neural network architectures, training, and optimization</li>\n\n\n\n<li>Physics-informed neural networks</li>\n\n\n\n<li>Neural operators&nbsp;</li>\n\n\n\n<li>Data and uncertainty quantification</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\" target=\"_blank\" rel=\"noreferrer noopener\">High-performance computing (HPC)</a> and the <a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a> open-source framework</li>\n</ul>\n\n\n\n<p>This content is ideal for educators in engineering and science. The modular design enables instructors to develop their own custom version of the course to suit the needs of their students.&nbsp;</p>\n\n\n\n<p>The teaching kit includes dedicated modules for physics-ML, due to its potential to transform HPC simulation workflows across disciplines. These include multi-disciplinary physics including computational fluid dynamics, structural mechanics, and computational chemistry. Because of its broad applicability across science and engineering domains, physics-ML is well suited for modeling real-world multiphysics systems.&nbsp;</p>\n\n\n\n<p>AI surrogate models can help develop a wide range of solutions including weather forecasting, <a href=\"https://developer.nvidia.com/blog/reducing-power-plant-greenhouse-gasses-using-ai-and-digital-twins/\">reducing power plant greenhouse gasses</a>, and <a href=\"https://blogs.nvidia.com/blog/2022/03/22/siemens-gamesa-wind-farms-digital-twins/\">accelerating clean energy transitions</a>. Such physically consistent surrogate models can underpin the deployment of large-scale digital twins of real-world systems.</p>\n\n\n\n<p>\u201cTo this end,\u201d Karniadakis said, \u201cthe focus of the course is how to solve forward and inverse problems given sparse and noisy data, how to discover governing physical laws, how to construct proper surrogate models for digital twins, and how to quantify uncertainties associated with models and data.&#8221;&nbsp;</p>\n\n\n\n<p>The kit leverages the open-source <a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a> framework with hands-on tutorials for project-based learning. Modulus enables engineering and scientific communities that may not have AI or programming expertise. With a Python-based interface, Modulus provides the right tools to combine the governing partial differential equations and other attributes of the problem like the physical geometry and boundary conditions with the training dataset in a simple way.&nbsp;</p>\n\n\n\n<p>It also provides a variety of reference applications as starting points, across domains (computational fluid dynamics, structures, thermal) applied to problems in different segments from manufacturing to healthcare. To learn more, see <a href=\"https://developer.nvidia.com/blog/physics-ml-platform-modulus-is-now-open-source/\">Physics-Informed Machine Learning Platform NVIDIA Modulus Is Now Open Source</a>.</p>\n\n\n\n<p>Given the rapid pace of change in the field of AI, educators can anticipate that the teaching material will be updated, as needed. NVIDIA is committed to providing the best educational materials, and feedback is welcome.&nbsp;</p>\n\n\n\n<p>&#8220;The NVIDIA Teaching Kit on physics-ML has provided me with great resources for use in my machine learning course targeted for our engineering students,\u201d said Hadi Meidani, associate professor of Civil &amp; Environmental Engineering at University of Illinois Urbana-Champaign. \u201cThe examples and code greatly enable hands-on learning experiences on how machine learning is applied to scientific and engineering problems.&#8221;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started&nbsp;</h2>\n\n\n\n<p>Educators can get full, free access to the Deep Learning for Science and Engineering Teaching Kit (and many more) by joining the <a href=\"https://developer.nvidia.com/teaching-kits\">NVIDIA DLI Teaching Kit Program</a>. The entire lecture part of the course is also openly available through <a href=\"https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/\">NVIDIA On-Demand</a>.</p>\n\n\n\n<p>For access to tutorial content, <a href=\"mailto:modulus-team@nvidia.com\">reach out to the Modulus team</a>. To get started with NVIDIA Modulus hands-on experience, see <a href=\"https://courses.nvidia.com/courses/course-v1:DLI+S-OV-04+V1/\">Introduction to Physics-Informed Machine Learning with Modulus</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI is quickly becoming an integral part of diverse industries, from transportation and healthcare to manufacturing and finance. AI powers chatbots, recommender systems, computer vision applications, fraud prevention, and autonomous vehicles. It also has broad applications in engineering and science.&nbsp; Physics-informed machine learning (physics-ML) leverages knowledge of the physical world to train AI models. It &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/\">Continued</a></p>\n", "protected": false}, "author": 909, "featured_media": 72371, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1296459", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/272711", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [1949, 1913, 2375, 2964, 2614, 1935, 453, 2216, 1958, 3281, 61], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-deep-learning-institute-teaching-kit-logo.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iPb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72365"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/909"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72365"}], "version-history": [{"count": 40, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72365/revisions"}], "predecessor-version": [{"id": 73577, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72365/revisions/73577"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72371"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72365"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72365"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72365"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72720, "date": "2023-11-13T09:13:02", "date_gmt": "2023-11-13T17:13:02", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72720"}, "modified": "2023-11-16T11:16:39", "modified_gmt": "2023-11-16T19:16:39", "slug": "simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/", "title": {"rendered": "Simplifying GPU Programming for HPC with NVIDIA Grace Hopper Superchip"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The new hardware developments in <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA Grace Hopper Superchip</a> systems enable some dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that the user can develop their application for both processors while using a single, unified address space.&nbsp;</p>\n\n\n\n<p>Each processor retains its own physical memory that is designed with the bandwidth, latency, and capacity characteristics that are matched to the workloads most suited for each processor. Code written for existing discrete-memory GPU systems will continue to run performantly without modification for the new NVIDIA Grace Hopper architecture.</p>\n\n\n\n<p>Our recent post, <a href=\"https://developer.nvidia.com/blog/simplifying-gpu-application-development-with-heterogeneous-memory-management/\">Simplifying GPU Application Development with Heterogeneous Memory Management</a>, details some of the benefits that a single-address space brings to developers and how it works on systems with NVIDIA GPUs connected to x86_64 CPUs through PCIe. All application threads (GPU or CPU) can directly access all of the application\u2019s system allocated memory, removing the need to copy data between processors.&nbsp;</p>\n\n\n\n<p>This new ability to directly read or write to the full application memory address space significantly improves programmer productivity for all programming models built on top of CUDA: CUDA C++, CUDA Fortran, standard parallelism in ISO C++ and ISO Fortran, OpenACC, OpenMP, and many others.&nbsp;</p>\n\n\n\n<p>This post continues the Heterogeneous Memory Management (HMM) discussion in the context of Grace Hopper hardware, which provides all of the same programming model improvements as HMM-enabled systems, but with added hardware support to make it even better.&nbsp;</p>\n\n\n\n<p>Notably, any workload that is bottlenecked by host-to-device or device-to-host transfers can get up to a 7x speedup due to the <a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\">chip-to-chip (C2C) interconnect</a> in Grace Hopper systems. Cache coherency enables this performance without having to pin the memory (using <code>cudaHostRegister</code>, for example) if huge pages are used. While HMM and CUDA Managed Memory have historically been limited to migrating whole pages of data reactively on page faults, Grace Hopper is able to make better decisions on where data should reside and when it should migrate.&nbsp;</p>\n\n\n\n<p>We will detail in this post how the NVIDIA HPC compilers take advantage of these new hardware capabilities to simplify GPU programming with ISO C++, ISO Fortran, OpenACC, and CUDA Fortran.</p>\n\n\n\n<p>NVIDIA Grace Hopper systems provide the best performance available combined with a simplified GPU developer experience. HMM also brings this simplified developer experience to systems that are not Grace Hopper, while providing the optimal performance available when using PCIe. Developers can use these improved and simplified programming models in a portable way to get the best performance available on a wide variety of systems using NVIDIA GPUs.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Expanding stdpar with Grace Hopper unified memory&nbsp;</h2>\n\n\n\n<p>Standard languages such as ISO C++ and ISO Fortran have been gaining features in recent years to enable developers to express the parallelism that is present in their applications directly from the base language itself, without using extensions or compiler directives. The <a href=\"https://developer.nvidia.com/hpc-sdk\">NVIDIA HPC compilers</a> can build these applications to run with high performance on NVIDIA GPUs. These features have been detailed in <a href=\"https://developer.nvidia.com/blog/search-posts/?q=stdpar\">previous posts</a>.&nbsp;</p>\n\n\n\n<p>More specifically, we showed how using standard language parallelism, also known as stdpar, can be used to greatly improve developer productivity and simplify GPU application development. However, we also pointed out a few limitations due to the nature of the separate memory spaces of the CPU and GPU. These include the inability to use some types of data in the C++ parallel algorithms, such as data allocated on the stack, global data, or data captured by reference in lambda captures.&nbsp;</p>\n\n\n\n<p>For Fortran <code>do concurrent</code> loops, global variables could not be used in routines called from within the <code>do concurrent</code> loop, and size detection of data by the compiler for assumed-size arrays was limited. Now, Grace Hopper and its unified memory capabilities clear these limitations, and developing applications using stdpar becomes even simpler.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Simplifying OpenACC and CUDA Fortran&nbsp;</h2>\n\n\n\n<p>GPU application developers have long preferred both <a href=\"https://www.openacc.org/\">OpenACC</a> and <a href=\"https://developer.nvidia.com/cuda-fortran\">CUDA Fortran</a> due to their convenience and power. Along with <a href=\"https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf\">CUDA C++</a>, they have stood the test of time and are used in production by a large number of applications in HPC centers around the world. Both of these models provide robust ways of managing data residency and optimizing data transfer and movement.&nbsp;</p>\n\n\n\n<p>Now with the unified memory capabilities of Grace Hopper, application development can be dramatically simplified because these considerations for data location and movement can be automatically handled by the system. This reduces the effort spent on porting applications to run on GPUs and leaves more time for algorithm development.&nbsp;</p>\n\n\n\n<p>For fine-tuning performance and optimizations, the developer may choose to selectively add information about data locality using the facilities already available in OpenACC and CUDA Fortran. The data information in existing applications written for discrete memory devices can be used to optimize for Grace Hopper\u2019s unified memory without code changes.</p>\n\n\n\n<h1 class=\"wp-block-heading\">Evaluating application performance using unified memory</h1>\n\n\n\n<p>The following sections explore several benchmarks and applications to understand how these new features not only simplify code development, but also impact expected runtime performance.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">SPECaccel 2023 benchmark</h2>\n\n\n\n<p>The <a href=\"http://www.spec.org/accel2023\">SPECaccel\u00ae 2023 benchmark suite</a> focuses on single-accelerator performance using directives, OpenACC, and OpenMP. The benchmarks are intended to show general GPU performance and are a good proxy for how many HPC applications can use the new unified memory features of Grace Hopper.&nbsp;</p>\n\n\n\n<p>Figure 1 compares the performance of OpenACC data directives to unified memory enabled through the NVHPC SDK compiler flag <code>-gpu=unified</code>.&nbsp; While the results followed the benchmark\u2019s run rule requirements, they were measured on preproduction hardware, so are considered estimated.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1194\" height=\"687\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_.png\" alt=\"A bar chart comparing the estimated performance of 13 benchmarks in the SPECaccel 2023 suite when using data directives compared to unified memory. The majority of the bars show very little performance difference between the two versions. \" class=\"wp-image-72968\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_.png 1194w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-300x173.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-625x360.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-179x103.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-768x442.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-645x371.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-500x288.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-156x90.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-362x208.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-191x110.png 191w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/estimated-performance-specaccel-2023-benchmarks-unifed-memory-comparison_-1024x589.png 1024w\" sizes=\"(max-width: 1194px) 100vw, 1194px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Estimated performance of multiple SPECaccel 2023 benchmarks using data directives compared to unified memory</em></figcaption></figure></div>\n\n\n<p>Most of the benchmarks show very little difference between using unified memory and memory managed with the OpenACC data directives with an overall slow-down of only ~1%.&nbsp; 463.swim, which primarily measures the memory performance, gains 28% with unified memory. With the data directives, an entire array is copied back for each time cycle, though only the inner triangular portion of the array is used on the host.&nbsp;</p>\n\n\n\n<p>Given that the printed data is non-contiguous, with data directives, it is advantageous to copy the entire array as one large block rather than many smaller blocks. With unified memory, far less data is accessed on the host, with only a portion of the array fetched from GPU memory.</p>\n\n\n\n<p>The only significant slow-down is with the 404.lbm benchmark at 22%. The kernel times for each iteration have a slight overhead of 2 ms when using unified memory. Given that the kernel is executed 2,000 times, the overhead gets accumulated for about 3% of the difference. The larger issue is the entire 5 GB results array is check-pointed every 63 iterations, which needs to be accessed from the host. In this case, the CPU accessing the GPU memory roughly doubles the time, accounting for the remaining 19% of the difference.</p>\n\n\n\n<p>While unified memory makes porting code significantly easier, and as in the case of SPECaccel, it generally gives the same performance as using data directives. Programmers still need to be mindful of data placement, as for any other multi-socket system with non-uniform memory access (NUMA) characteristics. However, in most cases, data directives can now be considered a performance tuning option for cases such as 404.lbm where large portions of data are accessed on both the CPU and GPU, rather than a requirement to port code to the GPU.</p>\n\n\n\n<p><em>SPEC and SPECaccel are registered trademarks of the <a href=\"http://www.spec.org/accel2023/\">Standard Performance Evaluation Corporation</a>.&nbsp;</em></p>\n\n\n\n<h2 class=\"wp-block-heading\">LULESH</h2>\n\n\n\n<p><a href=\"https://asc.llnl.gov/codes/proxy-apps/lulesh\">LULESH </a>is a mini-application designed to simulate a simplified version of shock hydrodynamics representative of LLNL\u2019s ALE3D application. It has been used for over a decade to understand C++ parallel programming models and their interaction with compilers and memory allocators.&nbsp;&nbsp;</p>\n\n\n\n<p>The stdpar implementation of LULESH uses C++ standard library containers for all the data structures on the GPU, and they depend on the automatic migration of memory between CPU and GPU.&nbsp;&nbsp;</p>\n\n\n\n<p>Figure 2 shows that using unified memory does not affect the performance of LULESH, which makes sense. Both managed and unified memory options lead to a LULESH figure-of-merit (FOM) of 2.09e5 on <a href=\"https://www.nvidia.com/en-us/data-center/dgx-gh200/\">NVIDIA DGX GH200</a>, which is 40% higher than the FOM than with an <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100</a> PCIe GPU and 6.5x faster than a 56-core Intel Xeon 8480+ CPU system.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"743\" height=\"398\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1.png\" alt=\"A bar chart comparing the performance of LULESH when run in multiple ways. The performance on an Intel Xeon 8480+ is the baseline. The H100 PCIe bar is 4.61x faster. The performance of the GH200 using managed memory is 6.51x and with the compiler\u2019s unified mode is 6.49x. \n\" class=\"wp-image-72961\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1.png 743w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-300x161.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-625x335.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-645x346.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-500x268.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-362x194.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-lulesh-performance-1-205x110.png 205w\" sizes=\"(max-width: 743px) 100vw, 743px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Comparison of LULESH performance using managed and unified memory options on NVIDIA GH200 with NVIDIA H100 PCIe and a modern CPU</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">POT3D</h2>\n\n\n\n<p>POT3D approximates solar coronal magnetic fields by computing potential field solutions. It is developed by Predictive Science Inc. using modern Fortran. The application has historically run on GPUs using OpenACC, but the authors have now adopted a mixture of Fortran <code>do concurrent</code> to express data parallel loops and OpenACC for optimizing data movement with the GPU.&nbsp;</p>\n\n\n\n<p>As presented in the GTC session, <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41318/\">From Directives to DO CONCURRENT: A Case Study in Standard Parallelism</a>, the stdpar version of the code performed roughly 10% slower than the optimized OpenACC code. If OpenACC is used to optimize the data movement of the stdpar version, the performance is nearly identical. This means that the same performance was achieved while maintaining roughly 2,000 fewer lines of code. Does unified memory change this?</p>\n\n\n\n<p>Figure 3 shows the performance of POT3D on Grace Hopper built in two ways. The blue bar is the performance baseline, which is Fortran <code>do concurrent</code> loops for parallelism and OpenACC data directives to optimize data movement. The green bar is built using the <code>-gpu=unified</code> option on Grace Hopper and with all OpenACC directives removed.&nbsp;</p>\n\n\n\n<p>The performance of the code is now the same as the fully optimized code without requiring any OpenACC. With the performance and productivity enhancements unified memory brings, POT3D can now be written in pure Fortran and get the same performance as the previously tuned OpenACC code.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"744\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison.png\" alt=\"A bar chart comparing performance of managing memory explicitly in POT3D and using the new unified memory mode. The performance using OpenACC for data management and building without any data directives is equal.\n\" class=\"wp-image-72959\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison.png 744w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-300x142.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-625x296.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-645x305.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-500x237.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/pot3d-performance-comparison-233x110.png 233w\" sizes=\"(max-width: 744px) 100vw, 744px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. POT3D performance using OpenACC data directives compared to Grace Hopper unified memory</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">How to enable and use unified memory in the NVIDIA HPC SDK&nbsp;</h2>\n\n\n\n<p>As of NVHPC SDK release 23.11, developers aiming to use GPUs with unified memory capability can benefit from simplified programming interfaces. This release introduces a novel compilation mode to the <code>nvc++</code>, <code>nvc</code>, and <code>nvfortran</code> compilers, which can be enabled by passing the flag <code>-gpu=unified</code>.&nbsp;</p>\n\n\n\n<p>This section is a deep dive into the specific enhancements for unified memory across various programming models supported by the NVHPC SDK, which leverage the capabilities of the underlying hardware and the CUDA runtime to automatically handle data placement and memory migration between CPU and GPU physical memory.</p>\n\n\n\n<h3 class=\"wp-block-heading\">stdpar</h3>\n\n\n\n<p>For stdpar, all data access restrictions have been removed. This means global variables can be accessed from CPU or GPU, and unified memory compilation is now the default setting on compatible machines. However, when cross-compiling for different targets, <code>-gpu=unified</code> flag needs to be passed explicitly to enable the new programming interface.</p>\n\n\n\n<p>In <a href=\"https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/\">the original release of stdpar C++ within <code>nvc++</code></a>, lambda functions in parallel algorithms had several restrictions. These have now been completely lifted. Developers can freely use data across different parallel algorithms and sequential code. This enables capturing variables by reference and accessing global variables within parallel algorithms:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nint init_val = 123;\nvoid foo() {\n  int my_array&#91;ARRAY_SIZE];\n  auto r = std::views::iota(0, ARRAY_SIZE);\n  std::for_each(std::execution::par_unseq, r.begin(), r.end(),\n                &#91;&amp;](auto i) { my_array&#91;i] = init_val; });\n}\n</pre></div>\n\n\n<p>If this code is compiled as shown below, the array <code>my_array</code> can be safely initialized on the GPU with each element being set in parallel using the value from the global variable <code>init_val</code>. Previously, accessing both <code>my_array</code> and <code>init_val</code> was not supported.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nnvc++ -std=c++20 -stdpar -gpu=unified example.cpp\n</pre></div>\n\n\n<p>It is now also possible to use <code>std::array</code> safely with parallel algorithms, as illustrated by the example:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nstd::array&lt;int, 10000&gt; my_array = ...;\nstd::sort(std::execution::par, my_array.begin(), my_array.end());\n</pre></div>\n\n\n<p>The removal of data access limitations is a notable improvement, but it is important to remember that data races are still possible. For example, accessing global variables within parallel algorithms with simultaneous updates in different lambda instances running on the GPU.</p>\n\n\n\n<p>Porting existing code to stdpar C++ and integrating third-party libraries is also simplified. When pointers to data used in parallel algorithms originate in allocation statements from separate files, those files no longer require compilation with <code>nvc++</code> or <code>-stdpar</code>.</p>\n\n\n\n<p>For standard Fortran, some variable uses were previously unsupported. Now, it is possible to access global variables in routines called from <code>do concurrent</code> loops. Additionally, there were cases where the compiler could not accurately determine variable sizes for implicit data movements between GPU and CPU. These cases can now be handled correctly on targets with unified memory:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nsubroutine r(a, b)\n  integer :: a(*)\n  integer :: b(:)\n  do concurrent (i = 1 : size(b))\n    a(b(i)) = i \n  enddo\nend subroutine\n</pre></div>\n\n\n<p>In the example above, an access region of an assumed-size array <code>a</code> inside the <code>do concurrent</code> construct cannot be determined at compile time because the element index positions are taken from another array <code>b</code> initialized outside or the routine. This is no longer an issue when such code is compiled as follows:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nnvfortran -stdpar -gpu=unified example.f90\n</pre></div>\n\n\n<p>The crucial aspect is that the compiler no longer requires precise knowledge of the data segment accessed within the loop. Automatic data transfers between GPU and CPU are now handled seamlessly by the CUDA runtime.</p>\n\n\n\n<p><strong>OpenACC</strong></p>\n\n\n\n<p>Now with unified memory mode, OpenACC programs no longer require explicit data clauses and directives. All variables are now accessible from the OpenACC compute regions. This implementation closely adheres to the shared memory mode detailed in the OpenACC specification.</p>\n\n\n\n<p>The following C example illustrates an OpenACC parallel loop region that can now be executed correctly on GPUs without requiring any data clauses:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\nvoid set(int* ptr, int i, int j, int dim){\n  int idx = i * dim + j;\n  ptr&#91;idx] = someval(i, j);\n}\n\nvoid fill2d(int* ptr, int dim){\n#pragma acc parallel loop\n  for (int i = 0; i &lt; dim; i++)\n    for (int j = 0; j &lt; dim; j++)\n      set(ptr, i, j, dim);\n}\n</pre></div>\n\n\n<p>In C/C++, native language arrays are implicitly decayed into pointers when passed to functions. Therefore, the original array shape and size information is not preserved across the function invocations. Moreover, arrays with the dynamic size are represented by pointers. The use of pointers poses major challenges for the automatic code optimizations, as compilers lack essential information about the original data.&nbsp;</p>\n\n\n\n<p>While the OpenACC compiler has strong support for detecting the data segment accessed in loops to perform the data movements to GPU implicitly, it cannot determine the segment in this case because the array is updated through <code>ptr</code> in another function <code>set</code> called inside the loop. Previously, supporting such cases in C was not possible. However, with \u200cunified memory mode enabled as shown below, such examples are now fully supported:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nnvc -acc -gpu=unified example.c\n</pre></div>\n\n\n<p>Without <code>-gpu=unified</code> the only way to guarantee correctness for this example is to change the line with the pragma directive:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n#pragma acc parallel loop create(ptr&#91;0:dim*dim]) copyout(ptr&#91;0:dim*dim])\n</pre></div>\n\n\n<p>This explicitly instructs the OpenACC implementation about the precise data segment used within the parallel loop.</p>\n\n\n\n<p>The Fortran example below illustrates how a global variable can now be accessed in the OpenACC routine without requiring any explicit annotations.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nmodule m\ninteger :: globmin = 1234\ncontains\nsubroutine findmin(a)\n!$acc routine seq\n  integer, intent(in)  :: a(:)\n  integer :: i\n  do i = 1, size(a)\n    if (a(i) .lt. globmin) then\n      globmin = a(i)\n    endif\n  end do\nend subroutine\nend module m\n</pre></div>\n\n\n<p>When this example is compiled as shown below, the source does not need any OpenACC directives in order to access module variable <code>globmin</code> to read or update its value in the routine invoked from CPU and GPU.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nnvfortran -acc -gpu=unified example.f90\n</pre></div>\n\n\n<p>Moreover, any access to <code>globmin</code> will be made to the same exact instance of the variable from CPU and GPU keeping its value synchronized automatically. Previously, such behavior could only be achieved by adding a combination of OpenACC <code>declare</code> and <code>update</code> directives in the source code.</p>\n\n\n\n<p>In binaries compiled with <code>-gpu=unified</code>, the OpenACC runtime leverages data action information such as create/delete or copyin/copyout as optimizations to indicate preferable data placement to the CUDA runtime by means of memory hint APIs. For more details, see <a href=\"https://developer.nvidia.com/blog/simplifying-gpu-application-development-with-heterogeneous-memory-management/\">Simplifying GPU Application Development with Heterogeneous Memory Management</a>.&nbsp;</p>\n\n\n\n<p>Such actions originate either from the explicit data clauses in the source code or implicitly determined by the compiler. These optimizations can be used to fine-tune application performance by minimizing the amount of \u200cautomatic data migrations.&nbsp;</p>\n\n\n\n<p>For the C example above, while adding data clauses <code>create(ptr[0:dim*dim])</code> and <code>copyout(ptr[0:dim*dim])</code> is optional with <code>-gpu=unifie</code>d, their use in the OpenACC parallel loop directive may lead to a performance uplift.</p>\n\n\n\n<h3 class=\"wp-block-heading\">CUDA Fortran</h3>\n\n\n\n<p>The addition of -gpu=unified also simplifies CUDA Fortran programming by removing restrictions on CPU-declared variables passed as arguments to global or device routines executing on the GPU. Moreover, it now permits referencing module or common block variables in such routines without requiring explicit attributes. This change does not affect variables explicitly annotated with existing data attributes: device, managed, constant, shared, or pinned.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nmodule m\ninteger :: globval\ncontains \nattributes(global) subroutine fill(a)\n  integer :: a(*)\n  i = threadIdx%x\n  a(i) = globval\nend subroutine\nend module m\nprogram example\n  use m\n  integer :: a(N)\n  globval = 123\n  call fill&lt;&lt;&lt;1, N&gt;&gt;&gt; (a)\n  e = cudaDeviceSynchronize()\nend program\n</pre></div>\n\n\n<p>In the example above, the CPU stack-allocated array a is initialized in the kernel fill on the GPU with the value from the global variable globval assigned in the CPU code. As shown, a kernel routine, which is an entry point for execution on GPU, is now enables to directly access variables declared in the regular CPU host.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Details common across programming models</h3>\n\n\n\n<p>Binaries that have not been compiled with the new <code>-gpu=unified</code> flag will retain their existing performance characteristics on systems with and without unified memory alike. However, binaries compiled with <code>-gpu=unified</code> can not guarantee correct execution on targets without unified memory capability. When linking the final binary for unified memory targets, passing <code>-gpu=unified</code> in the linker command line is required for correctness.</p>\n\n\n\n<p>Many applications transitioning to architectures with unified memory can seamlessly recompile with <code>-gpu=unified</code> without any code modifications. In addition, stdpar C++ and CUDA Fortran object files, whether compiled with or without <code>-gpu=unified</code>, can be linked together. However, linking object files containing OpenACC directives or Fortran DC compiled differently with and without <code>-gpu=unified</code> is currently not supported.&nbsp;</p>\n\n\n\n<p>Manual performance tuning for memory usage is currently achievable through the CUDA memory hints APIs for all the programming models that support unified memory, as well as through data directives for OpenACC programs.</p>\n\n\n\n<p>The HPC SDK will continue to enhance support for unified memory in upcoming releases. For in-depth information regarding the current status, limitations, and future updates on this new functionality, refer to the <a href=\"https://docs.nvidia.com/hpc-sdk//index.html\">NVIDIA HPC SDK documentation</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary&nbsp;</h2>\n\n\n\n<p>The features and performance explained in this post are just the beginning of what NVIDIA Grace Hopper Superchip architecture and the NVIDIA software stack are bringing to developers. Future developments in the driver, CUDA software stack, and the NVIDIA HPC compilers are expected to remove even more restrictions on the way that users write their code, and to improve the performance of the resulting applications.&nbsp;</p>\n\n\n\n<ul>\n<li>Learn more about \u200ccompiler support on the <a href=\"https://developer.nvidia.com/hpc-sdk\">NVIDIA HPC SDK</a> page.</li>\n\n\n\n<li>Read <a href=\"https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/\">Developing Accelerated Code with Standard Language Parallelism</a>.</li>\n\n\n\n<li>Check out the blog post series on <a href=\"https://developer.nvidia.com/blog/using-fortran-standard-parallel-programming-for-gpu-acceleration/\">Fortran Standard Parallelism</a>.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/nvidia-hpc-sdk-downloads\">Download the NVIDIA HPC SDK</a> for free.</li>\n</ul>\n\n\n\n<p><em>SPEC and SPECaccel are registered trademarks of the <a href=\"http://www.spec.org/accel2023/\">Standard Performance Evaluation Corporation</a>.&nbsp;</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>The new hardware developments in NVIDIA Grace Hopper Superchip systems enable some dramatic changes to the way developers approach GPU programming. Most notably, the bidirectional, high-bandwidth, and cache-coherent connection between CPU and GPU memory means that the user can develop their application for both processors while using a single, unified address space.&nbsp; Each processor retains &hellip; <a href=\"https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/\">Continued</a></p>\n", "protected": false}, "author": 759, "featured_media": 72817, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1296451", "discourse_permalink": "https://forums.developer.nvidia.com/t/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/272710", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [23, 453, 3099, 2780, 608, 1258, 53], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-grace-hopper.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iUU", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72720"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/759"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72720"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72720/revisions"}], "predecessor-version": [{"id": 73483, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72720/revisions/73483"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72817"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72720"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72720"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72720"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72916, "date": "2023-11-13T08:58:27", "date_gmt": "2023-11-13T16:58:27", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72916"}, "modified": "2024-01-10T10:06:50", "modified_gmt": "2024-01-10T18:06:50", "slug": "using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/", "title": {"rendered": "Using Synthetic Data to Address Novel Viewpoints for Autonomous Vehicle Perception"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Autonomous vehicles (AV) come in all shapes and sizes, ranging from small passenger cars to multi-axle semi-trucks. However, a perception algorithm deployed on these vehicles must be trained to handle similar situations, like avoiding an obstacle or a pedestrian.</p>\n\n\n\n<p>The datasets used to develop and validate these algorithms are typically collected by one type of vehicle\u2014 for example sedans outfitted with cameras, radars, lidars, and ultrasonic sensors.</p>\n\n\n\n<p>Perception algorithms trained on fleet sensor data can perform reliably on similar sensor configurations. However, when deploying the same algorithm on a vehicle with a different sensor configuration, perception performance can degrade, as it is seeing the world from a new point of view.</p>\n\n\n\n<p>Addressing any loss in perception accuracy requires measuring the sensitivity of the deep neural network (DNN) to new sensor positions. Using the sensitivity analysis, it is possible to retrain the perception algorithm with data from multiple points of view to improve robustness in a targeted manner.</p>\n\n\n\n<p>However, performing a sensitivity analysis and retraining perception both require the collection and annotation of datasets across a variety of sensor configurations. This is a time- and cost-prohibitive process.</p>\n\n\n\n<p>This post shows how synthetic datasets in <a href=\"https://www.nvidia.com/en-us/self-driving-cars/simulation/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA DRIVE Sim</a> and the latest NVIDIA research in novel view synthesis (NVS) fill these data gaps and help recover perception accuracy otherwise lost when deploying to new sensor configurations.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Measuring DNN sensitivity</h2>\n\n\n\n<p>Before creating synthetic datasets for different sensor viewpoints, the first step is to create a digital twin of the test fleet vehicle in NVIDIA DRIVE Sim, along with a sensor rig of simulated cameras that are calibrated to <a href=\"https://developer.nvidia.com/blog/validating-drive-sim-camera-models/\" target=\"_blank\" rel=\"noreferrer noopener\">match real-world sensor configurations</a>.</p>\n\n\n\n<p>Synthetic data is generated by driving the ego-vehicle through a predefined scenario where it follows a specific trajectory, and saving the simulated camera data. For each run of the scenario, aspects are varied, such as sensor rig height, pitch, and mount position to emulate other vehicle types.</p>\n\n\n\n<p>Using the capabilities of <a href=\"https://developer.nvidia.com/omniverse/replicator\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Omniverse Replicator</a>, generate the ground truth (GT) labels, such as 3D bounding boxes, and object classes needed to evaluate perception algorithms. This entire workflow is repeatable and enables running well-defined experiments to quickly measure perception sensitivity.</p>\n\n\n\n<p>After running DNN inference on the generated datasets, compare the network\u2019s predictions with the GT labels to measure the network\u2019s accuracy for each sensor configuration for different camera heights, as shown in Figures 1 and 2. Each dataset is the same scenario but from different sensor view points. In Figure 1, blue boxes represent GT labels while green boxes show the network\u2019s predictions. In Figure 2, blue boxes represent GT labels while red boxes show the network\u2019s predictions.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1280\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/VehicleMultiSensorLowRes.gif\" alt=\"Side-by-side images from four sensor configurations showing a simulated driving scene with bounding boxes around other cars and objects in each frame.\" class=\"wp-image-73078\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. An example of an object detection DNN running on four different synthetic datasets, focusing on the vehicle object class</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1280\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/PedestrianMultiSensorLowRes.gif\" alt=\"Side-by-side images from four sensor configurations showing a simulated driving scene with bounding boxes around other cars and objects in each frame.\" class=\"wp-image-73079\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Example of an object detection DNN running on four different synthetic datasets, focusing on the pedestrian object class</em></figcaption></figure></div>\n\n\n<p>Given that the network was trained on data from one vehicle type, detections are more accurate for similar camera positions, and degrade as camera positions change significantly.</p>\n\n\n\n<p>Addressing these gaps in perception and deploying on a new vehicle type requires a targeted dataset for viewpoints that differ from the original data. While existing fleet data can be used with traditional augmentations, this approach does not fully satisfy the need for datasets captured from new points of view.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Novel view synthesis</h3>\n\n\n\n<p>NVS is a computer vision method for generating new, unseen views of a scene from a set of existing images. This capability makes it possible to create images of a scene from different viewpoints or angles not originally captured by the vehicle\u2019s camera.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"2468\" height=\"450\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes.png\" alt=\"Five side-by-side images illustrating the NVS pipeline. It starts with the source image, then depth estimation, followed by mesh creation. Finally, a diagram showing sensors being adjusted to a new viewpoint, resulting in the final novel view image.\" class=\"wp-image-73066\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes.png 2468w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-300x55.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-625x114.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-179x33.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-768x140.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-1536x280.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-2048x373.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-645x118.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-500x91.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-160x29.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-362x66.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-603x110.png 603w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_Pipelines_HighRes-1024x187.png 1024w\" sizes=\"(max-width: 2468px) 100vw, 2468px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The complete novel view synthesis pipeline</em></figcaption></figure></div>\n\n\n<p>The NVIDIA research team recently presented an NVS method that can transform dynamic driving data from one sensor position to new viewpoints emulating other camera heights, pitches, and angles. For details, see <a rel=\"noreferrer noopener\" href=\"https://nvlabs.github.io/viewpoint-robustness/assets/tzofi2023view.pdf\" target=\"_blank\">Towards Viewpoint Robustness in Bird\u2019s Eye View Segmentation</a>.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/SVt1UTtxbSY?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent&#038;listType=playlist&#038;list=PLZHnYvH1qtOYkElUMqYiHDMrGTPnqRhSr\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. NVIDIA DRIVE Labs video series walks through the latest research on transforming existing driving data to new sensor viewpoints</em></figcaption></figure>\n\n\n\n<p>Our approach builds on <a href=\"https://worldsheet.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">Worldsheet</a>, a method for using depth estimation and 3D meshing to synthesize new viewpoints of a static scene. A 3D scene mesh is created by warping a lattice grid onto a scene based on predicted depth values. Then, a texture sampler is used to \u201csplat,\u201d or project, the RGB pixel intensities from the original image onto the texture map of the 3D mesh. This approach expands on prior work in this area by using lidar-based depth supervision and automasking to improve the quality of the depth estimation and handle occlusions.</p>\n\n\n\n<p>The NVS model can now be used to generate data as if it was acquired from different vehicle types, unblocking existing fleet data for use in all future AV development.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"2528\" height=\"840\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes.png\" alt=\"A grid of camera images from a vehicle\u2019s point of view, each showing slight adjustments in camera pitch, depth, and height.\" class=\"wp-image-73069\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes.png 2528w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-300x100.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-625x208.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-179x59.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-768x255.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-1536x510.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-2048x681.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-645x214.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-500x166.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-362x120.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-331x110.png 331w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NVS_examples_HighRes-1024x340.png 1024w\" sizes=\"(max-width: 2528px) 100vw, 2528px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Examples of NVS-transformed images, generating viewpoints with changes in pitch, depth and height</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Validating NVS and improving perception performance</h2>\n\n\n\n<p>Before incorporating NVS-generated data into the training dataset, first validate that it accurately represents the real world and is effective for perception training.</p>\n\n\n\n<p>To do this, validate the NVS algorithm by training a perception algorithm on a combination of fleet data and NVS-transformed data. In the absence of real data to test the model\u2019s performance from multiple sensor viewpoints, generate synthetic data and GT labels in DRIVE Sim, similar to the sensitivity testing previously discussed.</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1832\" height=\"1588\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91.png\" alt=\"A grid of camera images from a vehicle\u2019s point of view, each showing slight adjustments in camera pitch, depth, and height.\" class=\"wp-image-73368\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91.png 1832w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-300x260.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-625x542.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-133x115.png 133w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-768x666.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-1536x1331.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-645x559.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-346x300.png 346w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-104x90.png 104w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-362x314.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-127x110.png 127w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-91-1024x888.png 1024w\" sizes=\"(max-width: 1832px) 100vw, 1832px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. A set of camera images generated in DRIVE Sim with varied pitch, depth, and height for perception validation</em></figcaption></figure>\n\n\n\n<p>Running inference on these synthetic datasets shows that using NVS-generated data for training can improve perception performance. Specifically:</p>\n\n\n\n<ul>\n<li>NVS-generated data quality is best for changes in sensor pitch and lowest for large changes in height.</li>\n\n\n\n<li>NVS-transformed data for training enables recovering valuable perception performance that would only have been possible by collecting new data for each new sensor configuration.</li>\n</ul>\n\n\n\n<p>This approach unlocks a new approach to AV development, where data only needs to be collected once, then repurposed for multiple vehicle types\u2014significantly reducing cost and time to deployment.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Conclusion</h2>\n\n\n\n<p>Developing a perception stack that works robustly across different vehicle types is a massive data challenge. However, synthetic data generation and AI techniques for novel view synthesis enable the systematic measurement of perception sensitivity. This significantly multiplies the value of existing datasets and reduces the time to deploy a perception stack for any vehicle.</p>\n\n\n\n<p>We invite the research community to add to this body of work. Accordingly, we are releasing the synthetic data from DRIVE Sim as reported in <a href=\"https://nvlabs.github.io/viewpoint-robustness/assets/tzofi2023view.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Towards Viewpoint Robustness in Bird\u2019s Eye View Segmentation</a>. <a href=\"https://nvlabs.github.io/viewpoint-robustness/\" target=\"_blank\" rel=\"noreferrer noopener\">Explore this data and learn more</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Autonomous vehicles (AV) come in all shapes and sizes, ranging from small passenger cars to multi-axle semi-trucks. However, a perception algorithm deployed on these vehicles must be trained to handle similar situations, like avoiding an obstacle or a pedestrian. The datasets used to develop and validate these algorithms are typically collected by one type of &hellip; <a href=\"https://developer.nvidia.com/blog/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/\">Continued</a></p>\n", "protected": false}, "author": 1754, "featured_media": 73071, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1296443", "discourse_permalink": "https://forums.developer.nvidia.com/t/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/272709", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [3366, 453, 2571, 1718], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/KeyVisual_HighRes.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iY4", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72916"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1754"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72916"}], "version-history": [{"count": 19, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72916/revisions"}], "predecessor-version": [{"id": 75682, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72916/revisions/75682"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73071"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72916"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72916"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72916"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72724, "date": "2023-11-13T08:00:00", "date_gmt": "2023-11-13T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72724"}, "modified": "2023-11-20T10:42:51", "modified_gmt": "2023-11-20T18:42:51", "slug": "optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/", "title": {"rendered": "Optimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Computational <a href=\"https://www.nvidia.com/en-us/glossary/energy-efficiency/\">energy efficiency</a> has become a primary decision criterion for most supercomputing centers. Data centers, once built, are capped in terms of the amount of power they can use without expensive and time-consuming retrofits. Maximizing insight in the form of workload throughput then means maximizing workload per watt. NVIDIA products have, for several generations, focused on maximizing real application performance per kilowatt hour (kWh) used.&nbsp;</p>\n\n\n\n<p>This post explores measuring and optimizing energy usage of multi-node hybrid density functional theory-(DFT) calculations with the <a href=\"https://www.vasp.at/\">Vienna Ab initio Simulation Package</a> (VASP). VASP is a computer program for atomic-scale materials modeling, such as electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.&nbsp;</p>\n\n\n\n<p>Material property research is an active area for researchers using supercomputing facilities for cases as broad as high-temperature, low-pressure superconductors, to the next generation of solar cells. VASP is a primary tool in these digital investigations.&nbsp;</p>\n\n\n\n<p>This post follows our 2022 investigation on multi-node VASP scalability for varying system sizes of a simple compound hafnia (HfO<sub>2</sub>). For details, see <a href=\"https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/\">Scaling VASP with NVIDIA Magnum IO</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Experiment setup</h2>\n\n\n\n<p>The environment and setup used for this energy-focused extension of our previous work is largely the same. This section provides details about our experiment setup for reproducing our results.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Hardware: NVIDIA GPU system</h3>\n\n\n\n<ul>\n<li><a href=\"https://top500.org/system/179842/\">NVIDIA Selene cluster</a>&nbsp;</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/dgx-a100/\">NVIDIA DGX A100</a></li>\n\n\n\n<li>AMD EPYC 7742 64C 2.25 GHz</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100</a> Tensor Core GPUs (80 GB) (eight per node)</li>\n\n\n\n<li>NVIDIA HDR InfiniBand (eight per node)</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Hardware: CPU system</h3>\n\n\n\n<ul>\n<li>Dual-socket Intel 8280 CPUs (28 cores per socket)</li>\n\n\n\n<li>192 GB per node</li>\n\n\n\n<li>NVIDIA HDR InfiniBand per node</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Software</h3>\n\n\n\n<p>Updated versions for all the following components of the NVIDIA GPU software stack are available. However, we intentionally used the same components as for our <a href=\"https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/\">previous work</a> to retain comparability. VASP was updated to version 6.4 released in 2023.</p>\n\n\n\n<ul>\n<li>NVIDIA HPC SDK 22.5 (formerly known as PGI)</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/magnum-io/\">NVIDIA Magnum IO</a><sup>&nbsp; </sup>NCCL 2.12.9</li>\n\n\n\n<li>CUDA 11.7</li>\n\n\n\n<li>FFT\n<ul>\n<li>GPU: FFT lib \u2013 cuFFT 10.7.2 (GPU side)</li>\n\n\n\n<li>CPU: FFTW interface comes from Intel MKL 2020.0.166</li>\n</ul>\n</li>\n\n\n\n<li>MPI: open MPI 4.1.4rc2 compiled with PGI</li>\n\n\n\n<li>UCX</li>\n\n\n\n<li>VASP 6.4.0</li>\n</ul>\n\n\n\n<p>For runs on an Intel CPU-only cluster, we employed the accordingly optimal toolchain consisting of the most recent version available at the time of writing:</p>\n\n\n\n<ul>\n<li>Rocky Linux release 9.2 (Blue Onyx)</li>\n\n\n\n<li>Intel oneAPI HPC Toolkit 2022.3.1</li>\n\n\n\n<li>MPI: hpcx-2.15</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Extrapolating runtime and energy</h2>\n\n\n\n<p>As detailed in <a href=\"https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/\">Scaling VASP with NVIDIA Magnum IO</a>, we shortened the benchmarking runs and extrapolated to the full results to save resources. In other words, we only used a fraction of the energy presented in the following formula:</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=t_%7Btotal%7D+%3D+t_%7Binit%7D+%2B+19+t_%7Biter%7D+%2B+t_%7Bpost%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"t_{total} = t_{init} + 19 t_{iter} + t_{post}\" class=\"latex\" /></p>\n\n\n\n<p>The method was extended for energy by assuming the energy used for one iteration was similarly constant:</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=E_%7Btotal%7D+%3D+E_%7Binit%7D+%2B+19+E_%7Biter%7D+%2B+E_%7Bpost%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"E_{total} = E_{init} + 19 E_{iter} + E_{post}\" class=\"latex\" /></p>\n\n\n\n<h3 class=\"wp-block-heading\">Chemistry and models</h3>\n\n\n\n<ul>\n<li>Chemistry: hafnia (HfO<sub>2</sub>)</li>\n\n\n\n<li>Models:\n<ul>\n<li>3x3x2: 216 atoms, 1,280 orbitals</li>\n\n\n\n<li>3x3x3: 324 atoms, 1,792 orbitals</li>\n\n\n\n<li>4x4x3: 576 atoms, 3,072 orbitals</li>\n\n\n\n<li>4x4x4: 768 atoms, 3,840 orbitals</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Capturing energy usage</h2>\n\n\n\n<p>The GPU benchmarks were done on the NVIDIA Selene supercomputer, which is equipped with smart PDUs that can provide information on currently used power through the SNMP protocol. We collected the data using a simple Python script launched in the background of each node before starting the application.&nbsp;</p>\n\n\n\n<p>We collected \u200cpower usage with a frequency of 1 Hz combined with timestamps. Given that, for GPU runs at hybrid-DFT level, VASP leaves the CPU mostly idle, this logging comes at almost no overhead. Based on the information in the files and timestamps included in the outputs from VASP, we calculated the energy usage for each part of the code and project as described above.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Optimizing for best energy efficiency (MaxQ)</h2>\n\n\n\n<p>By default, NVIDIA GPUs operate at their maximum clock frequency with sufficient load to ensure best possible performance, and hence time to solution. However, the performance of certain parts of an application might not be primarily limited by clock frequency to begin with.&nbsp;</p>\n\n\n\n<p>Higher clock frequencies require higher voltages, and this in turn leads to a higher energy uptake. Hence, the sweet spot for the maximum GPU clock frequency for solving a problem in the shortest time to solution might be different from those necessary to achieve the lowest energy to solution.</p>\n\n\n\n<p>Looking at hypothetical applications that are entirely limited by memory loads and stores, one would expect the lowest frequency that suffices to still saturate memory bandwidth should give a better energy to solution while not impairing performance. </p>\n\n\n\n<p>Given that real applications have mixed computational profiles and the dependence on frequency varies with the workload, the ideal frequency can be determined on a case-by-case basis. This was done for the VASP hafnia workloads presented here. However, we \u200chave observed that our findings also work well for other <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">high-performance computing (HPC)</a> applications.</p>\n\n\n\n<p>The frequencies can be controlled through the <a href=\"https://developer.nvidia.com/nvidia-system-management-interface\">NVIDIA System Management Interface (SMI)</a>, as shown in the code snippets below:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n-lgc --lock-gpu-clocks= Specifies &lt;minGpuClock,maxGpuClock&gt; clocks as a pair (1500,1500) that defines\nthe range of desired locked GPU clock speed in MHz. \nSetting this will supersede application clocks and take effect regardless if an app is running. \nInput can also be a singular desired clock value (&lt;GpuClockValue&gt;).\n\nFor example:\n# nvidia-smi -pm 1\n# nvidia-smi -i 0 -pl 250\n# nvidia-smi -i 0 -lgc 1090,1355\n</pre></div>\n\n\n<p>Additional data collected includes:</p>\n\n\n\n<ul>\n<li>CPU multi-node performance</li>\n\n\n\n<li>Single-node, SM frequency sweep for MaxQ</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Results</h2>\n\n\n\n<p>This section showcases the influence of GPU clock frequency on energy usage in VASP simulations, emphasizing the trade-offs between computational speed and energy usage. It also explores the complexities of optimizing the balance between performance and energy in HPC by analyzing data and heatmaps to minimize both time to solution and energy usage.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GPU clock frequency for efficiency</h3>\n\n\n\n<p>In the pursuit of a maximum rate of scientific insight for minimum energy cost, the GPU clock frequency can be set dynamically at a rate less than maximum. For <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100</a> GPUs, the maximum is 1,410 MHz.</p>\n\n\n\n<p>Lowering the GPU clock frequency has two effects: it lowers the maximum theoretical computational rate the GPU can achieve, which reduces \u200cenergy usage by the GPU. But it also reduces the amount of heat the GPU generates as it performs computations.&nbsp;</p>\n\n\n\n<p>In Figures 1 and 2, the data are normalized to the energy used by one node with NCCL enabled, running at the maximum frequency of 1,410 MHz. All the data shown are for the 216-atom case of hafnia. The vertical axes are matched, so relative energy usage can be seen between NCCL enabled and disabled.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"469\" height=\"422\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms.png\" alt=\"A chart showing energy use relative to default GPU clock of 1,410 MHz as the GPU clock is changed from less than 800 MHz to 1,410. The plots show a smooth curve, and minimum at 1,250 MHz for both NCCL enabled and disabled.  NCCL enabled at 128 nodes shows a 60% increase over the baseline, NCCL disabled shows a 200+% increase over the baseline.\n\" class=\"wp-image-72743\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms.png 469w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms-300x270.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms-333x300.png 333w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms-100x90.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms-362x326.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-enabled-216-atoms-122x110.png 122w\" sizes=\"(max-width: 469px) 100vw, 469px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Relative energy use at different GPU clocks (x-axis) for different node counts (separate lines) for NCCL enabled and the 216-atom case</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"469\" height=\"415\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms.png\" alt=\"Chart showing energy use relative to default GPU clock of 1,410 MHz as the GPU clock is changed from less than 800 MHz to 1,410. The plots show a smooth curve, and minimum at 1,250 MHz for both NCCL enabled and disabled.  NCCL enabled at 128 nodes shows a 60% increase over the baseline, NCCL disabled shows a 200+% increase over the baseline.\n\" class=\"wp-image-72746\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms.png 469w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms-300x265.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms-130x115.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms-339x300.png 339w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms-102x90.png 102w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms-362x320.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/relative-energy-use-gpu-clocks-nccl-disabled-216-atoms-124x110.png 124w\" sizes=\"(max-width: 469px) 100vw, 469px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Relative energy use at different GPU clocks (x-axis) for different node counts (separate lines) for NCCL disabled and the 216-atom case</em></em></figcaption></figure>\n\n\n\n<p>For both the NCCL enabled and disabled cases, reducing the GPU clock offers, at best, a 10% reduction in energy usage compared to the single-node, maximum frequency energy usage. In both cases, the minimum energy usage for most runs is close to the 1,250 MHz GPU clock.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Scalability and energy usage</h2>\n\n\n\n<p>Our previous investigation showed the significant performance available to NVIDIA GPU users when calculating large atomic systems in VASP within the hybrid DFT level of theory. Though the focus of this work is energy usage and efficiency, performance remains a crucial concern.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"580\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-625x580.png\" alt=\"Chart showing the near-linear scaling for the three models: 96, 216, and 423 atoms. \" class=\"wp-image-73110\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-625x580.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-300x279.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-124x115.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-768x713.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-645x599.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-323x300.png 323w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-97x90.png 97w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-362x336.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-118x110.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1-1024x951.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-speedup-1.png 1446w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. CPU scalability compared to&nbsp;<em><em>ideal for 96, 216, and 324 atoms</em></em>  </em></em></figcaption></figure></div>\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"584\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-625x584.png\" alt=\"Chart showing the energy usage of the three models at about 10 kWh for 96 atoms, 100 for 216 atoms, about 700 kWh for 324 atoms.\n\" class=\"wp-image-73108\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-625x584.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-300x280.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-123x115.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-768x717.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-645x602.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-321x300.png 321w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-96x90.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-362x338.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-118x110.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy-1024x956.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-scaling-energy.png 1447w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. CPU energy usage scaling for 96, 216, and 324 atoms</em></em></figcaption></figure>\n\n\n\n<p>To view the energy use trends, we began with a comparison of CPU-only and GPU performance, and energy. Though the CPUs for these systems are two generations behind current state-of-the-art Intel CPUs, they exhibit excellent scalability and a roughly linear trend for energy use. For the range of a single node up to 32 nodes, the energy usage for the 96-atoms case increases by 24%, for 216 atoms it increases by 13%, and for the 324-atom case it increases by 12%.&nbsp;</p>\n\n\n\n<p>By comparison, for the GPU runs at the same scales with NCCL enabled, energy increases by 10% at 216 atoms, and 3% at 324 atoms. Though it is not plotted, parallel efficiency for all three CPU-based runs stays above 80%, so the scaling is very good.&nbsp;</p>\n\n\n\n<p>In Figures 5 and 6, GPU performance data is shown for the maximum available GPU frequency of 1,410 MHz with NCCL enabled on four A100 GPUs per node. Note that the GPU system is more than an order of magnitude faster than the CPU system, and the scalability (slope of the lines) are essentially parallel, so both are scaling at the same rate.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"720\" height=\"526\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2.png\" alt=\"Log-log plot of elapsed time compared to number of nodes, where GPUs are more than an order of magnitude faster than CPUs for both the 216 and 324 atom cases, and both scale roughly linearly \n\" class=\"wp-image-74039\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2.png 720w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-300x219.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-625x457.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-157x115.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-645x471.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-411x300.png 411w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-123x90.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-362x264.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-elapsed-time-compared-gpu-based-systems_-2-151x110.png 151w\" sizes=\"(max-width: 720px) 100vw, 720px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. CPU elapsed time compared to GPU-based systems for 216 and 324 atoms</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"618\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-625x618.png\" alt=\"A semi-log plot of energy where GPUs use more than a factor of two less energy for both 216 and 324 atom cases.\n\" class=\"wp-image-73117\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-625x618.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-300x296.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-116x115.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-768x759.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-645x637.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-304x300.png 304w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-91x90.png 91w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-362x358.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-111x110.png 111w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1-1024x1012.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cpu-gpu-scaling-energy-1.png 1502w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. CPU energy compared to GPU-based systems for 216 and 324 atoms</em></em></figcaption></figure>\n\n\n\n<p>Figure 5 shows that the performance achieved for the 324-atom case on 32 nodes of CPUs is about the same speed as for a single node of A100 GPUs. GPU systems, even though they run at higher power, use less than 20% of the energy compared to CPU-based simulations.&nbsp;</p>\n\n\n\n<p>The other item of note is the near invariance in energy use between one and 32 nodes for the GPU system. For the case of hafnia at these sizes, the A100 GPU systems are 5x as energy efficient while delivering more than 32x the throughput in the same elapsed time.&nbsp;</p>\n\n\n\n<p>Figures 7 and 8 show the scalability of VASP with NCCL enabled and disabled. NCCL offers about twice the performance at 128 nodes (or 1,024 GPUs). Our previous work showed that the performance for VASP 6.4.0 is slightly better than 6.3.2.&nbsp;</p>\n\n\n\n<p>Both figures are plotted relative to the single-node VASP 6.3.2 results. They show a 128-node speedup of 107x compared to 114x for the 576-atom case; and 113x compared to 115x for the 768-atom case.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"640\" height=\"555\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3.png\" alt=\"Speedup plot showing good scaling for 216, 324 atom cases up to 64 nodes (1,024 GPUs) and almost a factor of 2 in performance at 64 nodes between NCCL enabled and disabled.\" class=\"wp-image-73175\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3.png 640w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-300x260.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-625x542.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-133x115.png 133w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-346x300.png 346w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-104x90.png 104w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-362x314.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-216-324-atoms-3-127x110.png 127w\" sizes=\"(max-width: 640px) 100vw, 640px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 7. Strong scaling speedup relative to single-node performance at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes for 216 and 324 atoms</em></em></figcaption></figure></div>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"638\" height=\"565\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3.png\" alt=\"Near-perfect scaling to 128 nodes (2,048 GPUs) for the 576 and 768 atom cases for NCCL enabled, with less scalability for NCCL disabled, and a factor of 2 in performance difference.\" class=\"wp-image-73177\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3.png 638w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-300x266.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-625x553.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-130x115.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-339x300.png 339w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-102x90.png 102w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-362x321.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/vasp-speedup-with-without-nccl-576-768-atoms-3-124x110.png 124w\" sizes=\"(max-width: 638px) 100vw, 638px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 8. Strong scaling speedup relative to single-node performance at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes for 576 and 768 atom</em></em>s</figcaption></figure></div>\n\n\n\n<p>The most important effect to note is the performance of the models ranging from 216 atoms to 768. This enables using NCCL instead of only MPI, offering the end user more than 2x the performance for a given number of GPUs. Or similarly, at a given parallel efficiency, an NCCL-enabled VASP hybrid-DFT calculation can be scaled to much higher numbers of GPUs to compress the elapsed time.&nbsp;</p>\n\n\n\n<p>Occasionally concerns are expressed that GPU-enabled servers require 4x to 8x the power that a CPU-based server requires. While it is true that GPU servers do require more power, accelerated applications always use far less energy to complete a task compared to a CPU. Energy is power multiplied by time. So, though the GPU server may draw more power (watts) while it is running, it runs for less time and so uses less total energy.&nbsp;</p>\n\n\n\n<p>Figure 9 shows a comparison of a GPU and CPU workload, where the CPU workload runs for a long time at low power and the GPU workload finishes much earlier though at a higher power, which enables GPUs to use less energy. The GPU workload runs very quickly at high power. Energy is the area of each of the time histories.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"265\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload.png\" alt=\"Power compared to time plot showing that GPUs run at high power for short time, CPUs run at low power for a long time. Energy is the area under the curve for each. \" class=\"wp-image-73134\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload-500x212.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/comparison-gpu-cpu-workload-259x110.png 259w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 9. Comparison of a GPU and CPU workload</em></em></figcaption></figure></div>\n\n\n<p>Figure 10 shows the energy advantage that using NCCL over MPI-only can provide. Both MPI and NCCL have the GPU server running at roughly the same power level, but because using NCCL scales better, the runtimes are shorter, and thereby less energy is used.&nbsp;</p>\n\n\n\n<p>The gap in energy between the two grows with node count simply because the scalability of MPI-only is significantly worse. As the parallel efficiency drops, the simulation runs for a longer time while not doing more productive work, and thereby uses more energy.</p>\n\n\n\n<p>Quantitatively, a hafnia hybrid-DFT calculation shows up to 1.8x reduction in energy usage for models between 216 and 768 atoms running at maximum GPU frequency at 128 nodes for using NCCL (Figures 10 and 11).</p>\n\n\n\n<p>Using lower numbers of nodes reduces the energy difference because the MPI-only simulations have a relatively higher parallel efficiency when running on fewer nodes. The tradeoff is that the runtime per simulation is extended.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"829\" height=\"464\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_.png\" alt=\"Energy use for 216 and 324 atom cases from one to 128 nodes where NCCL enabled and disabled start at the same energy, and diverge with NCCL disabled using 1.5-2.0x more energy at 128 nodes.\" class=\"wp-image-72805\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_.png 829w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-300x168.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-625x350.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-768x430.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-645x361.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-500x280.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-216-324-atoms_-197x110.png 197w\" sizes=\"(max-width: 829px) 100vw, 829px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 10. Energy used at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes from 216 atoms and 324 atoms</em></em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"832\" height=\"454\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_.png\" alt=\"Energy use for 576 and 768 atom cases from one to 128 nodes where NCCL enabled / disabled start at the same energy, and diverge with NCCL disabled using 1.5-2.0x more energy at 128 nodes.\" class=\"wp-image-72808\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_.png 832w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-300x164.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-625x341.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-768x419.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-645x352.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-500x273.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-362x198.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-used-nccl-enabled-disabled-hafnia-576-768-atoms_-202x110.png 202w\" sizes=\"(max-width: 832px) 100vw, 832px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 11. Energy used at GPU clock 1,410 MHz for NCCL enabled and disabled and hafnia model sizes from for 576 and 768 atoms</em></em></figcaption></figure></div>\n\n\n<p>As a VASP user or an HPC center manager, you may be asking yourself, \u201cFor a given large system of atoms, what is the most efficient point, or how many nodes are needed per simulation?\u201d This is a very good question\u2014one that we expect more and more people will be asking in the near future.</p>\n\n\n\n<p>Generally speaking, the fewer the nodes, the closer to 100% parallel efficiency the run will be, and will thereby use less energy. Figure 7 of <a href=\"https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/\">Scaling VASP with NVIDIA Magnum IO</a> shows the parallel efficiency, and Figures 10 and 11 of this post show the energy.&nbsp;</p>\n\n\n\n<p>However, other factors such as researcher time, publishing deadlines, and external influences can increase the value of having simulation results sooner. In these cases, maximizing the number of runs completed in a given time or minimizing the end-to-end wait time for results suggests less energy will be used if the algorithm is configured to obtain the best parallel efficiency. For example, a run with NCCL enabled.&nbsp;</p>\n\n\n\n<p>Maximizing application performance against energy usage constraints also helps optimize the return on investment for the HPC center manager.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Balancing speed and energy</h2>\n\n\n\n<p>Figure 10 or 11 might be interpreted as a recommendation to run a VASP simulation on as few nodes as possible. For HPC centers more focused on maximum efficiency than scientific output, that may be the right decision. We do not anticipate such an attitude to be common.&nbsp;</p>\n\n\n\n<p>Doing so, however, could easily ignore costs, which are not captured in a single metric like energy per simulation. For instance, researcher time is arguably the most precious resource in the scientific toolchain. As such, most HPC centers will want to explore a more balanced approach of reducing energy usage while minimizing the performance impact to their users.&nbsp;</p>\n\n\n\n<p>This is a multi-objective optimization problem with a family of solutions depending on the relative weight of the time-to-solution and energy-to-solution objectives. We therefore expect a rigorous analysis to produce a pareto front of optimal solutions.&nbsp;</p>\n\n\n\n<p>A quick approach, however, is to plot energy to solution on the vertical axis, and time to solution on the horizontal axis. By visualizing the data this way, the best compromise between these two is the datapoint closest to the origin.</p>\n\n\n\n<p>Figure 12 shows the separation between NCCL enabled and NCCL disabled as the cluster of dotted lines and the cluster of solid lines, where the solid lines reach an area much closer to the optimum. It also shows some difference between the maximum performance line (blue) and maximum efficiency line (green) for both NCCL enabled and disabled.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"453\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-625x453.png\" alt=\"Plot of energy to solution compared to time to solution where lower of both parameters is labeled \u201cbetter\u201d for NCCL enabled and disabled for four different GPU clocks: 1,410, 1,350, 1,250, 1,005.\n\" class=\"wp-image-74041\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-625x453.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-300x218.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-159x115.png 159w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-768x557.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-645x468.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-414x300.png 414w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-362x263.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1-152x110.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/energy-per-jobs-runtime-comparison-216-atom-hafnia_-1.png 867w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 12. Energy per jobs compared to runtime used on a 216-atom hafnia study</em></em></figcaption></figure>\n\n\n\n<p>It is difficult, however, to draw a conclusion about the optimum point to run. Is it 32 or 64 nodes? To help answer this question, Figure 13 shows a heat map calculating the distance to the origin, where green is closest to the optimum.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"819\" height=\"344\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1.png\" alt=\"Red to green heat map of distance from the origin, where the minimum distance represents the best compromise between time to solution and energy to solution. Best are 1,350 MHz and 1,250 MHz frequency, NCCL-enabled, 16, 32 nodes. Worst is NCCL enabled or disabled for one node at 510 MHz. \" class=\"wp-image-72973\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1.png 819w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-300x126.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-625x263.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-179x75.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-768x323.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-645x271.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-500x210.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-160x67.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-362x152.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/heat-map-distance-from-origin-1-262x110.png 262w\" sizes=\"(max-width: 819px) 100vw, 819px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 13. Heat map of distance from the origin, where green is closest to the optimum</em></em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Energy will continue to be a precious commodity for the foreseeable future. We have shown here and in our previous post that the <a href=\"https://www.nvidia.com/en-us/data-center/solutions/accelerated-computing/\">NVIDIA accelerated computing platform</a> of hardware and software is vastly more performant for multi-node simulations using VASP for medium and large-sized molecular hybrid-DFT calculations&nbsp; compared to both a CPU-only platform as well as an accelerated platform that uses MPI only.</p>\n\n\n\n<p>Based on these results, we urge researchers using VASP for anything but the smallest systems to do so using the NVIDIA accelerated platform. This approach uses less energy per expended kWh and enables more work per unit of time.</p>\n\n\n\n<p>The results of this investigation show that the energy use of simulations varies by more than two orders of magnitude depending on the atom count. However, the optimization opportunity is not as large as the total use.</p>\n\n\n\n<p>The energy savings opportunity associated with using NVIDIA Magnum IO NCCL ranges from 41 kWh in the 216-atom case at 128 nodes at best time to solution (1,410 MHz for the A100 GPU) to 724 kWh per simulation for the 768-atom case at 128 nodes. Running for best energy to solution (1,250 MHz) doesn&#8217;t change the number materially for 216 atoms, and drops the difference between NCCL enabled and disabled to 709 kWh per simulation for 768 atoms.</p>\n\n\n\n<p>In order of most energy savings for minimum impact to runtime, our recommendation for running multi-node, multi-GPU simulations for large VASP systems follows:</p>\n\n\n\n<ol>\n<li>Run VASP on the NVIDIA accelerated GPU platform rather than on CPUs only.</li>\n\n\n\n<li>Use NVIDIA Magnum IO NCCL to maximize parallel efficiency.</li>\n\n\n\n<li>Run a 216-atom hybridDFT calculation between 16 and 64 nodes (128 to 512 A100 GPUs); more for larger systems, and less for smaller.</li>\n\n\n\n<li>Run at the MaxQ point of 1,250 MHz (GPU clock) to get the last 5-10% energy savings.</li>\n</ol>\n\n\n\n<p>Looking beyond the hybrid DFT in VASP analyzed in this post, software developers can also conserve energy by (in descending order of impact):</p>\n\n\n\n<ol>\n<li>Accelerating applications with GPUs</li>\n\n\n\n<li>Optimizing as much as possible, including hiding unproductive, but necessary parts</li>\n\n\n\n<li>Having users run at optimized frequencies</li>\n</ol>\n\n\n\n<p>For more information about NCCL and NVIDIA Magnum IO, watch the GTC sessions, <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51111/\">Scaling Deep Learning Training: Fast Inter-GPU Communication with NCCL</a> and <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52087/\">Optimizing Energy Efficiency for Applications on NVIDIA GPUs</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Computational energy efficiency has become a primary decision criterion for most supercomputing centers. Data centers, once built, are capped in terms of the amount of power they can use without expensive and time-consuming retrofits. Maximizing insight in the form of workload throughput then means maximizing workload per watt. NVIDIA products have, for several generations, focused &hellip; <a href=\"https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/\">Continued</a></p>\n", "protected": false}, "author": 1584, "featured_media": 74069, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1296408", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/272701", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [1937, 453, 608, 53, 99, 1914], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/hfo2-332-polyhedra-resized-002_16x9.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iUY", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72724"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1584"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72724"}], "version-history": [{"count": 97, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72724/revisions"}], "predecessor-version": [{"id": 74070, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72724/revisions/74070"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/74069"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72724"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72724"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72724"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 73001, "date": "2023-11-09T16:16:46", "date_gmt": "2023-11-10T00:16:46", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=73001"}, "modified": "2023-11-16T11:16:41", "modified_gmt": "2023-11-16T19:16:41", "slug": "enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/", "title": {"rendered": "Enabling Greater Patient-Specific Cardiovascular Care with AI Surrogates"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>A Stanford University team is transforming heart healthcare with near real-time cardiovascular simulations driven by the power of AI. Harnessing physics-informed machine learning surrogate models, the researchers are generating accurate and patient-specific blood flow visualizations for a non-invasive window into cardiac studies. The technology has far-reaching scope, from evaluating coronary artery aneurysms to pioneering new surgical methods for congenital heart disease and enhancing medical device efficacy. With enormous potential in advancing cardiovascular medicine, the work could offer innovative methods for combating the leading cause of death in the US.&nbsp;</p>\n\n\n\n<p>Cardiovascular simulations are important enablers for patient-specific treatment for several heart-related ailments. 3D computational fluid dynamics (CFD) simulations of the blood flow using finite-element methods are a computationally challenging task, especially in clinical practice. As an alternative, physics-based reduced-order models (ROMs) are often employed due to their increased efficiency.&nbsp;</p>\n\n\n\n<p>However, such ROMs rely on simplified assumptions of vessel geometry, complexity, or simplified mathematical models. They often fail to model the quantities of interest, such as pressure losses at vascular junctions accurately, which would otherwise require full 3D simulations. Conventional data-driven reduced-order approaches, such as projection-based methods, don&#8217;t offer much flexibility with respect to changes to the domain geometry, which is crucial for patient-specific cardiovascular simulations.&nbsp;</p>\n\n\n\n<p>As an alternative, employing deep learning-based surrogates can model these complex physical processes. Physics-informed machine learning (physics-ML) enables training deep learning models that can offer both computational efficiency as well as flexibility, through parameterizable models. Recently, graph neural network (GNN) based architectures have been proposed to build physics-ML models for emulating mesh-based simulations. This approach offers generalization over different meshes, boundary conditions, and other input parameters, which makes it a perfect candidate for patient-specific cardiovascular simulations.</p>\n\n\n\n<p>The research team from Stanford University leveraged <a href=\"https://github.com/NVIDIA/modulus-launch/tree/main/examples/cfd/vortex_shedding_mgn\">MeshGraphNet</a>, a graph neural network (GNN)-based architecture, to devise a one-dimensional Reduced Order Model (1D ROM) for simulating blood flow. The team implemented this approach into NVIDIA Modulus, a platform equipped with an optimized implementation of MeshGraphNet. The reference MeshGraphnet implementation in NVIDIA Modulus brings several code optimizations such as data parallelism, model parallelism, gradient checkpointing, <a href=\"https://github.com/rapidsai/cugraph\">cuGraphs</a>, and multi-GPU and multi-node training. All of which are useful for the development of GNNs for cardiovascular simulation.</p>\n\n\n\n<p>&nbsp;<a href=\"https://developer.nvidia.com/blog/physics-ml-platform-modulus-is-now-open-source\">NVIDIA Modulus is an open-source framework</a> geared towards the development of physics-informed machine learning models. It enhances the efficacy of high-fidelity, complex multi-physics simulations by enabling the exploration and development of surrogate models. This framework enables seamless integration of datasets with first principles, whether described by the governing partial differential equations or other system attributes, such as physical geometry and boundary conditions. Furthermore, it provides the capability to parameterize the input space, fostering the development of parameterized surrogate models critical for applications like digital twins.&nbsp;</p>\n\n\n\n<p>NVIDIA Modulus provides a variety of reference applications across domains from CFD, thermal, structural, and electromagnetic simulations, and can be used for numerous industry solutions ranging across climate modeling, manufacturing, automotive, healthcare, and more. These examples can serve as a foundation for the work of researchers, such as vortex shedding, in this case for the Stanford research team.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Modeling cardiovascular simulation as an AI problem</h2>\n\n\n\n<p>In this research study, the Stanford team&#8217;s objective was to develop a one-dimensional physics-ML model. They chose GNNs for developing a surrogate that infers the pressure and flow rate along the centerline of compliant vessels. The team used \u200cgeometry from 3D vascular models and generated a directed graph consisting of a set of nodes along the centerline of the geometry.&nbsp;</p>\n\n\n\n<p>The nodes and edges of the graph capture the state of the system at a given time. For instance, the cross-sectional average pressure, flow rate, and area of the vessel&#8217;s lumen along the centerlines of the vessel are included as \u200cnode features. The GNN takes the state of the system at time t and infers the state of the system at the next time step. This can be iteratively applied to a rollout for simulating the cardiovascular cycle as shown in Figure 2.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"729\" height=\"394\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver.png\" alt=\"The dataset preparation includes 3D simulation data from the Finite Element solver, averaged over the cross-section slice for 1D centerline representation, and the generation of a directed graph.\" class=\"wp-image-73003\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver.png 729w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-300x162.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-625x338.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-179x97.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-645x349.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-500x270.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-362x196.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Finite-Element-solver-204x110.png 204w\" sizes=\"(max-width: 729px) 100vw, 729px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The steps involved in modeling the simulation problem as a graph representation</em></figcaption></figure></div>\n\n\n<p>The boundary conditions at the inlet (red) and outlet (yellow) are required for determining the hemodynamics in the vessels. These are modeled as special edges of the graph to account for the effect and the complexity of these boundary conditions. Boundary condition parameters, one-hot vector encoding for differentiating between nodes (branch, junction, inlet, outlet), and minimum diastolic and maximum systolic pressure in the cardiac cycle are also included as node features.&nbsp;</p>\n\n\n\n<p>For the details on the modeling of the physics, the mapping of graph features, and boundary conditions, refer to the paper <a href=\"https://arxiv.org/pdf/2303.07310.pdf\">Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1652\" height=\"900\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation.png\" alt=\"Schematics of MeshGraphNet, rollout phase to update the state of the system and the encoder, processor, and decoder of the MeshGraphnet architecture.\" class=\"wp-image-73004\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation.png 1652w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-300x163.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-625x340.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-768x418.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-1536x837.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-645x351.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-500x272.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-362x197.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-202x110.png 202w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/MeshGraphNet-computation-1024x558.png 1024w\" sizes=\"(max-width: 1652px) 100vw, 1652px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Three steps of MeshGraphNet computation\u2014encode, process, and decode that are iteratively executed to the rollout to get simulation results&nbsp;</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">AI surrogate: Dataset, architecture, and experiments</h2>\n\n\n\n<p>The researchers used models from the <a href=\"http://www.vascularmodel.com\">Vascular model </a>repository, containing about 252 cardiovascular models. They picked eight models, capturing typically challenging scenarios. Figure 3 shows three of the eight models: an aortofemoral model featuring an aneurysm (Model 1), a healthy pulmonary model (Model 2), and an aorta model affected by coarctation (Model 3). These include features such as multiple junctions (Model 2) or stenoses (Model 3), which can be challenging to handle using 1D physics-based models.</p>\n\n\n\n<p>Data was generated using 3D finite-element simulations of the unsteady Navier-Stokes flow. This was done using the <a href=\"http://www.simvascular.org\">SimVascular software suite</a>. The simulation data was then transformed into a 1D centerline representation by averaging the quantities of interest along orthogonal sections at each node.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1886\" height=\"736\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository.png\" alt=\"Model 1 shows aortofemoral model affected by an aneurysm. Model 2 shows a healthy pulmonary model. Model 3 shows an aorta model affected by coarctation.\" class=\"wp-image-73005\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository.png 1886w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-300x117.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-625x244.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-768x300.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-1536x599.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-645x252.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-500x195.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-362x141.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-282x110.png 282w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Cardiovascular-models-Vascular-Model-Repository-1024x400.png 1024w\" sizes=\"(max-width: 1886px) 100vw, 1886px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Cardiovascular models from the Vascular Model Repository that were considered as part of the dataset</em></figcaption></figure></div>\n\n\n<p>Simulations were done with different boundary conditions and for each geometry. Two cardiac cycles were simulated using 50 random configurations of boundary conditions to generate the training dataset. You can download the training dataset using the scripts from the <a href=\"https://github.com/NVIDIA/modulus-launch/tree/main/examples/healthcare/bloodflow_1d_mgn\">NVIDIA Modulus GitHub repo</a>.</p>\n\n\n\n<p>The MeshGraphNet architecture was chosen to model the system and was modified to suit the cardiovascular simulations. Like the base architecture, the model consists of three components: an encoder, a processor, and a decoder (Figure 2). The encoder transforms the node and edge features into latent representations using a fully connected neural network (FCNN).&nbsp;</p>\n\n\n\n<p>Then the processor performs several rounds of message passing along edges, updating both node and edge embeddings. The decoder extracts nodal pressure and flow rate at each node, which is used to update the mesh in an auto-regressive manner. For the exact details of the Meshgraphnet architecture, you can refer to the <a href=\"https://arxiv.org/abs/2010.03409v4\">Learning Mesh-Based Simulation with Graph Networks</a>.</p>\n\n\n\n<p>The team ran various experiments to analyze:</p>\n\n\n\n<ul>\n<li>The convergence of the rollout error as a function of the dataset size.&nbsp;</li>\n\n\n\n<li>Sensitivity analysis to evaluate which node and edge features were more important to the accuracy of predicting the flow rate and pressure</li>\n\n\n\n<li>Direct comparisons with physics-driven one-dimensional models \u200cshowed superior performance of the GNN, especially when handling complex geometries such as those with many junctions (Figure 4).</li>\n\n\n\n<li>Different approaches to train the algorithm: training networks specific to different cardiovascular regions instead of a single network able to handle different geometries (GNN-A vs GNN-Bg in Figure 4).&nbsp;</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1112\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models.png\" alt=\"The figure plots the pressure and flow rate predicted by two of the GNN models, the 1D physics-based ROM, and how they compare to the ground truth for a complex model with stenosis.\" class=\"wp-image-73006\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-300x167.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-625x348.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-768x427.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-1536x854.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-645x359.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-500x278.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-362x201.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-198x110.png 198w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/1D-ROM-GNN-models-1024x570.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Comparison between the 1D physics-based ROM and two GNN models against the ground truth</em></figcaption></figure></div>\n\n\n<p>The research team is continuing to explore further work related to the generalization of the GNN to more geometries as well as exploring the optimal feature set for improving the performance. Furthermore, they are looking to extend this work to 3D models but also to leverage such physics-ML models into their Simvascular software suite to accelerate simulations.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Using NVIDIA Modulus for your research</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/physics-ml-platform-modulus-is-now-open-source/\">NVIDIA Modulus</a> is an open-source project under the Apache 2.0 license to support the growing physics-ML community. If you are an AI researcher working in the field of physics-informed machine learning and want to get started with NVIDIA Modulus go to the <a href=\"https://github.com/NVIDIA/modulus\">Modulus GitHub repo.</a>\u00a0</p>\n\n\n\n<p>If you would like to contribute your work to the project, follow the <a href=\"https://github.com/NVIDIA/modulus/blob/main/CONTRIBUTING.md\">contribution guidelines</a> in the project or reach out to the <a href=\"https://github.com/NVIDIA/modulus-launch/blob/main/examples/cfd/ahmed_body_mgn/modulus-team@nvidia.com\">NVIDIA Modulus team</a>.</p>\n\n\n\n<p>NVIDIA is celebrating developer contributions across use cases, demonstrating how to build and train physics-ML models using the NVIDIA Modulus framework. Equally important is the effort to systematically organize such innovative work in the Modulus open-source project for the community and the ecosystem to leverage for their engineering and science surrogate modeling problems.&nbsp;</p>\n\n\n\n<p>To learn more about how Modulus is being used by the industry, you can refer to the Modulus <a href=\"https://resources.nvidia.com/l/en-us-modulus-pathfactory-explore-page\">resources page.</a></p>\n", "protected": false}, "excerpt": {"rendered": "<p>A Stanford University team is transforming heart healthcare with near real-time cardiovascular simulations driven by the power of AI. Harnessing physics-informed machine learning surrogate models, the researchers are generating accurate and patient-specific blood flow visualizations for a non-invasive window into cardiac studies. The technology has far-reaching scope, from evaluating coronary artery aneurysms to pioneering new &hellip; <a href=\"https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/\">Continued</a></p>\n", "protected": false}, "author": 1915, "featured_media": 73019, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1294842", "discourse_permalink": "https://forums.developer.nvidia.com/t/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/272377", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [1949, 453, 3052, 1948, 3281], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/AI-Surrogates-Modulus.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-iZr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73001"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1915"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=73001"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73001/revisions"}], "predecessor-version": [{"id": 73204, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/73001/revisions/73204"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73019"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=73001"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=73001"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=73001"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71512, "date": "2023-11-09T12:17:05", "date_gmt": "2023-11-09T20:17:05", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71512"}, "modified": "2023-12-05T10:56:29", "modified_gmt": "2023-12-05T18:56:29", "slug": "accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/", "title": {"rendered": "Accelerating Neurosymbolic AI with RAPIDS and Prometheux Vadalog Parallel"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>As the scale of available data continues to grow, so does the need for scalable and intelligent data processing systems to swiftly harness useful knowledge. Especially in high-stakes domains such as life sciences and finance, alongside scalability, transparency of data-driven processes becomes paramount to ensure the utmost trustworthiness.</p>\n\n\n\n<p>Started by scientists coming from the Knowledge Graph labs across the University of Oxford and TU Wien, Prometheux, an NVIDIA Inception company, builds AI capable of explaining its exact logical process. From drug repurposing for AstraZeneca to financial data processing with the Applied Research Team of the Central Bank of Italy, Prometheux technology powers highly scalable and explainable reasoning over some of the world&#8217;s largest knowledge graphs.</p>\n\n\n\n<p>Prometheux has seamlessly integrated the <a href=\"https://resources.nvidia.com/en-us-spark\" data-type=\"link\" data-id=\"https://resources.nvidia.com/en-us-spark\" target=\"_blank\" rel=\"noreferrer noopener\">RAPIDS accelerator for Apache Spark</a> into their proprietary knowledge graph management system, <a href=\"https://www.nvidia.com/en-us/gpu-accelerated-applications/?search=prometheux\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/gpu-accelerated-applications/?search=prometheux\" target=\"_blank\" rel=\"noreferrer noopener\">Vadalog Parallel</a>, to leverage NVIDIA GPUs. They achieve significant speedups and cost savings for their clients when processing large knowledge graphs with hundreds of millions of entities and billions of relations.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Knowledge graphs and reasoning</h2>\n\n\n\n<p>Over the last decades, due to the ever-growing scale of available data, there has been a rapid rise in the popularity of large enterprise knowledge graphs. And a corresponding rise in the scalable and intelligent processing systems with which to exploit them.</p>\n\n\n\n<p>Knowledge graphs serve as a backbone for data integration and provide a common representation structure to enable query answering across large data sources. A <em>knowledge graph</em> can be defined as a semi-structured data model composed of the following:</p>\n\n\n\n<ul>\n<li>An <strong>extensional component</strong>: Existing entities and relations integrating knowledge from heterogeneous data sources.</li>\n\n\n\n<li>An <strong>intensional component</strong>: Domain knowledge, either in the form of statistical and ML models or defined declaratively with logical rules.</li>\n\n\n\n<li>A <strong>derived extensional component</strong>: Produced through the application of domain knowledge to the extensional component in the so-called reasoning process.</li>\n</ul>\n\n\n\n<p>From intricate company ownership graphs to protein interaction networks, knowledge graphs provide a concise and intuitive abstraction for a variety of domains and can be used to model real-world entities and their interrelations.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Vadalog Parallel</h2>\n\n\n\n<p>Resembling the interplay of intuition and logical thinking in human intelligence, AI is also moving towards <em>neurosymbolic</em>, a synergistic combination of ML and logic-based reasoning.</p>\n\n\n\n<p>Building towards neurosymbolic AI, Prometheux provides Vadalog Parallel, its knowledge graph management system (KGMS) that offers a deductive fully explainable framework. This KGMS combines data with domain logic and automates complex reasoning tasks over large knowledge graphs with high scalability and transparency at its core.</p>\n\n\n\n<p>Vadalog Parallel acts as middleware between heterogeneous enterprise data sources and applications on top of them. It serves as a backbone for data integration without data migration. Encoding domain logic at a high level enables the rapid development of entirely new solutions without having to program extensive code or design algorithms.</p>\n\n\n\n<p>Vadalog Parallel offers database-agnostic compatibility, seamlessly connecting to all major databases (RDBMS, RDF, and NoSQL, for example, Neo4j and Mongo, and so on), as well as diverse data sources (CSV, Parquet, JSON, and so on). It enables modeling and reasoning over graphs, hypergraphs, and simplicial complexes without any constraints on the arity, whether dealing with tuples, triples, or <em>n</em>-tuples.</p>\n\n\n\n<p>Domain logic is encoded declaratively at a high level, enabling domain scientists to directly extract insights from large knowledge graphs in an automated, explainable, and intuitive logical manner, saving time and computational resources.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"768\" height=\"691\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1.png\" alt=\"Graph  shows a portion of the billions of logical steps that led to an indication expansion prediction.\" class=\"wp-image-73131\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-300x270.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-625x562.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-645x580.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-333x300.png 333w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-100x90.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-362x326.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/logical-steps-to-prediction-768x691-1-122x110.png 122w\" sizes=\"(max-width: 768px) 100vw, 768px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Portion of the billions of logical steps that led to an indication expansion prediction</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"319\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1.png\" alt=\"The graph shows a human-understandable explanation of an indication expansion prediction. Other nodes and edges represent the logical steps that brought Vadalog Parallel to the prediction, for example, the potential treatment was deduced using compound similarity, disease similarities and known treatments. Compound similarity, in turn, was deduced using paths and genes.\" class=\"wp-image-73147\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1-300x153.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1-500x256.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Compact-logical-explanation-of-an-indication-expansion-prediction-1-215x110.png 215w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Compact logical explanation of an indication expansion prediction</em></figcaption></figure></div>\n\n\n<p>For each task, Vadalog Parallel natively provides the step-by-step, logical full explanation of the reasoning process. Up to billions of logical steps are computed at lightning speed, accompanied by a compact visual explanation to enable faster interactions with domain experts.&nbsp;</p>\n\n\n\n<p>Thanks to its expressive framework and its distributed processing, Vadalog Parallel achieves low computational complexity and scalability in practice and enables modeling complex domains with advanced data analysis features.&nbsp;</p>\n\n\n\n<p>Indeed, it enables efficient graph traversal and captures both regular path queries (for navigating graphs using pattern matching, for example, Cypher), and SPARQL under the OWL2 QL regime (for querying the semantic web). It supports counterfactual and temporal reasoning, full recursion, and existential quantification, unlike other state-of-the-art systems, and outperforms existing big data analytics tools.</p>\n\n\n\n<p>With the unprecedented scale of datasets that we encounter today, new requirements on the scalability and flexibility of reasoning engines have emerged so that AI techniques can execute sufficiently fast. To guarantee such desiderata, Prometheux precisely studies and develops compilation techniques that shift classical reasoning, inference methodologies, and tools onto Big Data platforms. </p>\n\n\n\n<p>Vadalog Parallel already achieves unmatched scalability even when operating solely on CPUs. However, when reasoning over some of the world&#8217;s largest knowledge graphs (with hundreds of millions of entities and billions of relations), GPUs become an essential asset.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Explainable drug repurposing and beyond</h2>\n\n\n\n<p>Vadalog Parallel finds <a href=\"https://ceur-ws.org/Vol-3485/paper9111.pdf\">diverse applications</a> in the field of computational biology, including its proven tangible impact on indication expansion.</p>\n\n\n\n<p>Prometheux enables life sciences organizations to scalably reason over large biological (and other kinds of) knowledge graphs in an automated and logically explainable manner. This enables more informed decisions and faster interactions with domain experts, resulting in accelerated pipeline development globally towards trustworthy precision medicine.</p>\n\n\n\n<p>Through the automated analysis of both proprietary and client datasets, Prometheux unravels hidden insights through logical reasoning and unlocks the potential of existing drugs for new therapeutic purposes. By computing billions of logical steps at lightning speed, Vadalog Parallel produces hundreds of successfully validated indications (and many more to study), effectively providing a dynamic and explainable recommendation system for indication expansion.</p>\n\n\n\n<p>The compact visual explanation of the recommendations enables faster interactions with domain experts. It leads to rapid adaptation to feedback bringing everyone from initial knowledge to novel indications in a fraction of the time of traditional techniques.</p>\n\n\n\n<p>In the following sections, we also provide an experimental analysis of Vadalog Parallel with NVIDIA GPUs for a series of reasoning tasks. We use Prometheux\u2019s internal pilot biological knowledge graph (BIO KG), a starting point for life sciences organizations that have yet to build their own knowledge graphs.</p>\n\n\n\n<p>BIO KG has ~4.7M data points spanning compounds, diseases, genes, biological pathways, symptoms, and more. Vadalog Parallel achieves up to 9x speedups with Spark RAPIDS.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Financial institutions</h2>\n\n\n\n<p>Another compelling application of Vadalog Parallel unfolds within the dynamic realms of finance and economics. Prometheux\u2019s knowledge graph-aided approach empowers an enhanced understanding of intricate interconnections between financial entities, whether they are institutions, companies, financial intermediaries, other types of shareholders, or transactions.</p>\n\n\n\n<p>It can be employed for FinTech, RegTech, SupTech, and InsurTech applications encoding international regulations, as well as other domain logic. The goal is to automatically reason over knowledge graphs and achieve AI-aided banking supervision, compliance checks, creditworthiness evaluation, anti-money laundering, fraud detection, shock propagation, company control, detection of takeovers, and more.</p>\n\n\n\n<p>This comprehensive approach empowers analysts to proactively manage risks, respond to challenges, optimize strategies, and foster financial stability.</p>\n\n\n\n<p>Also for this domain, we show Vadalog Parallel performances with NVIDIA GPUs when reasoning over a company ownership graph (Company KG).&nbsp;</p>\n\n\n\n<p>Company KG is a synthetically constructed knowledge graph reflecting the <a href=\"https://link.springer.com/article/10.1007/s00181-011-0512-x\">well-known topology of Italian companies</a>, counting 8M ownership edges between companies and shareholders. On such graphs, Vadalog Parallel achieves up to 3x speedups with Spark RAPIDS.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Strategy and solution design</h2>\n\n\n\n<p>The Vadalog Parallel architecture has the following key components to execute reasoning tasks efficiently on top of distributed frameworks such as Spark, Flink, GraphX, and more:</p>\n\n\n\n<ul>\n<li>Rule parsing</li>\n\n\n\n<li>Logic optimization</li>\n\n\n\n<li>Query planning</li>\n\n\n\n<li>Planner optimization</li>\n\n\n\n<li>Query compilation</li>\n</ul>\n\n\n\n<p>Overall, Vadalog Parallel exposes a reasoning API with the following interface:</p>\n\n\n\n<p><code>reason(kg_ref,domain_logic)</code></p>\n\n\n\n<p>A client application issues calls to the reasoning API specifying a reference to a knowledge graph <code>kg_ref</code> in which it activates the reasoning process. Vadalog Parallel connects to and handles repositories of knowledge graphs, each with unique identifiers<em> </em><code>kg_ref</code>. The reasoning engine encodes <code>domain_logic</code> into a set of distributed operations (narrow, wide transformations, that is, shuffling), and computes the answer to the reasoning task. It either expands the knowledge graph with new knowledge or materializes the output in the specified output data sources.</p>\n\n\n\n<p>To ensure even faster and reliable processing, Prometheux has seamlessly integrated Spark-RAPIDS into Vadalog Parallel going beyond traditional Spark capabilities.&nbsp;</p>\n\n\n\n<p>The RAPIDS Shuffle Manager, an integral part of Spark-rapids, provides a significant advantage by introducing custom mechanisms for exchanging shuffle data. This innovation offers two distinct modes of operation: Multi-Threaded and UCX, which can be configured to leverage GPU-to-GPU communication and RDMA capabilities. It unlocks unmatched levels of performance and efficiency for the reasoning tasks with Vadalog Parallel.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Scenarios and data</h2>\n\n\n\n<p>Navigating graphs can be very challenging without powerful recursion. Standard SQL, for instance, lacks native support for recursion.</p>\n\n\n\n<p>The Vadalog Parallel framework enables full recursion. In this post, we show how both graph traversal and graph analytics tasks are significantly sped up by integrating GPUs through Spark-RAPIDS.</p>\n\n\n\n<p>We categorized the tasks into the following distinct types:</p>\n\n\n\n<ul>\n<li><strong>Non-recursive:</strong> The execution plan for the set of operations is structured as a tree and each distributed operation is executed one time.</li>\n</ul>\n\n\n\n<ul>\n<li><strong>Recursive:</strong> The execution plan for the set of operations is structured as a graph and the set of distributed operations is applied until the knowledge graph has been exhaustively explored.</li>\n</ul>\n\n\n\n<p>In doing so, we provide the first assessment of NVIDIA Spark-RAPIDS with recursive operations.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Experimental results</h3>\n\n\n\n<p>Figure 3 shows the four knowledge graph analytics tasks containing recursive and non-recursive operations, two over the Bio KG and two over the Company KG. For all measurements, we ran each single experiment 10 times and averaged the results.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1214\" height=\"868\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1.png\" alt=\"Four charts compare CPU and GPU performance on knowledge graphs, with GPUs achieving between 2.6-9x speedups.\" class=\"wp-image-71632\" style=\"width:911px;height:651px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1.png 1214w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-300x214.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-625x447.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-161x115.png 161w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-768x549.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-645x461.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-420x300.png 420w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-126x90.png 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-750x535.png 750w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-362x259.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-154x110.png 154w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/experiemental-analysis-vadalog-1-1024x732.png 1024w\" sizes=\"(max-width: 1214px) 100vw, 1214px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Experimental analysis of Vadalog Parallel reasoning with GPUs using Spark-RAPIDS</em></figcaption></figure></div>\n\n\n<p>Figure 3 shows four reasoning tasks over the knowledge graphs introduced earlier. We provide the relevant experimental evaluation with particular attention to the speedups achieved with NVIDIA GPUs and Spark RAPIDS.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Hardware and software configuration</h4>\n\n\n\n<p>In all the experimental analyses, Vadalog Parallel was executed on top of a Spark 3.3.2 standalone cluster integrated with Spark-RAPIDS v23.08.1, with CUDA v12.0 and Java v8 as Spark language. The cluster was locally installed on an Amazon EC2 AMI p3.16xlarge with 64 vCPU 8 GPU, 480 GB RAM, and eight NVIDIA V100 Tensor Cores, each having 16 GB of GPU core memory.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Test setup</h4>\n\n\n\n<p>All the tasks were required to perform the following steps:&nbsp;</p>\n\n\n\n<ol>\n<li>Call the Vadalog Parallel reasoning API passing in input the domain logic describing the task and the reference to one of the two knowledge graphs.&nbsp;</li>\n\n\n\n<li>Establish the connection with the Spark cluster.</li>\n\n\n\n<li>Extract the input subgraph from the specific knowledge graph.</li>\n\n\n\n<li>Execute the task.</li>\n\n\n\n<li>Write the output in the Parquet file.</li>\n</ol>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>KG</strong></td><td><strong>Edges</strong></td><td><strong>Task</strong></td><td><strong>Description</strong></td><td><strong>Operations</strong></td><td><strong>Reasoning times (sec)</strong></td></tr><tr><td><br>BIO KG</td><td><br>~4.7M</td><td>Compound/Gene/Disease Similarity</td><td><br>Determine the pairwise similarity for genes, compounds, and diseases based on common features</td><td>Non-recursive Wide Transformations: 3 Joins, 3 Aggregations;<br>Non-recursive Narrow Transformations: 3 Maps, 3 Filters;</td><td><br><br>CPU: 386<br>GPU: 43</td></tr><tr><td><br><br>BIO KG</td><td><br><br>~4.7M</td><td><br><br>Guilt by Association</td><td><br>Recommend new indications for each compound based on similar ones treating a set of similar diseases&nbsp;</td><td>Non-recursive Wide Transformations: 11 Joins, 11 Aggregations;<br>Non-recursive Narrow Transformations: 52 Maps, 26 Filters;</td><td><br><br>CPU: 520<br>GPU: 220</td></tr><tr><td><br><br><br><br><br><br>Company KG</td><td><br><br><br><br><br><br><br>~8M</td><td><br><br><br><br><br>All Company-To-Company Links</td><td><br><br><br><br><br><br>Determine the pairwise connectivity between all nodes</td><td>Non-recursive Wide Transformations: 1 Aggregation;<br>Non-recursive Narrow Transformations: 2 Maps, 1 Filters;<br>Recursive Wide Transformations: 1 Join, 2 Aggregations;<br>Recursive Narrow Transformations: 4 Maps, 4 Filters</td><td><br><br><br><br><br><br><br><br>CPU: 141<br>GPU: 46</td></tr><tr><td><br><br><br><br>Company KG</td><td><br><br><br><br><br>~8M</td><td><br><br><br><br>Company Control</td><td><br><br><br>Find all pairs of controllers for each company</td><td>Non-recursive Wide Transformations: 2 Aggregations;<br>Non-recursive Narrow Transformations: 2 Maps;<br>Recursive Wide Transformations: 1 Join, 1 Aggregation;<br>Recursive Narrow Transformations: 1 Map</td><td><br><br><br><br><br>CPU: 94<br>GPU: 36</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Comparisons of CPU and GPU performance on knowledge graph-based predictions</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>In this post, we discussed the rapid rise in the popularity of reasoning with logic over large enterprise knowledge graphs as well as the scalable and intelligent processing systems to exploit them. We showcased Vadalog Parallel, Prometheux&#8217;s knowledge graph management system. Vadalog Parallel is a powerful framework to combine data with domain logic and automate complex reasoning tasks. It\u2019s a solution advancing us toward <em>neurosymbolic AI</em>, a synergistic combination of ML and logic-based reasoning.&nbsp;</p>\n\n\n\n<p>We also discussed the application of Vadalog Parallel in the financial and life sciences realm. The integration of RAPIDS enables significant speedups and cost savings leveraging NVIDIA GPUs when processing some of the world\u2019s largest knowledge graphs.</p>\n\n\n\n<p>For more information about <a href=\"https://www.linkedin.com/company/prometheux\" target=\"_blank\" rel=\"noreferrer noopener\">Prometheux</a>, contact them via <a href=\"mailto:business@prometheux.co.uk\">email</a>. For Spark 3.0 and RAPIDS, see the <a href=\"https://forums.developer.nvidia.com/tag/rapids\">RAPIDS developer forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As the scale of available data continues to grow, so does the need for scalable and intelligent data processing systems to swiftly harness useful knowledge. Especially in high-stakes domains such as life sciences and finance, alongside scalability, transparency of data-driven processes becomes paramount to ensure the utmost trustworthiness. Started by scientists coming from the Knowledge &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/\">Continued</a></p>\n", "protected": false}, "author": 1575, "featured_media": 71637, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1294756", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/272360", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 1968], "tags": [278, 453, 3550], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/knowledge-graph-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iBq", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71512"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1575"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71512"}], "version-history": [{"count": 24, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71512/revisions"}], "predecessor-version": [{"id": 73159, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71512/revisions/73159"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71637"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71512"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71512"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71512"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72570, "date": "2023-11-08T13:30:00", "date_gmt": "2023-11-08T21:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72570"}, "modified": "2023-11-16T11:36:02", "modified_gmt": "2023-11-16T19:36:02", "slug": "new-workshop-rapid-application-development-using-large-language-models", "status": "publish", "type": "post", "link": "https://nvda.ws/3MtIen0", "title": {"rendered": "New Workshop: Rapid Application Development Using Large Language Models"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Interested in developing LLM-based applications? Get started with this exploration of the open-source ecosystem.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Interested in developing LLM-based applications? Get started with this exploration of the open-source ecosystem.</p>\n", "protected": false}, "author": 1466, "featured_media": 72575, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/3MtIen0", "_links_to_target": "_blank"}, "categories": [1050, 3110], "tags": [3312, 2964, 1935, 453, 2136, 2932, 61, 369, 2143], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/rapid-application-development-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iSu", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72570"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72570"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72570/revisions"}], "predecessor-version": [{"id": 72585, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72570/revisions/72585"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72575"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72570"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72570"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72570"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71819, "date": "2023-11-08T09:55:53", "date_gmt": "2023-11-08T17:55:53", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71819"}, "modified": "2023-11-16T11:16:42", "modified_gmt": "2023-11-16T19:16:42", "slug": "whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx/", "title": {"rendered": "Whole Human Brain Neuro-Mapping at Cellular Resolution on NVIDIA DGX"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Whole human brain imaging of 100 brains at a cellular level within a 2-year timespan, and subsequent analysis and mapping, requires accelerated supercomputing and computational tools. This need is well matched by NVIDIA technologies, which range across hardware, computational systems, high-bandwidth interconnects, domain-specific libraries, accelerated toolboxes, curated deep-learning models, and container runtimes. NVIDIA accelerated computing spans the <a href=\"https://colab.humanbrain.in/index.html\">IIT Madras Brain Centre\u2019s</a> technology journey from solution building, rollout, optimization, management, and scaling.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/W4I4r3ctlUc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. The Sudha Gopalakrishnan Brain Center</em></figcaption></figure>\n\n\n\n<p>Cellular resolution imaging is readily possible today for smaller brains of insects like <a href=\"https://www.science.org/doi/10.1126/science.add9330\">flies</a> and mammals like <a href=\"https://www.nature.com/articles/nature13186\">mice</a> and <a href=\"https://elifesciences.org/articles/40042\">small monkeys</a>. However, the process of acquiring, converting, processing, analyzing, and interpreting, turns into an even more arduous, skill\u2013 and time-intensive activity for whole human brains, which are orders of magnitude larger and more complex.</p>\n\n\n\n<p>The key big data characteristics of IITM Brain Centre\u2019s imaging pipeline are volume<strong> </strong>and velocity. At a scanning rate of 250 GB per hour per scanner, operating multiple scanners simultaneously, the center generates 2 TB per hour of high-resolution uncompressed images. All the images must be processed to map every imaged cell. For a computer vision object detection model, the equivalent incidence rate is approximately 10K objects per second.</p>\n\n\n\n<p>Handling such large-scale primary neuroanatomy data needs mathematical and computational methods to reveal the complex biological principles governing brain structure, organization, and developmental evolution, at multiple spatio-temporal scales. </p>\n\n\n\n<p>This important and challenging scientific pursuit involves the analysis of carefully acquired cellular-resolution brain images for establishing quantitative descriptions of the following: </p>\n\n\n\n<ul>\n<li>Spatial layout</li>\n\n\n\n<li>Cellular composition</li>\n\n\n\n<li>Neuronal pathways</li>\n\n\n\n<li>Compartmental organization</li>\n\n\n\n<li>Whole-brain architecture</li>\n</ul>\n\n\n\n<p>It extends to studying inter-brain similarities and relationships at all these levels.</p>\n\n\n\n<p>The new Brain Centre of IIT Madras has taken up this challenge and is powering a large-scale, multi-disciplinary effort to map more than 100 human brains at a cellular level. Using their proprietary technology platform, the center is imaging post-mortem human brains of different types and ages. </p>\n\n\n\n<p>Their goal is to create an unprecedented cell resolution, uniformly studied, digital collection of multiple types of human brains, which will be queryable from cell level to whole brain level. This requires the capability to enumerate 100B neurons per brain, over 100 brains, and the connectivity across different brain regions.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Meeting the computational demands of cellular resolution brain imaging</h2>\n\n\n\n<p>The center has developed a world-class computing platform to store, address, access, process, and visualize such high-resolution digital human brain data at the scale of 100+ petabytes, through nothing more than a web browser interface.&nbsp;</p>\n\n\n\n<p>This endeavor can be related to the mapping of multiple whole planets to uniformly study patterns, trends, and differences through high-resolution, cross-sectional imaging data through the planet\u2019s volume. Satellite images of the earth\u2019s surface would reach terabytes, a manageable size in today\u2019s computers and web browsers. These geospatial rendering techniques power the likes of Google Maps and others. </p>\n\n\n\n<p>However, volumetric imaging of whole brains at cellular resolution yields petabytes of digital data per brain, a challenge to visualize, process, analyze, and query over a web interface.</p>\n\n\n\n<p>The computational challenges behind the scenes are equally tremendous:&nbsp;</p>\n\n\n\n<ul>\n<li>A human-in-the-loop large image data pipeline</li>\n\n\n\n<li>Automation right from indexing the data from multiple concurrent imaging systems</li>\n\n\n\n<li>Transporting the images to a central uniform parallel file storage cluster</li>\n\n\n\n<li>Encoding in a format convenient for random access at an arbitrary scale</li>\n\n\n\n<li>Machine learning models for multiple automated tasks like the detection of tissue outlines</li>\n\n\n\n<li>Quality control for imaging</li>\n\n\n\n<li>Deep learning tasks for large image normalization</li>\n\n\n\n<li>Cellular-level object detection, classification, and region delineation</li>\n\n\n\n<li>Advanced mathematical models for the geometric alignment of images across modalities and resolutions, followed by computational geometry for deriving quantitative informatics</li>\n\n\n\n<li>Formatting into a high-volume rapid retriable metadata and informatics data store that can transform the prohibitive to the possible: performing single-cell to whole-brain queries on demand and reverting with timely answers for a web-based interaction</li>\n</ul>\n\n\n\n<p>To contemplate the scale of the challenge, look at just one of these tasks, a seemingly well-posed task of object detection. This is a task for which modern deep learning convolutional neural networks are known to perform quite well, achieving or in some cases exceeding human-level abilities in identifying and labeling objects.&nbsp;</p>\n\n\n\n<p>However, these models are trained to work on megapixel images, containing a few tens of objects. A single-cell resolution image is multi-gigapixel and can contain millions of distinct objects. Processing such large data requires specialized computational workhorses.&nbsp;</p>\n\n\n\n<p>This is why the Brain Centre has turned to NVIDIA, a leader in GPU-based <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">HPC</a> offerings, and has operationalized a cluster of DGX A100 systems to do the complete processing of 10 to 20 brains. As the center scales to 100 brains and more, they look to a DGX SuperPOD to provide scalable performance, with industry-leading computing, storage, networking, and infrastructure management. </p>\n\n\n\n<p>With eight <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPUs</a> per DGX node, the same data that requires a minimum of 1 hour to detect cells has been reduced to less than 10 minutes on an NVIDIA DGX. This enables whole-brain analysis in a month&#8217;s time frame, and scaling to 100 brains becomes practical.</p>\n\n\n\n<p>&#8220;I am delighted to see IITM Brain Centre collaborating with NVIDIA in tackling this challenge of analyzing the very large and complex cellular-level human brain data we are generating,&#8221; said Kris Gopalakrishan, a distinguished alumnus of IIT Madras and co-founder of Infosys. He played a pivotal role in setting up and supporting the IITM Brain Centre. &#8220;By working with an industry leader like NVIDIA, we look forward to creating breakthroughs in this area leading to global impact.&#8221;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Solving the computation challenges</h2>\n\n\n\n<p>A gigapixel whole slide image making up 0.25 million tiles of 256 X 256 images takes just 420 seconds for inferencing on an A100 GPU. It was possible through end-to-end pipeline optimization using NVIDIA libraries and application frameworks:&nbsp;</p>\n\n\n\n<ul>\n<li>Accelerated tile creation and batching are performed using NVIDIA DALI.&nbsp;</li>\n\n\n\n<li>The MONAI Core model is optimized by TensorRT.&nbsp;</li>\n\n\n\n<li>The TensorRT plan file selects a mix of INT8, FP16, and TF32 at different parts of the network, and produces the engine.&nbsp;</li>\n\n\n\n<li>Three engines are put in one A100 GPU for distributed inferencing.</li>\n\n\n\n<li>The NVIDIA accelerated image processing library cuCIM is used for accelerated image registration.&nbsp;</li>\n\n\n\n<li>NVIDIA IndeX is used for multi-GPU volume visualization for various zoom levels. Soon, MONAI Label\u2019s AI-assisted annotation, along with NVIDIA federated learning SDK-Flare, will be used to further refine various other MONAI core models and the pipeline will be deployed using MONAI Deploy.</li>\n</ul>\n\n\n\n<p>\u201cThe NVIDIA technology stack empowers the pioneers at the IIT Madras Brain Centre to effectively address the computational needs for high-resolution brain imaging at the cellular level, thereby propelling forward neuroscience research on a national and global scale,\u201d said Vishal Dhupar, managing director of South Asia at NVIDIA.</p>\n\n\n\n<p>MONAI and TensorRT are available with NVIDIA AI Enterprise, which is included with the NVIDIA DGX infrastructure.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1125\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai.png\" alt=\"Diagram shows different stages of the NVIDIA accelerated computing workflow for medical imaging, from digital pathology scanner to MONAI Deploy.\" class=\"wp-image-71823\" style=\"width:1000px;height:563px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/accelerated-computing-workflow-medical-imaging-monai-1024x576.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1.</em><strong><em> </em></strong><em>NVIDIA accelerated computing workflow for medical imaging</em></figcaption></figure></div>\n\n\n<p>The compute capability of NVIDIA DGX systems, with dual 64-core CPUs and eight NVIDIA A100 GPUs with 640 GB of GPU memory, along with 2 TB of RAM and 30 TB of flash storage, represent the highest class of server compute available in a single 4U chassis.&nbsp;</p>\n\n\n\n<p>Further, the DGX is scalable. NVIDIA offers an ecosystem of software and networking to interconnect multiple DGX systems and match the scale and performance requirements of IITM\u2019s Brain Centre for data processing of pipelined batch jobs, as well as on-demand burst computations.</p>\n\n\n\n<p>The effective processing rate of a single NVIDIA A100 GPU for CNN inference as benchmarked on the Brain Centre data is 60 GB per hour (data in uint8, inference in FP16 precision), or 2.4 TB per hour over five DGX servers (40 A100 GPUs), which matches the current imaging rate. This makes pipelining of imaging and computation bottleneck-free. Owing to the scalability of the DGX compute nodes, any surge in data inflow rate can also be matched through scale-out growth.</p>\n\n\n\n<p>The A100 GPU is mostly targeted for deep learning training for large datasets and large models that might not fit in smaller GPU vRAM. In the context of the IITM Brain Centre, the A100 GPUs within the DGX systems are used in CNN inference, in a multi-engine per A100 GPU fashion, with the data being mapped out across multi-GPU and multi-node, for scaling from 1\u201340x across five DGX servers.&nbsp;</p>\n\n\n\n<p>This enables the handling of variable image sizes, corresponding to the physical size variability in human brains across ages from fetal to adult (1\u201332x scale variation). Also, the CPU compute capabilities and the storage type of the DGX A100 systems are well-used in the Brain Centre\u2019s compute pipeline for workloads that are CPU-intensive or data access or movement-intensive, as well as for remote visualization.&nbsp;</p>\n\n\n\n<p>The NVIDIA technology stack offers tools and optimized libraries for every step in the process, in a uniform format as container runtimes, facilitating the adoption with minimal effort and ensuring best practices and automated operation.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Transforming the landscape of deep learning in medical imaging</h2>\n\n\n\n<p>Deep learning technology used to focus on engineering the best methods or tuning at training time for incremental boosts in performance. It has now shifted to inference with proven foundational models across domains like computer vision (object detection, semantic and panoptic segmentation, DL-based image registration) and natural language. The results are realizing applications previously held as non-amenable for computational automation.&nbsp;</p>\n\n\n\n<p>The emphasis now is on implementing software guardrails around new applications powered by deep learning inference. Integrated hardware systems and software stacks are not just a convenience in this new direction but are a vehicle of scale and simplification. The NVIDIA technology stack is a way to leapfrog solution building, deployment, and scaling.</p>\n\n\n\n<p>A detailed map of the earth is today accessible for everyone and has emerged as a platform enabling new applications, and businesses. It now guides and shapes globally how we move about. The efforts of the IIT Madras Brain Centre are targeted to produce a similar, transformative platform that will yield new outcomes in brain science, shape and guide brain surgery and therapy, and expand our understanding of medicine\u2019s last frontier, the human brain.</p>\n\n\n\n<p>For more information, see the <a href=\"https://github.com/HTIC-Medical-Imaging/hbp_hover_net\">/HTIC-Medical-Imaging</a> GitHub repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Whole human brain imaging of 100 brains at a cellular level within a 2-year timespan, and subsequent analysis and mapping, requires accelerated supercomputing and computational tools. This need is well matched by NVIDIA technologies, which range across hardware, computational systems, high-bandwidth interconnects, domain-specific libraries, accelerated toolboxes, curated deep-learning models, and container runtimes. NVIDIA accelerated computing &hellip; <a href=\"https://developer.nvidia.com/blog/whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx/\">Continued</a></p>\n", "protected": false}, "author": 1895, "featured_media": 71820, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1293855", "discourse_permalink": "https://forums.developer.nvidia.com/t/whole-human-brain-neuro-mapping-at-cellular-resolution-on-nvidia-dgx/272223", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1235, 852], "tags": [453, 90], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/Whole-Human-Brain-Mapping.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iGn", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71819"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1895"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71819"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71819/revisions"}], "predecessor-version": [{"id": 73014, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71819/revisions/73014"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71820"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71819"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71819"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71819"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72467, "date": "2023-11-08T09:00:00", "date_gmt": "2023-11-08T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72467"}, "modified": "2023-11-24T10:36:30", "modified_gmt": "2023-11-24T18:36:30", "slug": "setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/", "title": {"rendered": "Setting New Records at Data Center Scale Using NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">Generative AI</a> is rapidly transforming computing, unlocking new use cases and turbocharging existing ones. <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large language models</a> (LLMs), such as OpenAI\u2019s GPT models and Meta\u2019s Llama 2, skillfully perform a variety of tasks on text-based content. These tasks include summarization, translation, classification, and generation of new content such as computer code, marketing copy, poetry, and much more.&nbsp;</p>\n\n\n\n<p>In addition, diffusion models, such as Stable Diffusion developed by Stability.ai, enable users to generate incredible images using simple text prompts.&nbsp;</p>\n\n\n\n<p>The capabilities enabled by state-of-the-art generative AI are considerable\u2014but so, too, is the amount of compute performance required to train them. LLMs, for example, have grown into hundreds of billions of parameters, with each model being trained on enormous amounts of data. The large and growing compute demands of training state-of-the-art LLMs are beyond the scope of a single GPU or even a single node packed with GPUs.</p>\n\n\n\n<p>Instead, training these LLMs requires accelerated computing at the scale of an entire data center. This is why the NVIDIA accelerated computing platform scales to many thousands of high-performance GPUs, interconnected with the highest-bandwidth networking fabrics, all efficiently orchestrated by carefully crafted software.</p>\n\n\n\n<p>MLPerf Training v3.1 is the latest edition of the long-running suite of AI training benchmarks, and serves as a trusted, peer-reviewed measure of AI training performance. After adding an LLM training benchmark in the prior round based on OpenAI\u2019s GPT-3 175B parameter model, this round sees the addition of a text-to-image generative AI training benchmark based on Stable Diffusion.</p>\n\n\n\n<p>Reflecting the rapid convergence of traditional <a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\">HPC</a> workloads with AI, MLPerf HPC v3.0 measures the training performance of AI models that are used for scientific computing applications. In this round, a protein structure prediction test using OpenFold was added, complementing the existing tests which cover climate atmospheric river identification, cosmology parameter prediction, and quantum molecular modeling.\u00a0</p>\n\n\n\n<p>In just one MLPerf Training round, NVIDIA demonstrated unprecedented performance and scalability for LLM training, tripling the submission scale and nearly tripling performance. This shattered the performance record previously set by the NVIDIA platform and <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPUs</a> just 6 months ago. With continued software improvements, the <a href=\"https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/\">NVIDIA MLPerf submissions</a> also boosted the per-accelerator performance of the H100 GPU, translating into faster time to train and lower cost to train.\u00a0</p>\n\n\n\n<p>NVIDIA also submitted results on the newly added text-to-image training benchmark, achieving \u200crecord performance both on a per-accelerator basis as well as at scale.</p>\n\n\n\n<p>In addition, NVIDIA set new performance records at scale on the DLRM-dcnv2, BERT-large, RetinaNet, and 3D U-Net workloads, extending the record-setting performance achieved by the NVIDIA platform and H100 GPUs in the prior round. For these benchmarks, collective operations were accelerated using <a href=\"https://www.nvidia.com/en-us/networking/quantum2/\">NVIDIA Quantum-2 InfiniBand</a> switches and in-network computing with NVIDIA SHARP to help achieve record performance at scale.&nbsp;</p>\n\n\n\n<p>Finally, NVIDIA also made its first MLPerf HPC submissions with H100 GPUs and ran all workloads and scenarios.&nbsp;</p>\n\n\n\n<p>The following sections take a closer look at these incredible results.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Supercharging GPT-3 175B training performance</h2>\n\n\n\n<p>In MLPerf Training v3.1, NVIDIA raised the bar for LLM training performance through both dramatically greater submission scale as well as software enhancements that achieved greater performance per GPU.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Record-setting performance and excellent scaling efficiency</h3>\n\n\n\n<p>NVIDIA made several large-scale LLM submissions at a maximum submission scale of 10,752 H100 GPUs. This tripled the maximum scale submitted in the prior round, representing the largest number of accelerators ever used in an MLPerf submission. NVIDIA achieved a time-to-train score of 3.92 minutes in its largest-scale submission, a 2.8x performance boost and a new LLM benchmark record.\u00a0</p>\n\n\n\n<p>In addition, NVIDIA partnered closely with Microsoft Azure on a joint LLM submission, also using 10,752 H100 GPUs and Quantum-2 InfiniBand networking, achieving a time to train of 4.01 minutes, nearly identical to that of the NVIDIA submission. Achieving this level of performance on two entirely different, giant-scale systems in the same MLPerf round is a singular technical achievement.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"986\" height=\"610\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1.png\" alt=\"A chart showing that the NVIDIA and Microsoft Azure MLPerf Training v3.1 LLM submissions this round were approximately 2.8x that of the MLPerf Training v3.0 submission using 3,584 H100 GPUs.\" class=\"wp-image-72480\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1.png 986w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-performance-graph-1-178x110.png 178w\" sizes=\"(max-width: 986px) 100vw, 986px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Relative performance of the NVIDIA and Microsoft Azure 10,752 H100 GPU joint submission on the MLPerf Training LLM test</em></em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">MLPerf Training v3.0 and v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.0-2003, 3.1-2002, 3.1-2007. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See <a href=\"https://mlcommons.org/en/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>The NVIDIA submissions also demonstrated near-linear performance scaling, as submission sizes scaled from 4,096 H100 GPUs to 10,752 H100 GPUs.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"891\" height=\"448\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph.png\" alt=\"A chart that shows that, compared to the baseline MLPerf Training v3.0 LLM submission using 3,584 H100 GPUs, NVIDIA\u2019s MLPerf Training v3.1 submissions achieve up to 2.8x more LLM training performance with up to 3x more H100 GPUs for near-linear performance scaling. \n\" class=\"wp-image-72484\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph.png 891w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-300x151.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-625x314.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-179x90.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-768x386.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-645x324.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-500x251.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-160x80.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-362x182.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-training-llm-scaling-graph-219x110.png 219w\" sizes=\"(max-width: 891px) 100vw, 891px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. MLPerf Training LLM performance compared to H100 GPU count used to achieve that performance</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Training v3.0 and v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.0-2003, 3.1-2005, 3.1-2007, 3.1-2008, 3.1-2009. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See <a href=\"https://mlcommons.org/en/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>To keep the benchmark duration within a reasonable time, MLPerf benchmarks represent a fraction of whole end-to-end workloads. As reported in <a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large Language Models</a>, a GPT-3 175B model requires 3.7T tokens to train on to be compute-optimal. Projecting our MLPerf 10,752-GPU record of 3.92 minutes, the compute-optimal training on 3.7T tokens would complete in 8 days.</p>\n\n\n\n<p>This result was achieved through the full array of NVIDIA technologies, including the latest, fourth-generation NVLink interconnect combined with the latest third-generation NVSwitch chip to enable 900 GB/s all-to-all communication between H100 GPUs, NVIDIA Quantum-2 InfiniBand networking, as well as NVIDIA\u2019s exceptional software stack, including the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo framework</a>, <a href=\"https://docs.nvidia.com/deeplearning/transformer-engine/index.html\">NVIDIA Transformer Engine</a> library, <a href=\"https://docs.nvidia.com/cuda/cublas/index.html\">NVIDIA cuBLAS</a> library, and the NVIDIA Magnum IO <a href=\"https://docs.nvidia.com/deeplearning/nccl/index.html\">NCCL</a> documentation.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Increasing performance per H100 GPU</h3>\n\n\n\n<p>NVIDIA made significant software improvements that yielded about a 10% faster time to train in both 512-GPU and 768-GPU submissions compared to the corresponding submissions in the prior round, achieving 797 TFLOPS of training throughput per H100 GPU. These improvements were seen in the larger-scale NVIDIA submissions as well, with the 4,096 H100 GPU submission demonstrating nearly 28% more performance with only 14% more GPUs.\u00a0\u00a0</p>\n\n\n\n<p>By continuing to deliver more performance from NVIDIA GPUs with ongoing software enhancements, customers benefit from greater productivity through shorter model training times, faster time to deployment, and lower costs, particularly in the cloud. This is achieved by requiring fewer GPU hours to perform the same work, and the ability to train increasingly complex models on the same hardware.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">Setting the standard for Stable Diffusion training</h2>\n\n\n\n<p>The NVIDIA platform and H100 GPUs submitted record-setting results for the newly added Stable Diffusion workloads. The NVIDIA submission using 64 H100 GPUs completed the benchmark in just 10.02 minutes, and that time to train was reduced to just 2.47 minutes using 1,024 H100 GPUs.\u00a0</p>\n\n\n\n<p>Figure 3 shows some of the optimizations used in the NVIDIA Stable Diffusion submission. All the optimizations are available in NeMo Multimodal release 23.09 (EA).\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"979\" height=\"641\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph.png\" alt=\"A chart that shows the absolute time-to-train values for the 64 and 1,024 NVIDIA H100 MLPerf Stable Diffusion submissions at 10.02 minutes and 2.47 minutes, respectively. \n\" class=\"wp-image-72490\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph.png 979w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-300x196.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-625x409.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-176x115.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-768x503.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-645x422.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-458x300.png 458w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-137x90.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-362x237.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-stable-diffusion-time-to-train-nvidia-h100-graph-168x110.png 168w\" sizes=\"(max-width: 979px) 100vw, 979px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Time to train for the 64 and 1,024 NVIDIA H100 submissions on the MLPerf text-to-image (Stable Diffusion) benchmark</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Training v3.1. Results retrieved from www.mlperf.org on November 8, 2023, from entries 3.1-2050, 3.1-2060. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See <a href=\"https://mlcommons.org/en/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GroupNorm with Channels Last support</h3>\n\n\n\n<p>Most diffusion models are composed of two basic building blocks, <code>ResBlock</code> and <code>SpatialTransformer</code>. For <code>ResBlock</code> and other convolutional neural network (CNN)-based networks, using a channels-last memory format is preferred for performance. To avoid layout transformations around <code>GroupNorm</code>, which would incur additional memory accesses, the NVIDIA Stable Diffusion submission added support for a <code>GroupNorm</code> block that employs a channels-last memory format. Using a channels-last layout in combination with <a href=\"https://github.com/NVIDIA/apex/blob/master/apex/contrib/group_norm/group_norm.py\">APEX GroupNorm</a> led to a 14% performance improvement.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">FlashAttention-2</h3>\n\n\n\n<p>Each <code>SpatialTransformer</code> module contains a self-attention and a cross-attention block. Using <a href=\"https://github.com/Dao-AILab/flash-attention\">FlashAttention-2</a> for these blocks achieved a 21% speedup.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Host-device synchronization removals</h3>\n\n\n\n<p>PyTorch models can suffer from significant CPU overheads. To reduce this overhead, several synchronization points between the CPU and the GPU were eliminated. This removes the host from the critical path and enables end-to-end application performance to correlate much better with GPU performance.\u00a0</p>\n\n\n\n<p>Host execution also results in significant execution time variability, causing variations in iteration times across different workers. This is further exacerbated in multi-GPU settings, where all workers need to synchronize one time at each iteration. These optimizations also reduce runtime variations across each worker, improving multi-GPU training performance. Overall, these optimizations boosted performance by 11%.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">CUDA graphs</h3>\n\n\n\n<p>The NVIDIA submission also uses CUDA graphs to further reduce runtime overhead, boosting GPU utilization. The use of CUDA graphs for the U-Net model that is part of Stable Diffusion delivered a 4% performance increase. Similar to the previous optimization, this helps with reducing CPU overhead, as well as multi-GPU performance by reducing runtime variability across workers.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Reduced kernel launch overheads</h3>\n\n\n\n<p>Diffusion models tend to include many small CUDA kernels, and they can occur at many points across the model. To both reduce kernel launch overheads and minimize the trips to GPU memory, we applied fusion engines such as <code>TorchInductor</code>, yielding a 6% speedup.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Removing type-casting overheads</h3>\n\n\n\n<p>To make full use of NVIDIA Tensor Cores, modern diffusion models adopt <a href=\"https://developer.nvidia.com/automatic-mixed-precision\">Automatic Mixed Precision</a> (AMP) to enable training using lower precision data formats, such as FP16. The PyTorch implementation of AMP stores model parameters in FP32 precision, which means that dynamic type-casting operations between FP16 and FP32 need to be performed. To avoid such overhead, the model is first cast to FP16 precision, resulting in FP16 model parameters and gradients that can be directly used without the need for type-casting operations.&nbsp;</p>\n\n\n\n<p>The optimizer is also modified to maintain an FP32 copy of the model parameters, which are used to update the model instead of the FP16 model parameters. This enables faster model training while maintaining accuracy.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Unlocking faster DLRMv2 training</h2>\n\n\n\n<p>As in the prior round, NVIDIA made submissions using 8, 64, and 128 H100 GPUs. Due to software improvements, the performance of each of these submissions increased round-over-round, with the largest improvement of 57% achieved in the 128 H100 GPU submission. </p>\n\n\n\n<p>Key to our DLRMv2 submissions this round was an extensive focus on removing host CPU bottlenecks. This is mostly achieved by using statically linked CUDART instead of dynamically linking it at runtime. This is because dynamically linked CUDART has additional locking overhead due to its requirement of allowing loading and unloading of modules at any point.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"988\" height=\"366\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph.png\" alt=\"A chart that shows the relative speedup at each of 8-, 64-, and 128-GPU scales on the for the DLRMv2 workload in MLPerf Training v3.1 compared to results submitted in MLPerf Training v3.0. \n\" class=\"wp-image-72492\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph.png 988w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-300x111.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-625x232.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-179x66.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-768x285.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-645x239.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-500x185.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-160x59.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-362x134.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/mlperf-relative-speedups-dlrmv2-graph-297x110.png 297w\" sizes=\"(max-width: 988px) 100vw, 988px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. DLRMv2 performance increases in MLPerf Training v3.1 compared to the prior MLPerf Training v3.0 round</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Training v3.1. Results retrieved from www.mlperf.org on November 8, 2023 from entries: 128x 3.0-2065, 128x 3.1-2051. The MLPerf name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See <a href=\"https://mlcommons.org/en/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>By making use of data-parallel embeddings for small embedding tables, the NVIDIA submission this round also reduced the all-to-all traffic associated with embedding operations. Although data-parallel embedding naturally avoids all-to-all operations, it still requires an <code>AllReduce</code> operation to accumulate gradients across GPUs. So, to further enhance performance, a grouped <code>AllReduce</code> technique that fuses the <code>AllReduce</code> operation with the <code>AllReduce</code> operation from the data-parallel dense network is used.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">Boosting RetinaNet performance</h2>\n\n\n\n<p>In this round, the NVIDIA submissions demonstrated both performance increases at the same scales submitted in the prior round, as well as a dramatic boost in maximum scale to 2,048 H100 GPUs (compared to 768 H100 GPUs in the previous round). As a result, NVIDIA set a new record time to train of 0.92 minutes for the MLPerf RetinaNet benchmark, a 64% performance boost compared to the previous NVIDIA submission.</p>\n\n\n\n<p>These improvements were fueled by several optimizations, described in the following sections.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Avoiding gradient copies in PyTorch DDP</h3>\n\n\n\n<p>In the NVIDIA maximum-scale configuration with 2,048 H100 GPUs, device-to-device memory copies that were exposed between the end of backpropagation and the start of the optimizer step were observed. These were found to be copies between the model gradients and the <code>AllReduce</code> communication buckets used by PyTorch <code>DistributedDataParallel</code>.\u00a0</p>\n\n\n\n<p>Setting <code>gradient_as_bucket_view=True</code> for PyTorch <code>DistributedDataParallel</code> turns gradients into views (pointers) into the existing <code>AllReduce</code> buckets and avoids creating an extra buffer, reducing peak memory usage and avoiding the overhead of memory copies. This optimization improved training throughput by 3% in the largest-scale NVIDIA submission.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Improved evaluation performance</h3>\n\n\n\n<p>At large scales, evaluation time increasingly becomes an important factor in time to train. Evaluation in RetinaNet consists of two phases: inference, which is performed on GPUs, and asynchronous scoring, which is performed on CPUs. To improve inference performance at max-scale, the NVIDIA submission this round uses a specifically optimized batch size rather than the one used for training. It also binds processes to CPU cores and increases the number of CPU threads to improve scoring performance.</p>\n\n\n\n<h3 class=\"wp-block-heading\">cuDNN optimizations</h3>\n\n\n\n<p>The cuDNN library has made improvements to computational kernels that use the fourth-generation Tensor Cores of the H100 GPU. This led to increased performance of convolutions across all sizes used in RetinaNet, delivering up to 3% higher training throughput compared to the previous NVIDIA submission. Improved convolution kernel performance in cuDNN has also led to up to 7% throughput improvement in other benchmarks, such as ResNet-50.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Greater spatial parallelism for 3D U-Net</h2>\n\n\n\n<p>For the maximum-scale NVIDIA submission on the 3D U-Net workload in this round, the system configuration grew from 54 nodes (consisting of 42 training nodes and 12 evaluation nodes in the prior round) to 96 nodes (consisting of 84 training nodes and 12 training nodes).</p>\n\n\n\n<p>By leveraging the capabilities of H100 GPUs and the latest version of NVLink, the NVIDIA submission in this round increased the level of spatial parallelism to 8x from a prior 4x. This means that instead of training one image across a set of four H100 GPUs, the latest NVIDIA submission trains one image across eight H100 GPUs, resulting in an end-to-end throughput improvement of over 6.2%.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Improving BERT score at-scale</h2>\n\n\n\n<p>The max-scale NVIDIA submission using 434 nodes achieved a 1.12x speedup over our previous submission using 384 nodes. This round, we also introduced a sample re-ordering optimization to improve load balancing for small per-GPU batch sizes, as is the case for max-scale. This resulted in a 6% throughput improvement for the 434-node scale submission.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Sample reordering</h3>\n\n\n\n<p>Prior large-scale NVIDIA submissions used a packed dataset format where every sample had one to three actual sequences, so that the combined length is the same or slightly smaller than the maximum sequence length. </p>\n\n\n\n<p>Although this optimization minimizes the load imbalance among GPUs for large enough per-GPU batch sizes, the number of actual sequences processed by GPUs is different. This causes jitter at large-scale runs using small per-GPU batch size. Sample reordering optimization bucketizes the packed samples such that the number of actual sequences processed in each iteration remains constant across GPUs, thus minimizing jitter. We have verified that this optimization does not impact convergence for this benchmark.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">Bringing H100 to MLPerf HPC v3.0</h2>\n\n\n\n<p>In MLPerf HPC v3.0, NVIDIA made its first submissions using H100 GPUs. The newer GPUs delivered a significant boost compared to the NVIDIA MLPerf HPC v2.0 submissions, which used prior-generation NVIDIA A100 GPUs.\u00a0</p>\n\n\n\n<p>The following sections cover some of the software work that was done to make optimized submissions using H100 on all MLPerf HPC v3.0 tests, including the new test based on OpenFold.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">OpenFold</h3>\n\n\n\n<p>A common method for reducing training time is data parallelism, which equally distributes the global batch across GPUs. The degree of parallelism of data-parallel task partitioning is limited by the global batch size. Larger batch sizes, however, result in lower accuracy, limiting the global batch size for OpenFold and preventing the use of data parallelism.\u00a0</p>\n\n\n\n<p>FastFold introduces dynamic axial parallelism (DAP), a way to increase the degree of parallelism. With DAP, the model can continue scaling up, but the scaling efficiency of DAP is low. In the initial phase, increasing GPU count by 4x yields only a 1.7x speedup, with no additional performance increase when the GPU count is scaled to 8x.\u00a0</p>\n\n\n\n<p>In addition to scaling efficiency challenges, OpenFold contains about 150K operators, most of which are memory bandwidth\u2013bound, leading to suboptimal GPU utilization and high CPU launch overhead. Moreover, in the data load stage, all samples need to be cropped or padded into a fixed size. The time required to prepare these batches depends on both sequence length and the number of multiple sequence alignments (MSAs), which vary significantly. We observed that 0.1% batches are slow to prepare, blocking the training stage.\u00a0</p>\n\n\n\n<p>To resolve these problems, the NVIDIA OpenFold submission employs the following optimizations:</p>\n\n\n\n<ol>\n<li>Incorporate DAP into the submission, enabling continued GPU scaling to 8x.&nbsp;</li>\n\n\n\n<li>Create a new data loader with a non-blocking priority queue, which enables slow batches to be skipped and processed later when they are ready.\u00a0</li>\n\n\n\n<li>Implement three efficient NVIDIA Triton kernels for critical patterns, multi-head attention, <code>LayerNorm</code>, and <code>fusedAdam</code> with stochastic weight averaging (SWA).</li>\n\n\n\n<li>Apply the torch compiler to automatically fuse memory-bound operations.&nbsp;</li>\n\n\n\n<li>Enable BF16 training and CUDA graphs.&nbsp;</li>\n\n\n\n<li>Use asynchronous evaluation to free training nodes from evaluation and implement an evaluation dataset cache to speed up the performance of evaluation.</li>\n</ol>\n\n\n\n<h3 class=\"wp-block-heading\">OpenCatalyst20</h3>\n\n\n\n<p>The NVIDIA OpenCatalyst20 submissions incorporate several optimizations to maximize performance. First, the submissions use FP16 precision. Next, the submissions perform evaluation asynchronously, which frees the training nodes from needing to perform evaluation. Finally, by disabling PyTorch garbage collection, CPU overhead was reduced.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">DeepCAM</h3>\n\n\n\n<p>For the time-to-train metric, the NVIDIA DeepCAM submission on 2,048 GPUs uses a GPU local batch size of one. In the previous NVIDIA DeepCAM submissions, the PyTorch <code>SyncBN</code> kernel was used for distributed batch normalization. In the NVIDIA MLPerf HPC v3.0 submission, this kernel was replaced with a more efficient single-node group batch normalization kernel from APEX, boosting end-to-end performance.\u00a0</p>\n\n\n\n<p>For the throughput test, the NVIDIA DeepCAM submission ran on two nodes with a GPU local batch size of eight. In this scenario, a large portion of the dataset resides on the NVMe storage, and reading a large portion of this dataset during training thrashes the node\u2019s page cache frequently. This resulted in stalls in the input pipeline, an issue that manifested itself when running on H100 GPUs, which feature much more compute performance than A100 GPUs.&nbsp;</p>\n\n\n\n<p>To overcome this issue, the NVIDIA submission implements <code>O_DIRECT</code> in the data reader, bypassing the page cache entirely. This ensured that I/O operations can be hidden almost entirely behind the computations, leading to a 1.8x performance increase compared to the unimproved code.\u00a0</p>\n\n\n\n<p>Finally, enhancements to group convolution kernels designed for the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper architecture</a> contributed another 2% speedup.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">CosmoFlow</h3>\n\n\n\n<p>For the CosmoFlow throughput benchmark, the NVIDIA submission runs on 32 GPUs using a global batch size of 32 and a local batch size of one. As the compute performance of H100 GPUs is much greater than A100 GPUs, the training pipeline began to stall, waiting for additional data to arrive.&nbsp;</p>\n\n\n\n<p>To fix this, the NVIDIA submission implements support for <code>O_DIRECT</code> in the data reader.\u00a0 Previously, the GPU would make requests to the CPU, the CPU would perform the data read access, and then pass the data back to the GPU. With <code>O_DIRECT</code>, the GPU makes the data requests directly and receives the data back, bypassing the CPU. This decreased \u200cdata latency and enabled the data input pipeline to remain mostly hidden behind the math computation.</p>\n\n\n\n<p>The CosmoFlow network is primarily a series of 3D convolutions and <code>MaxPool</code> layers. For both the time-to-train and throughput benchmark runs, the individual kernels were slower than their maximum theoretical speed.\u00a0 Several low-level optimizations were made to the kernels to increase their performance in the forward and backward passes. In the time-to-train configuration, CosmoFlow ran on 512 GPUs with a global batch size of 512 and a local batch size of one. Kernel optimizations also improved the time to train by a few percentage points.</p>\n\n\n\n<h2 class=\"wp-block-heading\">MLPerf Training v3.1 and HPC v3.0 takeaways</h2>\n\n\n\n<p>The NVIDIA platform, powered by NVIDIA H100 GPUs and Quantum-2 InfiniBand, yet again raised the bar for AI training. From powering two LLM submissions at an unprecedented scale of 10,752 H100 GPUs, to unmatched performance on the newly added text-to-image test, to continued performance boosts for classic MLPerf workloads, the NVIDIA platform continues to demonstrate the highest performance and the greatest versatility for the most demanding AI training challenges.&nbsp;</p>\n\n\n\n<p>The NVIDIA platform also demonstrated how it is accelerating the convergence of HPC and AI with the latest MLPerf HPC results, with the H100 GPU enabling substantial speedups compared to prior NVIDIA submissions using the A100 GPU. In addition to delivering the highest performance across all MLPerf HPC workloads and scenarios, only the NVIDIA platform ran every test, demonstrating remarkable versatility.\u00a0</p>\n\n\n\n<p>In addition to exceptional performance and versatility, the NVIDIA platform is broadly available from both cloud service providers and system makers worldwide. All software used for NVIDIA MLPerf submissions is available from the MLPerf repository, enabling users to reproduce results. All NVIDIA AI software used to achieve these results is also available in the enterprise-grade software suite, <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI is rapidly transforming computing, unlocking new use cases and turbocharging existing ones. Large language models (LLMs), such as OpenAI\u2019s GPT models and Meta\u2019s Llama 2, skillfully perform a variety of tasks on text-based content. These tasks include summarization, translation, classification, and generation of new content such as computer code, marketing copy, poetry, and &hellip; <a href=\"https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/\">Continued</a></p>\n", "protected": false}, "author": 1355, "featured_media": 72471, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1293838", "discourse_permalink": "https://forums.developer.nvidia.com/t/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-nvidia-quantum-2-infiniband/272218", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 503], "tags": [1592, 453, 2779, 658, 2932, 973], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/hpc-mlperf-training-graphic-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iQP", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72467"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1355"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72467"}], "version-history": [{"count": 28, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72467/revisions"}], "predecessor-version": [{"id": 74293, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72467/revisions/74293"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72471"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72467"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72467"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72467"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72600, "date": "2023-11-08T06:00:00", "date_gmt": "2023-11-08T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72600"}, "modified": "2023-12-07T09:01:54", "modified_gmt": "2023-12-07T17:01:54", "slug": "accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/", "title": {"rendered": "Accelerating NetworkX on NVIDIA GPUs for High Performance Graph Analytics"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NetworkX states in its documentation that it is \u201c\u2026a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\u201d Since its first public release in 2005, it&#8217;s become the most popular Python graph analytics library available. This may explain why NetworkX amassed 27M PyPI downloads just in September of 2023. </p>\n\n\n\n<p>How is NetworkX able to achieve such mass appeal? Are there use cases where NetworkX falls short, and if so, what can be done to address those? I examine these questions and more in this post.</p>\n\n\n\n<div class='stb-container stb-style-info stb-no-caption'><div class='stb-caption'><div class='stb-logo'><img class='stb-logo__image' src='data:image/png;base64,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' alt='img'/></div><div class='stb-caption-content'></div><div class='stb-tool'></div></div><div class='stb-content'>Watch the <a style=\"color: #0000ff;\" href=\"https://www.nvidia.com/en-us/events/ai-data-science-virtual-summit\">Accelerating NetworkX: the Future of Easy Graph Analytics</a> session replay from the recent AI and Data Science Virtual Summit hosted by NVIDIA.</div></div>\n\n\n\n<h2 class=\"wp-block-heading\">NetworkX: Easy graph analytics</h2>\n\n\n\n<p>There are several reasons NetworkX is so popular among data scientists, students, and many others interested in graph analytics.&nbsp; NetworkX is open-source and backed by a large and friendly community, eager to answer questions and help. The code is mature and well-documented, and the package itself is easy to install and requires no additional dependencies.&nbsp; But most of all, NetworkX has a plethora of algorithms that cover something for everyone (including plotting!) with an easy-to-use API.</p>\n\n\n\n<p>With just a few lines of simple code, you can load and analyze graph data using any of the algorithms provided. Here is an example of finding the shortest weighted path of a simple four-node graph:</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"503\" height=\"391\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1.png\" alt=\"Example of finding the shortest weighted path of a simple four-node graph\" class=\"wp-image-72823\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1.png 503w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1-300x233.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1-148x115.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1-386x300.png 386w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1-116x90.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1-362x281.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure1-142x110.png 142w\" sizes=\"(max-width: 503px) 100vw, 503px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A simple weighted graph with four nodes and four edges</em></figcaption></figure></div>\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; G = nx.Graph()\n&gt;&gt;&gt; G.add_edge(&quot;A&quot;, &quot;B&quot;, weight=4)\n&gt;&gt;&gt; G.add_edge(&quot;B&quot;, &quot;D&quot;, weight=2)\n&gt;&gt;&gt; G.add_edge(&quot;A&quot;, &quot;C&quot;, weight=3)\n&gt;&gt;&gt; G.add_edge(&quot;C&quot;, &quot;D&quot;, weight=4)\n&gt;&gt;&gt; nx.shortest_path(G, &quot;A&quot;, &quot;D&quot;, weight=&quot;weight&quot;)\n&#091;'A', 'B', 'D']\n</pre></div>\n\n\n<p>In just a few lines, easily typed at a Python prompt, you can interactively explore your graph data.</p>\n\n\n\n<h2 class=\"wp-block-heading\">So what\u2019s missing?</h2>\n\n\n\n<p>While NetworkX provides a tremendous amount of usability right out of the box, performance and scalability for medium-to-large-sized networks are far from best-in-class and can significantly limit a data scientist\u2019s productivity.</p>\n\n\n\n<p>To get an idea of how graph size and algorithm options impact runtime, here\u2019s an interesting analytic that answers questions about a real-world dataset.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Examining influential U.S. patents using betweenness centrality</h3>\n\n\n\n<p>The , provided by the <a href=\"https://snap.stanford.edu/data/cit-Patents.html\" data-type=\"link\" data-id=\"https://snap.stanford.edu/data/cit-Patents.html\" target=\"_blank\" rel=\"noreferrer noopener\">Stanford Network Analysis Platform (SNAP)</a>, is a citation graph of patents granted between 1975 and 1999, totaling 16,522,438 citations. If you know which patents are more central than others, you may get an idea of their relative importance.</p>\n\n\n\n<p>The citation graph can be processed using the pandas library to create a DataFrame containing graph edges. The DataFrame has two columns: one for the source node and another for the destination node for each edge. NetworkX can then take this DataFrame and create a graph object, which can then be used to run betweenness centrality.</p>\n\n\n\n<p><em>Betweenness centrality</em> is a metric that quantifies the extent to which nodes act as intermediaries between other nodes, as determined by the number of shortest paths they are a part of. In the context of patent citations, it may be used to measure the extent to which a patent connects other patents.</p>\n\n\n\n<p>Using NetworkX, you can run <code>betweenness_centrality</code> to find these central patents. NetworkX selects <code>k</code> nodes at random for the shortest path analysis used by the betweenness centrality computation. A higher value of <code>k</code> leads to more accurate results at the cost of increased computation time.</p>\n\n\n\n<p>The following code example loads the citation graph data, creates a NetworkX graph object, and runs betweenness_centrality.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n###############################################################################\n# Run Betweenness Centrality on a large citation graph using NetworkX\nimport sys\nimport time\n\nimport networkx as nx\nimport pandas as pd\n\nk = int(sys.argv&#091;1])\n\n# Dataset from https://snap.stanford.edu/data/cit-Patents.txt.gz\nprint(&quot;Reading dataset into Pandas DataFrame as an edgelist...&quot;, flush=True,\n      end=&quot;&quot;)\npandas_edgelist = pd.read_csv(\n    &quot;cit-Patents.txt&quot;,\n    skiprows=4,\n    delimiter=&quot;\\t&quot;,\n    names=&#091;&quot;src&quot;, &quot;dst&quot;],\n    dtype={&quot;src&quot;: &quot;int32&quot;, &quot;dst&quot;: &quot;int32&quot;},\n)\nprint(&quot;done.&quot;, flush=True)\nprint(&quot;Creating Graph from Pandas DataFrame edgelist...&quot;, flush=True, end=&quot;&quot;)\nG = nx.from_pandas_edgelist(\n    pandas_edgelist, source=&quot;src&quot;, target=&quot;dst&quot;, create_using=nx.DiGraph\n)\nprint(&quot;done.&quot;, flush=True)\n\nprint(&quot;Running betweenness_centrality...&quot;, flush=True, end=&quot;&quot;)\nst = time.time()\nbc_result = nx.betweenness_centrality(G, k=k)\nprint(f&quot;done, BC time with {k=} was: {(time.time() - st):.6f} s&quot;)\n</pre></div>\n\n\n<p>Pass a <code>k</code> value of 10 when you run the code:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 97.553809 s\n</pre></div>\n\n\n<p>As you can see, running <code>betweenness_centrality</code> on a moderately large graph with just a value of <code>k=10</code> and a fast, modern CPU (Intel Xeon Platinum 8480CL) takes almost 98 seconds. Achieving a higher level of accuracy that aligns with your expectations for a graph of this scale would necessitate significantly increasing the value of <code>k</code>. However, this would result in considerably longer runtimes, as highlighted in the benchmark results later in this post, where execution times extend to several hours.</p>\n\n\n\n<h2 class=\"wp-block-heading\">RAPIDS cuGraph: Speed and NetworkX interoperability</h2>\n\n\n\n<p>The RAPIDS cuGraph project was created to bridge the gap between fast, scalable, GPU-based graph analytics and NetworkX ease-of-use. For more information, see <a href=\"https://medium.com/rapids-ai/rapids-cugraph-networkx-compatibility-d119e417557c\">RAPIDS cuGraph adds NetworkX and DiGraph Compatibility</a>.</p>\n\n\n\n<p>cuGraph was designed with NetworkX interoperability in mind, which can be seen when you replace only the <code>betweenness_centrality</code> call from the prior example with cuGraph\u2019s <code>betweenness_centrality</code> and leave the rest of the code as-is.</p>\n\n\n\n<p>The result is a greater-than-12x speedup with only a few lines of code changed:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n###############################################################################\n# Run Betweenness Centrality on a large citation graph using NetworkX\n# and RAPIDS cuGraph.\n# NOTE: This demonstrates legacy RAPIDS cuGraph/NetworkX interop. THIS CODE IS\n# NOT PORTABLE TO NON-GPU ENVIRONMENTS! Use nx-cugraph to GPU-accelerate\n# NetworkX with no code changes and configurable CPU fallback.\nimport sys\nimport time\n\nimport cugraph as cg\nimport pandas as pd\n\nk = int(sys.argv&#091;1])\n\n# Dataset from https://snap.stanford.edu/data/cit-Patents.txt.gz\nprint(&quot;Reading dataset into Pandas DataFrame as an edgelist...&quot;, flush=True,\n      end=&quot;&quot;)\npandas_edgelist = pd.read_csv(\n    &quot;cit-Patents.txt&quot;,\n    skiprows=4,\n    delimiter=&quot;\\t&quot;,\n    names=&#091;&quot;src&quot;, &quot;dst&quot;],\n    dtype={&quot;src&quot;: &quot;int32&quot;, &quot;dst&quot;: &quot;int32&quot;},\n)\nprint(&quot;done.&quot;, flush=True)\nprint(&quot;Creating Graph from Pandas DataFrame edgelist...&quot;, flush=True, end=&quot;&quot;)\nG = cg.from_pandas_edgelist(\n    pandas_edgelist, source=&quot;src&quot;, destination=&quot;dst&quot;, create_using=cg.Graph(directed=True)\n)\nprint(&quot;done.&quot;, flush=True)\n\nprint(&quot;Running betweenness_centrality...&quot;, flush=True, end=&quot;&quot;)\nst = time.time()\nbc_result = cg.betweenness_centrality(G, k=k)\nprint(f&quot;done, BC time with {k=} was: {(time.time() - st):.6f} s&quot;)\n</pre></div>\n\n\n<p>When you run the new code on the same machine with the same <code>k</code> value, you can see that it\u2019s over 12x faster:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ python cg_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 7.770531 s\n</pre></div>\n\n\n<p>This example showcases cuGraph interoperability with NetworkX quite well. However, there are instances that require you to make more significant changes when adding cuGraph to your code.</p>\n\n\n\n<p>Many differences are intentional (different options to better map to GPU implementations for performance reasons, options not supported, and so on), while others are unavoidable (cuGraph has fewer algorithms implemented, cuGraph requires a GPU, and so on). These differences require you to add special case code to convert options or check if the code is running on a cuGraph-compatible system and call the equivalent NetworkX API if they intend to support environments without GPUs or cuGraph.</p>\n\n\n\n<p>cuGraph is an easy-to-use Python library on its own, but it\u2019s not intended to be a drop-in replacement for NetworkX.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Meanwhile, NetworkX adds dispatching\u2026</h2>\n\n\n\n<p>NetworkX has recently added the ability to dispatch API calls to different analytic backends provided by third parties.&nbsp;These backends can provide alternate implementations for various NetworkX APIs that can greatly improve performance.</p>\n\n\n\n<p>Backends can be specified by either an additional <code>backend=keyword</code> argument on supported APIs or by setting the <code>NETWORKX_AUTOMATIC_BACKENDS</code> environment variable.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"367\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-625x367.png\" alt=\"NetworkX can be configured to raise an informative error or automatically fall back to its default implementation to satisfy the call.\" class=\"wp-image-72831\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-625x367.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-300x176.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-179x105.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-645x379.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-500x294.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-153x90.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-362x213.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1-187x110.png 187w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Figure2-1.png 693w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NetworkX dispatching can call alternate backends or use the default implementation based on user configuration</em></figcaption></figure></div>\n\n\n<p>If a NetworkX API call is made and a backend isn&#8217;t available to support that call, NetworkX can be configured to raise an informative error or automatically fall back to its default implementation to satisfy the call (Figure 2).</p>\n\n\n\n<p>Dispatching is opt-in. Even when backends are installed and available, NetworkX uses the default implementation if you don&#8217;t specify one or more backends to use.&nbsp;</p>\n\n\n\n<p>By enabling other graph libraries to easily extend NetworkX through backends, NetworkX becomes a standard graph analytics frontend.&nbsp; This means more users can use the capabilities of other graph libraries without the learning curve and integration time associated with a new library.</p>\n\n\n\n<p>Library maintainers also benefit from NetworkX dispatching because they can reach more users without the overhead of maintaining a user-facing API. Instead, they can just focus on delivering a backend.</p>\n\n\n\n<h2 class=\"wp-block-heading\">GPU-accelerated NetworkX using nx-cugraph</h2>\n\n\n\n<p>NetworkX dispatching opened the door for the RAPIDS cuGraph team to create nx-cugraph, a new project that adds a backend for NetworkX based on the graph analytic engine provided by RAPIDS cuGraph.</p>\n\n\n\n<p>This approach also enables nx-cugraph to have fewer dependencies and avoid code paths that the cuGraph Python library adds for efficient integration with RAPIDS cuDF, which is not needed for NetworkX.</p>\n\n\n\n<p>With nx-cugraph, NetworkX users can finally have everything: ease of use, portability between GPU and non-GPU environments, and performance, all without code changes.</p>\n\n\n\n<p>But maybe best of all, you can now unlock use cases that were not practical before due to excessive runtime, just by adding GPUs and nx-cugraph. For more information, see the benchmark section later in this post.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Installing nx-cugraph</h2>\n\n\n\n<p>Assuming NetworkX version 3.2 or later has been installed, nx-cugraph can be installed using either conda or pip.</p>\n\n\n\n<p><strong>conda</strong></p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nconda install -c rapidsai-nightly -c conda-forge -c nvidia nx-cugraph\n</pre></div>\n\n\n<p><strong>pip</strong></p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\npython -m pip install nx-cugraph-cu11 --extra-index-url https://pypi.nvidia.com\n</pre></div>\n\n\n<p>Nightly wheel builds are not available until the 23.12 release, therefore the index URL for the stable release version is being used in the <code>pip install</code> command.</p>\n\n\n\n<p>For more information about installing any RAPIDS package, see <a href=\"https://rapids.ai/#quick-start\">Quick Local Install</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Revisiting NetworkX betweenness centrality with nx-cugraph</h2>\n\n\n\n<p>When you install nx-cugraph and specify the cugraph backend, NetworkX dispatches the <code>betweenness_centrality</code> call to nx-cugraph. You don\u2019t have to change your code to see the benefits of GPU acceleration.</p>\n\n\n\n<p>The following runs were done on the same system used in the benchmark section later in this post. These demonstrations also did not include a warmup run, which can improve performance, but the benchmarks shown later did.</p>\n\n\n\n<p>Here&#8217;s the initial NetworkX run on the U.S. Patent dataset with k=10:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 97.553809 s\n</pre></div>\n\n\n<p>With no changes to the code, set the environment variable <code>NETWORKX_AUTOMATIC_BACKENDS</code> to cugraph to use nx-cugraph for the <code>betweenness_centrality</code> run and observe a 6.8x speedup:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 10\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=10 was: 14.286906 s\n</pre></div>\n\n\n<p>Larger <code>k</code> values result in a significant slowdown for the default NetworkX implementation:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ python nx_bc_demo.py 50\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=50 was: 513.636750 s\n</pre></div>\n\n\n<p>Using the cugraph backend on the same <code>k</code> value results in a 31x speedup:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 50\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=50 was: 16.389574 s\n</pre></div>\n\n\n<p>As you can see, when you increase <code>k</code>, you see the speedup increase. The larger <code>k</code> value has little impact on the runtime when using the cugraph backend due to the high parallel processing capability of the GPU.</p>\n\n\n\n<p>In fact, you can go much higher with <code>k</code> to increase accuracy with little difference to the overall runtime:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nbash:~$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python nx_bc_demo.py 500\nReading dataset into Pandas DataFrame as an edgelist...done.\nCreating Graph from Pandas DataFrame edgelist...done.\nRunning betweenness_centrality...done, BC time with k=500 was: 18.673590 s\n</pre></div>\n\n\n<p>Setting <code>k</code> to 500 when using the default NetworkX implementation takes over an hour but adds only a few seconds when using the cugraph backend. For more information, see the next section on benchmarks.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Benchmarks</h2>\n\n\n\n<p>Benchmark results for NetworkX with and without nx-cugraph are shown in Tables 1-3 using the following dataset and system hardware configuration:</p>\n\n\n\n<ul>\n<li><strong>Dataset:</strong> directed graph, 3.7M nodes, 16.5M edges</li>\n\n\n\n<li><strong>CPU:</strong> Intel Xeon Platinum 8480CL, 2TB</li>\n\n\n\n<li><strong>GPU:</strong> NVIDIA H100, 80 GB</li>\n</ul>\n\n\n\n<p>These benchmarks were run using pytest with the pytest-benchmark plugin. Each run includes a warmup step for both NetworkX and nx-cugraph, which improves performance for the measured run.</p>\n\n\n\n<p>The benchmark code is available in the <a rel=\"noreferrer noopener\" href=\"https://github.com/rapidsai/cugraph/blob/branch-23.12/benchmarks/nx-cugraph/pytest-based/bench_algos.py\" target=\"_blank\">cuGraph Github repo</a>.</p>\n\n\n\n<pre class=\"wp-block-preformatted\">nx.betweenness_centrality(G, k=k)</pre>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td></td><td>k=10</td><td>k=20</td><td>k=50</td><td>k=100</td><td>k=500</td><td>k=1000</td></tr><tr><td>NetworkX</td><td>97.28 s</td><td>184.77 s</td><td>463.15 s</td><td>915.84 s</td><td>4,585.96 s</td><td>9,125.48 s</td></tr><tr><td>nx-cugraph</td><td>8.71 s</td><td>8.26 s</td><td>8.91 s</td><td>8.67 s</td><td>11.31 s</td><td>14.37 s</td></tr><tr><td>speedup</td><td>11.17 X</td><td>22.37 X</td><td>51.96 X</td><td>105.58 X</td><td>405.59 X</td><td>634.99 X</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. nx.betweenness_centrality: default implementation (NetworkX) vs. cugraph backend (nx-cugraph)</em></figcaption></figure>\n\n\n\n<pre class=\"wp-block-preformatted\">nx.edge_betweenness_centrality(G, k=k)</pre>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td></td><td>k=10</td><td>k=20</td><td>k=50</td><td>k=100</td><td>k=500</td><td>k=1000</td></tr><tr><td>NetworkX</td><td>112.22 s</td><td>211.52 s</td><td>503.57 s</td><td>993.15 s</td><td>4,937.70 s</td><td>9,858.11 s</td></tr><tr><td>nx-cugraph</td><td>19.62 s</td><td>19.93 s</td><td>21.26 s</td><td>22.48 s</td><td>41.65 s</td><td>57.79 s</td></tr><tr><td>speedup</td><td>5.72 X</td><td>10.61 X</td><td>23.69 X</td><td>44.19 X</td><td>118.55 X</td><td>170.59 X</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. nx.edge_betweenness_centrality: default implementation (NetworkX) vs. cugraph backend (nx-cugraph)</em></figcaption></figure>\n\n\n\n<pre class=\"wp-block-preformatted\">nx.community.louvain_communities(G)</pre>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td>NetworkX</td><td>2834.86 s</td></tr><tr><td>nx-cugraph</td><td>21.59 s</td></tr><tr><td>speedup</td><td>131.3 X</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. nx.community.louvain_communities: default implementation (NetworkX) vs. cugraph backend (nx-cugraph)</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>NetworkX dispatching is a new chapter in the evolution of NetworkX, which will result in the adoption of NetworkX by even more users for use cases not previously feasible.</p>\n\n\n\n<p>Interchangeable, third-party backends enable NetworkX to become a standardized frontend, where you no longer have to rewrite your Python code to use different graph analytic engines. nx-cugraph adds cuGraph-based GPU acceleration and scalability directly to NetworkX, so you can finally have the speed and scalability missing from NetworkX without code changes.</p>\n\n\n\n<p>Because both NetworkX and nx-cugraph are open-source projects, feedback, suggestions, and contributions are welcome. If there\u2019s something you\u2019d like to see, such as specific algorithms in nx-cugraph or additional dispatchable NetworkX APIs, leave a suggestion with the appropriate GitHub project:</p>\n\n\n\n<ul>\n<li><a rel=\"noreferrer noopener\" href=\"https://github.com/networkx/networkx\" target=\"_blank\">NetworkX</a>&nbsp;</li>\n\n\n\n<li><a rel=\"noreferrer noopener\" href=\"https://github.com/rapidsai/cugraph\" target=\"_blank\">cuGraph</a> &nbsp;</li>\n</ul>\n\n\n\n<p>To learn more about accelerating NetworkX on GPUs with nx-cugraph, register for the <a href=\"https://www.nvidia.com/en-us/events/ai-data-science-virtual-summit/\">AI and Data Science Virtual Summit</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NetworkX states in its documentation that it is \u201c\u2026a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\u201d Since its first public release in 2005, it&#8217;s become the most popular Python graph analytics library available. This may explain why NetworkX amassed 27M PyPI downloads just in September &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/\">Continued</a></p>\n", "protected": false}, "author": 1908, "featured_media": 72603, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1293764", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/272200", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 276], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/NetworkX-easy-graph-analytics-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iSY", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72600"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1908"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72600"}], "version-history": [{"count": 24, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72600/revisions"}], "predecessor-version": [{"id": 75003, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72600/revisions/75003"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72603"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72600"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72600"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72600"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72591, "date": "2023-11-08T06:00:00", "date_gmt": "2023-11-08T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72591"}, "modified": "2023-11-16T14:44:24", "modified_gmt": "2023-11-16T22:44:24", "slug": "rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/", "title": {"rendered": "RAPIDS cuDF Accelerates pandas Nearly 150x with Zero Code Changes"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA announced that RAPIDS cuDF can now bring GPU acceleration to 9.5M million pandas users without requiring them to change their code.</p>\n\n\n\n<div class='stb-container stb-style-info stb-no-caption'><div class='stb-caption'><div class='stb-logo'><img class='stb-logo__image' src='data:image/png;base64,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' alt='img'/></div><div class='stb-caption-content'></div><div class='stb-tool'></div></div><div class='stb-content'>Watch the <a style=\"color: #0000ff;\" href=\"https://www.youtube.com/watch?v=0fJ3TcY8bO8\">keynote replay</a> from the recent AI and Data Science Virtual Summit hosted by NVIDIA.</div></div>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/pandas-python/\">pandas</a>, a flexible and powerful data analysis and manipulation library for Python, is a top choice for data scientists because of its easy-to-use API. However, as dataset sizes grow, it struggles with processing speed and efficiency in CPU-only systems.</p>\n\n\n\n<p><a href=\"https://rapids.ai/\">RAPIDS</a> is an open-source suite of GPU-accelerated Python libraries designed to improve data science and analytics pipelines. RAPIDS cuDF is a GPU DataFrame library that provides a pandas-like API for loading, filtering, and manipulating data. In earlier releases of cuDF, it was meant for GPU-only development workflows.</p>\n\n\n\n<p>With the latest release of RAPIDS v23.10, cuDF now brings accelerated computing to pandas workflows with no code changes through a unified CPU/GPU user experience with its new <a href=\"https://rapids.ai/cudf-pandas\">pandas accelerator mode</a>. It\u2019s available today in the open-source <a href=\"https://docs.rapids.ai/install\">RAPIDS v23.10</a> release as an open beta and will be supported in <a href=\"https://nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> soon.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/kmj1QOY71Ps?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Accelerate Pandas by Nearly 150X with RAPIDS cuDF</em></figcaption></figure>\n\n\n\n<p>In the video, you can see identical pandas workflows running side-by-side: one uses pandas with CPU-only and the other uses pandas accelerator mode in RAPIDS cuDF.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Bringing a unified CPU/GPU experience to pandas workflows</h2>\n\n\n\n<p>cuDF has always provided users with top DataFrame library performance using a pandas-like API. However, adopting cuDF has sometimes required workarounds:</p>\n\n\n\n<ul>\n<li>Working around any pandas functionality not yet implemented or supported in cuDF.</li>\n\n\n\n<li>Designing separate code paths for CPU and GPU execution in codebases that require running on heterogeneous hardware.</li>\n\n\n\n<li>Manually switching between cuDF and pandas when interacting with other PyData libraries or organization-specific tooling designed for pandas.</li>\n</ul>\n\n\n\n<p>Starting with the RAPIDS <a href=\"https://docs.rapids.ai/install\">v23.10 release</a>, cuDF now provides a pandas accelerator mode to address these challenges, in addition to the existing GPU-only experience.</p>\n\n\n\n<p>This feature was built for data scientists who want to continue using pandas as data sizes grow into the gigabytes and pandas performance slows. In cuDF\u2019s pandas accelerator mode, operations execute on the GPU where possible and on the CPU (using pandas) otherwise, synchronizing under the hood as needed. This enables a unified CPU/GPU experience that brings best-in-class performance to your pandas workflows.&nbsp;</p>\n\n\n\n<p>With the latest release, cuDF now provides the following features:</p>\n\n\n\n<ul>\n<li><strong>Zero code change acceleration</strong>: Just load the cuDF Jupyter Notebook extension or use the cuDF Python module option.</li>\n\n\n\n<li><strong>Third-party library compatibility</strong>: pandas accelerator mode is compatible with most third-party libraries that operate on pandas objects. It will even accelerate pandas operations within these libraries.</li>\n\n\n\n<li><strong>Unified CPU/GPU workflows</strong>: Develop, test, and run in production with a single code path, regardless of hardware.</li>\n</ul>\n\n\n\n<p>To bring GPU acceleration into your pandas workflows in a Jupyter notebook, load the <code>cudf.pandas</code> extension:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n%load_ext cudf.pandas\nimport pandas as pd\n</pre></div>\n\n\n<p>To access it when running Python scripts, use the <code>cudf.pandas</code> module option:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\npython -m cudf.pandas script.py\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\">Bringing top performance to pandas workflows</h2>\n\n\n\n<p>As data sizes scale into the gigabytes, using pandas often becomes challenging due to slower performance, causing some data scientists to grudgingly give up the pandas API they love. With the new RAPIDS cuDF, you can keep using pandas as your primary tool and access the highest performance.</p>\n\n\n\n<p>You can see this in action by running the pandas portion of the popular <a href=\"https://duckdb.org/2023/04/14/h2oai.html\">DuckDB Database-like Ops Benchmark</a> originally developed by H2o.ai. DuckDB\u2019s benchmark setup compares popular CPU-based DataFrame and SQL engines on a series of common analytics tasks such as joining data together or computing statistical measures on a per-group basis.</p>\n\n\n\n<p>With 5 GB of data, pandas performance slows to a crawl, taking minutes to perform the series of join and advanced groupby operations.</p>\n\n\n\n<p>Historically, running this benchmark with cuDF rather than pandas has required changing the code and working around missing functionality. With cuDF\u2019s new pandas accelerator mode, this is no longer an issue. You can run the pandas benchmark code unchanged and achieve significant speedups, using the GPU for most of the operations and the CPU for a small portion to ensure that the workflow succeeds.</p>\n\n\n\n<p>The results are excellent. The cuDF unified CPU/GPU experience turns minutes of processing into just 1 or 2 seconds with no code change required (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1764\" height=\"1322\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32.png\" alt=\"Bar chart shows a 150x speed increase using pandas with RAPIDS cuDF on NVIDIA Grace Hopper. The 10 join operations take only 1 second with RAPIDS cuDF as opposed to 336 seconds with pandas on CPU, while the 10 groupby advanced operations take 2 seconds with RAPIDS cuDF compared to 288 seconds with pandas on CPU.\" class=\"wp-image-72835\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32.png 1764w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-625x468.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-768x576.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-1536x1151.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-645x483.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-362x271.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-147x110.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/image-32-1024x767.png 1024w\" sizes=\"(max-width: 1764px) 100vw, 1764px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1.</em><strong><em> </em></strong><em><em>Performance comparison between</em>&nbsp;Traditional pandas&nbsp;<em>v1.5 on Intel Xeon Platinum 8480CL CPU and pandas v1.5 with RAPIDS cuDF on NVIDIA Grace Hopper</em></em></figcaption></figure></div>\n\n\n<p>For more information about these benchmark results and how to reproduce them, see the <a href=\"https://docs.rapids.ai/api/cudf/stable/\">cuDF documentation</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>pandas is the most popular DataFrame library in the Python ecosystem, but it slows down as data sizes grow on CPUs.&nbsp;</p>\n\n\n\n<p>With cuDF\u2019s pandas accelerator mode now available in open beta as part of the RAPIDS v23.10 release, you can now bring accelerated computing to your pandas workflows without needing to change your code. Based on an analytics benchmark processing a 5 GB dataset, you can achieve 150x faster processing times.</p>\n\n\n\n<p>Take cuDF\u2019s new pandas accelerator mode for a <a href=\"http://nvda.ws/rapids-cudf\">test drive with this detailed walkthrough notebook</a> in a free GPU-enabled environment on Google Colab.</p>\n\n\n\n<p>For more information, see the <a href=\"https://rapids.ai/cudf-pandas\">cuDF pandas accelerator mode</a> page on the RAPIDS website.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA announced that RAPIDS cuDF can now bring GPU acceleration to 9.5M million pandas users without requiring them to change their code. pandas, a flexible and powerful data analysis and manipulation library for Python, is a top choice for data scientists because of its easy-to-use API. However, as dataset sizes grow, it struggles with processing &hellip; <a href=\"https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/\">Continued</a></p>\n", "protected": false}, "author": 1130, "featured_media": 72593, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1293765", "discourse_permalink": "https://forums.developer.nvidia.com/t/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/272201", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 1731], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/rapids-ai-day-announcement-press-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iSP", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72591"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1130"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72591"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72591/revisions"}], "predecessor-version": [{"id": 73995, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72591/revisions/73995"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72593"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72591"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72591"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72591"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72424, "date": "2023-11-07T14:22:37", "date_gmt": "2023-11-07T22:22:37", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72424"}, "modified": "2023-12-05T11:04:45", "modified_gmt": "2023-12-05T19:04:45", "slug": "cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo/", "title": {"rendered": "CUDA-Accelerated Robot Motion Generation in Milliseconds with NVIDIA cuRobo"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Real-time autonomous robot navigation powered by a fast motion-generation algorithm can enable applications in several industries such as food and services, warehouse automation, and machine tending. Motion generation for manipulators is extremely challenging, as it requires satisfying complex constraints and minimizing several cost terms.&nbsp;</p>\n\n\n\n<p>In addition, manipulators can have many articulations, complex link geometries, entire goal regions beyond a single configuration, task constraints, and nontrivial kinematic and torque limitations. Prior approaches mitigated this complexity by first planning collision-free geometric paths and then optimizing them locally for smoother plans.&nbsp;</p>\n\n\n\n<p>However, research has increasingly shown that trajectory optimization can be a powerful tool to address problems larger than just trajectory smoothing. Our modern understanding of this robot navigation problem is that it is a large global motion optimization problem.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/FqGsAyjFCAM?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Figure 1. cuRobo\u2019s approach to motion generation</em></figcaption></figure>\n\n\n\n<p>In the video, the steps in motion generation with cuRobo are visualized, starting with inverse kinematics iterations, followed by iterations of trajectory optimization.&nbsp;&nbsp;</p>\n\n\n\n<p><a href=\"https://curobo.org/\">NVIDIA cuRobo</a> formulates the motion generation problem as a global optimization problem and leverages GPUs to solve it with many parallel seeds. cuRobo first performs collision-free inverse kinematics (IK) to find collision-free final joint configurations, followed by trajectory optimization leveraging the final joint configurations as seeds (Figure 1).&nbsp;</p>\n\n\n\n<p>cuRobo also implements a GPU-accelerated, fast graph planner to use as a seed for trajectory optimization for use in extreme cases.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/t2O9sj3yQMs?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Figure 2. cuRobo\u2019s solution to motion generation problems from the </em><a href=\"https://developer.nvidia.com/blog/improving-robot-motion-generation-with-motion-policy-networks/\"><em>motion policy networks</em></a><em> and </em><a href=\"https://arxiv.org/abs/2112.06402\"><em>motionbenchmaker</em></a><em> datasets</em></figcaption></figure>\n\n\n\n<p>In the video, a robot manipulator moves through space avoiding obstacles to reach targets. </p>\n\n\n\n<p>cuRobo is implemented in PyTorch, enabling you to easily implement your own cost terms for motion generation. cuRobo comes with a library of custom robotics CUDA kernels for common and time-consuming tasks. It uses several NVIDIA technologies:&nbsp;</p>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/warp-python\">NVIDIA Warp</a> for mesh distance queries.</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_nvblox\">NVIDIA nvblox</a> for signed distance from depth images.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/cuda-graphs/\">CUDA Graphs</a> for reducing kernel launch overheads.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim</a> for rendering and examples.&nbsp;</li>\n</ul>\n\n\n\n<p>cuRobo also runs on the NVIDIA Jetson enabling embedded applications.</p>\n\n\n\n<p>Results show that cuRobo can generate motion plans within 100 ms (median) on NVIDIA AGX Orin. Figure 3 shows an example integration of cuRobo running on an NVIDIA Jetson AGX Orin on a UR10.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Bpa1gEr5RXU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Figure 3. cuRobo generates collision-free minimum-jerk motions&nbsp;</em></figcaption></figure>\n\n\n\n<p>In the video, cuRobo generates smooth motion for a UR10 robot rapidly for dynamically appearing targets. </p>\n\n\n\n<p>cuRobo provides CUDA-accelerated implementations of several motion generation components including kinematics, collision checking, inverse kinematics, numerical optimization solvers, trajectory optimization, and motion generation. Results show that cuRobo solves complex problems in milliseconds, significantly faster than existing approaches (Figure 4).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"549\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range.jpg\" alt=\"Bar chart shows compute time for Forward Kinematics, Collision Checking, Inverse Kinematics, Collision-Free Inverse Kinematics, Geometric Planning, Motion Generation on NVIDIA RTX 4090, and Motion Generation on NVIDIA AGX Orin.\" class=\"wp-image-72796\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range.jpg 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-300x82.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-625x172.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-179x49.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-768x211.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-1536x422.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-645x177.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-500x137.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-160x44.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-362x99.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-401x110.jpg 401w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/compute-time-range-1024x281.jpg 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Median compute time across different modules</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">More resources</h2>\n\n\n\n<p>cuRobo code is available at <a href=\"https://www.github.com/nvlabs/curobo\">/NVlabs/curobo</a>. For more information, see <a href=\"https://curobo.org\">CuRobo: CUDA Accelerated Robot Library</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Real-time autonomous robot navigation powered by a fast motion-generation algorithm can enable applications in several industries such as food and services, warehouse automation, and machine tending. Motion generation for manipulators is extremely challenging, as it requires satisfying complex constraints and minimizing several cost terms.&nbsp; In addition, manipulators can have many articulations, complex link geometries, entire &hellip; <a href=\"https://developer.nvidia.com/blog/cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo/\">Continued</a></p>\n", "protected": false}, "author": 1905, "featured_media": 72722, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1293124", "discourse_permalink": "https://forums.developer.nvidia.com/t/cuda-accelerated-robot-motion-generation-in-milliseconds-with-nvidia-curobo/272121", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2758, 63, 503], "tags": [453, 3581, 264], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/cuRobo.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-iQ8", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72424"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1905"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72424"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72424/revisions"}], "predecessor-version": [{"id": 72883, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72424/revisions/72883"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72722"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72424"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72424"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72424"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72651, "date": "2023-11-07T13:30:00", "date_gmt": "2023-11-07T21:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72651"}, "modified": "2024-01-12T13:02:18", "modified_gmt": "2024-01-12T21:02:18", "slug": "getting-started-with-large-language-models-for-enterprise-solutions", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/", "title": {"rendered": "Getting Started with Large Language Models for Enterprise Solutions"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large language models (LLMs)</a> are <a href=\"https://www.nvidia.com/en-us/glossary/data-science/deep-learning/\">deep learning</a> algorithms that are trained on Internet-scale datasets with hundreds of billions of parameters. LLMs can read, write, code, draw, and augment human creativity to improve productivity across industries and solve the world\u2019s toughest problems.&nbsp;</p>\n\n\n\n<p>LLMs are used in a wide range of industries, from retail to healthcare, and for a wide range of tasks. They learn the language of protein sequences to generate new, viable compounds that can help <a href=\"https://blogs.nvidia.com/blog/2023/07/13/generative-ai-for-industries/\">scientists develop groundbreaking, life-saving vaccines</a>. They help software programmers generate code and fix bugs based on natural language descriptions. And they provide productivity co-pilots so humans can do what they do best\u2014create, question, and understand.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-625x352.png\" alt=\"A prompt is submitted into a large language model and can be leveraged for many different use cases, from content generation to summarization, translation, classification, or chatbots. \" class=\"wp-image-76527\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-graphic-vertical-1.png 1920w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. LLMs are used to generate content, summarize, translate, classify, answer questions, and much more</em></figcaption></figure></div>\n\n\n<p>Effectively leveraging LLMs in enterprise applications and workflows requires understanding key topics such as model selection, customization, optimization, and deployment. This post explores the following enterprise LLM topics:</p>\n\n\n\n<ol>\n<li><a href=\"#enterprise-llms-how-organizations-use\">How organizations are using LLMs</a></li>\n\n\n\n<li><a href=\"#use-customize-build\">Use, customize, or build an LLM?</a>&nbsp;</li>\n\n\n\n<li><a href=\"#foundation-models\">Begin with foundation models</a></li>\n\n\n\n<li><a href=\"#custom-language-model\">Build a custom language model</a></li>\n\n\n\n<li><a href=\"#connect-llm-external-data\">Connect an LLM to external data</a></li>\n\n\n\n<li><a href=\"#keep-llms-secure\">Keep LLMs secure and on track</a></li>\n\n\n\n<li><a href=\"#optimize-llm-inference\">Optimize LLM inference in production</a></li>\n\n\n\n<li><a href=\"#get-started-using-llms\">Get started using LLMs</a></li>\n</ol>\n\n\n\n<p>Whether you are a data scientist looking to build custom models or a chief data officer exploring the potential of LLMs for your organization, read on for valuable insights and guidance.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"enterprise-llms-how-organizations-use\">How organizations are using LLMs</h2>\n\n\n\n<p>LLMs are used in a wide variety of applications <a href=\"https://blogs.nvidia.com/blog/2023/07/13/generative-ai-for-industries/\">across industries</a> to efficiently recognize, summarize, translate, predict, and generate text and other forms of content based on knowledge gained from massive datasets. For example, companies are leveraging LLMs to develop chatbot-like interfaces that can support users with customer inquiries, provide personalized recommendations, and assist with internal knowledge management.&nbsp;</p>\n\n\n\n<p>LLMs also have the potential to <a href=\"https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\">broaden the reach of AI across industries and enterprises</a> and enable a new wave of research, creativity, and productivity. They can help generate complex solutions to challenging problems in fields such as <a href=\"https://blogs.nvidia.com/blog/2023/06/07/nyu-large-language-model-patient-readmission-nature/\">healthcare</a> and <a href=\"https://nvidianews.nvidia.com/news/nvidia-unveils-large-language-models-and-generative-ai-services-to-advance-life-sciences-r-d#:~:text=GTC%E2%80%94NVIDIA%20today%20announced%20an,chemistry%2C%20biology%20and%20molecular%20dynamics.\">chemistry</a>. LLMs are also used to create reimagined search engines, tutoring chatbots, composition tools, <a href=\"https://blogs.nvidia.com/blog/2023/08/08/writer-nemo-generative-ai/?nvid=nv-int-bnr-397231\">marketing materials</a>, and more.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Intelligent AI assistants and agents</h3>\n\n\n\n<p><a href=\"https://nvidianews.nvidia.com/news/servicenow-and-nvidia-announce-partnership-to-build-generative-ai-across-enterprise-it\">Collaboration between ServiceNow and NVIDIA</a> will help drive new levels of automation to fuel productivity and maximize business impact. Generative AI use cases being explored include developing intelligent virtual assistants and agents to help answer user questions and resolve support requests and using generative AI for automatic issue resolution, knowledge-base article generation, and chat summarization.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Language accessibility</h3>\n\n\n\n<p>A consortium in Sweden is developing a state-of-the-art language model with NVIDIA NeMo Megatron and will make it available to any user in the Nordic region. The team aims to train an LLM with a whopping 175 billion parameters that can handle all sorts of language tasks in the Nordic languages of Swedish, Danish, Norwegian, and potentially Icelandic.&nbsp;</p>\n\n\n\n<p>The project is seen as a strategic asset, a keystone of digital sovereignty in a world that speaks thousands of languages across nearly 200 countries. To learn more, see <a href=\"https://blogs.nvidia.com/blog/2022/06/19/ai-sweden-nlp/\">The King\u2019s Swedish: AI Rewrites the Book in Scandinavia</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Smarter contact centers</h3>\n\n\n\n<p>The leading mobile operator in South Korea, KT, has developed a billion-parameter LLM using the <a href=\"https://www.nvidia.com/en-us/data-center/dgx-superpod/\">NVIDIA DGX SuperPOD</a> platform and <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo framework</a>. NeMo is an end-to-end, cloud-native enterprise framework that provides prebuilt components for building, training, and running custom LLMs.&nbsp;</p>\n\n\n\n<p>KT\u2019s LLM has been used to improve the understanding of the company\u2019s AI-powered speaker, GiGA Genie, which can control TVs, offer real-time traffic updates, and complete other home-assistance tasks based on voice commands. For details, see <a href=\"https://blogs.nvidia.com/blog/2022/09/20/kt-large-language-models/\">No Hang Ups With Hangul: KT Trains Smart Speakers, Customer Call Centers With NVIDIA AI</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\">What Are Large Language Models Used For?</a></li>\n\n\n\n<li><a href=\"https://info.nvidia.com/generative-ai-for-ai-teams.html?&amp;ondemandrgt=yes#\">What AI Teams Need to Know About Generative AI</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/\">Introduction to LLM Agents</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/2023/06/07/nyu-large-language-model-patient-readmission-nature/\">NYU, NVIDIA Collaborate on Large Language Model to Predict Patient Readmission</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/2023/08/08/writer-nemo-generative-ai/?nvid=nv-int-bnr-397231\">Startup \u201cWriter\u201d Pens Generative AI Success Story With NVIDIA NeMo</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"use-customize-build\">Use, customize, or build an LLM?</h2>\n\n\n\n<p>Organizations can choose to use an existing LLM, customize a pretrained LLM, or build a custom LLM from scratch. Using an existing LLM provides a quick and cost-effective solution, while customizing a pretrained LLM enables organizations to tune the model for specific tasks and embed proprietary knowledge. Building an LLM from scratch offers the most flexibility but requires significant expertise and resources.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NeMo</a> offers a choice of several <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">customization techniques</a> and is optimized for at-scale inference of models for language and image applications, with multi-GPU and multi-node configurations. For more details, see <a href=\"https://developer.nvidia.com/blog/unlocking-the-power-of-enterprise-ready-llms-with-nemo/\">Unlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo</a>.&nbsp;</p>\n\n\n\n<p>NeMo makes <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a> model development easy, cost-effective, and fast for enterprises. It is available across all major clouds, including Google Cloud as part of their A3 instances powered by <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPUs</a> to build, customize, and deploy LLMs at scale. To learn more, see <a href=\"https://developer.nvidia.com/blog/streamline-generative-ai-development-with-nvidia-nemo-on-gpu-accelerated-google-cloud/\">Streamline Generative AI Development with NVIDIA NeMo on GPU-Accelerated Google Cloud</a>.</p>\n\n\n\n<p>To quickly try generative AI models such as Llama 2,&nbsp;Mistral 7B, and Nemotron-3 directly from your browser with an easy-to-use interface, see <a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA AI Foundation Models</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources&nbsp;</h3>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">Large Language Models Explained</a></li>\n\n\n\n<li><a href=\"https://info.nvidia.com/building-generative-ai-applications-webinar.html\">Building Generative AI Applications for Enterprise Demands</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/building-your-first-llm-agent-application/\">Building Your First LLM Agent Application</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/unlocking-the-power-of-enterprise-ready-llms-with-nemo/\">Unlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo</a></li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/playground\"></a><a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA AI Foundation Models</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"foundation-models\">Begin with foundation models</h2>\n\n\n\n<p><a href=\"https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/\">Foundation models</a> are large AI models trained on enormous quantities of unlabeled data through self-supervised learning. Examples include Llama 2, GPT-3, and Stable Diffusion.&nbsp;</p>\n\n\n\n<p>The models can handle a wide variety of tasks, such as image classification, natural language processing, and question-answering, with remarkable accuracy.</p>\n\n\n\n<p>These foundation models are the starting point for building more specialized and sophisticated custom models. Organizations can <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">customize foundation models</a> using <a href=\"https://blogs.nvidia.com/blog/2023/03/21/nemo-large-language-models-enterprise-data/\">domain-specific labeled data</a> to create more accurate and context-aware models for specific use cases.&nbsp;</p>\n\n\n\n<p>Foundation models generate an enormous number of unique responses from a single prompt by generating a probability distribution over all items that could follow the input and then choosing the next output randomly from that distribution. The randomization is amplified by the model&#8217;s use of context. Each time the model generates a probability distribution, it considers the last generated item, which means each prediction impacts every prediction that follows.</p>\n\n\n\n<p>NeMo supports NVIDIA-trained foundation models, like Nemotron-3, as well as community models such as Llama 2, Falcon LLM, Mistral 7B, and MPT. You can experience a variety of optimized community and NVIDIA-built foundation models directly from your browser for free on <a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA NGC</a>. You can then customize the foundation model using your proprietary enterprise data. This results in a model that is an expert in your business and domain.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/\">What Are Foundation Models?</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\">NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/\">Build Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/2023/03/21/nemo-large-language-models-enterprise-data/\">Mind the Gap: Large Language Models Get Smarter With Enterprise Data</a></li>\n\n\n\n<li><a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA AI Foundation Models</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/amdocs-telco-industry/\">Ringing in the Future: NVIDIA and Amdocs Bring Custom Generative AI to Global Telco Industry</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"custom-language-model\">Build a custom language model</h2>\n\n\n\n<p>Enterprises will often need custom models to tailor \u200clanguage processing capabilities to their specific use cases and domain knowledge. Custom LLMs enable a business to generate and understand text more efficiently and accurately within a certain industry or organizational context. They empower enterprises to create personalized solutions that align with their brand voice, optimize workflows, provide more precise insights, and deliver enhanced user experiences, ultimately driving a competitive edge in the market.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a> is a powerful framework that provides components for building and training custom LLMs on-premises, across all leading cloud service providers, or in <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>. It includes a <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">suite of customization techniques</a> from prompt learning to parameter-efficient fine-tuning, to reinforcement learning through human feedback (RLHF). NVIDIA also released a new, open customization technique called <a href=\"https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/\">SteerLM</a> that allows for tuning during inference.</p>\n\n\n\n<p>When training an LLM, there is always the risk of it becoming \u201cgarbage in, garbage out.\u201d A large percentage of the effort is acquiring and curating the data that will be used to train or customize the LLM.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/\">NeMo Data Curator</a> is a scalable data-curation tool that enables you to curate trillion-token multilingual datasets for pretraining LLMs. The tool allows you to preprocess and deduplicate datasets with exact or fuzzy deduplication, so you can ensure that models are trained on unique documents, potentially leading to greatly reduced training costs.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">Mastering LLM Techniques: Customization</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/custom-generative-ai-model-development/\">NVIDIA Fast-Tracks Custom Generative AI Model Development for Enterprises</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/llm-semiconductors-chip-nemo/\">Silicon Volley: Designers Tap Generative AI for a Chip Assist</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/2023/10/10/customize-ai-models-steerlm/\">Take the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model\u2019s Responses During Inference</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/\">Curating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"connect-llm-external-data\">Connect an LLM to external data</h2>\n\n\n\n<p>Connecting an LLM to external enterprise data sources enhances its capabilities. This enables the LLM to perform more complex tasks and leverage data that has been created since it was last trained.&nbsp;</p>\n\n\n\n<p><a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">Retrieval augmented generation</a> (RAG) is an architecture that provides an LLM with the ability to use current, curated, domain-specific data sources that are easy to add, delete, and update. With RAG, external data sources are processed into vectors (using an embedding model) and placed into a vector database for fast retrieval at inference time.&nbsp;For more information about how to build a production-grade RAG pipeline, see the <a href=\"https://nvda.ws/47OvlMU\">/GenerativeAIExamples</a> GitHub repo.</p>\n\n\n\n<p>In addition to reducing computational and financial costs, RAG increases accuracy and enables more reliable and trustworthy AI-powered applications. <a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/\">Accelerating vector search</a> is one of the hottest topics in the AI landscape due to its applications in LLMs and generative AI.</p>\n\n\n\n<p><a href=\"https://nvidianews.nvidia.com/news/nemo-retriever-generative-ai-microservice\">NVIDIA NeMo Retriever</a> is a semantic-retrieval microservice to help organizations enhance their generative AI applications with enterprise-grade RAG capabilities.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">What Is Retrieval-Augmented Generation, aka RAG?</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/\">RAG 101: Demystifying Retrieval-Augmented Generation Pipelines</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/\">RAG 101: Retrieval-Augmented Generation Questions Answered</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/\">Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/\">Accelerating Vector Search: Using GPU-Powered Indexes with RAPIDS RAFT</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"keep-llms-secure\">Keep LLMs on track and secure</h2>\n\n\n\n<p>To ensure an LLM&#8217;s behavior aligns with desired outcomes, it&#8217;s important to establish guidelines, monitor its performance, and customize as needed. This involves defining ethical boundaries, addressing biases in training data, and regularly evaluating the model&#8217;s outputs against predefined metrics, often in concert with a guardrails capability. For more information, see <a href=\"https://developer.nvidia.com/blog/nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems\">NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems</a>.</p>\n\n\n\n<p>To address this need, NVIDIA has developed <a href=\"https://blogs.nvidia.com/blog/2023/04/25/ai-chatbot-guardrails-nemo/\">NeMo Guardrails</a>, an open-source toolkit that helps developers ensure their generative AI applications are accurate, appropriate, and safe. It provides a framework that works with all LLMs, including OpenAI\u2019s ChatGPT, to make it easier for developers to build safe and trustworthy LLM conversational systems that leverage foundation models.</p>\n\n\n\n<p>Keeping LLMs secure is of paramount importance for generative AI-powered applications. NVIDIA has also introduced accelerated <a href=\"https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/\">Confidential Computing</a>, a groundbreaking security feature that mitigates threats while providing access to the unprecedented acceleration of NVIDIA H100 Tensor Core GPUs for AI workloads. This feature ensures that sensitive data remains secure and protected, even during processing.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/\">Best Practices for Securing LLM-enabled Applications</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems\">NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/2023/04/25/ai-chatbot-guardrails-nemo/\">Right on Track\u2014NVIDIA Open-Source Software Helps Developers Add Guardrails to AI Chatbots</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/\">Confidential Compute on NVIDIA Hopper H100</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"optimize-llm-inference\">Optimize LLM inference in production</h2>\n\n\n\n<p>Optimizing LLM inference involves techniques such as model quantization, hardware acceleration, and efficient deployment strategies. Model quantization reduces the memory footprint of the model, while hardware acceleration leverages specialized hardware like GPUs for faster inference. Efficient deployment strategies ensure scalability and reliability in production environments.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a> is an open-source software library that supercharges large LLM inference on NVIDIA accelerated computing. It enables users to convert their model weights into a new FP8 format and compile their models to take advantage of optimized FP8 kernels with NVIDIA H100 GPUs. TensorRT-LLM can accelerate inference performance by 4.6x compared to NVIDIA A100 GPUs. It provides a faster and more efficient way to run LLMs, making them more accessible and cost-effective.</p>\n\n\n\n<p>These custom generative AI processes involve pulling together models, frameworks, toolkits, and more. Many of these tools are open source, requiring time and energy to maintain development projects. The process can become incredibly complex and time-consuming, especially when trying to collaborate and deploy across multiple environments and platforms.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">NVIDIA AI Workbench </a>helps simplify this process by providing a single platform for managing data, models, resources, and compute needs. This enables seamless collaboration and deployment for developers to create cost-effective, scalable generative AI models quickly.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/vmware/\">NVIDIA and VMware are working together</a> to transform the modern data center built on VMware Cloud Foundation and bring AI to every enterprise. Using the NVIDIA AI Enterprise suite and NVIDIA\u2019s most advanced GPUs and data processing units (DPUs), VMware customers can securely run modern, accelerated workloads alongside existing enterprise applications on <a href=\"https://www.nvidia.com/en-us/data-center/products/certified-systems/\">NVIDIA-Certified Systems</a>.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">Mastering LLM Techniques: Inference Optimization</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/\">NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/\">New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/ignite-rtx-ai-tensorrt-llm-chat-api/\">New Models, Tools and Resources for Windows Development on RTX PCs</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/vmware/\">Modernizing the Data Center with VMware and NVIDIA</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"get-started-using-llms\">Get started using LLMs</h2>\n\n\n\n<p>Getting started with LLMs requires weighing factors such as cost, effort, training data availability, and business objectives. Organizations should evaluate the trade-offs between using existing models and customizing them with domain-specific knowledge versus building custom models from scratch in most circumstances. Choosing tools and frameworks that align with specific use cases and technical requirements is important, including those listed below.</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/launchpad/ai/generative-ai-knowledge-base-chatbot/\">Generative AI Knowledge Base Chatbot</a> lab \u200cshows you how to adapt an existing AI foundational model to accurately generate responses for your specific use case. This free lab provides hands-on experience with customizing a model using prompt learning, ingesting data into a vector database, and chaining all components to create a chatbot.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, available on all major cloud and data center platforms, is a cloud-native suite of AI and data analytics software that provides over 50 frameworks, including the NeMo framework, pretrained models, and development tools optimized for accelerated GPU infrastructures. You can try this end-to-end enterprise-ready software suite is with a <a href=\"http://www.nvidia.com/ai-enterprise-eval\">free 90-day trial</a>.</p>\n\n\n\n<p><a href=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/intro.html\">NeMo</a> is an end-to-end, cloud-native enterprise framework for developers to build, customize, and deploy generative AI models with billions of parameters. It is optimized for at-scale inference of models with multi-GPU and multi-node configurations. The framework makes generative AI model development easy, cost-effective, and fast for enterprises. Explore the <a href=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/starthere/tutorials.html\">NeMo tutorials</a> to get started.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/learn/organizations/\">NVIDIA Training</a> helps organizations train their workforce on the latest technology and bridge the skills gap by offering comprehensive technical hands-on workshops and courses. The <a href=\"https://nvdam.widen.net/s/rvsgdxpfkz/dli-generative-ai-llm-learning-path-2740963\">LLM learning path</a> developed by NVIDIA subject matter experts spans fundamental to advanced topics that are relevant to software engineering and IT operations teams. <a href=\"https://www.nvidia.com/en-us/learn/organizations/request/\">NVIDIA Training Advisors</a> are available to help develop customized training plans and offer team pricing.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Key resources</h3>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/launchpad/ai/generative-ai-knowledge-base-chatbot/\">Generative AI Knowledge Base Chatbot</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/ai-enterprise/workflows-generative-ai/0.1.0/technical-brief.html\">AI Chatbot with Retrieval Augmented Generation</a></li>\n\n\n\n<li><a href=\"https://info.nvidia.com/building-intelligent-ai-chatbots-using-rag-webinar.html?ondemandrgt=yes#\" data-type=\"link\" data-id=\"https://info.nvidia.com/building-intelligent-ai-chatbots-using-rag-webinar.html?ondemandrgt=yes#\">Building Intelligent AI Chatbots Using RAG</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise Free Trial</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/starthere/tutorials.html\">NVIDIA NeMo Tutorials</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/learn/organizations/\">NVIDIA Training</a></li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>As enterprises race to keep pace with AI advancements, identifying the best approach for adopting LLMs is essential. Foundation models help jumpstart the development process. Using key tools and environments to efficiently process and store data and customize models can significantly accelerate productivity and advance business goals.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) are deep learning algorithms that are trained on Internet-scale datasets with hundreds of billions of parameters. LLMs can read, write, code, draw, and augment human creativity to improve productivity across industries and solve the world\u2019s toughest problems.&nbsp; LLMs are used in a wide range of industries, from retail to healthcare, and &hellip; <a href=\"https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/\">Continued</a></p>\n", "protected": false}, "author": 1557, "featured_media": 72652, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1293104", "discourse_permalink": "https://forums.developer.nvidia.com/t/getting-started-with-large-language-models-for-enterprise-solutions/272112", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [453, 2932, 1133], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/ai-model-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iTN", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72651"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1557"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72651"}], "version-history": [{"count": 50, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72651/revisions"}], "predecessor-version": [{"id": 76606, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72651/revisions/76606"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72652"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72651"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72651"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72651"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72433, "date": "2023-11-07T08:07:22", "date_gmt": "2023-11-07T16:07:22", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72433"}, "modified": "2023-11-16T11:16:46", "modified_gmt": "2023-11-16T19:16:46", "slug": "video-exploring-speech-ai-from-research-to-practical-production-applications", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/video-exploring-speech-ai-from-research-to-practical-production-applications/", "title": {"rendered": "Video: Exploring Speech AI from Research to Practical Production Applications"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The integration of speech and translation AI into our daily lives is rapidly reshaping our interactions, from virtual assistants to call centers and augmented reality experiences. <a href=\"https://www.nvidia.com/en-us/events/speech-ai-day/\">Speech AI Day</a> provided valuable insights into the latest advancements in speech AI, showcasing how this technology addresses real-world challenges.&nbsp;</p>\n\n\n\n<p>In this first of three Speech AI Day sessions, experts from <a href=\"https://www.cmu.edu/\">Carnegie Mellon University</a>, <a href=\"https://www.hippocraticai.com/\">Hippocratic AI</a>, <a href=\"https://www.suno.ai/\">Suno</a>, and <a href=\"https://www.wipro.com/ai/\">Wipro</a> discussed deploying speech AI to maximize business investment.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Key takeaways</h2>\n\n\n\n<ul>\n<li><strong>Unified compatible framework</strong>: Establishing a standardized speech AI development framework ensures seamless compatibility between different components. This fosters easier speech AI solutions development and deployment and ultimately enhances the overall quality of speech AI services.</li>\n\n\n\n<li><strong>Efficiency through MLOps</strong>: Implementing MLOps streamlines model management from research to production, enabling companies to overcome the challenges associated with transitioning from proof-of-concepts to full-scale&nbsp;production implementations.</li>\n\n\n\n<li><strong>Rigorous reliability testing</strong>: A thorough testing and validation process is vital for ensuring the accuracy and reliability of speech AI solutions. This involves evaluating the solution\u2019s understanding of various speech types and its ability to handle errors and unexpected inputs effectively.</li>\n\n\n\n<li><strong>Versatility in handling audio: </strong>Speech AI&#8217;s capability to process both verbal and non-verbal audio expands its utility across diverse applications, enhancing its practicality and applicability.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/PrqsHUbRF_0?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video: Exploring Speech AI From Research to Practical Production Applications</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>The advancements in <a href=\"https://www.nvidia.com/en-us/ai-data-science/solutions/speech-ai/?nvid=nv-int-bnr-618956\">speech AI</a> research are revolutionizing the development of multilingual applications, allowing concurrent understanding of different languages. Cutting-edge multilingual speech technologies empower you to create applications and deliver superior user experiences transcending cultural and national boundaries.</p>\n\n\n\n<p>For in-depth insights into the latest trends and techniques in speech and translation AI, including automatic speech recognition (ASR), text-to-speech (TTS), and neural machine translation (NMT), see the following resources:</p>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/events/speech-ai-day/\">Speech AI Day</a>: Access all three Speech AI Day sessions on-demand, featuring presentations from leading companies such as Motorola and Deloitte.</li>\n\n\n\n<li><a href=\"https://resources.nvidia.com/en-us-speech-ai-ebooks\">Speech AI Ebook</a>: Get a comprehensive overview of the speech AI landscape, understanding its functionalities and significance across various industries.&nbsp;</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a>: Dive into NVIDIA Riva, a GPU-accelerated speech and translation AI with automatic speech recognition, text-to-speech, and neural machine translation skills ideal for conversational applications across cloud platforms, on-premises, at the edge, and embedded devices.</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>The integration of speech and translation AI into our daily lives is rapidly reshaping our interactions, from virtual assistants to call centers and augmented reality experiences. Speech AI Day provided valuable insights into the latest advancements in speech AI, showcasing how this technology addresses real-world challenges.&nbsp; In this first of three Speech AI Day sessions, &hellip; <a href=\"https://developer.nvidia.com/blog/video-exploring-speech-ai-from-research-to-practical-production-applications/\">Continued</a></p>\n", "protected": false}, "author": 902, "featured_media": 72437, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1292999", "discourse_permalink": "https://forums.developer.nvidia.com/t/video-exploring-speech-ai-from-research-to-practical-production-applications/272091", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050], "tags": [453, 3545, 1976, 3166, 106], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/exploring-speech-ai-possibilities-video-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iQh", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72433"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/902"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72433"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72433/revisions"}], "predecessor-version": [{"id": 72617, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72433/revisions/72617"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72437"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72433"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72433"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72433"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72341, "date": "2023-11-06T14:20:18", "date_gmt": "2023-11-06T22:20:18", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72341"}, "modified": "2023-11-16T11:16:46", "modified_gmt": "2023-11-16T19:16:46", "slug": "level-up-your-lighting-qa-with-lighting-artist-ted-mebratu", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/level-up-your-lighting-qa-with-lighting-artist-ted-mebratu/", "title": {"rendered": "Level Up Your Lighting: Q&A with Lighting Artist Ted Mebratu"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA recently caught up with veteran-level lighting artist Ted Mebratu to find out how he pushed real-time lighting to its limits with the <a href=\"https://www.unrealengine.com/marketplace/en-US/product/d189ac5607b8425e88a1ec6df2dfaa65\">Rainy Neon Lights scene</a> created by environment artist Maarten Hof.&nbsp; Using an <a href=\"https://www.nvidia.com/en-me/geforce/graphics-cards/30-series/rtx-3090-3090ti/\">NVIDIA RTX 3090Ti</a> and the <a href=\"https://developer.nvidia.com/game-engines/unreal-engine/rtx-branch\">NVIDIA RTX Branch of Unreal Engine (NvRTX</a>), Mebratu spoke to NVIDIA about what his aspirations were for the scene and pushing the limits of real-time lighting.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/iHlnaTgTbZc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1.&nbsp; A dynamically lit scene with hundreds of shadow-casting lights</em></figcaption></figure>\n\n\n\n<p><strong>What do you want to achieve with this scene?</strong></p>\n\n\n\n<p><strong>Mebratu: </strong>Ultimately, I wanted to explore the potential future of real-time lighting for the next generation of games and real-time graphics. Most current and upcoming games still need to rely on a prebaked global illumination system to some extent. This is because ray tracing and real-time global illumination remain quite resource-intensive. There&#8217;s a significant concern regarding artistic freedom when working with a fully dynamic GI system.</p>\n\n\n\n<p>With a prebaked lighting setup, you can incorporate thousands of baked lights without impacting performance, except for the bake time. This enables significant artistic control over where your light goes, what it affects, and how far and intensely indirect bounces travel across the scene. However, when using real-time global illumination, your options become constrained because each additional light that you introduce comes with a performance cost.</p>\n\n\n\n<p>Technologies like RTXDI and DLSS have the potential to narrow this gap, enabling much greater artistic control and capability while maintaining performance efficiency.</p>\n\n\n\n<p><strong>Why did you choose RTXDI for this project?</strong></p>\n\n\n\n<p><strong>Mebratu: </strong>I&#8217;m always looking to push the limits of real-time lighting with Unreal Engine 5 in my personal projects. When I came across <a href=\"https://developer.nvidia.com/rtx/ray-tracing/rtxdi\">RTXDI</a>, the decision was a no-brainer for me. Having an unlimited runtime light budget combined with a real-time global illumination technique like Lumen creates an incredible combination for achieving results that are near to path-traced quality.&nbsp; For the scene, there were a total of 141 dynamic shadow-casting lights with the attenuation radius set to maximum.&nbsp; This enabled me to focus on the aesthetics and not worry about technical limitations.</p>\n\n\n\n<p><strong>Was installing and getting up to speed with RTXDI through NvRTX a smooth process?</strong></p>\n\n\n\n<p><strong>Mebratu:</strong> I have experience building engines from sources like <a href=\"https://www.nvidia.com/en-gb/geforce/technologies/vxgi/technology/\">voxel-based GI (VXGI)</a>, so compiling NvRTX was a smooth process.&nbsp; I compiled the UE5.2 version of the <a href=\"https://developer.nvidia.com/game-engines/unreal-engine/rtx-branch\">NvRTX</a> branch and did a series of extensive lighting tests and scenarios. Specifically, I selected scenes that were densely populated with numerous light sources and approached the scene lighting from a real-world perspective. Since I didn&#8217;t have to worry about faking lighting effects for the sake of performance, I placed point, spot, and area lights with infinite radii and adjusted the sizes and shapes of the lights to closely match the actual shapes of the light sources.</p>\n\n\n\n<h2 class=\"wp-block-heading\">More resources</h2>\n\n\n\n<p>For more artwork by Ted, check out his <a href=\"https://www.artstation.com/tedm\">ArtStation page</a>.&nbsp; Learn more about <a href=\"https://developer.nvidia.com/game-engines/unreal-engine\">NVIDIA resources for Unreal Engine developers</a> and <a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-da3472ed-1de5-492d-a6de-b03f1063ecaa/\">NvRTX training resources</a>, and join the <a href=\"https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia\">Level Up with NVIDIA webinar series</a> to ask questions directly about your NVIDIA RTX integrations in Unreal Engine.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA recently caught up with veteran-level lighting artist Ted Mebratu to find out how he pushed real-time lighting to its limits with the Rainy Neon Lights scene created by environment artist Maarten Hof.&nbsp; Using an NVIDIA RTX 3090Ti and the NVIDIA RTX Branch of Unreal Engine (NvRTX), Mebratu spoke to NVIDIA about what his aspirations &hellip; <a href=\"https://developer.nvidia.com/blog/level-up-your-lighting-qa-with-lighting-artist-ted-mebratu/\">Continued</a></p>\n", "protected": false}, "author": 1380, "featured_media": 72342, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1292245", "discourse_permalink": "https://forums.developer.nvidia.com/t/level-up-your-lighting-q-a-with-lighting-artist-ted-mebratu/271990", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [453, 3536, 1480, 582], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ted-mebratu-qa-featured-neon-lights.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iON", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72341"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1380"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72341"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72341/revisions"}], "predecessor-version": [{"id": 72650, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72341/revisions/72650"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72342"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72341"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72341"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72341"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72618, "date": "2023-11-06T13:37:40", "date_gmt": "2023-11-06T21:37:40", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72618"}, "modified": "2023-11-16T11:16:46", "modified_gmt": "2023-11-16T19:16:46", "slug": "join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/", "title": {"rendered": "Join the First NVIDIA LLM Developer Day: Elevate Your App-Building Skills"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/events/llm-developer-day/\">NVIDIA LLM Developer Day</a> is a virtual event providing hands-on guidance for developers exploring and building LLM-based applications and services. You can gain an understanding \u200cof key technologies, their pros and cons, and explore example applications. The sessions also cover how to create, customize, and deploy applications using managed APIs, self-managed LLMs, and Retrieval Augmented Generation.</p>\n\n\n\n<p>The NVIDIA Deep Learning Institute is hosting the event and sessions on November 17, starting at 8 a.m. PT (5 p.m. CEST). Participation is free of charge.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Learn about key LLM techniques, tools, and resources&nbsp;</h2>\n\n\n\n<p>As the <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a> technology ecosystem explodes, offering a myriad of choices across different levels of the stack, navigating where to begin is challenging.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://event.on24.com/eventRegistration/EventLobbyServlet?target=reg20.jsp&amp;eventid=4371300&amp;sessionid=1&amp;key=9C96C2D84F0E9BB46E757F807129A8C3&amp;groupId=5023819&amp;sourcepage=register\">sessions</a> at LLM Developer Day are designed to help developers evaluate the starting point for their use case, while providing resources and tools to begin.</p>\n\n\n\n<p>Session highlights include:</p>\n\n\n\n<ul>\n<li><strong>The Fast Path to Developing with LLMs: </strong>Explore practical techniques for deploying LLM-powered systems using popular APIs.\u00a0</li>\n\n\n\n<li><strong>Tailoring LLMs to Your Use Case:</strong> Learn how to push the limits of off-the-shelf models and APIs by customizing your models for domain-specific applications.&nbsp;</li>\n\n\n\n<li><strong>Running Your Own LLM:</strong> Discover how to leverage open, commercially licensed LLMs running on commonly available hardware and optimizers for lower-latency and higher-throughput inference, reducing compute needs.</li>\n\n\n\n<li><strong>Live Q&amp;A:</strong> Connect with NVIDIA experts and get your questions answered.&nbsp;</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Attend LLM Developer Day</h2>\n\n\n\n<p>Save the date, spread the word, and <a href=\"https://www.nvidia.com/en-us/events/llm-developer-day/\">join us on November 17</a> as we unveil new tools and technologies, and share insights to help you lead the AI wave.</p>\n\n\n\n<p>As an event participant, you will also receive a special discount for continuing your learning journey through the <a href=\"https://www.nvidia.com/en-us/training/\">NVIDIA Deep Learning Institute</a>. Check out our<a href=\"https://nvdam.widen.net/s/rvsgdxpfkz/dli-generative-ai-llm-learning-path-2740963\"> LLM Training paths</a>, with instructor-led hands-on workshops, and earn a certificate upon successful completion of your training.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA LLM Developer Day is a virtual event providing hands-on guidance for developers exploring and building LLM-based applications and services. You can gain an understanding \u200cof key technologies, their pros and cons, and explore example applications. The sessions also cover how to create, customize, and deploy applications using managed APIs, self-managed LLMs, and Retrieval Augmented &hellip; <a href=\"https://developer.nvidia.com/blog/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/\">Continued</a></p>\n", "protected": false}, "author": 1565, "featured_media": 72619, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1292238", "discourse_permalink": "https://forums.developer.nvidia.com/t/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/271988", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1903], "tags": [2964, 452, 453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/LLM-Developer-Day.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iTg", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72618"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1565"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72618"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72618/revisions"}], "predecessor-version": [{"id": 72649, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72618/revisions/72649"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72619"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72618"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72618"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72618"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72637, "date": "2023-11-06T11:17:06", "date_gmt": "2023-11-06T19:17:06", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72637"}, "modified": "2023-11-16T11:16:47", "modified_gmt": "2023-11-16T19:16:47", "slug": "icymi-leveraging-the-power-of-gpus-with-cupy-in-python", "status": "publish", "type": "post", "link": "https://nvda.ws/3u5L3Es", "title": {"rendered": "ICYMI: Leveraging the Power of GPUs with CuPy in Python"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>See how KDNuggets achieved 500x speedup using CuPy and NVIDIA CUDA on 3D arrays.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>See how KDNuggets achieved 500x speedup using CuPy and NVIDIA CUDA on 3D arrays.</p>\n", "protected": false}, "author": 1288, "featured_media": 72644, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3u5L3Es", "_links_to_target": "_blank"}, "categories": [696], "tags": [3273, 453, 61, 783], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/Leveraging-power-gpus-cupy-python-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iTz", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72637"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1288"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72637"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72637/revisions"}], "predecessor-version": [{"id": 72645, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72637/revisions/72645"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72644"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72637"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72637"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72637"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72417, "date": "2023-11-06T09:20:42", "date_gmt": "2023-11-06T17:20:42", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72417"}, "modified": "2023-11-16T11:16:47", "modified_gmt": "2023-11-16T19:16:47", "slug": "webinar-accelerate-edge-ai-development-with-nvidia-metropolis-apis-and-microservices-on-nvidia-jetson", "status": "publish", "type": "post", "link": "https://info.nvidia.com/metropolis-on-jetson-webinar.html", "title": {"rendered": "Webinar: Accelerate Edge AI Development with NVIDIA Metropolis APIs and Microservices on NVIDIA Jetson"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Explore how Metropolis APIs and microservices on NVIDIA Jetson can significantly reduce vision AI development timelines from years to months.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Explore how Metropolis APIs and microservices on NVIDIA Jetson can significantly reduce vision AI development timelines from years to months.</p>\n", "protected": false}, "author": 1115, "featured_media": 72418, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1292162", "discourse_permalink": "https://forums.developer.nvidia.com/t/webinar-accelerate-edge-ai-development-with-nvidia-metropolis-apis-and-microservices-on-nvidia-jetson/271965", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://info.nvidia.com/metropolis-on-jetson-webinar.html", "_links_to_target": "_blank"}, "categories": [2724, 2758, 63], "tags": [453, 90], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/jetson-ai-summit-blog-2964350-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iQ1", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72417"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72417"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72417/revisions"}], "predecessor-version": [{"id": 72423, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72417/revisions/72423"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72418"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72417"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72417"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72417"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]