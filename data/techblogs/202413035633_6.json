[{"id": 72501, "date": "2023-11-03T08:00:00", "date_gmt": "2023-11-03T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72501"}, "modified": "2023-11-16T11:37:36", "modified_gmt": "2023-11-16T19:37:36", "slug": "analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim/", "title": {"rendered": "Analyze, Visualize, and Optimize Real-World Processes with OpenUSD in FlexSim"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>For manufacturing and industrial enterprises, efficiency and precision are essential. To streamline operations, reduce costs, and enhance productivity, companies are turning to <a href=\"https://blogs.nvidia.com/blog/2021/12/14/what-is-a-digital-twin/\">digital twins</a> and discrete-event simulation.\u00a0</p>\n\n\n\n<p>Discrete-event simulation enables manufacturers to optimize processes by experimenting with different inputs and behaviors that can be modeled and tested step by step.</p>\n\n\n\n<p><a href=\"https://www.flexsim.com/\">FlexSim</a> is a simulation modeling software provider that specializes in discrete event simulation and enables users to analyze, visualize, and optimize real-world processes across various industries. With a powerful toolset for creating and running simulations, it\u2019s being used in industries such as manufacturing, warehousing, and healthcare to improve complex systems and operations.</p>\n\n\n\n<p>Recently, FlexSim developed an <a href=\"https://www.nvidia.com/en-us/omniverse/ecosystem/\">NVIDIA Omniverse Connector</a>, enabling engineers, designers, and simulation experts to seamlessly convert FlexSim models to <a href=\"https://developer.nvidia.com/usd\">Universal Scene Description (OpenUSD)</a> format. </p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/omniverse\">NVIDIA Omniverse</a> is a development computing platform that enables developers to build interoperable 3D workflows and tools based on OpenUSD. OpenUSD is an extensible open-source framework for describing, composing, and collaborating within 3D worlds, initially developed by Pixar Animation Studios.</p>\n\n\n\n<p>FlexSim leverages the Omniverse Connector to enable users to visualize their simulation models in real time, with photoreal quality, using the Omniverse platform\u2019s native RTX Renderer. This advanced visualization, now powered by DLSS 3.5, aids in decision-making, as stakeholders gain a better understanding of complex systems through interactive 3D models.</p>\n\n\n\n<p>With the most recent update to FlexSim, users can now export 3D models and assets to USD, bridging the gap between their simulation data and real-time 3D visualization in Omniverse.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/m3xdFom5qnk?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Export FlexSim models in USD format for real-time, collaborative editing in NVIDIA Omniverse</em></figcaption></figure>\n\n\n\n<p>FlexSim has long been a favored tool among conveyor, automated guided vehicle (AGV), and wire-guided industrial robotics system planners. In recent years, the proliferation of free-roaming autonomous mobile robots (AMRs) occupying more manufacturing and warehouse space has spurred the development of <a href=\"https://docs.flexsim.com/en/23.2/Reference/Tools/AStar/AStar.html\">FlexSim A* navigation</a>.\u00a0</p>\n\n\n\n<p>Coupling FlexSim\u2019s advanced material-handling toolkit with collaborative layout tools in Omniverse can bring increased flexibility to the simulation-planning phase and greater visual fidelity to the digital twin operational phase.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Seamless digital twin collaboration with OpenUSD</h2>\n\n\n\n<p>OpenUSD is becoming more widely accepted as a standard across various sectors, including visual effects, architecture, design, robotics, and manufacturing, due to its power and versatility. The incorporation of USD into FlexSim&#8217;s workflow has unlocked a range of advantages, benefiting both the FlexSim team and users in various ways.</p>\n\n\n\n<p>One significant advantage is the enhanced collaboration it enables. FlexSim\u2019s Omniverse Connector streamlines collaboration, offering a platform where multiple teams can work together live in a unified digital twin environment on various aspects of a project. This promotes cross-functional communication and synergy among engineers, designers, and simulation experts.</p>\n\n\n\n<p>Data consistency is another critical benefit. FlexSim models often serve as the definitive reference for complex industrial systems. Building support for USD into FlexSim ensures the preservation of not just geometric data, but also essential simulation metadata. This data consistency maintains accuracy throughout the design and simulation phases, ultimately enhancing decision-making.</p>\n\n\n\n<p>In addition, adopting USD aligns FlexSim with the broader 3D design and engineering community. CAD packages increasingly use USD as a standard exchange format. This interoperability simplifies data exchange and accelerates project workflows, improving collaboration with partners and clients who use these tools.</p>\n\n\n\n<p>FlexSim&#8217;s OpenUSD support also includes the ability to use a USD Stage within FlexSim. Located in the 3D Object Library&#8217;s Visual area, the USD Stage functions as a container for 3D objects that enables users to load OpenUSD assets into a FlexSim model.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1919\" height=\"1081\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt.png\" alt=\"The FlexSim user interface, showing a simulation for a conveyor belt in a factory setting\n\" class=\"wp-image-72506\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt.png 1919w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-1536x865.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-usd-stage-conveyor-belt-1024x577.png 1024w\" sizes=\"(max-width: 1919px) 100vw, 1919px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. A USD Stage in FlexSim with a conveyor belt that can be used in Omniverse</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Developing the Connector</h2>\n\n\n\n<p>To develop the Connector, FlexSim\u2019s development team started with the \u201cHello World\u201d C++ and Python sample, which serves as a comprehensive example for developers seeking to build Connectors. The sample program creates a USD Stage on an <a href=\"https://docs.omniverse.nvidia.com/nucleus/latest/index.html\">Omniverse Nucleus</a> server and demonstrates various functionalities, such as:</p>\n\n\n\n<ul>\n<li>Creating a custom mesh and adding it to the stage</li>\n\n\n\n<li>Uploading an MDL material and its textures to a Nucleus server</li>\n\n\n\n<li>Tweaking skeletal mesh animation data with live updates</li>\n</ul>\n\n\n\n<p>The team then built a Live Session experience using the Live Session sample to enable multiple FlexSim users to collaborate on the same scene in real time. The sample demonstrates how to implement numerous live session functionalities, including:</p>\n\n\n\n<ul>\n<li>Setting the edit target to the .live layer so changes replicate to other clients</li>\n\n\n\n<li>Displaying the owner and connected users in a live session</li>\n\n\n\n<li>Merging changes from the .live session back to the root stage</li>\n</ul>\n\n\n\n<p>With live sessions implemented, users can create or join live sessions with real-time, bidirectional collaborative editing of USD Stages. These stages can include assets from various 3D software tools, enabling cross-functional teams to collaborate on the same scene.</p>\n\n\n\n<p>They also used Omni CLI to demonstrate how to use the Client Library API to interact with Nucleus.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"640\" height=\"360\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/live-sync-simulation-model-flexsim-usd-composer.gif\" alt=\"Omniverse Live connection of a simulation model in FlexSim and USD Composer.\" class=\"wp-image-72511\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. A live-sync simulation model in FlexSim and USD Composer</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Get started with FlexSim on Omniverse</h2>\n\n\n\n<p>Download both <a href=\"https://account.flexsim.com/register\">FlexSim</a> and <a href=\"https://www.nvidia.com/en-us/omniverse/download/\">Omniverse</a> for free. In FlexSim, find Omniverse Connector Properties in the Toolbox under Connectivity. You can choose which properties to export to USD, including meshes, camera, textures, and object property tables.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1920\" height=\"1080\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export.png\" alt=\"Screenshot of the FlexSim UI with Omniverse Connector Properties window for exporting assets to USD.\n\" class=\"wp-image-72514\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export.png 1920w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/flexsim-select-properties-usd-export-1024x576.png 1024w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Select properties in FlexSim to export to USD</em></em></figcaption></figure>\n\n\n\n<p>To learn more about developing extensions, Connectors, and applications on Omniverse, see <a href=\"https://developer.nvidia.com/omniverse/get-started/\">Get Started Building on Omniverse</a>.</p>\n\n\n\n<p>To learn about Universal Scene Description, see the <a href=\"https://developer.nvidia.com/usd\">OpenUSD resources</a>. You can also take the self-paced online course, <a href=\"https://courses.nvidia.com/courses/course-v1:DLI+S-FX-02+V1/\">Getting Started with USD for Collaborative 3D Workflows</a>. And join the conversation on the <a href=\"https://forum.aousd.org/\">Alliance for OpenUSD (AOUSD) forums</a> and <a href=\"https://discord.com/channels/827959428476174346/1147014434485715044\">Discord</a>.\u00a0</p>\n\n\n\n<p>Try the free <a href=\"https://developer.nvidia.com/usd/validator\">RunUSD Validation Service</a> to validate the compatibility of your OpenUSD assets and applications against a range of OpenUSD versions and configurations.&nbsp;&nbsp;</p>\n\n\n\n<p><em>Stay up to date on the platform by subscribing to the </em><a href=\"https://nvda.ws/3u5KPv1\"><em>newsletter</em></a><em> and following NVIDIA Omniverse on </em><a href=\"https://www.instagram.com/nvidiaomniverse/\"><em>Instagram</em></a><em>, </em><a href=\"https://www.linkedin.com/showcase/nvidia-omniverse\"><em>LinkedIn</em></a><em>, </em><a href=\"https://medium.com/@nvidiaomniverse\"><em>Medium</em></a><em>, </em><a href=\"https://www.threads.net/@nvidiaomniverse\"><em>Threads</em></a><em> and </em><a href=\"https://twitter.com/nvidiaomniverse\"><em>Twitter</em></a><em>. For more, check out our </em><a href=\"https://forums.developer.nvidia.com/c/omniverse/300\"><em>forums</em></a><em>, </em><a href=\"https://discord.com/invite/XWQNJDNuaC\"><em>Discord server</em></a><em>, </em><a href=\"https://www.twitch.tv/nvidiaomniverse\"><em>Twitch</em></a><em> and </em><a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\"><em>YouTube</em></a><em> channels.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>For manufacturing and industrial enterprises, efficiency and precision are essential. To streamline operations, reduce costs, and enhance productivity, companies are turning to digital twins and discrete-event simulation.\u00a0 Discrete-event simulation enables manufacturers to optimize processes by experimenting with different inputs and behaviors that can be modeled and tested step by step. FlexSim is a simulation modeling &hellip; <a href=\"https://developer.nvidia.com/blog/analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim/\">Continued</a></p>\n", "protected": false}, "author": 1353, "featured_media": 72527, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1290577", "discourse_permalink": "https://forums.developer.nvidia.com/t/analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim/271713", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [97, 1235, 63, 503], "tags": [2375, 637, 453, 1409, 3280, 3096], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/manufacturing-conveyor-boxes2.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iRn", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72501"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1353"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72501"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72501/revisions"}], "predecessor-version": [{"id": 72587, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72501/revisions/72587"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72527"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72501"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72501"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72501"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72331, "date": "2023-11-02T09:00:00", "date_gmt": "2023-11-02T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72331"}, "modified": "2023-11-02T13:24:27", "modified_gmt": "2023-11-02T20:24:27", "slug": "webinar-next-gen-lighting-with-nvidia-nvrtx", "status": "publish", "type": "post", "link": "https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia", "title": {"rendered": "Webinar: Next-Gen Lighting with NVIDIA NvRTX"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how game developers can add leading-edge NVIDIA RTX technologies to Unreal Engine with custom branches.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how game developers can add leading-edge NVIDIA RTX technologies to Unreal Engine with custom branches.</p>\n", "protected": false}, "author": 1480, "featured_media": 72332, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1289971", "discourse_permalink": "https://forums.developer.nvidia.com/t/webinar-next-gen-lighting-with-nvidia-nvrtx/271617", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia", "_links_to_target": "_blank"}, "categories": [1235, 503], "tags": [3284, 453, 483, 582], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/webinar-nvrtx-lighting.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iOD", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72331"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1480"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72331"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72331/revisions"}], "predecessor-version": [{"id": 72334, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72331/revisions/72334"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72332"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72331"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72331"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72331"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72335, "date": "2023-11-01T14:02:45", "date_gmt": "2023-11-01T21:02:45", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72335"}, "modified": "2023-11-02T11:14:27", "modified_gmt": "2023-11-02T18:14:27", "slug": "workshop-computer-vision-for-industrial-inspection", "status": "publish", "type": "post", "link": "https://www.nvidia.com/en-us/training/instructor-led-workshops/computer-vision-for-industrial-inspection/", "title": {"rendered": "Workshop: Computer Vision for Industrial Inspection"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In this instructor-led workshop, learn how to create an end-to-end hardware-accelerated industrial inspection pipeline to automate defect detection. Using real NVIDIA production data set as an example, we show how the application can be easily applied to a variety of manufacturing use cases.&nbsp;You also learn how to extract meaningful insights from the provided data set using pandas <code>DataFrame</code>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In this instructor-led workshop, learn how to create an end-to-end hardware-accelerated industrial inspection pipeline to automate defect detection. Using real NVIDIA production data set as an example, we show how the application can be easily applied to a variety of manufacturing use cases.&nbsp;You also learn how to extract meaningful insights from the provided data set &hellip; <a href=\"https://www.nvidia.com/en-us/training/instructor-led-workshops/computer-vision-for-industrial-inspection/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 72337, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1289265", "discourse_permalink": "https://forums.developer.nvidia.com/t/workshop-computer-vision-for-industrial-inspection/271488", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://www.nvidia.com/en-us/training/instructor-led-workshops/computer-vision-for-industrial-inspection/", "_links_to_target": "_blank"}, "categories": [2724], "tags": [2964, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvt-computer-vision-for-industrial-inspection.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iOH", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72335"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72335"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72335/revisions"}], "predecessor-version": [{"id": 72447, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72335/revisions/72447"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72337"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72335"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72335"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72335"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71735, "date": "2023-11-01T09:00:00", "date_gmt": "2023-11-01T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71735"}, "modified": "2023-11-02T11:52:56", "modified_gmt": "2023-11-02T18:52:56", "slug": "cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing/", "title": {"rendered": "CUDA Toolkit 12.3 Delivers New Features for Accelerated Computing"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The latest release of <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA Toolkit</a> continues to push the envelope of accelerated computing performance using the latest NVIDIA GPUs. New features of this release, <a href=\"https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\">version 12.3</a>, include:</p>\n\n\n\n<ul>\n<li>Lazy loading default on Windows</li>\n\n\n\n<li>Single-step CUDA uninstall on Windows</li>\n\n\n\n<li>Enhanced <a href=\"https://developer.nvidia.com/nsight-compute\">NVIDIA Nsight Compute</a> and <a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA Nsight Systems</a> developer tools</li>\n</ul>\n\n\n\n<p>CUDA and the CUDA Toolkit continue to provide the foundation for all accelerated computing applications in data science, machine learning and deep learning, generative AI with LLMs for both training and inference, graphics and simulation, and scientific computing. CUDA is fundamental to helping solve the world&#8217;s most complex computing problems.</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA Nsight Developer Tools</h2>\n\n\n\n<p>The latest versions of <a href=\"https://developer.nvidia.com/tools-overview\">NVIDIA Nsight Developer Tools</a> are included in the CUDA Toolkit to help you optimize and debug your CUDA applications on <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA Grace Hopper</a> platforms.\u00a0</p>\n\n\n\n<h3 class=\"wp-block-heading\">Nsight Compute</h3>\n\n\n\n<p><a href=\"https://developer.nvidia.com/nsight-compute\">Nsight Compute</a> provides detailed profiling and analysis for CUDA kernels, and version 2023.3 debuts with CUDA Toolkit 12.3. This version includes features that improve performance and data collection and analysis capabilities.\u00a0</p>\n\n\n\n<p>The new PM Sampling feature adds time-correlated kernel performance data. Previously, most performance metrics were aggregated across an entire kernel. This frequently requested feature can help users uncover performance issues that occur in phases within a kernel and temporal effects such as the tail effect (Figure 1). It is included in the <code>--full</code> metric set. It can be added as the PM Sampling section in the GUI, or by adding the <code>--section PmSampling</code> flag to the CLI.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1513\" height=\"515\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling.png\" alt=\"Screenshot of Nsight Compute kernel profiler.\" class=\"wp-image-71744\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling.png 1513w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-300x102.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-625x213.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-179x61.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-768x261.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-645x220.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-500x170.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-160x54.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-362x123.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-323x110.png 323w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-nsight-compute-pm-sampling-1024x349.png 1024w\" sizes=\"(max-width: 1513px) 100vw, 1513px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Nsight Compute PM Sampling</em></em></figcaption></figure></div>\n\n\n<p>Nsight Compute 2023.3 also introduces the ability to compare source code changes across profiles to see how modifications have impacted performance at the source level. To use this feature, set one report as a baseline, and click the Source Comparison button from another report to view highlighted source differences and the associated performance metrics.&nbsp;</p>\n\n\n\n<p>Use the <code>\u2013-lineinfo</code> flag when compiling the kernel to enable source resolution and if the source file is modified in place. Use the Import Source option or <code>--import-source</code> flag to preserve the original source code.&nbsp;</p>\n\n\n\n<p>To learn more about Nsight Compute 2023.3 features, see <a href=\"https://developer.nvidia.com/tools-overview/nsight-compute/get-started\">Getting Started with Nsight Compute</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Nsight Systems</h3>\n\n\n\n<p>CUDA Toolkit 12.3 also includes Nsight Systems 2023.3, a performance tuning tool that profiles hardware metrics and CUDA apps, APIs, and libraries on a unified timeline.</p>\n\n\n\n<p>The latest version of <a href=\"https://developer.nvidia.com/nsight-systems\">Nsight Systems</a> introduces support for <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\">NVIDIA Grace CPU</a>, enabling you to drill into Grace CPU cycles in the context of your application\u2019s performance. Nsight Systems 2023.3 also adds new features, including network interface card (NIC) profiling from the GUI.&nbsp;</p>\n\n\n\n<p>As the primary way that data moves between hardware units on a server, understanding internode communication from the network will help diagnose bottlenecks. Nsight Systems monitors NIC throughput, charting the volume of bytes sent and received. Extended NIC wait times are a strong indication that the internode network needs optimization. Nsight Systems can also profile <a href=\"https://www.nvidia.com/en-us/networking/products/infiniband/\">NVIDIA Quantum InfiniBand</a> switch throughput.&nbsp;</p>\n\n\n\n<p>To learn more about Nsight Systems 2023.3 features, see <a href=\"https://developer.nvidia.com/nsight-systems/get-started\">Getting Started with Nsight Systems</a>.<strong> </strong>For a deeper dive into how Nsight Systems supports development at data center scale, see <a href=\"https://developer.nvidia.com/blog/accelerating-data-center-and-hpc-performance-analysis-with-nvidia-nsight-systems/\">Accelerating Data Center and HPC Performance Analysis with NVIDIA Nsight Systems</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>The CUDA Toolkit 12.3 release enriches the foundational NVIDIA driver and runtime software for accelerated computing while continuing to provide enhanced support for the newest NVIDIA GPUs, accelerated libraries, compilers, and developer tools.&nbsp;</p>\n\n\n\n<p>To learn more, see the <a href=\"https://docs.nvidia.com/cuda/\">CUDA documentation</a>, check out the latest <a href=\"https://www.nvidia.com/en-us/training/\">NVIDIA Deep Learning Institute</a> offerings, and browse the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/collections/cuda_toolkit\">NGC Catalog</a>. Ask questions and join the conversation in the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/cuda/206\">CUDA Developer Forums.</a>&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The latest release of CUDA Toolkit continues to push the envelope of accelerated computing performance using the latest NVIDIA GPUs. New features of this release, version 12.3, include: CUDA and the CUDA Toolkit continue to provide the foundation for all accelerated computing applications in data science, machine learning and deep learning, generative AI with LLMs &hellip; <a href=\"https://developer.nvidia.com/blog/cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing/\">Continued</a></p>\n", "protected": false}, "author": 1257, "featured_media": 71736, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1282678", "discourse_permalink": "https://forums.developer.nvidia.com/t/cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing/270386", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 503, 1903], "tags": [21, 453, 3099, 2780, 529, 197], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/cuda-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iF1", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71735"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1257"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71735"}], "version-history": [{"count": 30, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71735/revisions"}], "predecessor-version": [{"id": 72005, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71735/revisions/72005"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71736"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71735"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71735"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71735"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72317, "date": "2023-10-31T13:00:00", "date_gmt": "2023-10-31T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72317"}, "modified": "2023-11-02T12:19:18", "modified_gmt": "2023-11-02T19:19:18", "slug": "workshop-generative-ai-with-diffusion-models", "status": "publish", "type": "post", "link": "https://nvda.ws/40ge7oP", "title": {"rendered": "New Workshop: Generative AI with Diffusion Models"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Take a deep dive into denoising diffusion models, from building a U-Net to training a text-to-image model.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Take a deep dive into denoising diffusion models, from building a U-Net to training a text-to-image model.</p>\n", "protected": false}, "author": 1466, "featured_media": 72319, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/40ge7oP", "_links_to_target": "_blank"}, "categories": [1235, 3110, 503, 1903], "tags": [3312, 3147, 2964, 1935, 453, 271], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/generative-ai-workshop.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iOp", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72317"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72317"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72317/revisions"}], "predecessor-version": [{"id": 72330, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72317/revisions/72330"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72319"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72317"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72317"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72317"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72286, "date": "2023-10-30T13:00:00", "date_gmt": "2023-10-30T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72286"}, "modified": "2023-11-02T11:14:28", "modified_gmt": "2023-11-02T18:14:28", "slug": "webinar-accelerate-av-development-with-nvidia-dgx-cloud-and-nvidia-ai-enterprise", "status": "publish", "type": "post", "link": "https://info.nvidia.com/accelerating-autonomous-vehicle-development-with-dgx-cloud-and-nvidia-ai-enterprise.html?ncid=em-news-180848", "title": {"rendered": "Webinar: Accelerate AV Development with NVIDIA DGX Cloud and NVIDIA AI Enterprise"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how to leverage NVIDIA AI-powered infrastructure and software to accelerate AV development for maximum efficiency.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how to leverage NVIDIA AI-powered infrastructure and software to accelerate AV development for maximum efficiency.</p>\n", "protected": false}, "author": 1466, "featured_media": 72288, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://info.nvidia.com/accelerating-autonomous-vehicle-development-with-dgx-cloud-and-nvidia-ai-enterprise.html?ncid=em-news-180848", "_links_to_target": "_blank"}, "categories": [852, 503], "tags": [1592, 1051, 453, 1958, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/av-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iNU", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72286"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72286"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72286/revisions"}], "predecessor-version": [{"id": 72295, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72286/revisions/72295"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72288"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72286"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72286"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72286"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72201, "date": "2023-10-29T09:55:33", "date_gmt": "2023-10-29T16:55:33", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72201"}, "modified": "2023-11-02T13:22:40", "modified_gmt": "2023-11-02T20:22:40", "slug": "how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data/", "title": {"rendered": "How to Train Autonomous Mobile Robots to Detect Warehouse Pallet Jacks Using Synthetic Data"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://blogs.nvidia.com/blog/2021/06/08/what-is-synthetic-data/\">Synthetic data</a> can play a key role when training perception AI models that are deployed on autonomous mobile robots (AMRs). This process is becoming increasingly important in manufacturing. For an example of using synthetic data to generate a pretrained model that can detect pallets in a warehouse, see <a href=\"https://developer.nvidia.com/blog/developing-a-pallet-detection-model-using-openusd-and-synthetic-data/\">Developing a Pallet Detection Model Using OpenUSD and Synthetic Data</a>.&nbsp;</p>\n\n\n\n<p>This post explores how to train AMRs to detect warehouse pallet jacks using synthetic data. Pallet jacks are commonly used in warehouses to lift and transport heavy pallets. In a crowded warehouse, it\u2019s important for the AMR to detect and avoid colliding with a pallet jack. </p>\n\n\n\n<p>To achieve this goal, it\u2019s necessary to train the AI model with a large and diverse set of data under varying lighting conditions and occlusions.&nbsp;Real data can rarely capture the full range of potential scenarios. <a href=\"https://www.nvidia.com/en-us/omniverse/synthetic-data/\">Synthetic data generation (SDG)</a>, which is annotated data generated from a 3D simulation, enables developers to overcome the data gap and bootstrap the model training process.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/X5SdELeBhFw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Synthetic data generation using NVIDIA Omniverse Replicator for NVIDIA Isaac Sim</em></figcaption></figure>\n\n\n\n<p>This use case will again take a data-centric approach by manipulating the data, as opposed to changing the model parameters to fit the data. The process begins by generating synthetic data using <a href=\"https://developer.nvidia.com/omniverse/replicator\">NVIDIA Omniverse Replicator</a> in <a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim</a>. Next, train the model with synthetic data in <a href=\"https://developer.nvidia.com/tao-toolkit\">NVIDIA TAO Toolkit</a>. Finally, visualize the model\u2019s performance on real data, and modify the parameters to generate better synthetic data to reach the desired level of performance.</p>\n\n\n\n<p>Omniverse Replicator is a core extension of <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a>, a computing platform that enables individuals and teams to develop workflows based on <a href=\"https://developer.nvidia.com/usd\">Universal Scene Description (OpenUSD)</a>. Replicator enables developers to build custom synthetic data generation pipelines to generate data to bootstrap the training of computer vision models.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Iterating with synthetic data to improve model performance</h2>\n\n\n\n<p>The sections below explain how the team iterated with synthetic data to improve the real-world performance of our object detection model. It walks through the steps using Python scripts that work with the Omniverse Replicator APIs.&nbsp;</p>\n\n\n\n<p>For each iteration, we incrementally changed various parameters in the model and generated new sets of training data. The model\u2019s performance was then validated against real data. We continued this process until we were able to close the sim-to-real gap.&nbsp;</p>\n\n\n\n<p>The process of varying object or scene parameters is called <a href=\"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/randomizer_details.html\">domain randomization</a>. You can randomize many parameters, including location, color, texture, background, lighting of objects and scene, allowing you to generate new data quickly for your model training.&nbsp;</p>\n\n\n\n<p>OpenUSD, an extensible framework, 3D scene description, and the foundation for NVIDIA Omniverse, makes it easy to experiment with different parameters of a scene. Parameters can be modified and tested in individual layers, and users can author non-destructive overrides on top of those layers.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Preparation</h2>\n\n\n\n<p>To get started with this example, you\u2019ll need a system with NVIDIA RTX GPUs and the latest version of <a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim</a> installed. Isaac Sim is a scalable robotics simulation application that leverages the core functionality of Omniverse Replicator for generating synthetic data. For details on installation and configuration, see the <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/installation/requirements.html\">documentation</a> section.&nbsp;</p>\n\n\n\n<p>When Isaac Sim is up and running, you can then download all the assets from&nbsp; <a href=\"https://github.com/NVIDIA-AI-IOT/synthetic_data_generation_training_workflow\">NVIDIA-AI-IOT/synthetic_data_generation_training_workflow</a> on GitHub.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Iteration 1: Changing color and camera position</h2>\n\n\n\n<p>For the first iteration, the team varied the color and pose of the pallet jack, along with the pose of the camera. Follow the steps below to replicate this scenario in your own session.&nbsp;</p>\n\n\n\n<p>Start by loading the stage:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nENV_URL = &quot;/Isaac/Environments/Simple_Warehouse/warehouse.usd&quot;\nopen_stage(prefix_with_isaac_asset_server(ENV_URL))\n</pre></div>\n\n\n<p>Then add pallet jacks and a camera to the scene. The pallet jacks can be loaded from the <a href=\"https://developer.nvidia.com/omniverse/simready-assets\">SimReady Asset</a> library.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nPALLETJACKS = &#91;&quot;http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Scale_A/PalletTruckScale_A01_PR_NVD_01.usd&quot;,\n            &quot;http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Heavy_Duty_A/HeavyDutyPalletTruck_A01_PR_NVD_01.usd&quot;,\n            &quot;http://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/DigitalTwin/Assets/Warehouse/Equipment/Pallet_Trucks/Low_Profile_A/LowProfilePalletTruck_A01_PR_NVD_01.usd&quot;]\n\ncam = rep.create.camera(clipping_range=(0.1, 1000000))\n</pre></div>\n\n\n<p>SimReady, or simulation-ready, assets are physically-accurate 3D objects that encompass accurate physical properties and behavior. They are preloaded with the metadata and annotation required for model training.&nbsp;</p>\n\n\n\n<p>Next, add domain randomization for the pallet jacks and the camera:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nwith cam:\n            \trep.modify.pose(position=rep.distribution.uniform((-9.2, -11.8,     0.4), (7.2, 15.8, 4)),look_at=(0, 0, 0))\n\n     # Get the Palletjack body mesh and modify its color\n     with rep.get.prims(path_pattern=&quot;SteerAxles&quot;):\n          \trep.randomizer.color(colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1)))\n\n   # Randomize the pose of all the added palletjacks\n   with rep_palletjack_group:\n      rep.modify.pose(\n   position=rep.distribution.uniform((-6, -6, 0), (6, 12, 0)),\n   rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\n   scale=rep.distribution.uniform((0.01, 0.01, 0.01), (0.01, 0.01, 0.01)))\n</pre></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"900\" height=\"474\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2.png\" alt=\"Composite of three synthetic images showing pallet jacks of different colors and a randomized camera position.\" class=\"wp-image-72243\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2.png 900w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-625x329.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-768x404.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-645x340.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-362x191.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-randomized-color-location2-209x110.png 209w\" sizes=\"(max-width: 900px) 100vw, 900px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Synthetic images showing randomized color and location of the pallet jacks and randomized camera position</em></figcaption></figure></div>\n\n\n<p>Finally, configure writers for annotating data:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nwriter = rep.WriterRegistry.get(&quot;KittiWriter&quot;) \nwriter.initialize(output_dir=output_directory,\n                    omit_semantic_type=True,)\n</pre></div>\n\n\n<p>Note that this example uses the KittiWriter provided with Replicator to store the annotations in KITTI format for object detection labels. This will ensure easier compatibility with training pipelines.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Results&nbsp;</h3>\n\n\n\n<p>For this first batch of synthetic data, the team used the <a href=\"https://github.com/tum-fml/loco\">LOCO dataset</a>, which is a scene understanding dataset for logistics covering the problem of detecting logistics-specific objects to visualize the real-world model performance.&nbsp;</p>\n\n\n\n<p>The resulting images show that the model is still trying to detect the pallet jack in a crowded warehouse (Figure 2). Many bounding boxes have been created around objects surrounding the pallet jack. This result is somewhat expected, given that it is the first training iteration. Reducing the domain gap will be a focus for subsequent iterations.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"898\" height=\"472\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one.png\" alt=\"Composite of real-world images showing many false positives.One image shows two bounding boxes with one pallet jack. Another shows many more objects in addition to the pallet jack.\n\" class=\"wp-image-72247\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one.png 898w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-625x329.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-768x404.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-645x339.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-one-209x110.png 209w\" sizes=\"(max-width: 898px) 100vw, 898px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Real-world images showing many false positives after validating the model against real data</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Iteration 2: Adding textures and changing ambient lighting</h2>\n\n\n\n<p>In this iteration, the team randomized the texture and the ambient lighting, in addition to the pallet color and camera position from the first iteration.&nbsp;</p>\n\n\n\n<p>Activate the randomization for both textures and lighting:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Randomize the lighting of the scene\n    with rep.get.prims(path_pattern=&quot;RectLight&quot;):\n     rep.modify.attribute(&quot;color&quot;, rep.distribution.uniform((0, 0, 0), (1, 1, 1)))\n     rep.modify.attribute(&quot;intensity&quot;, rep.distribution.normal(100000.0, 600000.0))\n     rep.modify.visibility(rep.distribution.choice(&#91;True, False, False, False, False, False, False]))\n\n# select floor material\nrandom_mat_floor = rep.create.material_omnipbr(diffuse_texture=rep.distribution.choice(textures),                                                    roughness=rep.distribution.uniform(0, 1),                                               metallic=rep.distribution.choice(&#91;0, 1]),                                                    emissive_texture=rep.distribution.choice(textures),           emissive_intensity=rep.distribution.uniform(0, 1000),)\n        \n        \n     with rep.get.prims(path_pattern=&quot;SM_Floor&quot;):\n          rep.randomizer.materials(random_mat_floor)\n</pre></div>\n\n\n<p>Figure 3 shows the resulting synthetic images. Notice the various textures that have been added to the background, along with different types of ambient light incident on the objects.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"904\" height=\"476\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2.png\" alt=\"Composite of three synthetic images showing pallet jacks with different texture backgrounds.\" class=\"wp-image-72250\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2.png 904w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-625x329.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-768x404.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-645x340.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-362x191.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-background-texture-variation2-209x110.png 209w\" sizes=\"(max-width: 904px) 100vw, 904px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Synthetic images of pallet jacks with backgrounds of different textures</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Results</h3>\n\n\n\n<p>This iteration shows a reduction in the number of false positives, with the addition of texture and lighting randomization. One crucial factor when generating synthetic data is to ensure a good diversity of data in the resulting dataset. Similar or repetitive data from the synthetic domain will likely not help to improve \u200creal-world model performance.&nbsp;</p>\n\n\n\n<p>To improve the diversity of the dataset, add more objects in the scene with randomization. This is addressed in the third iteration and should help improve model robustness.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"902\" height=\"472\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two.png\" alt=\"Composite of three real-world images showing red bounding boxes around pallet jacks. \" class=\"wp-image-72251\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two.png 902w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-625x327.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-768x402.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-645x338.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-500x262.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-362x189.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-two-210x110.png 210w\" sizes=\"(max-width: 902px) 100vw, 902px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Real-world images showing that the model detects pallet jacks with higher precision after training on randomized texture and lighting images</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Iteration 3: Adding distractors</h2>\n\n\n\n<p>This iteration introduces additional objects, called distractors, into the scene. These distractors add more diversity to the dataset. This iteration also includes all the changes shown in the first two iterations.&nbsp;</p>\n\n\n\n<p>Add distractors to the scene:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nDISTRACTORS_WAREHOUSE = &#91;&quot;/Isaac/Environments/Simple_Warehouse/Props/S_TrafficCone.usd&quot;,\n                            &quot;/Isaac/Environments/Simple_Warehouse/Props/S_WetFloorSign.usd&quot;,\n                            &quot;/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_01.usd&quot;,\n                            &quot;/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_02.usd&quot;,\n                            &quot;/Isaac/Environments/Simple_Warehouse/Props/SM_BarelPlastic_A_03.usd&quot;]\n\n# Modify the pose of all the distractors in the scene\n   with rep_distractor_group:\n        rep.modify.pose(\nposition=rep.distribution.uniform((-6, -6, 0), (6, 12, 0)),\n            rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\n            scale=rep.distribution.uniform(1, 1.5))\n</pre></div>\n\n\n<p>Note that all the assets used in this project are available with the default Isaac Sim installation. Load them by specifying their path on the nucleus server.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"897\" height=\"467\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1.png\" alt=\"Composite of synthetic images showing pallet jacks surrounded by common warehouse objects such as a caution sign and bottles.\" class=\"wp-image-72252\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1.png 897w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-300x156.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-625x325.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-179x93.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-768x400.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-645x336.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-500x260.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-160x83.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-362x188.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks-with-distractors2-1-211x110.png 211w\" sizes=\"(max-width: 897px) 100vw, 897px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Synthetic images of pallet jacks surrounded by common warehouse objects (distractors)</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Results</h3>\n\n\n\n<p>Figure 6 shows results from the third iteration. The model can accurately detect the pallet jacks, and there are fewer bounding boxes. The model performance has improved significantly compared to the first iteration.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"898\" height=\"470\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three.png\" alt=\"Composite of real-world images with red bounding boxes around pallet jacks. \" class=\"wp-image-72253\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three.png 898w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-625x327.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-768x402.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-645x338.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-500x262.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-362x189.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/results-iteration-three-210x110.png 210w\" sizes=\"(max-width: 898px) 100vw, 898px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Real-world images showing that the model detects pallet jacks with high precision&nbsp;</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Continue iterating</h2>\n\n\n\n<p>The team used 5,000 images to train the model for each iteration. You can continue to iterate on this workflow by generating more variations, along with increasing the size of your synthetic data, to reach the desired level of accuracy. </p>\n\n\n\n<p>We used <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/resources/tao-getting-started\">NVIDIA TAO Toolkit</a> to train a DetectNet_v2 model with a resnet18 backbone for these experiments. Using this model is not a workflow requirement. You can leverage the data generated with the annotations to train a model of your architecture and framework choice.&nbsp;&nbsp;</p>\n\n\n\n<p>We leveraged the KITTI writer in our experiments. However, you can write your own custom writer with Omniverse Replicator to generate data in the correct annotations format. This enables seamless compatibility with your training workflows.&nbsp;</p>\n\n\n\n<p>You can also experiment with mixing real and synthetic data during your training process. The final model can be optimized and deployed on NVIDIA Jetson in the real world after obtaining satisfactory evaluation metrics.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Develop synthetic data pipelines with Omniverse Replicator</h2>\n\n\n\n<p>With Omniverse Replicator, you can build your own custom synthetic data generation pipeline or tools to programmatically generate large sets of diverse synthetic data to bootstrap your model, and iterate quickly. Introducing various types of randomizations adds the necessary diversity to the dataset, enabling the model to recognize the object or objects of interest in a variety of conditions.\u00a0</p>\n\n\n\n<p>To get started with the workflow featured in this post, visit <a href=\"https://github.com/NVIDIA-AI-IOT/synthetic_data_generation_training_workflow\">NVIDIA-AI-IOT/synthetic_data_generation_training_workflow</a> on GitHub.\u00a0To see the full workflow in action, join  Rishabh Chadha of NVIDIA and Jenny Plunkett of Edge Impulse as they showcase how to use Omniverse Replicator and synthetic data to train object detection models for manufacturing processes (Video 2).</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/PDEFs79BjOg?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Learn how to train computer vision models with synthetic data</em></figcaption></figure>\n\n\n\n<p>To build your own custom synthetic data generation pipeline, <a href=\"https://www.nvidia.com/en-us/omniverse/download/\">download Omniverse free</a> and follow the instructions for getting started with <a href=\"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/getting_started.html\">Replicator in Omniverse Code</a>. You can also take the self-paced online course, <a href=\"https://courses.nvidia.com/courses/course-v1:DLI+S-OV-10+V1/\">Synthetic Data Generation for Training Computer Vision Models</a> and <a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-35d98b97-8abf-4f92-883a-c898801f28b4/\">watch the latest Omniverse Replicator tutorials</a>.</p>\n\n\n\n<p>NVIDIA recently released Omniverse Replicator 1.10 with new support for developers building low-code SDG workflows. For details, see <a href=\"https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/\">Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10</a>.&nbsp;</p>\n\n\n\n<p>NVIDIA Isaac ROS 2.0 and NVIDIA Isaac Sim 2023.1 are also now available with major updates to performant perception and high-fidelity simulation. To learn more, see <a href=\"https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/\">Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform</a>.</p>\n\n\n\n<p><em>Stay up to date with NVIDIA Omniverse by subscribing to the </em><a href=\"https://nvda.ws/3u5KPv1\"><em>newsletter</em></a><em> and following Omniverse on </em><a href=\"https://www.instagram.com/nvidiaomniverse/\"><em>Instagram</em></a><em>, </em><a href=\"https://www.linkedin.com/showcase/nvidia-omniverse\"><em>LinkedIn</em></a><em>, </em><a href=\"https://medium.com/@nvidiaomniverse\"><em>Medium</em></a><em>, </em><a href=\"https://www.threads.net/@nvidiaomniverse\"><em>Threads</em></a><em>, and </em><a href=\"https://twitter.com/nvidiaomniverse\"><em>Twitter</em></a><em>. For more, check out our </em><a href=\"https://forums.developer.nvidia.com/c/omniverse/300\"><em>forums</em></a><em>, </em><a href=\"https://discord.com/invite/XWQNJDNuaC\"><em>Discord server</em></a><em>, </em><a href=\"https://www.twitch.tv/nvidiaomniverse\"><em>Twitch</em></a><em> and </em><a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\"><em>YouTube</em></a><em> channels.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Synthetic data can play a key role when training perception AI models that are deployed on autonomous mobile robots (AMRs). This process is becoming increasingly important in manufacturing. For an example of using synthetic data to generate a pretrained model that can detect pallets in a warehouse, see Developing a Pallet Detection Model Using OpenUSD &hellip; <a href=\"https://developer.nvidia.com/blog/how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data/\">Continued</a></p>\n", "protected": false}, "author": 891, "featured_media": 72204, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1284945", "discourse_permalink": "https://forums.developer.nvidia.com/t/how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data/270685", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [453, 1409, 3327, 1410, 1718, 2056, 3096], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/warehouse-pallet-jacks.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iMx", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72201"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/891"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72201"}], "version-history": [{"count": 36, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72201/revisions"}], "predecessor-version": [{"id": 72281, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72201/revisions/72281"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72204"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72201"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72201"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72201"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72096, "date": "2023-10-28T10:35:57", "date_gmt": "2023-10-28T17:35:57", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72096"}, "modified": "2023-11-02T13:22:18", "modified_gmt": "2023-11-02T20:22:18", "slug": "accelerate-genomic-analysis-for-any-sequencer-with-parabricks-v4-2", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerate-genomic-analysis-for-any-sequencer-with-parabricks-v4-2/", "title": {"rendered": "Accelerate Genomic Analysis for Any Sequencer with NVIDIA Parabricks v4.2"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Parabricks version 4.2 has been released, furthering its mission to deliver unprecedented speed, cost-effectiveness, and accuracy in genomics sequencing analysis. The latest version delivers a newly accelerated workflow for Oxford Nanopore sequencing (in the featured image), enables Parabricks to be run on the latest NVIDIA GPUs, and furthers Parabricks\u2019 accelerated deep learning variant calling initiative to support data types from all major sequencer types.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Analyzing a long-read whole genome in under an hour</h2>\n\n\n\n<p>Parabricks v4.2 includes upgraded WDL and NextFlow workflows, as best practices for deploying Parabricks tools, available on the <a href=\"https://github.com/clara-parabricks-workflows\">Parabricks Workflows</a> GitHub repo and including both short\u2013 and long-read workflows.</p>\n\n\n\n<p>This latest release of Parabricks delivers an updated Oxford Nanopore germline workflow, delivering high-speed analysis on NVIDIA H100 GPUs.</p>\n\n\n\n<p>Following on from the success of the <a href=\"https://developer.nvidia.com/blog/boosting-ultra-rapid-nanopore-sequencing-analysis-on-nvidia-dgx-a100/\">Ultrarapid Nanopore Analysis Pipeline (UNAP)</a> released by NVIDIA in 2022, this new workflow includes the basecalling, alignment, and small and structural variant calling steps. It has updated software from Guppy to Dorado, and from PEPPER-MARGIN-DeepVariant to the newly integrated long-read variant calling of DeepVariant 1.5, deployed with Parabricks v4.2.</p>\n\n\n\n<p>Figure 1 shows the workflow for the Oxford Nanopore germline sequencing analysis.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"601\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic.png\" alt=\"Schematic shows basecalling and integrated alignment with Dorado/Minimap2, small variant calling with DeepVariant in Parabricks, and structural variant calling with Sniffles2.\" class=\"wp-image-72183\" style=\"width:1000px;height:301px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-300x90.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-625x188.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-179x54.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-768x231.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-1536x462.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-645x194.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-500x150.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-160x48.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-362x109.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-366x110.png 366w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-schematic-1024x308.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Workflow schematic for analysis of Oxford Nanopore germline sequencing data</em></figcaption></figure></div>\n\n\n<p>This latest Oxford Nanopore workflow was <a href=\"https://blogs.oracle.com/cloud-infrastructure/post/oci-delivers-clinical-sequence-analysis\">recently benchmarked by Oracle Cloud</a> on eight NVIDIA H100 GPUs, achieving an end-to-end runtime of under an hour on a single 55x coverage whole genome.</p>\n\n\n\n<p>High-speed Oxford Nanopore sequencing and Parabricks workflows also have the potential to provide rapid turnaround times to clinical sequencing. </p>\n\n\n\n<p>In terms of ongoing development, this workflow will be further benchmarked and optimized as part of a collaboration between NVIDIA and the Clinical Long-read Genome Initiative (lonGER) consortium. This consists of four institutes across Germany aimed at optimizing the analysis of nanopore data both in time to results and in clinical-grade accuracy of methods, to identify the most relevant clinical genomic alterations.</p>\n\n\n\n<p>The National Institutes of Health Center for Alzheimer\u2019s and Related Dementias (CARD) has developed a protocol for highly accurate, whole-genome sequencing at scale. That example study, among others, shows how Oxford Nanopore sequencing and rapid analysis can provide a comprehensive view of haplotype-resolved variation and methylation.</p>\n\n\n\n<p>In a recent <a href=\"https://www.nature.com/articles/s41592-023-01993-x\">Nature Methods paper</a>, the CARD team described how this makes large-scale, long, native DNA sequencing projects feasible due to the lower cost and higher throughput of Oxford Nanopore\u2019s PromethION when compared with alternative sequencing methods.</p>\n\n\n\n<h2 class=\"wp-block-heading\">High-accuracy variant calling for all sequencers with optimized DeepVariant models</h2>\n\n\n\n<p>DeepVariant, the CNN-based, high-accuracy germline variant caller, is accelerated on GPUs as part of Parabricks. </p>\n\n\n\n<p>Recently, Parabricks v4.1 introduced an accelerated framework for re-training the underlying CNN model, to more easily enable custom models, and bring more accurate variant calls to analysis workflows. This brings greater accuracy by learning the error profiles of different sequencers, or the unique artifacts introduced in different high-throughput labs.</p>\n\n\n\n<p>Parabricks v4.2 now comes with accelerated models pretrained for a variety of sequencer data types, as part of DeepVariant in Parabricks:</p>\n\n\n\n<ul>\n<li>Illumina</li>\n\n\n\n<li>Oxford Nanopore</li>\n\n\n\n<li>PacBio</li>\n\n\n\n<li>Ultima</li>\n\n\n\n<li>Singular</li>\n\n\n\n<li>\u2026and more</li>\n</ul>\n\n\n\n<p>The acceleration factors of these models can reach over 80x acceleration, from hours on CPU instances to under 4 minutes on NVIDIA GPUs.</p>\n\n\n<div class=\"wp-block-image is-resized\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"937\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow.png\" alt=\"Benchmarks shown are for a single HG002 whole genome sequencing sample from different sequencer types. Oxford Nanopore reference sample was sequenced to a higher depth (~55x).\" class=\"wp-image-72185\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-625x293.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-768x360.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-1536x720.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-645x302.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-500x234.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-235x110.png 235w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-germline-workflow-1024x480.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Runtime performance of DeepVariant in Parabricks on the NVIDIA DGX A100 compared to a cpu-only M5.24xlarge (96 vCPU cores) instance</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Unprecedented speed on NVIDIA GPUs</h2>\n\n\n\n<p>In high-throughput settings, moving genomic analysis workflows to GPU with Parabricks results in hugely reduced processing time. </p>\n\n\n\n<p>One example of this is in Cancer Research UK\u2019s TRACERx EVO, the latest project of TRACERx, which is the world\u2019s largest long-term lung cancer research program, and is driven by infrastructure at the Francis Crick Institute, University College London, and the University of Manchester.</p>\n\n\n\n<p>Initial results from the Francis Crick Institute show that the end-to-end analysis of whole human genomes (including FastQ alignment and deep variant calling) can be done in just over 2 hours with NVIDIA Parabricks, compared to approximately 13 hours on their NEMO CPU cluster. This performance gain is anticipated to be pushed even further on their latest GPU cluster.</p>\n\n\n\n<p>For the TRACERx EVO project alone, they estimate this will save nearly 9 years of bioinformatics processing time, an improvement described as a \u201cgame-changer in terms of the feasibility of the analysis pipelines for the project,\u201d by Mark S. Hill, principal research fellow at TRACERx EVO.</p>\n\n\n\n<p>For the latest GPU architectures, the newest NVIDIA Hopper architecture has been dubbed the engine of the world\u2019s AI infrastructure, with an order-of-magnitude performance leap for a diverse range of workloads.</p>\n\n\n\n<p>High-performance computing applications being run in data centers benefit from NVIDIA Hopper\u2019s multi-GPU scalability, and its advancements in tensor core technology, meaning impressive results such as 30x acceleration in AI inference over previous generations.</p>\n\n\n\n<p>For genomics specifically, NVIDIA Hopper architectures include new <a href=\"https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/\">dynamic programming instructions</a> (DPX) designed to solve complex recursive problems. Dynamic programming is used commonly across multiple fields such as in graph analytics or in route optimizations. This includes in genomics with the Smith-Waterman algorithm, which underlies most aligners and multiple variant callers. The new DPX instructions accelerate these algorithms by 40x compared to CPU-only, and 7x compared to the previous NVIDIA Ampere architecture.</p>\n\n\n\n<p>Combining all these advances means that the latest NVIDIA GPU architectures are incredibly well-suited to accelerate bioinformatics tools like the BWA-MEM aligner, which can run in just 8 minutes on eight NVIDIA H100 GPUs, or the deep learning\u2013based DeepVariant variant caller, which can run in just 3 minutes on eight H100 GPUs. These runtimes mean an end-to-end germline workflow can be achieved in just 14 minutes with H100 GPUs and Parabricks.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"956\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing.png\" alt=\"Benchmarks shown are for a single 30x HG002 whole genome Illumina sequencing sample, run with Parabricks DeepVariant germline pipeline.\" class=\"wp-image-72184\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-300x143.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-625x299.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-179x86.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-768x367.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-1536x735.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-645x308.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-500x239.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-160x77.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-362x173.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-230x110.png 230w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/hg002-genome-sequencing-1024x490.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Runtime performance of the Parabricks germline workflow on 8 x NVIDIA H100 GPUs compared to a cpu-only M5.24xlarge (96 vCPU cores) instance</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">NVIDIA Parabricks v4.2 now available on NGC</h2>\n\n\n\n<p>Parabricks v4.2 integrates seamlessly into genomics workflows, with continued support for GPU-accelerated versions of well-established workflows with tools like BWA-MEM, and GATK and the ability to quickly train custom models for DeepVariant variant calling. In providing these for new GPU architectures, and across both short\u2013 and long-read sequencing devices, Parabricks is a truly universal full-stack acceleration platform, for gold-standard genomics analysis on GPU.</p>\n\n\n\n<p>The Parabricks v4.2 container is freely available now under the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/collections/claraparabricks\">NVIDIA Parabricks Collection on NGC</a>. For WDL and NextFlow reference workflows, see the <a href=\"https://github.com/clara-parabricks-workflows\">Parabricks Workflows</a> GitHub repo.</p>\n\n\n\n<p>For more information about Parabricks, see <a href=\"https://www.nvidia.com/en-us/clara/genomics/\">Whole Genome Sequencing Analysis</a>, which includes customer success stories with analysis at scale, deployment in sequencers and devices, and cutting-edge research.</p>\n\n\n\n<p>If you require enterprise support, <a href=\"https://www.nvidia.com/en-us/clara/genomics/free-trial-vs-buy/\">contact NVIDIA sales</a> to access enterprise benefits, including access to NVIDIA experts to ensure optimization at scale, guaranteed critical support response times, and enterprise training services</p>\n\n\n\n<p>For more information about what\u2019s new, tutorials, and deployment guides for cloud service providers, see the <a href=\"https://docs.nvidia.com/clara/parabricks/latest/\">Parabricks documentation</a>.</p>\n\n\n\n<p>For more information about scaling sequencing analysis with Parabricks, see the <a href=\"https://developer.download.nvidia.com/healthcare/clara/docs/Genomics-POD-White-Paper-v3-gd-hwc.pdf\">NVIDIA DGX BasePOD solutions for genomic sequencing</a> whitepaper.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Parabricks version 4.2 has been released, furthering its mission to deliver unprecedented speed, cost-effectiveness, and accuracy in genomics sequencing analysis. The latest version delivers a newly accelerated workflow for Oxford Nanopore sequencing (in the featured image), enables Parabricks to be run on the latest NVIDIA GPUs, and furthers Parabricks\u2019 accelerated deep learning variant calling initiative &hellip; <a href=\"https://developer.nvidia.com/blog/accelerate-genomic-analysis-for-any-sequencer-with-parabricks-v4-2/\">Continued</a></p>\n", "protected": false}, "author": 1370, "featured_media": 72189, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1287511", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerate-genomic-analysis-for-any-sequencer-with-nvidia-parabricks-v4-2/271199", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [1910, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/parabricks-4-2-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iKQ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72096"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1370"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72096"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72096/revisions"}], "predecessor-version": [{"id": 72362, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72096/revisions/72362"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72189"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72096"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72096"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72096"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71317, "date": "2023-10-27T09:00:00", "date_gmt": "2023-10-27T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71317"}, "modified": "2023-11-02T13:23:13", "modified_gmt": "2023-11-02T20:23:13", "slug": "advanced-api-performance-descriptors", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-api-performance-descriptors/", "title": {"rendered": "Advanced API Performance: Descriptors"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>By using descriptor types, you can bind resources to shaders and specify how those resources are accessed. This creates efficient communication between the CPU and GPU and enables shaders to access the necessary data during rendering.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Recommended</h2>\n\n\n\n<ul>\n<li>Prefer a \u201cbindless\u201d design.\n\n\n<ul>\n<li>Use unbounded array descriptors pointing to big descriptor tables or sets with all known textures, buffers, and acceleration structures needed for the frame.</li>\n\n\n\n<li>Upload as much data upfront as possible (textures, per-draw constants, and per-frame constants) and make them accessible through these descriptor arrays.</li>\n\n\n\n<li>This design also makes it easier to implement ray tracing; that is, allowing access to every texture and buffers from each shader.</li>\n\n\n\n<li>Cache descriptors on GPU-visible descriptor heaps (DirectX 12) or sets (Vulkan) with a known offset. This lowers the CPU overhead and virtually eliminates the need for copying descriptors.</li>\n</ul>\n\n\n\n<ul>\n<li>Use multiple copies of the heap to handle descriptor changes gracefully, such as streaming textures and buffers. But don\u2019t exceed the 1M and 2K limits. For more information, see the <strong>Not Recommended </strong>section later in this post. </li>\n</ul>\n</li>\n\n\n\n<li>Use root (DirectX 12) or push (Vulkan) constants. They are the fastest way to transfer per-draw varying constants.</li>\n\n\n\n<li>On Pascal: Prefer CBVs over SRVs for constant data.\n<ul>\n<li>Generally, SRV buffers are slower than CBV buffers on &lt;= Pascal.</li>\n\n\n\n<li>Performance is equivalent on Volta and up.</li>\n\n\n\n<li>Better yet, try using root constants.\n<ul>\n<li>They can be faster, even for infrequently changing data (for example, material data, pass data, and per-frame data).</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">DirectX 12</h3>\n\n\n\n<ul>\n<li>Feel free to maximize the use of the full 64 <code>DWORD</code> data types available in the root signature.</li>\n\n\n\n<li>Performance ranking on both GPU and CPU:\n<ol>\n<li>Root constants are the fastest with no indirections, and they are directly indexable.</li>\n\n\n\n<li>Root CBV/SRV/UAV are the second fastest, with single indirection and no bounds checking.</li>\n\n\n\n<li>Descriptor tables are the slowest, with two indirections and bounds checking.</li>\n</ol>\n</li>\n\n\n\n<li>Use dynamic resource binding, such as HLSL SM 6.6.\n<ul>\n<li>This enables you to omit some descriptor tables from the root signature, for more space for root constants and other data. For more information, see <a href=\"https://devblogs.microsoft.com/directx/in-the-works-hlsl-shader-model-6-6/\">In the works: HLSL Shader Model 6.6</a>.</li>\n</ul>\n</li>\n\n\n\n<li>Switching root signatures is a fast operation.\n<ul>\n<li>The usage of multiple root signatures to improve binding efficiency could be a valid strategy.</li>\n\n\n\n<li>This especially holds true for a non-bindless design.</li>\n\n\n\n<li>It could be inefficient when having to rebind a lot of data unnecessarily. Switching root signature causes existing bindings to be lost.</li>\n</ul>\n</li>\n\n\n\n<li>Use Root Signature 1.1 to get slightly more performance in some cases.\n<ul>\n<li>In particular, using <code>DATA_STATIC_SET_AT_EXECUTE</code> where possible enables the driver to inline some data early.</li>\n\n\n\n<li>This is not a high priority; only use it whenever it is convenient to do so.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Vulkan</h3>\n\n\n\n<ul>\n<li>Try to keep the number of descriptor sets in pipeline layouts as low as possible.</li>\n\n\n\n<li>Use dynamic uniform and storage buffers for per-draw call changes.</li>\n\n\n\n<li>Prefer using combined image and sampler descriptors.</li>\n\n\n\n<li>Vulkan 1.2 enables passing device addresses of storage buffers as 64-bit values to shaders. This enables pointer-like workflows (such as casting) that are not available in DirectX or HLSL. GLSL exposes this through <code>GL_EXT_buffer_reference(2)</code> and uses <code>SPV_EXT_physical_storage_buffer</code>. Try to make optimal usage of the <code>buffer_reference_align</code> information, as the hardware can leverage wider memory load operations accordingly.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Not recommended</h2>\n\n\n\n<ul>\n<li>Do not exceed 1M active descriptors and 2K samplers in total for the whole application (GPU-visible).\n<ul>\n<li>Otherwise, pipeline stalls across the whole GPU could occur when switching descriptor heaps (DirectX 12).</li>\n\n\n\n<li>Whenever the limits are exceeded, it reduces the asynchronous execution efficiency of command lists.</li>\n\n\n\n<li>On Vulkan, the deduplication of descriptors is automatically performed by the driver. The limits mentioned earlier only count towards unique variations.\n<ul>\n<li>In general, try to keep under the thresholds described in <code>VkPhysicalDeviceLimits</code>.</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li>Avoid typed UAV loads or stores where possible.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">DirectX 12</h3>\n\n\n\n<ul>\n<li>Prevent the excessive creation or copying of descriptors during the frame.\n<ul>\n<li>Keep descriptors around persistently instead of re-allocating or copying them in each frame.</li>\n\n\n\n<li>Use root CBVs instead of CBVs in a descriptor table.\n<ul>\n<li>There&#8217;s no need to call <code>CreateConstantBufferView</code> with a root CBV.</li>\n</ul>\n</li>\n\n\n\n<li>The careful selection of smaller descriptor tables could also improve the situation.</li>\n</ul>\n</li>\n\n\n\n<li>Reduce duplicate descriptors to the same resources as much as possible.\n<ul>\n<li>Example: Texture 0 should not be referenced in Descriptor 0, 10, 20, 30, 40, 50, and so on.</li>\n\n\n\n<li>Instead, try changing the layout from the descriptor tables to be able to reuse the same descriptor multiple times.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Vulkan</h3>\n\n\n\n<ul>\n<li>Do not have excessively sparse binding offsets in a single descriptor set.\n<ul>\n<li>Keep bindings as tightly packed as possible.</li>\n\n\n\n<li>Unused binding indices waste memory and reduce cache efficiency.</li>\n</ul>\n</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>By using descriptor types, you can bind resources to shaders and specify how those resources are accessed. This creates efficient communication between the CPU and GPU and enables shaders to access the necessary data during rendering. Recommended DirectX 12 Vulkan Not recommended DirectX 12 Vulkan</p>\n", "protected": false}, "author": 1888, "featured_media": 66457, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1286476", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-api-performance-descriptors/270941", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 503], "tags": [2424, 514, 453, 693], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/Advanced-API-series.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iyh", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71317"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1888"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71317"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71317/revisions"}], "predecessor-version": [{"id": 71619, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71317/revisions/71619"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/66457"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71317"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71317"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71317"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71878, "date": "2023-10-24T13:00:00", "date_gmt": "2023-10-24T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71878"}, "modified": "2023-11-02T11:50:46", "modified_gmt": "2023-11-02T18:50:46", "slug": "webinar-transform-your-vision-ai-applications-with-generative-ai", "status": "publish", "type": "post", "link": "https://info.nvidia.com/metropolis-gen-ai-webinar.html", "title": {"rendered": "Webinar: Transform Your Vision AI Applications with Generative AI"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Explore new generative AI models from NVIDIA that will have a major impact on your vision AI developer stack.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Explore new generative AI models from NVIDIA that will have a major impact on your vision AI developer stack.</p>\n", "protected": false}, "author": 1466, "featured_media": 71883, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://info.nvidia.com/metropolis-gen-ai-webinar.html", "_links_to_target": "_blank"}, "categories": [2724, 2758, 3110, 1903], "tags": [453, 2932, 1472, 1958, 2143, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/metropolis-gen-ai-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iHk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71878"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71878"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71878/revisions"}], "predecessor-version": [{"id": 71890, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71878/revisions/71890"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71883"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71878"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71878"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71878"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72126, "date": "2023-10-24T12:00:00", "date_gmt": "2023-10-24T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72126"}, "modified": "2023-11-02T11:14:32", "modified_gmt": "2023-11-02T18:14:32", "slug": "reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library/", "title": {"rendered": "Reduce Apache Spark ML Compute Costs with New Algorithms in Spark RAPIDS ML Library"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://github.com/nvidia/spark-rapids-ml\">Spark RAPIDS ML</a> is an open-source Python package enabling NVIDIA GPU acceleration of <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#mllib-dataframe-based\">PySpark MLlib</a>. It offers PySpark MLlib DataFrame API compatibility and speedups when training with the supported algorithms. See <a href=\"https://developer.nvidia.com/blog/new-gpu-library-lowers-compute-costs-for-apache-spark-ml/\">New GPU Library Lowers Compute Costs for Apache Spark ML</a> for more details.</p>\n\n\n\n<p>PySpark MLlib DataFrame API compatibility means easier incorporation into existing PySpark ML applications, with only a package import change (at most). An example of the <a href=\"https://www.nvidia.com/en-us/glossary/data-science/k-means/\">K-means algorithm</a> is shown below. The package import change is the only additional step necessary to enable GPU acceleration using this library.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-4 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">PySpark MLlib</h5>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nfrom pyspark.ml.clustering import KMeans\n\nkmeans_estm = KMeans()\\\n.setK(100)\\\n.setFeaturesCol(&quot;features&quot;)\\\n.setMaxIter(30)\n\nkmeans_model = kmeans_estm.fit(pyspark_data_frame)\n\nkmeans_model.write().save(&quot;saved-model&quot;)\n\ntransformed = kmeans_model.transform(pyspark_data_frame)\n</pre></div></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">Spark RAPIDS ML</h5>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nfrom spark_rapids_ml.clustering import KMeans\n\nkmeans_estm = KMeans()\\\n.setK(100)\\\n.setFeaturesCol(&quot;features&quot;)\\\n.setMaxIter(30)\n\nkmeans_model = kmeans_estm.fit(pyspark_data_frame)\n\nkmeans_model.write().save(&quot;saved-model&quot;)\n\ntransformed = kmeans_model.transform(pyspark_data_frame)\n</pre></div></div>\n</div>\n\n\n\n<p>Training with supported algorithms in a benchmarking suite run in three-node Spark clusters on GPU-accelerated Databricks\u2019 AWS-hosted Spark service demonstrated significant time and cost advantages compared to CPU-based PySpark MLlib. Specifically, this achieved a <a href=\"https://developer.nvidia.com/blog/new-gpu-library-lowers-compute-costs-for-apache-spark-ml/#:~:text=The%20cost%20benefits%20graph%20in%20Figure%203%20shows%20the%20ratio%20of%20CPU%20compute%20cost%20to%20GPU%20compute%20cost%20as%20determined%20by%20the%20benchmark%20running%20times%20and%20Databricks%E2%80%99%20compute%20cost%20model%20(DBUs%20plus%20Amazon%20EC2%20instance%20costs).\">7x to 100x speedup</a> (depending on the algorithm) and <a href=\"https://developer.nvidia.com/blog/new-gpu-library-lowers-compute-costs-for-apache-spark-ml/#:~:text=Figure%203.%20GPU%2Dto%2DCPU%20speedup%20factors%20and%20corresponding%20cost%20benefits\">3x to 50x more in cost savings</a>. Moreover, the Spark RAPIDS ML library is built on top of the proven, highly optimized RAPIDS cuML GPU-accelerated ML library.</p>\n\n\n\n<p>The initial release of Spark RAPIDS ML supported GPU acceleration of a subset of PySpark MLlib algorithms with readily available counterparts in RAPIDS cuML, namely <a href=\"https://www.nvidia.com/en-us/glossary/data-science/linear-regression-logistic-regression/#:~:text=What%20is%20Linear%20Regression%3F\">linear regression</a>, <a href=\"https://www.nvidia.com/en-us/glossary/data-science/random-forest/\">random forest</a> classification, random forest regression, k-means, and pca. It also included a PySpark DataFrame API for the cuML distributed implementation of exact k-nearest neighbors (k-NN) for easy incorporation of this useful algorithm into Spark applications using a familiar API.</p>\n\n\n\n<p>The <a href=\"https://github.com/NVIDIA/spark-rapids-ml/releases\">Spark RAPIDS ML 23.08 release</a> includes GPU-accelerated PySpark MLlib APIs for three new algorithms:</p>\n\n\n\n<ul>\n<li>Binomial logistic regression with L-BFGS optimization</li>\n\n\n\n<li>Cross validation</li>\n\n\n\n<li>Uniform manifold approximation and projection (UMAP)</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/ebook-sign-up/?nvid=nv-int-tblg-212161-vt04\" target=\"_blank\" rel=\"noreferrer noopener\"><img decoding=\"async\" src=\"https://lh7-us.googleusercontent.com/IJN-l_hi8wGDjfuUMOaOrYWfUIXJ4P7ArxhZL03cmaGaGvV016gL5BTRaJ-48zn9BDgRK1GSNhpvcQdB2igg2JShXV2vYrbOqarl68ZEMYVwGlh5Lzm3SdB6z8hSCjaiOWu2a_5mUJ2a5u8GfG0mNw\" alt=\"Banner for e-book: Accelerating Apache Spark 3 \" /></a></figure></div>\n\n\n<h2 class=\"wp-block-heading\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/ebook-sign-up/?nvid=nv-int-tblg-212161-vt04\"></a>Binomial logistic regression with L-BFGS</h2>\n\n\n\n<p>Logistic regression is a well-known <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> (ML) classification algorithm that models the conditional probability distribution of a finite valued class variable as a generalized linear function (softmax or sigmoid and linear, for example) of a feature vector.</p>\n\n\n\n<p>The 23.08 release includes GPU-accelerated versions of the PySpark MLlib<code> classification.LogisticRegression</code> and <code>classification.LogisticRegressionModel</code> supporting accelerated fit and transform. This is initially for binary classification (binomial logistic regression) and L2 regularization, with full support (elastic net regularization and multi-class classification, for example) planned for upcoming releases.</p>\n\n\n\n<p>Supporting accelerated logistic regression was more involved than the previously released algorithms. Unlike with the algorithms in previous releases, there was no readily available distributed implementation in cuML to leverage.</p>\n\n\n\n<p>The first step was thus to contribute a multi-node multi-GPU (MNMG) extension of the cuML single GPU-accelerated L-BFGS-based logistic regression optimization algorithm (which is also used in Spark MLlib). To do this, the team followed the design pattern of the other cuML distributed implementations, with a design similar to a Message Passing Interface (MPI) centered around the GPU-optimized <a href=\"https://developer.nvidia.com/nccl\">NVIDIA Collective Communication Library (NCCL)</a>.&nbsp;&nbsp;</p>\n\n\n\n<p>The Spark RAPIDS ML bootstrapping of this implementation using the PySpark Barrier RDD and the MLlib API compatibility was then layered on top (Figure 1). As with the previously released algorithms, this design enables the GPU-accelerated distributed implementation to carry out communication in a manner that optimizes GPU utilization and over the best available interconnect between GPUs. These include Ethernet or higher-performance interconnects like <a href=\"https://www.nvidia.com/en-us/data-center/nvlink/\">NVLink</a> and InfiniBand.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"769\" height=\"354\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1.png\" alt=\"Stack diagram showing PySpark, Spark RAPIDS ML, cuML, GPU, and NCCL communication layers.\" class=\"wp-image-72162\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1.png 769w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-300x138.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-625x288.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-645x297.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-500x230.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-362x167.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stack-diagram-spark-rapids-ml-integration-1-239x110.png 239w\" sizes=\"(max-width: 769px) 100vw, 769px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Integration of Spark RAPIDS ML and the newly added cuML MNMG distributed logistic regression implementation</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Performance</h3>\n\n\n\n<p>The benchmarking setting used for the previously released algorithms was also used to compare GPU-accelerated Spark RAPIDS ML logistic regression with the baseline CPU-based Spark ML version. The PySpark RAPIDS MLlib implementation was 6x faster and 3x more cost-efficient than the PySpark MLlib CPU implementation.</p>\n\n\n\n<p>These benchmarks were run in three-node Spark clusters (one driver, two executors) on Databricks\u2019 AWS-hosted Spark service with the hardware configurations listed below.</p>\n\n\n\n<ul>\n<li>In the CPU cluster, the m5.2xlarge executor and driver nodes each have eight CPU cores and 32GB of RAM.</li>\n\n\n\n<li>In the GPU cluster, the g5.2xlarge executor nodes each have the same CPU and RAM as the m5.2xlarge nodes, along with <a href=\"https://www.nvidia.com/en-us/data-center/products/a10-gpu/\">NVIDIA A10</a> 24GB GPUs.</li>\n</ul>\n\n\n\n<p>The benchmarks were run on a 3,000-feature 12GB synthetic dataset generated with the scikit-learn synthetic data-generating routines and stored in Parquet format on Amazon S3. Note that the runtimes are for end-to-end data loading from Amazon S3 plus fit method execution, and the spark-rapids plugin was used to accelerate data loading for GPU runs.</p>\n\n\n\n<p>For more information including scripts related to this benchmark, visit <a href=\"https://github.com/NVIDIA/spark-rapids-ml/tree/main/python/benchmark\">NVIDIA/spark-rapids-ml</a> on GitHub. See also a sample <a href=\"https://github.com/NVIDIA/spark-rapids-ml/blob/main/notebooks/logistic-regression.ipynb\">Jupyter Notebook</a> demonstrating how to use the accelerated LogisticRegression APIs.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Cross validation</h2>\n\n\n\n<p>Cross validation is a well-known algorithm for optimizing model or training algorithm hyperparameters that are not tuned directly by the core training algorithm itself, such as the regularization parameters in logistic regression. It has long been supported in PySpark MLlib through the tuning.CrossValidator class.&nbsp;&nbsp;</p>\n\n\n\n<p>Thanks to the MLlib API compatibility of Spark RAPIDS ML, the supported accelerated algorithm Estimator classes can undergo PySpark CrossValidator hyperparameter tuning out of the box. It provides speedup and cost benefits over cross validation with CPU training, which is on par with a single training run GPU compared to CPU cases. However, it suffers from the inefficiency of repeatedly copying data from CPU to GPU for each change in hyperparameter values.&nbsp;&nbsp;</p>\n\n\n\n<p>Such excessive copying is a known performance bottleneck in GPU computing. It is even more pronounced for Spark RAPIDS ML, as these copies also exist between the JVM executors and Python workers over a local socket connection.&nbsp;</p>\n\n\n\n<p>To eliminate this inefficiency, Spark RAPIDS ML now includes a specialized <a href=\"https://nvidia.github.io/spark-rapids-ml/api/python/api/spark_rapids_ml.tuning.CrossValidator.html\">variant of the PySpark native CrossValidator</a> compatible with MLlib API. It copies data to Python workers and GPUs only once while hyperparameter values change for a given cross validation fold.</p>\n\n\n\n<p>The Spark RAPIDS ML specialized CrossValidator trains and evaluates all the models for the hyperparameter values under test in single respective training and evaluation Spark stages, copying the data once per stage. Figure 2 illustrates timeline traces showing patterns of copies and training and evaluation compute steps for the baseline PySpark MLlib and Spark RAPIDS ML CrossValidator versions.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"859\" height=\"183\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1.png\" alt=\"Above, a timeline for PySpark MLlib CrossValidator shows alternating copy, train, copy, and eval steps for each change of hyperparameter values. Below, a timeline for Spark RAPIDS ML CrossValidator shows a single copy for training followed by training on different values of hyperparameters and then a single copy for evaluation followed by evaluation on those same value of hyperparameters. \n\" class=\"wp-image-72168\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1.png 859w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-300x64.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-625x133.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-179x38.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-768x164.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-645x137.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-500x107.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-160x34.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-362x77.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-crossvalidator-timelines-1-516x110.png 516w\" sizes=\"(max-width: 859px) 100vw, 859px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The Spark RAPIDS ML CrossValidator eliminates redundant data copies</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Performance</h3>\n\n\n\n<p>Our team benchmarked the new specialized CrossValidator class on three-fold cross validation with four hyperparameter values per fold for GPU-accelerated <code>RandomForestClassifier</code>, <code>RandomForestRegression</code>, and <code>LinearRegression</code>. We observed a 2x speedup<strong><em> </em></strong>over the baseline CrossValidator operating on the GPU-accelerated implementations.&nbsp;&nbsp;</p>\n\n\n\n<p>Note that this speedup multiplies existing speedup factors due to GPU implementations of the core training algorithms when considering an overall comparison to pure CPU cross validation.&nbsp;&nbsp;</p>\n\n\n\n<p>Visit NVIDIA/spark-rapids-ml on GitHub for a sample <a href=\"https://github.com/NVIDIA/spark-rapids-ml/blob/branch-23.08/notebooks/cv-rf-regressor.ipynb\">Jupyter Notebook</a> showing the Spark MLlib API-compatible accelerated CrossValidator.</p>\n\n\n\n<h2 class=\"wp-block-heading\">UMAP</h2>\n\n\n\n<p>UMAP is a state-of-the-art non-linear dimensionality reduction algorithm that is highly effective in capturing structure from the high-dimensional data into the computed lower dimensional representations or embeddings. It can be used to simplify downstream ML tasks such as classification and clustering, or for visualization.&nbsp;</p>\n\n\n\n<p>The algorithm involves compute-intensive steps to arrive at the lower dimensional embeddings\u2014such as k-nearest neighbors (k-NN)\u2014in the original high-dimensional space and an iterative cross-entropy optimization of a random graph over the embeddings. It is thus a natural candidate for GPU acceleration and has been implemented in the cuML library, offering significant speedups over the <a href=\"https://github.com/lmcinnes/umap\">original CPU implementation</a>.</p>\n\n\n\n<p>In the latest Spark RAPIDS ML release, UMAP joins exact k-NN as a non-MLlib accelerated algorithm from cuML that is wrapped in a PySpark MLlib API for easy integration in Spark applications. The design is shown in Figure 3.&nbsp;&nbsp;</p>\n\n\n\n<p>The UMAP estimator\u2019s fit method implementation is single-node and operates on a random sample of the full dataset to create a UMAP model comprising that random sample along with its embedding. The transform method of the UMAP model then extends the embedding to the rest of the dataset in a scalable, distributed manner.&nbsp;</p>\n\n\n\n<p>It uses k-NN and cross-entropy optimization with respect to the original random sample and embedding captured in the model. The implementation overcomes serialization limitations in Spark to enable a large model size (many GBs).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1871\" height=\"539\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram.png\" alt=\"In a left-to-right flow, a random sample of a dataset is read from persistent storage by a worker running on a single server and GPU to carry out the Fit or training step. The trained model is broadcast to a collection of workers running on multiple servers and GPUs for the Transform step operating on the full distributed dataset. The UMAP embeddings computed by each worker in the Transform step are then available for storage or other downstream tasks.\" class=\"wp-image-72169\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram.png 1871w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-300x86.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-625x180.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-179x52.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-768x221.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-1536x442.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-645x186.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-500x144.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-160x46.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-362x104.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-382x110.png 382w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spark-rapids-ml-umap-diagram-1024x295.png 1024w\" sizes=\"(max-width: 1871px) 100vw, 1871px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Spark RAPIDS ML UMAP fit and transform implementations</em></figcaption></figure></div>\n\n\n<p>Visit NVIDIA/spark-rapids-ml on GitHub to see a sample <a href=\"https://github.com/NVIDIA/spark-rapids-ml/blob/main/notebooks/umap.ipynb\">Jupyter Notebook</a> demonstrating GPU-accelerated UMAP on Spark. For more details about the API, see the <a href=\"https://nvidia.github.io/spark-rapids-ml/api/python/api/spark_rapids_ml.umap.UMAP.html\">UMAP documentation</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>With Spark RAPIDS ML and its growing capabilities, you can dramatically accelerate Spark ML applications with a one-line code change while reducing your computing cost. The latest release of Spark RAPIDS ML extends these benefits of GPU acceleration to logistic regression and cross validation. In addition, GPU-accelerated UMAP is now available with a PySpark MLlib API for easier adoption in Spark ML applications.</p>\n\n\n\n<p>Visit <a href=\"https://github.com/NVIDIA/spark-rapids-ml/tree/main/python/benchmark\">NVIDIA/spark-rapids-ml</a> on GitHub to access Spark RAPIDS ML <a href=\"https://github.com/NVIDIA/spark-rapids-ml\">source code</a> and <a href=\"https://nvidia.github.io/spark-rapids-ml/\">documentation</a>, and to <a href=\"https://github.com/NVIDIA/spark-rapids-ml/issues/new\">provide feedback</a>. You can also check out the resources for <a href=\"https://nvidia.github.io/spark-rapids-ml/get-started/\">getting started with Spark RAPIDS ML</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Spark RAPIDS ML is an open-source Python package enabling NVIDIA GPU acceleration of PySpark MLlib. It offers PySpark MLlib DataFrame API compatibility and speedups when training with the supported algorithms. See New GPU Library Lowers Compute Costs for Apache Spark ML for more details. PySpark MLlib DataFrame API compatibility means easier incorporation into existing PySpark &hellip; <a href=\"https://developer.nvidia.com/blog/reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library/\">Continued</a></p>\n", "protected": false}, "author": 1719, "featured_media": 72131, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1283854", "discourse_permalink": "https://forums.developer.nvidia.com/t/reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library/270563", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696], "tags": [278, 1909, 1592, 453, 1953, 1595, 1958], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/graphs-abstract-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iLk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72126"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1719"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72126"}], "version-history": [{"count": 39, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72126/revisions"}], "predecessor-version": [{"id": 72194, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72126/revisions/72194"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72131"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72126"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72126"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72126"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71925, "date": "2023-10-24T09:00:00", "date_gmt": "2023-10-24T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71925"}, "modified": "2023-11-02T11:14:33", "modified_gmt": "2023-11-02T18:14:33", "slug": "efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/", "title": {"rendered": "Efficient CUDA Debugging: Memory Initialization and Thread Synchronization with NVIDIA Compute Sanitizer"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://developer.nvidia.com/nvidia-compute-sanitizer\">NVIDIA Compute Sanitizer</a> (NCS) is a powerful tool that can save you time and effort while improving the reliability and performance of your CUDA applications.&nbsp;</p>\n\n\n\n<p>In our previous post, <a href=\"https://developer.nvidia.com/blog/debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer/\">Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer</a>, we explored efficient debugging in the realm of parallel programming. We discussed how debugging code in the CUDA environment can be both challenging and time-consuming, especially when dealing with thousands of threads, and how NCS can help with this process.&nbsp;</p>\n\n\n\n<p>This post continues our exploration of efficient CUDA debugging. It highlights a few more NCS tools and walks through several examples.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA Compute Sanitizer</h2>\n\n\n\n<p>NCS is a suite of tools that can perform different types of checks on the functional correctness of your code. There are four main tools in NCS:</p>\n\n\n\n<ul>\n<li><strong>Memcheck</strong> for memory access error and leak detection</li>\n\n\n\n<li><strong>Racecheck</strong>, a shared memory data access hazard detection tool</li>\n\n\n\n<li><strong>Initcheck</strong>, an uninitialized device global memory access detection tool</li>\n\n\n\n<li><strong>Synccheck</strong> for thread synchronization hazard detection</li>\n</ul>\n\n\n\n<p>In addition to these tools, NCS capabilities include:</p>\n\n\n\n<ul>\n<li>An API to enable the creation of sanitizing and tracing tools that target CUDA applications</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/compute-sanitizer/SanitizerNvtxGuide/index.html\">Integration with NVIDIA Tools Extension</a> (NVTX)&nbsp;</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#coredump\">Coredump support</a> for use with <a href=\"https://docs.nvidia.com/cuda/cuda-gdb/index.html\">CUDA-GDB</a></li>\n\n\n\n<li>Suppression features for managing the output of the tool</li>\n</ul>\n\n\n\n<p>This post will focus on debugging code and catching bugs related to uninitialized device arrays using <code>initcheck</code>, and synchronization using <code>synccheck</code>. See <a href=\"https://developer.nvidia.com/blog/debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer/\">Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer</a> for details about using <code>memcheck</code> for discovering memory leaks and <code>racecheck</code> for finding race conditions.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Initialization checking</h2>\n\n\n\n<p>NCS Initcheck helps developers identify and resolve uninitialized memory access errors in CUDA code. Uninitialized memory access can lead to unpredictable behavior and incorrect results in CUDA applications.&nbsp;</p>\n\n\n\n<p>NCS Initcheck can detect uninitialized memory accesses to global memory in device code and provides detailed information about the location and timing of the access, as well as the stack trace of the accessing thread. This helps to reveal the root cause of the issue and resolve the problem.</p>\n\n\n\n<p>To provide an example, the code below benefits from initialization checking.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n#include &lt;stdio.h&gt;\n\n#define THREADS 32\n#define BLOCKS 2\n\n__global__ void addToVector(float *v) {\n  int tx = threadIdx.x + blockDim.x * blockIdx.x;\n  v&#91;tx] += tx;\n}\n\nint main(int argc, char **argv) {\n  float *d_vec = NULL;\n  float *h_vec = NULL;\n\n  h_vec = (float *)malloc(BLOCKS*THREADS * sizeof(float));\n  cudaMalloc((void**)&amp;d_vec, sizeof(float) * BLOCKS * THREADS);\n  cudaMemset(d_vec, 0, BLOCKS * THREADS); // Zero the array\n\n  addToVector&lt;&lt;&lt;BLOCKS, THREADS&gt;&gt;&gt;(d_vec);\n  cudaMemcpy(h_vec, d_vec, BLOCKS*THREADS * sizeof(float), cudaMemcpyDeviceToHost);\n  cudaDeviceSynchronize();\n  printf(&quot;After : Vector 0, 1 .. N-1: %f %f .. %f\\n&quot;, h_vec&#91;0], h_vec&#91;1], h_vec&#91;BLOCKS*THREADS-1]);\n\n  cudaFree(d_vec);\n  free(h_vec);\n  exit(0);\n}\n</pre></div>\n\n\n<p>This code contains a CUDA kernel called <code>addToVector</code> that performs a simple add of a value to each element in a vector, with the results written back to the same element. At \u200cfirst glance, it looks fine: allocate the vector on the device with <code>cudaMalloc</code>, then zero it with <code>cudaMemset</code>, then perform calculations in the kernel. It even prints out the correct answer:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n$ nvcc -lineinfo initcheck_example.cu -o initcheck_example\n$ ./initcheck_example\nAfter : Vector 0, 1 .. N-1: 0.000000 1.000000 .. 63.000000\n</pre></div>\n\n\n<p>But the code contains a small mistake. (Twenty points if you can spot it.)</p>\n\n\n\n<p>Use the NCS <code>initcheck</code> tool to check whether any of the accesses to the vector in global memory on the device are trying to read uninitialized values.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n$ compute-sanitizer --tool initcheck ./initcheck_example\n========= COMPUTE-SANITIZER\n========= Uninitialized __global__ memory read of size 4 bytes\n=========     at 0x70 in /home/pgraham/Code/BlogExamples/initcheck_example.cu:8:addToVector(float *)\n=========     by thread (16,0,0) in block (0,0,0)\n\n. . .\n\n========= Uninitialized __global__ memory read of size 4 bytes\n=========     at 0x70 in /home/pgraham/Code/BlogExamples/initcheck_example.cu:8:addToVector(float *)\n=========     by thread (17,0,0) in block (0,0,0)\n. . . \n=========\nAfter : Vector 0, 1 .. N-1: 0.000000 1.000000 .. 63.000000\n========= ERROR SUMMARY: 48 errors\n</pre></div>\n\n\n<p>This should print a lot of information (the output shown has been edited for brevity), but something is not right. A large quantity of the output is backtrace information, which can be hidden using the <code>--show-backtrace no</code> option:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n$ compute-sanitizer --tool initcheck --show-backtrace no ./initcheck_example\n</pre></div>\n\n\n<p>Looking at the output, you can see 48 errors in total. The report shows that they are all of the type, <code>Uninitialized __global__ memory read of size 4 bytes</code>.</p>\n\n\n\n<p>Each message refers to an attempt to read something from global device memory, and that something had a size of 4 bytes. A reasonable guess would be that the errors refer to attempts to access elements of the vector, made up of floats that are 4 bytes each.</p>\n\n\n\n<p>Looking at the first error, the next part of the message indicates which thread, and which thread block, caused the error. In this case, it was thread 16 in block 0. As the kernel is set up so each thread accesses a different element of the vector, element 17 of the vector, <code>d_vec[16]</code>, was uninitialized.</p>\n\n\n\n<p>In your output, you may see a different thread as the <em>first </em>one causing an error. The GPU can schedule warps (groups of 32 threads) in whatever order it sees fit. But check through the rest of the output, and convince yourself that the lowest element in the vector causing an error was element 17 (thread 16 from block 0).</p>\n\n\n\n<p>Next, look at the line of code that initialized (or should have initialized) the array:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\ncudaMemset(d_vec, 0, BLOCKS * THREADS); // Zero the array\n</pre></div>\n\n\n<p>Checking the definition of <code>cudaMemset</code>, it takes three arguments: the pointer to the device memory you want to set (<code>d_vec</code> in this case), the value to which <em>each byte</em> in that memory region should be set (0 in this case), and the number of bytes to set (<code>BLOCKS * THREADS</code>).&nbsp;</p>\n\n\n\n<p>Now the problem begins to become more clear. The vector contains 64 elements determined by <code>BLOCKS * THREADS</code>, but each element is a float, so the entire vector is 256 bytes long. <code>cudaMemset</code> was initializing only the first 64 <em>bytes</em><strong><em> </em></strong>(the first 16 elements), which means the remaining 192 bytes (equivalent to 48 elements) are uninitialized. These 48 elements correspond to the 48 errors.</p>\n\n\n\n<p>This ties in with the observation that element 17 (thread 16, block 0) was the first to cause an error. Bingo, problem found.&nbsp;</p>\n\n\n\n<p>To fix the problem, change the <code>cudaMemset</code> call:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\ncudaMemset(d_vec, 0, sizeof(float) * BLOCKS * THREADS);\n</pre></div>\n\n\n<p>And check to make sure that the sanitizer is happy.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Checking unused memory</h2>\n\n\n\n<p>Another feature of the <code>initcheck</code> tool is identifying allocated device memory that hasn\u2019t been accessed by the end of the application. In some programs, this may be deliberate\u2014using a large static buffer to handle a range of potential problem sizes, for example. But when this is more likely an error causing a bug, use <code>initcheck</code>, as shown below.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n#include &lt;stdio.h&gt;\n\n#define N 10\n\n__global__ void initArray(float* array, float value) {\n  int threadGlobalID = threadIdx.x + blockIdx.x * blockDim.x;\n  if (threadGlobalID &lt; N)\n    array&#91;threadGlobalID] = value;\n  return;\n}\n\nint main() {\n  float* array;\n\n  const int numThreadsPerBlock = 4;\n  const int numBlocks = 2;\n\n  cudaMalloc((void**)&amp;array, sizeof(float) * N);\n\n  initArray&lt;&lt;&lt;numBlocks, numThreadsPerBlock&gt;&gt;&gt;(array, 3.0);\n  cudaDeviceSynchronize();\n\n  cudaFree(array);\n  exit(0);\n}\n</pre></div>\n\n\n<p>This very basic code will reveal the potential error. It is initializing an array, but the number of threads and the number of blocks are hard-coded. The execution configuration <code>&lt;&lt;&lt; \u2026 &gt;&gt;&gt;</code> will launch a grid of eight threads while the dataset has 10 elements (the last two elements will go unused).</p>\n\n\n\n<p>Check this using the track-unused-memory option. Note that the required syntax will depend on the CUDA version in use. For versions before 12.3, supply an argument \u201cyes\u201d using the following:  </p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n--track-unused-memory yes ;\n</pre></div>\n\n\n<p>Beginning with version 12.3, it is not necessary to supply an argument, as shown below:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n$ nvcc -o unused -lineinfo unused.cu\n$ compute-sanitizer --tool initcheck --track-unused-memory ./unused\n========= COMPUTE-SANITIZER\n=========  Unused memory in allocation 0x7fe0a7200000 of size 40 bytes\n=========     Not written 8 bytes at offset 0x20 (0x7fe0a7200020)\n=========     20% of allocation were unused.\n=========\n========= ERROR SUMMARY: 1 error\n</pre></div>\n\n\n<p>Clearly, <code>track-unused-memory</code> indicates that the array of 40 bytes (10 x 4 byte floats) includes 8 bytes that were not written to. Use the array address (the first long 0x\u2026 number) and the offset (0 x 20, which is 32 in decimal, so 32 bytes or 8 floats along) to see which bytes were unused. As expected, floats 9 and 10 in the array were not used.</p>\n\n\n\n<p>To fix this, use <code>N</code> to define <code>numBlocks</code>:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nconst int numBlocks = (N + numThreadsPerBlock - 1) / numThreadsPerBlock;\n</pre></div>\n\n\n<p>Note that <code>--track-unused-memory</code> is designed to work for device memory assigned with <code>cudaMalloc</code>. The feature doesn\u2019t work for unified memory (<code>cudaMallocManaged</code> allocated memory, for example).</p>\n\n\n\n<h2 class=\"wp-block-heading\">Synchronization checking</h2>\n\n\n\n<p>The capability to synchronize threads at a variety of levels (beyond just block and warp) is a powerful CUDA feature, enabled by the Cooperative Groups programming model. Cooperative groups are device code APIs for defining, partitioning, and synchronizing groups of threads, giving much more flexibility and control compared to the standard <code>syncthreads</code> function, which synchronizes all the threads in a block. For more details, see <a href=\"https://developer.nvidia.com/blog/cooperative-groups/\">Cooperative Groups: Flexible CUDA Thread Programming</a>.</p>\n\n\n\n<p>Yet, this capability comes with greater opportunities to introduce bugs. This is where NCS <code>synccheck</code> can help to identify and resolve synchronization errors in CUDA code. <code>synccheck</code> can identify whether a CUDA application is correctly using synchronization primitives and their Cooperative Groups API counterparts.</p>\n\n\n\n<p>One interesting use of synchronization is the application of a mask to a warp of threads. Set up the warp so that some threads are true and others are false, enabling each thread to individually perform different operations depending on that property. For more details, see <a href=\"https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/\">Using CUDA Warp-Level Primitives</a>.</p>\n\n\n\n<p>A useful function to help with this is <code>__ballot_sync</code> defined as:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nunsigned int __ballot_sync(unsigned int mask, int predicate);\n</pre></div>\n\n\n<p><code>mask</code> is an initial mask, typically created with all bits set to 1, representing that all threads in the warp are initially active. <code>predicate</code> is a condition evaluated by each thread, where predicate evaluates to either true (non-zero) or false (zero) for each thread.</p>\n\n\n\n<p>The ballot function evaluates the predicate for each thread in the warp, and returns a mask representing the outcome for that thread. It also provides a synchronization point. All threads in the warp must reach this <code>__ballot_sync</code> before any of them can proceed further.&nbsp;</p>\n\n\n\n<p>For example, set up a mask where even threads in the warp are true and odd threads are false:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n__ballot_sync(0xffffffff, threadID % 2 == 0);\n</pre></div>\n\n\n<p>The initial mask <code>0xffffff</code> is a hexadecimal representation, and evaluates to <code>11111111111111111111111111111111</code> in binary. This ensures that all 32 threads are involved in the ballot.&nbsp;</p>\n\n\n\n<p>The outcome of the ballot is a mask, <code>0xaaaaaaaa</code>, which in binary is <code>10101010101010101010101010101010</code>. The even threads (thread ID 0, 2, 4 \u2026) are set to true, and odd threads are set to false.&nbsp;</p>\n\n\n\n<p>The ballot is often used in conjunction with a <code>__syncwarp</code>, which can synchronize threads in a warp, based on the mask provided.</p>\n\n\n\n<p>The following example uses both <code>_ballot_sync</code> and <code>_syncwarp</code>:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nstatic constexpr int NumThreads = 32 ;\n\n__shared__ int smem&#91;NumThreads];\n\n__global__ void sumValues(int *sum_out) {\n    int threadID = threadIdx.x;\n\n    unsigned int mask = __ballot_sync(0xffffffff, threadID &lt; (NumThreads / 2));\n\n    if (threadId &lt;= (NumThreads / 2)) {\n        smem&#91;threadId] = threadId;\n\n        __syncwarp(mask);\n\n        if (threadID == 0) {\n          *sum_out = 0;\n          for (int i = 0; i &lt; (NumThreads / 2); ++i)\n            *sum_out += smem&#91;i];\n        }\n    }\n\n    __syncThreads();\n}\n\nint main(){\n    int *sum_out = nullptr;\n\n    cudaMallocManaged((void**)&amp;sum_out, sizeof(int));\n\n    sumVaules&lt;&lt;&lt;1, NumThreads&gt;&gt;&gt;(sum_out);\n    cudaDeviceSynchronize();\n    \n    printf(&quot;Sum out = %d\\n&quot;, *sum_out);\n    cudaFree(sum_out);\n    return 0;\n}\n</pre></div>\n\n\n<p>Before reading further, take a look at the code and try and work out what it is doing given your understanding of the ballot and the <code>syncwarp</code> functionality. See if you can spot what&#8217;s wrong. (Fifty points for this one\u2014it\u2019s more challenging.)</p>\n\n\n\n<p>The purpose of this code is for each thread to assign a single value to shared memory, and then sum up all the values to get one answer. However, this is applied to only half the available threads. A single warp of 32 threads is set up through the execution configuration <code>&lt;&lt;&lt;1, numThreads&gt;&gt;&gt;</code> to execute the kernel <code>sumValues</code>.&nbsp;</p>\n\n\n\n<p>In that kernel, create a mask using <code>__ballot_sync</code> with <code>threadID &lt; NumThreads/2</code> as the predicate, which will evaluate to true for the first half of the warp where <code>threadID&lt;16</code> (threads 0, 1, .. 15).</p>\n\n\n\n<p>For those 16 threads, assign a value (the threadID) to shared memory, and perform a <code>__syncwarp(mask)</code> synchronization on those threads to ensure they have all\u200c written to shared memory. Then update sum_out the global sum based on those values.</p>\n\n\n\n<p>Next, try compiling and running the following code:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n$ nvcc -o ballot_example -lineinfo ballot_example.cu\n$ ./ballot_example\nSum out = 0\n</pre></div>\n\n\n<p>The answer, zero, is not correct. It should be 120 (15 + 14 + 13 + \u2026 + 2 + 1 + 0).</p>\n\n\n\n<p>Did you spot the mistake? The conditional section of code was executed using if <code>(threadId &lt;= (NumThreads / 2))</code>. This code uses <code>&lt;=</code> rather than <code>&lt;</code> as the comparator, meaning that the first 17 threads will execute.&nbsp;</p>\n\n\n\n<p>What happens when thread 17 tries to call <code>syncwarp</code> when it is not included as true in the mask? It\u200c causes the whole kernel to stop running, so the sum calculation is never reached. Hence the output is zero.&nbsp;</p>\n\n\n\n<p>All this fails silently, and only the incorrect output indicates a problem. In \u200cmore complicated code, this could be a nightmare to track down.</p>\n\n\n\n<p>Using <code>synccheck</code> provides the following:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n$ compute-sanitizer --tool synccheck --show-backtrace no ./ballot_example\n========= COMPUTE-SANITIZER\n========= Barrier error detected. Invalid arguments\n=========     at 0x220 in /home/pgraham/Code/devblog/NCS_Part2/ballot_example.cu:32:sumValues(int *)\n=========     by thread (0,0,0) in block (0,0,0)\n=========\n\n. . .\n\n========= Barrier error detected. Invalid arguments\n=========     at 0x220 in /home/pgraham/Code/devblog/NCS_Part2/ballot_example.cu:32:sumValues(int *)\n=========     by thread (16,0,0) in block (0,0,0)\n=========\nSum out = 0\n========= ERROR SUMMARY: 17 errors\n</pre></div>\n\n\n<p>Regarding these 17 errors, \u201cinvalid arguments,\u201d the <a href=\"https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#understanding-synccheck-reports\"><code>synccheck</code> documentation</a> states that the invalid argument can occur if not all threads reaching a <code>__syncwarp</code> declare themselves in the mask parameter.</p>\n\n\n\n<p>In this case, thread 17 or thread (16,0,0) is not<em> </em>active in the mask, so it shouldn\u2019t call the <code>syncwarp</code>. Note that this causes all the other threads calling the <code>syncwarp</code> to also register an error. They are individually calling <code>syncwarp</code>, but because one of them causes it to fail, all the other <code>syncwarp</code> calls must also fail. It is a collective operation that causes 17 errors in total.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>This post walked you through a few examples of how to debug code and catch bugs using the <code>initcheck</code> and <code>synccheck</code> features in <a href=\"https://developer.nvidia.com/nvidia-compute-sanitizer\">NVIDIA Compute Sanitizer</a>. To get started using NCS, download the <a href=\"https://developer.nvidia.com/cuda-downloads\">CUDA Toolkit.</a></p>\n\n\n\n<p>To learn more, visit <a href=\"https://github.com/NVIDIA/compute-sanitizer-samples\">NVIDIA/compute-sanitizer-samples</a> on GitHub, and read the <a href=\"https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html\">NCS documentation</a>. Join the conversation in the <a href=\"https://forums.developer.nvidia.com/c/developer-tools/cuda-developer-tools/compute-sanitizer/108\">NVIDIA Developer Forum</a> dedicated to sanitizer tools. Good luck on your bug hunt!</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Compute Sanitizer (NCS) is a powerful tool that can save you time and effort while improving the reliability and performance of your CUDA applications.&nbsp; In our previous post, Efficient CUDA Debugging: How to Hunt Bugs with NVIDIA Compute Sanitzer, we explored efficient debugging in the realm of parallel programming. We discussed how debugging code &hellip; <a href=\"https://developer.nvidia.com/blog/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/\">Continued</a></p>\n", "protected": false}, "author": 1790, "featured_media": 71928, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1283762", "discourse_permalink": "https://forums.developer.nvidia.com/t/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/270551", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 503], "tags": [21, 453, 608, 26, 2377], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/debugging-cuda.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iI5", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71925"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1790"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71925"}], "version-history": [{"count": 58, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71925/revisions"}], "predecessor-version": [{"id": 72200, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71925/revisions/72200"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71928"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71925"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71925"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71925"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72018, "date": "2023-10-22T21:03:00", "date_gmt": "2023-10-23T04:03:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72018"}, "modified": "2023-11-02T13:23:30", "modified_gmt": "2023-11-02T20:23:30", "slug": "differentiable-slang-example-applications", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/differentiable-slang-example-applications/", "title": {"rendered": "Differentiable Slang: Example Applications"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Differentiable Slang easily integrates with existing codebases\u2014from Python, PyTorch, and CUDA to HLSL\u2014to aid multiple computer graphics tasks and enable novel data-driven and neural research. In this post, we introduce several code examples using differentiable Slang to demonstrate the potential use across different rendering applications and the ease of integration.&nbsp;</p>\n\n\n\n<p class=\"has-text-align-right\"><em>This is part of a series on Differentiable Slang. For more information about differential programming and automatic gradient computation in the Slang language, see </em><a href=\"https://developer.nvidia.com/blog/differentiable-slang-a-shading-language-for-renderers-that-learn/\">Differentiable Slang: A Shading Language for Renderers That Learn</a><em>.</em></p>\n\n\n\n<h2 class=\"wp-block-heading\">Example application: Appearance-based BRDF optimization</h2>\n\n\n\n<p>One of the basic building blocks in computer graphics is BRDF texture maps representing multiple properties of materials and describing how the light interacts with the rendered surfaces. Artists author and preview textures, but then rendering algorithms transform them automatically, for instance, filtering, blending the BRDF properties, or creating mipmaps.</p>\n\n\n\n<p>Rendering is highly nonlinear, so linear operations on texture maps do not produce the correct linearly changing appearance. Various models were proposed to preserve appearance in applications like mipmap chain creation. Those models are approximate and often created only for a specific BRDF; new ones must be designed when rendering changes.</p>\n\n\n\n<p>Instead of refining those models, we propose to use differentiable rendering and a data-driven approach to build appearance-preserving mipmaps. For more information and code examples, see the <a href=\"https://github.com/shader-slang/slang-python/tree/main/examples/brdf-appearance-optimize-example\">/shader-slang</a> GitHub repo.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1105\" height=\"381\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based.png\" alt=\"Three columns of images showing the differences in rendering results of a surface with a brick-and-grass material.\" class=\"wp-image-72041\" style=\"width:829px;height:286px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based.png 1105w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-300x103.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-625x215.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-768x265.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-645x222.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-500x172.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-362x125.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-319x110.png 319w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/example-appearance-based-1024x353.png 1024w\" sizes=\"(max-width: 1105px) 100vw, 1105px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Inverse rendering enables appearance-preserving minification of a material: (left) naively downsampled material; (middle) low-resolution material optimized with Slang; (right) reference.</em></figcaption></figure></div>\n\n\n<p>In Figure 1, the left column shows the surface rendered with a naively downsampled material. The middle column shows the same surface rendered with a low-resolution material obtained from an optimization algorithm implemented using Slang\u2019s automatic differentiation feature. The right column shows the rendering result using the reference material without downsampling. The rendering using the optimized material preserves more details than using the naively downsampled material and matches much closer to the reference material.</p>\n\n\n\n<p>To demonstrate Slang\u2019s flexibility and compatibility with multiple existing frameworks, we write the optimization loop in PyTorch in a <a href=\"https://jupyter.org/\">Jupyter notebook</a>, enabling easy visualization, interactive debugging, and Markdown documentation of the code. The shading code is written in Slang, which will look familiar to graphics programmers. Easy Slang, Python, PyTorch, and Jupyter interoperability enable you to choose the best combination of languages for data-driven graphics development.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Example application: Texture compression</h2>\n\n\n\n<p>Texture compression is an optimization task that significantly reduces the texture file size and memory usage while trying to preserve the image quality. There are many approaches to texture compression and many different compressors available, with the most popular being hardware block compression (BC). We demonstrate how we can use gradient descent to find a close-to-optimal solution for BC7 Mode 6 texture compression automatically with Slang automatic differentiation capabilities.</p>\n\n\n\n<p>By using gradient descent, we don\u2019t need to write the compression code explicitly. Slang automatically generates gradients of BC7 block color interpolation through backward differentiation of the Mode 6 decoder:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n&#91;Differentiable]\nfloat4 decodeTexel() {\nreturn weight * maxEndPoint + (1 - weight) * minEndPoint;\n}\n</pre></div>\n\n\n<p>To facilitate compression, we provide an effective initial guess, with endpoints initialized to the color space box\u2019s corners enveloping a block and interpolation weights set to 0.5. We model the BC7 quantization and iteratively adjust endpoints and weights for each 4&#215;4 block, ensuring minimal difference between the original textures and its compressed version.</p>\n\n\n\n<p>This simple approach achieves a high compression quality, and for the best computational performance, we merge the forward (decoding) and backward (encoding) passes into a single compute shader. Every thread works independently on a BC7 block, improving efficiency by retaining all data in registers and avoiding atomic operations to accumulate gradients. On an NVIDIA RTX 4090, this method can compress 400 4k textures every second, achieving a compression speed of 6.5 GTexel/s.</p>\n\n\n\n<p>This example is written in Slang and the Python interface to the Falcor rendering infrastructure. For more information and code examples, see the <a href=\"https://github.com/NVIDIAGameWorks/Falcor/tree/master/scripts/python/TinyBC\">/NVIDIAGameWorks/Falcor</a> GitHub repo.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Example application: NVDIFFREC</h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"220\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-625x220.png\" alt=\"Figure shows that the pipeline takes as input 2D images of an ancient artifact, and a randomly initialized soup of triangles, and outputs a triangle mesh whose shape and material matches the input images.\" class=\"wp-image-72040\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-625x220.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-300x105.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-179x63.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-768x270.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-1536x539.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-645x227.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-500x176.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-160x56.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-362x127.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-313x110.png 313w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang-1024x360.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvdiffrec-slang.png 1825w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The inverse rendering pipeline </em><a href=\"https://github.com/NVlabs/nvdiffrec\"><em>nvdiffrec</em></a><em>, rewritten in Slang, running at equal performance to hand-differentiated CUDA kernels</em></figcaption></figure></div>\n\n\n<p><a href=\"https://nvlabs.github.io/nvdiffrec/\">Nvdiffrec</a> is a large inverse rendering library for joint shape, material, and lighting optimization. Nvdiffrec allows the reconstruction of various scene properties from a series of 2D observations and can be used in various inverse rendering and appearance reconstruction applications.</p>\n\n\n\n<p>Nvdiffrec is a large inverse rendering library for joint shape, material, and lighting optimization. Nvdiffrec reconstructs various scene properties from a series of 2D observations and can be used in various inverse rendering and appearance reconstruction applications.</p>\n\n\n\n<p>Originally, Nvdiffrec\u2019s performance-critical operations were accelerated using PyTorch extensions built with hand-differentiated CUDA kernels. The CUDA kernels perform the following tasks:</p>\n\n\n\n<ul>\n<li>Loss computation (log-sRGB mapping and warp-wide reduction)</li>\n\n\n\n<li>Tangent space normal mapping</li>\n\n\n\n<li>Vertex transforms (multiplication of a vertex array with a batch of 4\u00d74 matrices)</li>\n\n\n\n<li>Cube map pre-filtering (for the split-sum shading model)</li>\n</ul>\n\n\n\n<p>Slang generates automatically-differentiated CUDA kernels that achieve the same performance as the handwritten, manually-differentiated CUDA code. This reduces the number of lines of code considerably while staying compatible and interoperable with other CUDA kernels. Slang makes the code easier to maintain, extend, and connect to existing rendering pipelines and shading models.</p>\n\n\n\n<p>For more information about the Slang version of nvdiffrec, see the <a href=\"https://github.com/NVlabs/nvdiffrec/tree/slang\">/NVlabs/nvdiffrec</a> GitHub repo.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Example application: Differentiable path tracers</h2>\n\n\n\n<p>We converted a traditional, real-time path tracer into a differentiable path tracer, reusing over 5K lines of Slang code. The following are two different inverse path tracing examples in Slang:</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/NVIDIAGameWorks/Falcor/tree/master/scripts/inv-rendering/material_optimization\">Inverse-rendering optimization solving for material parameters via a differentiable path tracer</a></li>\n\n\n\n<li><a href=\"https://github.com/NVIDIAGameWorks/Falcor/tree/master/Source/RenderPasses/WARDiffPathTracer\">Differentiable path tracer with warped-area sampling for differential geometry</a></li>\n</ul>\n\n\n\n<figure class=\"wp-block-gallery has-nested-images columns-default is-cropped wp-block-gallery-5 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"768\" height=\"432\" data-id=\"72045\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left.png\" alt=\"Picture of 3D colored dots in a box.\" class=\"wp-image-72045\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-left-196x110.png 196w\" sizes=\"(max-width: 768px) 100vw, 768px\" /><figcaption class=\"wp-element-caption\">Initial set of material parameters</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"768\" height=\"432\" data-id=\"72046\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle.png\" alt=\"Picture of 3D dots with a color translation.\" class=\"wp-image-72046\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-middle-196x110.png 196w\" sizes=\"(max-width: 768px) 100vw, 768px\" /><figcaption class=\"wp-element-caption\">Optimized parameters</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"768\" height=\"432\" data-id=\"72047\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right.png\" alt=\"Picture of 3D dots that is similar to to the color translation version.\" class=\"wp-image-72047\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/inverse-rendering-right-196x110.png 196w\" sizes=\"(max-width: 768px) 100vw, 768px\" /><figcaption class=\"wp-element-caption\">Reference</figcaption></figure>\n<figcaption class=\"blocks-gallery-caption wp-element-caption\"><em>Figure 3. Inverse-rendering result that optimizes thousands of material parameters simultaneously</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>For more information, see the <a href=\"https://research.nvidia.com/labs/rtr/publication/bangaru2023slangd/\">SLANG.D: Fast, Modular and Differentiable Shader Programming</a> paper and begin exploring differentiable rendering with <a href=\"https://github.com/shader-slang/slang\">Slang</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Differentiable Slang easily integrates with existing codebases\u2014from Python, PyTorch, and CUDA to HLSL\u2014to aid multiple computer graphics tasks and enable novel data-driven and neural research. In this post, we introduce several code examples using differentiable Slang to demonstrate the potential use across different rendering applications and the ease of integration.&nbsp; This is part of a &hellip; <a href=\"https://developer.nvidia.com/blog/differentiable-slang-example-applications/\">Continued</a></p>\n", "protected": false}, "author": 1898, "featured_media": 72019, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1282081", "discourse_permalink": "https://forums.developer.nvidia.com/t/differentiable-slang-example-applications/270321", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [1932, 453, 1962, 527, 604], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-slang-example-apps-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iJA", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72018"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1898"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72018"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72018/revisions"}], "predecessor-version": [{"id": 72284, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72018/revisions/72284"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72019"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72018"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72018"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72018"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 72011, "date": "2023-10-22T21:02:00", "date_gmt": "2023-10-23T04:02:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=72011"}, "modified": "2023-11-02T13:23:44", "modified_gmt": "2023-11-02T20:23:44", "slug": "differentiable-slang-a-shading-language-for-renderers-that-learn", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/differentiable-slang-a-shading-language-for-renderers-that-learn/", "title": {"rendered": "Differentiable Slang: A Shading Language for Renderers That Learn"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA just released a SIGGRAPH Asia 2023 research paper, <a href=\"https://research.nvidia.com/labs/rtr/publication/bangaru2023slangd/\">SLANG.D: Fast, Modular and Differentiable Shader Programming</a>. The paper shows how a single language can serve as a unified platform for real-time, inverse, and differentiable rendering. The work is a collaboration between MIT, UCSD, UW, and NVIDIA researchers.</p>\n\n\n\n<p class=\"has-text-align-right\"><em>This is part of a series on Differentiable Slang. For more information about practical examples of Slang with various machine learning (ML) rendering applications, see </em><a href=\"https://developer.nvidia.com/blog/differentiable-slang-example-applications/\">Differentiable Slang: Example Applications</a><em>.</em></p>\n\n\n\n<p><a href=\"https://shader-slang.com/\">Slang</a> is an open-source language for real-time graphics programming that brings new capabilities for writing and maintaining large-scale, high-performance, cross-platform graphics codebases. Slang adapts modern language constructs to the high-performance demands of real-time graphics, and generates code for Direct 3D 12, Vulkan, OptiX, CUDA, and the CPU. </p>\n\n\n\n<p>While Slang began as a research project, it has grown into a practical solution used in the <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> and <a href=\"https://www.nvidia.com/en-us/geforce/rtx-remix/\">NVIDIA RTX Remix</a> renderers, and the <a href=\"https://github.com/NVIDIAGameWorks/Falcor\">NVIDIA Game Works Falcor research infrastructure</a>.</p>\n\n\n\n<p>The new research pioneers a co-design approach. This approach shows that the complexities of automatic differentiation can be handled elegantly if differentiation is incorporated as a first-class citizen in the entire system: </p>\n\n\n\n<ul>\n<li>Language</li>\n\n\n\n<li>Type system</li>\n\n\n\n<li>Intermediate representation (IR)</li>\n\n\n\n<li>Optimization passes</li>\n\n\n\n<li>Auto-completion engine</li>\n</ul>\n\n\n\n<p>Slang\u2019s automatic differentiation integrates seamlessly with Slang\u2019s modular programming model, GPU graphics pipelines, Python, and PyTorch. Slang supports differentiating arbitrary control flow, user-defined types, dynamic dispatch, generics, and global memory accesses. With Slang, existing real-time renderers can be made differentiable<em> </em>and learnable without major source code changes.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Bridging computer graphics and machine learning</h2>\n\n\n\n<p>Data-driven rendering algorithms are changing computer graphics, enabling powerful new representations for <a href=\"https://nvlabs.github.io/instant-ngp/\">shape</a>, <a href=\"https://research.nvidia.com/labs/rtr/neural_texture_compression/\">textures</a>, <a href=\"https://developer.nvidia.com/rendering-technologies/neuralvdb\">volumetrics</a>, <a href=\"https://research.nvidia.com/labs/rtr/neural_appearance_models/\">materials</a>, and <a href=\"https://www.nvidia.com/en-us/geforce/technologies/dlss/\">post-processing</a> algorithms that increase performance and image quality. In parallel, computer vision and ML researchers are increasingly leveraging computer graphics, for example, to <a href=\"https://nvlabs.github.io/nvdiffrecmc/\">improve 3D reconstruction</a> through inverse rendering.</p>\n\n\n\n<p>Bridging real-time graphics, ML, and computer vision development environments is challenging because of different tools, libraries, programming languages, and programming models. With the latest research, Slang enables you to easily take on the following tasks:</p>\n\n\n\n<ul>\n<li><strong>Bring learning to rendering</strong>. Slang enables graphics developers to use gradient-based optimization and solve traditional graphics problems in a data-driven manner, for example, learning mipmap hierarchies using appearance-based optimization.</li>\n\n\n\n<li><strong>Build differentiable renderers from existing graphics code.</strong> With Slang, we transformed a preexisting, real-time path tracer into a differentiable path tracer, reusing 90% of the Slang code.</li>\n\n\n\n<li><strong>Bring graphics to ML training frameworks</strong>. Slang generates custom PyTorch plugins from graphics shader code. In this post, we demonstrate using Slang in <a href=\"https://nvlabs.github.io/nvdiffrec/\">Nvdiffrec</a> to generate auto-differentiated CUDA kernels.</li>\n\n\n\n<li><strong>Bring ML training inside the renderer</strong>. Slang facilitates training small neural networks inside a real-time renderer, such as the model used in <a href=\"https://research.nvidia.com/publication/2021-06_real-time-neural-radiance-caching-path-tracing\">neural radiance caching</a>.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Differentiable programming and machine learning need gradients</h2>\n\n\n\n<figure class=\"wp-block-gallery has-nested-images columns-default is-cropped wp-block-gallery-7 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"512\" height=\"512\" data-id=\"72074\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny.png\" alt=\"Picture of a bunny figurine in a colored box.\" class=\"wp-image-72074\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny.png 512w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/rendered-bunny-110x110.png 110w\" sizes=\"(max-width: 512px) 100vw, 512px\" /><figcaption class=\"wp-element-caption\"><em>(l) Rendered scene</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"500\" height=\"500\" data-id=\"72075\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny.png\" alt=\"Picture of the bunny with a green background and color translation.\" class=\"wp-image-72075\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/derivative-bunny-110x110.png 110w\" sizes=\"(max-width: 500px) 100vw, 500px\" /><figcaption class=\"wp-element-caption\"><em>(m) Reference derivative for y-axis</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"500\" height=\"500\" data-id=\"72076\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny.png\" alt=\"A similar picture of the bunny with the green background and color translation.\" class=\"wp-image-72076\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/differentiable-bunny-110x110.png 110w\" sizes=\"(max-width: 500px) 100vw, 500px\" /><figcaption class=\"wp-element-caption\"><em>(r) Same derivative computed by Slang</em></figcaption></figure>\n<figcaption class=\"blocks-gallery-caption wp-element-caption\"><em>Figure 1. Comparison of a reference derivative vs. the Slang autodiff-computed derivative</em></figcaption></figure>\n\n\n\n<p>Figure 1 shows the Stanford Bunny placed inside the Cornell box scene. The left column shows the rendered scene. The middle column shows a reference derivative with respect to the bunny\u2019s translation in the y-axis. The right column shows the same derivative computed by the Slang\u2019s autodiff feature, which appears identical to the reference image.</p>\n\n\n\n<p>A key pillar of ML methods is gradient-based optimization. Specifically, most ML algorithms are powered by <em>reverse-mode automatic differentiation</em>, an efficient way to propagate derivatives through a series of computations. This applies not only to large neural networks but also to many simpler data-driven algorithms that require the use of gradients and gradient descent.</p>\n\n\n\n<p>Frameworks like PyTorch expose high-level operations on tensors (multi-dimensional matrices) that come with hand-coded reverse-mode kernels. As you compose tensor operations to create your neural network, PyTorch composes derivative computations automatically by chaining those kernels. The result is an easy-to-use system where you do not have to write gradient flow manually, which is one of the reasons behind ML research\u2019s accelerated pace.</p>\n\n\n\n<p>Unfortunately, some computations aren\u2019t easily captured by those high-level operations on arrays, creating difficulties in expressing them efficiently. This is the case with graphics components such as a rasterizer or ray tracer, where diverging control flow and complex access patterns require a lot of inefficient active-mask tracking and other workarounds. Those workarounds are not only difficult to write and read but also have a significant performance and memory usage overhead.</p>\n\n\n\n<p>As a result, most high-performance differentiable graphics pipelines, such as <a href=\"https://nvlabs.github.io/nvdiffrec/\">nvdiffrec</a>, <a href=\"https://nvlabs.github.io/instant-ngp/\">InstantNGP</a>, and <a href=\"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/\">Gaussian splatting</a>, are not written in pure Python. Instead, researchers write high-performance kernels in languages operating closer to the underlying hardware, such as CUDA, HLSL, or GLSL. </p>\n\n\n\n<p>Because these languages do not provide automatic differentiation, these applications use hand-derived gradients. Hand-differentiation is tedious and error-prone, making it difficult for others to use or modify those algorithms. This is where Slang comes in, as it can automatically generate differentiated shader code for multiple backends.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Designing Slang\u2019s automatic differentiation</h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"720\" height=\"235\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene.png\" alt=\"A split-screen image showing the original rendering of the Zero Day scene and the derivative of output color with respect to camera position.\" class=\"wp-image-72078\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene.png 720w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-300x98.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-625x204.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-179x58.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-645x211.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-500x163.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-160x52.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-362x118.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/propagated-derivatives-zero-day-scene-337x110.png 337w\" sizes=\"(max-width: 720px) 100vw, 720px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Propagated derivatives on the Zero Day scene</em></figcaption></figure></div>\n\n\n<p>Figure 2 shows propagated derivatives on the Zero Day scene computed by a differentiable path tracer written in the <a href=\"https://developer.nvidia.com/falcor\">Falcor framework</a>. The differentiable path tracer was built by reusing over 5K lines of preexisting shader code.</p>\n\n\n\n<p>Slang\u2019s roots can be traced to the <a href=\"https://dl.acm.org/doi/10.1145/1964921.1965002\">Spark</a> programming language presented at SIGGRAPH 2011 and, in its current form, to <a href=\"http://graphics.cs.cmu.edu/projects/slang/\">SIGGRAPH 2018</a>. Adding automatic differentiation to Slang required years of research and many iterations of language design. Every part of the language and the compiler\u2014including the parser, type system, standard library, IR, optimization passes, and the Intellisense engine\u2014needed to be revised to support auto-diff as a first-class member of the language.</p>\n\n\n\n<p>Slang\u2019s type system has been extended to treat differentiability as a first-class property of functions and types. The type system enables compile-time checks to guard against common mistakes when working in differentiable programming frameworks, such as dropping derivatives unintentionally through calls to non-differentiable functions. We describe those and many more challenges and solutions in our technical paper, <a href=\"https://research.nvidia.com/labs/rtr/publication/bangaru2023slangd/\">SLANG.D: Fast, Modular and Differentiable Shader Programming</a>.</p>\n\n\n\n<p>In Slang, automatic differentiation is represented as a composable operator on functions. Applying automatic differentiation on a function yields another function that can be used just as any other function. This functional design enables higher-order differentiation, which is absent in many other frameworks. The ability to differentiate a function multiple times in any combination of forward and reverse modes significantly eases the implementation of advanced rendering algorithms, such as <a href=\"http://people.csail.mit.edu/sbangaru/projects/was-2020/\">warped-area sampling</a> and <a href=\"https://people.csail.mit.edu/tzumao/h2mc/\">Hessian-Hamiltonian MLT</a>.</p>\n\n\n\n<p>Slang\u2019s standard library has also been extended to support differentiable computations, and most existing HLSL intrinsic functions are treated as differentiable functions, allowing existing code that uses these intrinsics to be automatically differentiated without modifications.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1.jpg\"><img decoding=\"async\" loading=\"lazy\" width=\"1920\" height=\"832\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1.jpg\" alt=\"Screenshot of Slang\u2019s Visual Studio Code extension showing the function signature of a backward-differentiated function at its call-site.\" class=\"wp-image-72079\" style=\"width:960px;height:416px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1.jpg 1920w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-300x130.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-625x271.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-179x78.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-768x333.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-1536x666.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-645x280.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-500x217.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-160x69.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-362x157.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-254x110.jpg 254w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/slang-vscode-extension-1-1024x444.jpg 1024w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Slang\u2019s Visual Studio Code extension providing interactive hinting on the signature of an automatically differentiated function.</em></figcaption></figure></div>\n\n\n<p>Slang offers a complete developer toolset, including a <a href=\"https://marketplace.visualstudio.com/items?itemName=shader-slang.slang-language-extension\">Visual Studio Code extension</a> with comprehensive hinting and auto-completion support for differentiable entities, which improves productivity in our internal projects.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Real-time graphics in the differentiable programming ecosystem</h2>\n\n\n\n<p>The Slang compiler can emit derivative function code in the following languages:</p>\n\n\n\n<ul>\n<li><strong>HLSL</strong>: For use with Direct3D pipelines.</li>\n\n\n\n<li><strong>GLSL or SPIR-V</strong>: For use with OpenGL and Vulkan.</li>\n\n\n\n<li><strong>CUDA or OptiX</strong>: For use in standalone applications, in Python, or with tensor frameworks such as PyTorch.</li>\n\n\n\n<li><strong>Scalar C++</strong>: For debugging.</li>\n</ul>\n\n\n\n<p>You can emit the same code to multiple targets. For instance, you could train efficient models with PyTorch optimizers and then deploy them in a video game or other interactive experience running on Vulkan or Direct3D without writing new or different code. A single representation written in one language is highly beneficial for long-term code maintenance and avoiding bugs arising if two versions are subtly different.</p>\n\n\n\n<p>Similarly to the <a href=\"https://developer.nvidia.com/warp-python\">NVIDIA WARP framework</a> for differentiable simulation, Slang contributes to the growing ecosystem of differentiable programming. Slang allows the generation of derivatives automatically and the use of them together with both lower and higher-level programming environments. It is possible to use Slang together with handwritten, heavily optimized CUDA kernels and <a href=\"https://developer.nvidia.com/cudnn\">libraries</a>.</p>\n\n\n\n<p>If you prefer a higher-level approach and use Python interactive notebooks for research and experimentation, you can use Slang through the <a href=\"https://shader-slang.com/slang/user-guide/a1-02-slangpy.html\"><em>slangpy</em></a> package (<code>pip install slangpy</code>) from environments like Jupyter notebooks. Slang can be a part of a rich notebook, Python, PyTorch, and NumPy ecosystem to interface with data stored in various formats, interact with it using widgets, and visualize with plotting and data analysis libraries while offering an additional programming model, more suited for certain applications.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Tensors vs. shading languages</h2>\n\n\n\n<p>PyTorch and other tensor-based libraries, such as NumPy, TensorFlow, and Jax, offer fundamentally different programming models from Slang and, in general, shading languages. PyTorch is designed primarily for <em>feed-forward</em> neural networks where operations on each element are relatively uniform without diverging control flow. The NumPy and PyTorch n-dimensional array (ndarray) model operates on whole tensors, making it trivial to specify horizontal reductions like summing over axes and large matrix multiplications.</p>\n\n\n\n<p>By contrast, shading languages occupy the other end of the spectrum and expose the <em>single-instruction-multiple-threads (SIMT)</em> model to enable you to specify programs operating on a single element or a small block of elements. This makes it easy to express intricate control flow where each set of elements executes a vastly different series of operations, such as when the rays of a path tracer strike different surfaces and execute different logic for their next bounce.&nbsp;</p>\n\n\n\n<p>Both models co-exist and should be treated as complementary, as they fulfill different goals: A reduce-sum operation on a tensor would take one line of ndarray code, but hundreds of lines of code and multiple kernel launches to express efficiently in the SIMT style. </p>\n\n\n\n<p>Conversely, a variable-step ray marcher can be written elegantly in the SIMT style using dynamic loops and stopping conditions, but the same ray marcher would devolve into complex and unmaintainable active-mask-tracking ndarray code. Such code is not only difficult to write and read but can perform worse as every branch gets executed for each element instead of only one or the other, depending on the active state.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1221\" height=\"316\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models.png\" alt=\"One diagram shows that the ndarray framework wavefront model with operations on full batches and intermediate results stored in global memory. The other diagram shows that the SIMT framework fused model compiles multiple passes into one optimized kernel, using local intermediate results to save memory and bandwidth.\" class=\"wp-image-72080\" style=\"width:916px;height:237px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models.png 1221w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-300x78.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-625x162.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-179x46.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-768x199.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-645x167.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-500x129.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-160x41.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-362x94.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-425x110.png 425w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/ndarray-vs-simt-models-1024x265.png 1024w\" sizes=\"(max-width: 1221px) 100vw, 1221px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Ndarray framework wavefront model (l) compared to SIMT framework fused model (r)</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Performance benefits</h2>\n\n\n\n<p>PyTorch and other ML frameworks are built for the training and inference of large neural networks. They use heavily optimized platform libraries to perform large matrix-multiply and convolution operations. </p>\n\n\n\n<p>While each individual operation is extremely efficient, the intermediate data between them is serialized to the main memory and checkpointed. During training, the forward and backpropagation passes are computed serially and separately. This makes the overhead of PyTorch significant for tiny neural networks and other differentiable programming uses in real-time graphics.</p>\n\n\n\n<p>Slang\u2019s automatic differentiation feature gives you control over how gradient values are stored, accumulated, and computed, which enables significant performance and memory optimizations. By avoiding multiple kernel launches, excessive global memory accesses, and unnecessary synchronizations, it enables fusing forward and backward passes and up to 10x training speedups compared to the same small-network and graphics workloads written with standard PyTorch operations. </p>\n\n\n\n<p>This speedup not only accelerates the training of ML models but also enables many novel applications that use smaller inline neural networks inside graphics workloads. Inline neural networks open up a whole new area of computer graphics research, such as neural radiance caching, neural texture compression, and neural appearance models.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Show me the code!</h2>\n\n\n\n<p>For more information about Slang\u2019s open-source repository and the <a href=\"https://shader-slang.com/slang/user-guide/a1-02-slangpy.html\">slangpy</a> Python package, see the <a href=\"https://github.com/shader-slang/slang\">/shader-slang</a> GitHub repo and <a href=\"https://shader-slang.com/slang/user-guide/a1-02-slangpy.html\">Using Slang to Write PyTorch Kernels</a>. The automatic differentiation language feature is documented in the <a href=\"https://shader-slang.com/slang/user-guide/07-autodiff.html\">Slang User Guide</a>. We also include several differentiable Slang tutorials that walk through the code for common graphics components in Slang while introducing Slang\u2019s object-oriented differentiable programming model.</p>\n\n\n\n<p>For more Slang and PyTorch tutorials using <a href=\"https://shader-slang.com/slang/user-guide/a1-02-slangpy.html\">slangpy</a>, see the following resources:</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/shader-slang/slang-python/tree/main/examples/fwd-rasterizer-example#readme\">1-Triangle Rasterizer (Non-Differentiable) </a>&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/shader-slang/slang-python/tree/main/examples/soft-rasterizer-example#readme\">1-Triangle Differentiable \u2018Soft\u2019 Rasterizer</a></li>\n\n\n\n<li><a href=\"https://github.com/shader-slang/slang-python/tree/main/examples/hard-rasterizer-example/#readme\">1-Triangle Differentiable Rasterizer using Monte Carlo Edge Sampling</a></li>\n\n\n\n<li><a href=\"https://github.com/shader-slang/slang-python/tree/main/examples/inline-mlp-example\">Image Fitting using Tiny Inline MLPs (using CUDA\u2019s WMMA API)</a></li>\n</ul>\n\n\n\n<p>For more examples, see <a href=\"https://developer.nvidia.com/blog/differentiable-slang-example-applications/\">Differentiable Slang: Example Applications</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>Differentiable rendering is a powerful tool for computer graphics, computer vision, and image synthesis. While researchers have advanced its capabilities, built systems, and explored applications for years, the resulting systems were difficult to combine with existing large codebases. Now, with Slang, <em>existing </em>real-time renderers can be made differentiable.&nbsp;</p>\n\n\n\n<p>Slang greatly simplifies adding shader code to ML pipelines, and the reverse, adding learned components to rendering pipelines.</p>\n\n\n\n<p>Real-time rendering experts can now explore building ML rendering components without rewriting the rendering code in ML frameworks. Slang facilitates data-driven asset optimization and improvement and aids the research of novel neural components in traditional rendering.&nbsp;</p>\n\n\n\n<p>On the other end of the spectrum, ML researchers can now leverage existing renderers and assets with complex shaders and incorporate expressive state-of-the-art shading models in new architectures.</p>\n\n\n\n<p>We are looking forward to seeing how bridging real-time graphics and machine learning contributes to new photorealistic neural and data-driven techniques. For more information about Slang\u2019s automatic differentiation feature, see the <a href=\"https://research.nvidia.com/labs/rtr/publication/bangaru2023slangd/\">SLANG.D: Fast, Modular and Differentiable Shader Programming</a> paper.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA just released a SIGGRAPH Asia 2023 research paper, SLANG.D: Fast, Modular and Differentiable Shader Programming. The paper shows how a single language can serve as a unified platform for real-time, inverse, and differentiable rendering. The work is a collaboration between MIT, UCSD, UW, and NVIDIA researchers. This is part of a series on Differentiable &hellip; <a href=\"https://developer.nvidia.com/blog/differentiable-slang-a-shading-language-for-renderers-that-learn/\">Continued</a></p>\n", "protected": false}, "author": 1898, "featured_media": 72159, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1282080", "discourse_permalink": "https://forums.developer.nvidia.com/t/differentiable-slang-a-shading-language-for-renderers-that-learn/270320", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [1932, 453, 1962, 527, 604], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/primal-and-derivative-zero-day.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iJt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72011"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1898"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=72011"}], "version-history": [{"count": 26, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72011/revisions"}], "predecessor-version": [{"id": 72285, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/72011/revisions/72285"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72159"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=72011"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=72011"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=72011"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71836, "date": "2023-10-19T15:17:37", "date_gmt": "2023-10-19T22:17:37", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71836"}, "modified": "2023-11-11T21:30:27", "modified_gmt": "2023-11-12T05:30:27", "slug": "bringing-generative-ai-to-life-with-jetson", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/", "title": {"rendered": "Bringing Generative AI to Life with NVIDIA Jetson"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Recently, NVIDIA unveiled <a href=\"https://www.jetson-ai-lab.com/\">Jetson Generative AI Lab</a>, which empowers developers to explore the limitless possibilities of generative AI in a real-world setting with NVIDIA Jetson edge devices. Unlike other embedded platforms, Jetson is capable of running large language models (LLMs), vision transformers, and stable diffusion locally. That includes the largest Llama-2-70B model on Jetson AGX Orin at interactive rates.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2.jpg\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"824\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-625x824.jpg\" alt=\"Four vertical bar graphs for large language models, vision language models, vision transformers, and stable diffusion.\" class=\"wp-image-71922\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-625x824.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-228x300.jpg 228w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-87x115.jpg 87w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-768x1012.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-1165x1536.jpg 1165w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-1554x2048.jpg 1554w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-645x850.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-68x90.jpg 68w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-362x477.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-83x110.jpg 83w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2-1024x1350.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/gen_ai_benchmarks_light_2x2.jpg 1625w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Inferencing performance of leading Generative AI models on Jetson AGX Orin</em></figcaption></figure></div>\n\n\n<p>To swiftly test the latest models and applications on Jetson, use the tutorials and resources provided on the Jetson Generative AI lab. Now you can focus on uncovering the untapped potential of generative AIs in the physical world.</p>\n\n\n\n<p>In this post, we explore the exciting generative AI applications that you can run and experience on Jetson devices, all of which are comprehensively covered in the lab tutorials.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Generative AI at the edge</h2>\n\n\n\n<p>In the rapidly evolving landscape of AI, the spotlight shines brightly on generative models and the following in particular:</p>\n\n\n\n<ul>\n<li><strong>LLMs</strong> that are capable of engaging in human-like conversations.</li>\n\n\n\n<li><strong>Vision language models</strong> (VLMs) that provide LLMs with the ability to perceive and understand the real world through a camera.</li>\n\n\n\n<li><strong>Diffusion models</strong> that can transform simple text prompts into stunning visual creations.&nbsp;</li>\n</ul>\n\n\n\n<p>These remarkable AI advancements have captured the imagination of many. However, if you delve into the infrastructure supporting this cutting-edge model inference, you would often find them tethered to the cloud, reliant on data centers for their processing power. This cloud-centric approach leaves certain edge applications, requiring high-bandwidth low-latency data processing, largely unexplored.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/BAMOw7qlVXw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. NVIDIA Jetson Orin Brings Powerful Generative AI Models to the Edge</em></figcaption></figure>\n\n\n\n<p>The emerging trend of running LLMs and other generative models in local environments is gaining momentum within developer communities. Thriving online communities, like <a href=\"https://www.reddit.com/r/LocalLLaMA/\">r/LocalLlama</a> on Reddit, provide a platform for enthusiasts to discuss the latest developments in generative AI technologies and their real-world applications. Numerous technical articles published on platforms like Medium delve into the intricacies of running open-source LLMs in local setups, with some taking advantage of NVIDIA Jetson.</p>\n\n\n\n<p>The Jetson Generative AI Lab serves as a hub for discovering the latest generative AI models and applications and learning how to run them on Jetson devices. As the field evolves at a rapid pace, with new LLMs emerging almost daily and advancements in quantization libraries reshaping benchmarks overnight, NVIDIA recognizes the importance of offering the most up-to-date information and effective tools. We offer easy-to-follow tutorials and prebuilt containers.&nbsp;</p>\n\n\n\n<p>The enabling force is <a href=\"https://github.com/dusty-nv/jetson-containers\">jetson-containers</a>, an open-source project thoughtfully designed and meticulously maintained to build containers for Jetson devices. Using GitHub Actions, it is building 100 containers in CI/CD fashion. These empower you to quickly test the latest AI models, libraries, and applications on Jetson without the hassle of configuring underlying tools and libraries.&nbsp;</p>\n\n\n\n<p>The Jetson Generative AI lab and jetson-containers enable you to focus on exploring the limitless possibilities of generative AI in real-world settings with Jetson.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Walkthrough</h2>\n\n\n\n<p>Here are some of the exciting generative AI applications that run on the NVIDIA Jetson device available in the Jetson Generative AI lab.</p>\n\n\n\n<h3 class=\"wp-block-heading\">stable-diffusion-webui</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1280\" height=\"720\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion.gif\" alt=\"GIF of Stable Diffusion interface working in a web browser to generate images from user prompts on Jetson\" class=\"wp-image-71862\" style=\"width:960px;height:540px\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Stable Diffusion interface</em></figcaption></figure></div>\n\n\n<p><a href=\"https://github.com/AUTOMATIC1111\">A1111</a>\u2019s stable-diffusion-webui provides a user-friendly interface to Stable Diffusion released by Stability AI. It enables you to perform many tasks, including the following:</p>\n\n\n\n<ul>\n<li><strong>Txt2img</strong>: Generates an image based on a text prompt.&nbsp;</li>\n\n\n\n<li><strong>img2img</strong><em>:</em> Generates an image from an input image and a corresponding text prompt.</li>\n\n\n\n<li><strong>inpainting</strong>: Fills in the missing or masked parts of the input image.&nbsp;</li>\n\n\n\n<li>outpainting: Expands the input image beyond its original borders.&nbsp;</li>\n</ul>\n\n\n\n<p>The web app downloads the Stable Diffusion v1.5 model automatically during the first start, so you can start generating your image right away. If you have a Jetson Orin device, it is as easy as executing the following commands, as explained in the tutorial.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\ngit clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\n./run.sh $(./autotag stable-diffusion-webui)\n</pre></div>\n\n\n<p>For more information about running stable-diffusion-webui, see the <a href=\"https://www.jetson-ai-lab.com/tutorial_stable-diffusion.html\">Jetson Generative AI lab tutorial</a>. Jetson AGX Orin is also capable of running the newer Stable Diffusion XL (SDXL) models, which generated the featured image at the top of this post.</p>\n\n\n\n<h3 class=\"wp-block-heading\">text-generation-webui</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/interactive-llama-chat.gif\" alt=\"GIF of text-generation-webui working in a web browser, showing a conversation with robot assistant about the comparison between UC Berkeley and Stanford University.\" class=\"wp-image-71858\"/><figcaption class=\"wp-element-caption\"><em>Figure 3. Interactive chat with Llama-2-13B on Jetson AGX Orin</em></figcaption></figure></div>\n\n\n<p><a href=\"https://github.com/oobabooga\">Oobabooga</a>\u2019s <a href=\"https://github.com/oobabooga/text-generation-webui\">text-generation-webui</a> is another popular Gradio-based web interface for running LLMs in a local environment. The official repository provides one-click installers for platforms, but jetson-containers offer an even easier method.&nbsp;</p>\n\n\n\n<p>Using the interface, you can easily download a model from the <a href=\"https://huggingface.co/models\">Hugging Face model repository</a>. With 4-bit quantization, the rule of thumb is that Jetson Orin Nano can generally accommodate a 7B parameter model, Jetson Orin NX 16GB can run a 13B parameter model, and Jetson AGX Orin 64GB can run whopping 70B parameter models.</p>\n\n\n\n<p>Many people are now working on <a href=\"https://ai.meta.com/llama/\">Llama-2</a>, Meta\u2019s open-source large language model, available for free for research and commercial use. There are Llama-2\u2013based models also trained using techniques like supervised fine-turning (SFT) and reinforcement learning from human feedback (RLHF). Some even claim that it is surpassing GPT-4 on some benchmarks.</p>\n\n\n\n<p>Text-generation-webui provides extensions and enables you to develop your own extensions. This can be used to integrate your application as you later see in the llamaspeak example. It also has support for multimodal VLMs like Llava and chatting about images.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"960\" height=\"540\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/quantized-llama-model-image-recognition.gif\" alt=\"GIF shows quantized Llama model responding to a query about the NASA logo.\" class=\"wp-image-71854\"/><figcaption class=\"wp-element-caption\"><em>Figure 4. Quantized Llava-13B VLM responding to image queries</em></figcaption></figure></div>\n\n\n<p>For more information about running text-generation-webui, see the <a href=\"https://www.jetson-ai-lab.com/tutorial_text-generation.html\">Jetson Generative AI lab tutorial</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">llamaspeak</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"325\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llamaspeak-conversation.gif\" alt=\"Screenshot of llamaspeak interface running in a web browser, showing voice conversation between the author and Llama, the AI assistant.\" class=\"wp-image-71853\"/><figcaption class=\"wp-element-caption\"><em>Figure 5. llamaspeak voice conversation with an LLM using Riva ASR/TTS</em></figcaption></figure></div>\n\n\n<p><a href=\"https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak\">Llamaspeak</a> is an interactive chat application that employs live NVIDIA Riva ASR/TTS to enable you to carry out verbal conversations with a LLM running locally. It is currently offered as a part of jetson-containers.</p>\n\n\n\n<p>To carry out a smooth and seamless voice conversation, minimizing the time to the first output token of an LLM is critical. On top of that, llamaspeak is designed to handle conversational interruption so that you can start talking while llamaspeak is still TTS-ing the generated response. Container microservices are used for Riva, the LLM, and the chat server.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"590\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow.png\" alt=\"Block diagram shows the conversation flow with live speech recognition, large language model, and speech synthesis\" class=\"wp-image-71849\" style=\"width:1000px;height:295px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-300x89.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-625x184.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-179x53.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-768x227.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-1536x453.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-645x190.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-500x148.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-160x47.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-362x107.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-373x110.png 373w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/live-conversation-control-flow-1024x302.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 6. Live conversation control flow with streaming ASR/LLM/TTS pipeline to web clients</em></figcaption></figure></div>\n\n\n<p>llamaspeak has a responsive interface with low-latency audio streaming from browser microphones or a microphone connected to your Jetson device. For more information about running it yourself, see the <a href=\"https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak\">jetson-containers documentation</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">NanoOWL</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"566\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/owl_small.gif\" alt=\"A video of two people high-fiving while body parts are interactively detected and highlighted.\" class=\"wp-image-71897\"/><figcaption class=\"wp-element-caption\"><em>Figure 7. NanoOWL can perform object detection in real time</em></figcaption></figure></div>\n\n\n<p>Open World Localization with Vision Transformers (OWL-ViT) is an approach for open-vocabulary detection, developed by Google Research. This model enables you to detect objects by providing text prompts for those objects.&nbsp;</p>\n\n\n\n<p>For example, to detect people and cars, prompt the system with text describing the classes:&nbsp;</p>\n\n\n\n<pre class=\"wp-block-preformatted\">prompt = \u201ca person, a car\u201d</pre>\n\n\n\n<p>This is incredibly valuable for rapidly developing new applications, without needing to train a new model. To unlock applications at the edge, our team developed a project, <a href=\"https://github.com/NVIDIA-AI-IOT/nanoowl\">NanoOWL</a>, which optimizes this model with NVIDIA TensorRT to obtain real-time performance on NVIDIA Jetson Orin Platforms (~95FPS encoding speed on Jetson AGX Orin). This performance means that you can run OWL-ViT well above the common camera frame rates. </p>\n\n\n\n<p>The project also contains a new tree detection pipeline that enables you to combine the accelerated OWL-ViT model with CLIP to enable zero-shot detection and classification at any level. For example, to detect faces and classify them as happy or sad, use the following prompt:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">prompt = \u201c[a face (happy, sad)]\u201d</pre>\n\n\n\n<p>To detect faces and then detect facial features in each region of interest, use the following prompt:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">prompt = \u201c[a face [an eye, a nose, a mouth]]\u201d</pre>\n\n\n\n<p>Combine them:</p>\n\n\n\n<pre class=\"wp-block-preformatted\">prompt = \u201c[a face (happy, sad)[an eye, a nose, a mouth]]\u201d</pre>\n\n\n\n<p>The list goes on. While the accuracy of this model may be better for some objects or classes than others, the ease of development means you can quickly try different prompts and find out if it works for you. We look forward to seeing what amazing applications that you develop!&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Segment Anything Model</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook.png\"><img decoding=\"async\" loading=\"lazy\" width=\"1187\" height=\"620\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook.png\" alt=\"Screenshot of a Jupyter notebook running a SAM example.\" class=\"wp-image-71848\" style=\"width:890px;height:465px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook.png 1187w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-625x326.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-179x93.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-768x401.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-645x337.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-500x261.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-362x189.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-211x110.png 211w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/sam-example-jupyter-notebook-1024x535.png 1024w\" sizes=\"(max-width: 1187px) 100vw, 1187px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 8. Jupyter notebook of Segment Anything model</em> <em>(SAM)</em></figcaption></figure></div>\n\n\n<p>Meta released the <a href=\"https://segment-anything.com/\">Segment Anything model</a> (SAM), an advanced image segmentation model designed to precisely identify and segment objects within images regardless of their complexity or context.</p>\n\n\n\n<p>Their official repository also has Jupyter notebooks to easily check the impact of the model, and jetson-containers offer a convenient container that has Jupyter Lab built in.</p>\n\n\n\n<h3 class=\"wp-block-heading\">NanoSAM&nbsp;</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"452\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/objects-on-desk.gif\" alt=\"Handheld camera video footage showing objects on a desk with a computer mouse highlighted.\" class=\"wp-image-71847\"/><figcaption class=\"wp-element-caption\"><em>Figure 9. NanoSAM working in real time to track and segment a computer mouse</em></figcaption></figure></div>\n\n\n<p>Segment Anything (SAM) is an incredible model that is capable of turning points into segmentation masks. Unfortunately, it does not run in real time, which limits its usefulness in edge applications.&nbsp;</p>\n\n\n\n<p>To get past this limitation, we\u2019ve recently released a new project, <a href=\"https://github.com/NVIDIA-AI-IOT/nanosam\">NanoSAM</a>, which distills the SAM image encoder into a lightweight model. It also optimizes the model with NVIDIA TensorRT to enable real-time performance on NVIDIA Jetson Orin platforms. Now, you can easily turn your existing bounding box or keypoint detector into an instance segmentation model, without any training required.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Track Anything Model</h3>\n\n\n\n<p>The Track Anything Model (TAM) is, as the <a href=\"https://arxiv.org/abs/2304.11968\">team\u2019s paper</a> explains, \u201cSegment Anything meets videos.\u201d Their open-sourced, Gradio-based interface enables you to click on a frame of an input video to specify anything to track and segment. It even showcases an additional capability of removing the tracked object by inpainting.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/track-anything-model.gif\" alt=\"GIF of a web browser running the TAM interface to process a cat video.\" class=\"wp-image-71843\"/><figcaption class=\"wp-element-caption\"><em>Figure 10. Track Anything interface</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">NanoDB</h3>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/ayqKpQNd1Jw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Hello AI World &#8211; Realtime Multi-Modal VectorDB on NVIDIA Jetson</em></figcaption></figure>\n\n\n\n<p>In addition to effectively indexing and searching your data at the edge, these vector databases are often used in tandem with LLMs for retrieval-augmented generation (RAG) for long-term memory beyond their built-in context length (4096 tokens for Llama-2 models). Vision-language models also use the same embeddings as inputs.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture.jpg\"><img decoding=\"async\" loading=\"lazy\" width=\"1475\" height=\"1999\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture.jpg\" alt=\"Multimodal agent architecture using retrieval-augmented generation (RAG) and plugins to interact with live and archived data at the edge for cyber-physical integration.\" class=\"wp-image-71842\" style=\"width:369px;height:500px\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture.jpg 1475w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-221x300.jpg 221w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-625x847.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-85x115.jpg 85w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-768x1041.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-1133x1536.jpg 1133w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-645x874.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-66x90.jpg 66w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-362x491.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-81x110.jpg 81w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/llm-vlm-architecture-1024x1388.jpg 1024w\" sizes=\"(max-width: 1475px) 100vw, 1475px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 11. Architecture diagram with an LLM/VLM at the core</em></figcaption></figure></div>\n\n\n<p>With all that incoming live data from the edge and the ability to understand it, they become agents capable of interacting with the real world. For more information about experimenting with using NanoDB on your own imagery and dataset, see the <a href=\"https://www.jetson-ai-lab.com//tutorial_nanodb.html\">lab tutorial</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>There you have it! Numerous exciting generative AI applications are emerging, and you can easily run them on Jetson Orin following these tutorials. To witness the incredible capabilities of generative AIs running locally, explore the <a href=\"https://www.jetson-ai-lab.com/\">Jetson Generative AI lab</a>.&nbsp;</p>\n\n\n\n<p>If you build your own generative AI application on Jetson and are interested in sharing your ideas, be sure to showcase your creation on the <a href=\"https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/jetson-projects/78\">Jetson Projects forum</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"625\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-625x625.jpg\" alt=\"Bringing Generative AI to Life with NVIDIA Jetson webinar\" class=\"wp-image-73335\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-625x625.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-300x300.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-115x115.jpg 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-768x768.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-645x645.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-90x90.jpg 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-32x32.jpg 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-50x50.jpg 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-64x64.jpg 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-96x96.jpg 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-128x128.jpg 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-150x150.jpg 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-362x362.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar-110x110.jpg 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/V2-Webinar.jpg 800w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure></div>\n\n\n<p>Join us Tuesday, November 7, 2023 at 9 a.m. PT for a <a href=\"https://info.nvidia.com/jetson-gen-ai-webinar.html\">webinar</a> diving even deeper into many of the topics discussed in this post, along with a live Q&amp;A!</p>\n\n\n\n<ul>\n<li>Accelerated APIs and quantization methods for deploying LLMs and VLMs on NVIDIA Jetson</li>\n\n\n\n<li>Optimizing vision transformers with NVIDIA TensorRT</li>\n\n\n\n<li>Multimodal agents and vector databases</li>\n\n\n\n<li>Live conversations with NVIDIA Riva ASR/TTS</li>\n</ul>\n\n\n\n<p><a href=\"https://info.nvidia.com/jetson-gen-ai-webinar.html\">Register now!</a></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Recently, NVIDIA unveiled Jetson Generative AI Lab, which empowers developers to explore the limitless possibilities of generative AI in a real-world setting with NVIDIA Jetson edge devices. Unlike other embedded platforms, Jetson is capable of running large language models (LLMs), vision transformers, and stable diffusion locally. That includes the largest Llama-2-70B model on Jetson AGX &hellip; <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/\">Continued</a></p>\n", "protected": false}, "author": 1456, "featured_media": 73334, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1280759", "discourse_permalink": "https://forums.developer.nvidia.com/t/bringing-generative-ai-to-life-with-nvidia-jetson/270043", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 3110, 63], "tags": [453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/V2-R2.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iGE", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71836"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1456"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71836"}], "version-history": [{"count": 22, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71836/revisions"}], "predecessor-version": [{"id": 73337, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71836/revisions/73337"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/73334"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71836"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71836"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71836"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71491, "date": "2023-10-19T13:26:15", "date_gmt": "2023-10-19T20:26:15", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71491"}, "modified": "2023-11-02T11:48:43", "modified_gmt": "2023-11-02T18:48:43", "slug": "ai-red-team-machine-learning-security-training", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-red-team-machine-learning-security-training/", "title": {"rendered": "AI Red Team: Machine Learning Security Training"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>At Black Hat USA 2023, NVIDIA hosted a two-day training session that provided security professionals with a realistic environment and methodology to explore the unique risks presented by machine learning (ML) in today&#8217;s environments.&nbsp;</p>\n\n\n\n<p>In this post, the NVIDIA AI Red Team shares what was covered during the training and other opportunities to continue learning about ML security.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Black Hat USA training</h2>\n\n\n\n<p>This has been a banner year for AI. Many security teams are being asked to evaluate and secure AI-enabled products without the skills and knowledge to appropriately evaluate their potential vulnerabilities.&nbsp;</p>\n\n\n\n<p>By providing this training at one of the world\u2019s leading security conferences, we were able to share the NVIDIA AI Red Team\u2019s experience and knowledge across many different industry verticals. We helped ensure that these organizations can begin securely using and developing AI solutions. We also come from that community, so it was a comfortable space to teach in.&nbsp;&nbsp;</p>\n\n\n\n<p>The two-day training consisted of over 20 Jupyter notebooks and 200 slides organized into the following modules:&nbsp;</p>\n\n\n\n<ul>\n<li>Introduction</li>\n\n\n\n<li>Evasion</li>\n\n\n\n<li>Extraction</li>\n\n\n\n<li>Assessments</li>\n\n\n\n<li>Inversion&nbsp;</li>\n\n\n\n<li>Membership Inference</li>\n\n\n\n<li>Poisoning</li>\n\n\n\n<li>And everything applied to large language models</li>\n</ul>\n\n\n\n<p>It was a lot. Attendees took slides and notebooks home to continue iterating on the coursework at their own pace.</p>\n\n\n\n<p>The course attempted to take students from all backgrounds and give them a solid foundation in the intersection of machine learning and security. It took students all the way from the basics of NumPy mechanics to algorithmic attacks against large language models. Each module gave some theory and then explored applied scenarios.&nbsp;</p>\n\n\n\n<p>Students were given a basic methodology based on our own framework. (<a href=\"https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/\">NVIDIA AI Red Team\u2019s assessment framework</a>). They were given an environment and code that they could take back to their organizations and iterate on. There\u2019s a lot of cool work still to be done.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Attendee questions and concerns</h2>\n\n\n\n<p>Outside of reshaping questions, there were a lot of questions about the effect and likelihood of attacks against machine learning systems. Machine learning has been in defensive products for a number of years at this point. \u201cML bypasses\u201d happen every day.&nbsp;</p>\n\n\n\n<p>Our goal was to help attendees understand the threat models, techniques, and attack vectors so that they could design and calibrate their security controls appropriately. Security isn\u2019t one-size-fits-all, but having a working knowledge of the systems inside an organization is a prerequisite for building meaningful defenses.&nbsp;</p>\n\n\n\n<p>Machine learning security has an established history in academia. This course gave students experience applying those techniques to familiar security scenarios.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Key lessons from the training</h2>\n\n\n\n<p>People are super smart and creative. It\u2019s easy to look at security and point out flaws (and we do, many of them in ML), but security has matured significantly over the last decade. We think the security industry will rise to the challenges presented by ML as well.</p>\n\n\n\n<p>The training was also a good exercise in industry baselining. We had a mix of professionals from all industries. Understanding where those industries are in adopting machine learning and securing systems was really interesting. It would be fair to say the majority of people are just beginning their journey.&nbsp;</p>\n\n\n\n<p>In general, we came away happy that people who are interested in this field have a base camp from which to operate. It\u2019s a fun space to be in at the moment and we enjoyed sharing our material with peers.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Opportunities to learn more about ML security</h2>\n\n\n\n<p>The next iteration of the NVIDIA Machine Learning Security course will be at <a href=\"https://www.blackhat.com/eu-23/training/schedule/index.html#black-hat-machine-learning-33946\">Black Hat EU</a> on December 4 and 5.&nbsp;</p>\n\n\n\n<p>We are also researching alternative delivery modalities and mechanisms. If you have a request, contact the <a href=\"mailto:threatops@nvidia.com\">AI Red Team</a>. Students who took the course will receive updates as we make them available!</p>\n", "protected": false}, "excerpt": {"rendered": "<p>At Black Hat USA 2023, NVIDIA hosted a two-day training session that provided security professionals with a realistic environment and methodology to explore the unique risks presented by machine learning (ML) in today&#8217;s environments.&nbsp; In this post, the NVIDIA AI Red Team shares what was covered during the training and other opportunities to continue learning &hellip; <a href=\"https://developer.nvidia.com/blog/ai-red-team-machine-learning-security-training/\">Continued</a></p>\n", "protected": false}, "author": 1594, "featured_media": 71495, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1280722", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-red-team-machine-learning-security-training/270027", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696], "tags": [3520, 1511, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-ai-red-team-ml-security-training-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iB5", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71491"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1594"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71491"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71491/revisions"}], "predecessor-version": [{"id": 71498, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71491/revisions/71498"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71495"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71491"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71491"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71491"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71648, "date": "2023-10-19T09:00:00", "date_gmt": "2023-10-19T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71648"}, "modified": "2023-11-16T08:46:41", "modified_gmt": "2023-11-16T16:46:41", "slug": "optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/", "title": {"rendered": "Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Today, NVIDIA announces the public release of TensorRT-LLM to accelerate and optimize inference performance for the latest LLMs on NVIDIA GPUs. This open-source library is now available for free on the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0\">/NVIDIA/TensorRT-LLM</a> GitHub repo and as part of the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo framework</a>.</p>\n\n\n\n<p>Large language models (LLMs) have revolutionized the field of artificial intelligence and created entirely new ways of interacting with the digital world. But, as organizations and application developers around the world look to incorporate LLMs into their work, some of the challenges with running these models become apparent.</p>\n\n\n\n<p>Put simply, LLMs are large. That fact can make them expensive and slow to run without the right techniques.</p>\n\n\n\n<p>Many optimization techniques have risen to deal with this, from model optimizations like <a href=\"https://arxiv.org/abs/2307.08691\">kernel fusion</a> and <a href=\"https://nvidia.github.io/TensorRT-LLM/precision.html\">quantization</a> to runtime optimizations like C++ implementations, KV caching, <a href=\"https://www.usenix.org/conference/osdi22/presentation/yu\">continuous in-flight batching</a>, and <a href=\"https://arxiv.org/pdf/2309.06180.pdf\">paged attention</a>. It can be difficult to decide which of these are right for your use case, and to navigate the interactions between these techniques and their sometimes-incompatible implementations.</p>\n\n\n\n<p>That\u2019s why <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\">NVIDIA introduced TensorRT-LLM</a>, a comprehensive library for compiling and optimizing LLMs for inference. TensorRT-LLM incorporates all of those optimizations and more while providing an intuitive Python API for defining and building new models.</p>\n\n\n\n<p>The TensorRT-LLM open-source library accelerates inference performance on the latest LLMs on NVIDIA GPUs. It is used as the optimization backbone for LLM inference in NVIDIA NeMo, an end-to-end framework to build, customize, and deploy generative AI applications into production. NeMo provides complete containers, including TensorRT-LLM and NVIDIA Triton, for generative AI deployments.</p>\n\n\n\n<p><a href=\"https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/\">TensorRT-LLM is also now available for native Windows</a> as a beta release. Application developers and AI enthusiasts can now benefit from accelerated LLMs running locally on PCs and Workstations powered by NVIDIA RTX and NVIDIA GeForce RTX GPUs. </p>\n\n\n\n<p>TensorRT-LLM wraps TensorRT\u2019s deep learning compiler and includes the latest optimized kernels made for cutting-edge implementations of <a href=\"https://arxiv.org/abs/2307.08691\">FlashAttention</a> and masked multi-head attention (MHA) for LLM execution. </p>\n\n\n\n<p>TensorRT-LLM also consists of pre\u2013 and post-processing steps and multi-GPU/multi-node communication primitives in a simple, open-source Python API for groundbreaking LLM inference performance on GPUs.</p>\n\n\n\n<p>Highlights of TensorRT-LLM include the following:</p>\n\n\n\n<ul>\n<li>Support for LLMs such as Llama 1 and 2, ChatGLM, Falcon, MPT, Baichuan, and Starcoder<strong><em></em></strong></li>\n\n\n\n<li>In-flight batching and paged attention<strong><em></em></strong></li>\n\n\n\n<li>Multi-GPU multi-node (MGMN) inference</li>\n\n\n\n<li>NVIDIA Hopper tansformer engine with FP8</li>\n\n\n\n<li>Support for NVIDIA Ampere architecture, NVIDIA Ada Lovelace architecture, and NVIDIA Hopper GPUs</li>\n\n\n\n<li>Native Windows support (beta)<strong><em></em></strong></li>\n</ul>\n\n\n\n<p>Over the past 2 years, NVIDIA has been working closely with leading LLM companies, including Anyscale, Baichuan, Cohere, Deci, Grammarly, Meta, Mistral AI, MosaicML, now part of Databricks, OctoML, Perplexity AI, Tabnine, Together.ai, Zhipu, and many others to accelerate and optimize LLM inference.</p>\n\n\n\n<p>To help you get a feel for the library and how to use it, here\u2019s an example of how to use and deploy<a href=\"https://ai.meta.com/llama/\"> Llama 2</a>, a popular publicly available LLM, with TensorRT-LLM and NVIDIA Triton on Linux. To get started with the beta release, see the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/windows\">TensorRT-LLM for native Windows</a> GitHub repo.</p>\n\n\n\n<p>For more information, including different models, different optimizations, and multi-GPU execution, see the full list of <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples\">TensorRT-LLM examples</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Getting started with installation</h2>\n\n\n\n<p>Start by cloning and building the TensorRT-LLM library. The easiest way to build TensorRT-LLM and retrieve all its dependencies is to use the included Dockerfile:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ngit lfs install\ngit clone -b release/0.5.0 https://github.com/NVIDIA/TensorRT-LLM.git\ncd TensorRT-LLM\ngit submodule update --init --recursive\nmake -C docker release_build\n</pre></div>\n\n\n<p>These commands pull a base container and install all the dependencies needed for TensorRT-LLM inside the container. It then builds and installs TensorRT-LLM itself in the container.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Retrieving the model weights</h2>\n\n\n\n<p>TensorRT-LLM is a library for LLM inference, and so to use it, you need to supply a set of trained weights. You can either use your own model weights trained in a framework like <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a>, or pull a set of pretrained weights from repositories like the HuggingFace Hub.</p>\n\n\n\n<p>The commands in this post automatically pull the weights and tokenizer files for the chat-tuned variant of the 7B parameter Llama 2 model from the <a href=\"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\">HuggingFace Hub</a>. You can also download the weights yourself to use offline with the following command. You just have to update the paths in later commands to point to this directory:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ngit lfs install\ngit clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n</pre></div>\n\n\n<p>Usage of this model is subject to a <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\">particular license</a>. To download the necessary files, agree to the terms and <a href=\"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf?clone=true\">authenticate with Hugging Face</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Compiling the model</h2>\n\n\n\n<p>The next step in the process is to compile the model into a TensorRT engine. For this, you need the model weights as well as a model definition written in the TensorRT-LLM Python API.</p>\n\n\n\n<p>The TensorRT-LLM repository contains a <a href=\"https://nvidia.github.io/TensorRT-LLM/python-api/tensorrt_llm.models.html\">wide variety</a> of predefined model architectures. For this post, you use the included <a href=\"https://nvidia.github.io/TensorRT-LLM/_modules/tensorrt_llm/models/llama/model.html#LLaMAModel\">Llama model definition</a> instead of writing your own. This is a minimal example of some of the optimizations available in TensorRT-LLM.</p>\n\n\n\n<p>For more information about available plug-ins and quantizations, see the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/llama\">full Llama example</a> and <a href=\"https://nvidia.github.io/TensorRT-LLM/precision.html\">Numerical Precision</a>.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Launch the Tensorrt-LLM container\nmake -C docker release_run LOCAL_USER=1\n\n# Log in to huggingface-cli\n# You can get your token from huggingface.co/settings/token\nhuggingface-cli login --token *****\n\n# Compile model\npython3 examples/llama/build.py \\\n    --model_dir meta-llama/Llama-2-7b-chat-hf \\\n    --dtype float16 \\\n    --use_gpt_attention_plugin float16 \\\n    --use_gemm_plugin float16 \\\n    --remove_input_padding \\\n    --use_inflight_batching \\\n    --paged_kv_cache \\\n    --output_dir examples/llama/out\n</pre></div>\n\n\n<p>When you create the model definition with the TensorRT-LLM API, you build a graph of operations from <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> primitives that form the layers of your neural network. These operations map to specific kernels: prewritten programs for the GPU.</p>\n\n\n\n<p>The TensorRT compiler can sweep through the graph to choose the best kernel for each operation and available GPU. Crucially, it can also identify patterns in the graph where multiple operations are good candidates for being <em>fused</em> into a single kernel. This reduces the required amount of memory movement and the overhead of launching multiple GPU kernels.</p>\n\n\n\n<p>TensorRT also compiles the graph of operations into a single <a href=\"https://developer.nvidia.com/blog/cuda-graphs/\">CUDA Graph</a> that can be launched all at one time, further reducing the kernel launch overhead.</p>\n\n\n\n<p>The TensorRT compiler is extremely powerful for fusing layers and increasing execution speed, but there are some complex layer fusions\u2014like <a href=\"https://arxiv.org/abs/2307.08691\">FlashAttention</a>\u2014that involve interleaving many operations together and which can\u2019t be automatically discovered. For those, you can explicitly replace parts of the graph with <a href=\"https://nvidia.github.io/TensorRT-LLM/architecture.html#plugins\">plugins</a> at compile time.</p>\n\n\n\n<p>In this example, you include the <code>gpt_attention</code> plug-in, which implements a <code>FlashAttention</code>-like fused attention kernel, and the <code>gemm</code> plug-in, which performs matrix multiplication with FP32 accumulation. You also call out your desired precision for the full model as FP16, matching the default precision of the weights that you downloaded from HuggingFace.</p>\n\n\n\n<p>Here&#8217;s what this script produces when you finish running it. In the <code>/examples/llama/out</code> folder, there are now the following files:</p>\n\n\n\n<ul>\n<li><code>Llama_float16_tp1_rank0.engine</code>: The main output of the build script, containing the executable graph of operations with the model weights embedded.</li>\n\n\n\n<li><code>config.json</code>: Includes detailed information about the model, like its general structure and precision, as well as information about which plug-ins were incorporated into the engine.</li>\n\n\n\n<li><code>model.cache</code>: Caches some of the timing and optimization information from model compilation, making successive builds quicker.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\"><a></a>Running the model</h2>\n\n\n\n<p>So, now that you\u2019ve got your model engine, what can you do with it?</p>\n\n\n\n<p>The engine file contains the information that you need for executing the model, but LLM usage in practice requires much more than a single forward pass through the model. TensorRT-LLM includes a highly optimized C++ runtime for executing built LLM engines and managing processes like sampling tokens from the model output, managing the KV cache, and batching requests together.</p>\n\n\n\n<p>You can use that runtime directly to execute the model locally, or you can use the TensorRT-LLM runtime backend for NVIDIA Triton Inference Server to serve the model for multiple users.</p>\n\n\n\n<p>To run the model locally, execute the following command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\npython3 examples/llama/run.py --engine_dir=examples/llama/out --max_output_len 100 --tokenizer_dir meta-llama/Llama-2-7b-chat-hf --input_text &quot;How do I count to nine in French?&quot;\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Deploying with Triton Inference Server</h2>\n\n\n\n<p>Beyond local execution, you can also use the <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a> to create a production-ready deployment of your LLM. </p>\n\n\n\n<p>NVIDIA is releasing a new <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend\">Triton Inference Server backend for TensorRT-LLM</a> that leverages the TensorRT-LLM C++ runtime for rapid inference execution and includes techniques like <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md#inflight-batching\">in-flight batching</a> and <a href=\"https://nvidia.github.io/TensorRT-LLM/gpt_attention.html#paged-kv-cache\">paged KV-caching</a>. Triton Inference Server with the TensorRT-LLM backend is <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags\">available as a pre-built container through NGC</a>.</p>\n\n\n\n<p>First, create a model repository so that Triton Inference Server can read the model and any associated metadata. The tensorrtllm_backend repository includes the <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend/tree/release/0.5.0/all_models/inflight_batcher_llm\">skeleton of an appropriate model repository</a> under <code>all_models/inflight_batcher_llm/</code> that you can use. In that directory are now four subfolders that hold artifacts for different parts of the model execution process:</p>\n\n\n\n<ul>\n<li>/<code>preprocessing</code> and <code>/postprocessing</code>: Contain scripts for the <a href=\"https://github.com/triton-inference-server/python_backend\">Triton Inference Server Python backend</a> for tokenizing the text inputs and detokenizing the model outputs to convert between strings and the token IDs on which the model operates.</li>\n\n\n\n<li><code>/tensorrt_llm</code>: Where you place the model engine that you previously compiled.</li>\n\n\n\n<li><code>/ensemble</code>: Defines a <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_5-Model_Ensembles\">model ensemble</a> that links the previous three components together and tells Triton Inference Server how to flow data through them.</li>\n</ul>\n\n\n\n<p>Pull down the example model repository and copy the model you compiled in the previous step over to it:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# After exiting the TensorRT-LLM docker container\ncd ..\ngit clone -b release/0.5.0 \\ \nhttps://github.com/triton-inference-server/tensorrtllm_backend.git\ncd tensorrtllm_backend\ncp ../TensorRT-LLM/examples/llama/out/*   all_models/inflight_batcher_llm/tensorrt_llm/1/\n</pre></div>\n\n\n<p>Next, modify some of the configuration files from the repository skeleton with information like the following:</p>\n\n\n\n<ul>\n<li>Where the compiled model engine is</li>\n\n\n\n<li>What tokenizer to use</li>\n\n\n\n<li>How to handle memory allocation for the KV cache when performing inference in batches</li>\n</ul>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\npython3 tools/fill_template.py --in_place \\\n      all_models/inflight_batcher_llm/tensorrt_llm/config.pbtxt \\\n      decoupled_mode:true,engine_dir:/all_models/inflight_batcher_llm/tensorrt_llm/1,\\\nmax_tokens_in_paged_kv_cache:,batch_scheduler_policy:guaranteed_completion,kv_cache_free_gpu_mem_fraction:0.2,\\\nmax_num_sequences:4\n\npython tools/fill_template.py --in_place \\\n    all_models/inflight_batcher_llm/preprocessing/config.pbtxt \\\n    tokenizer_type:llama,tokenizer_dir:meta-llama/Llama-2-7b-chat-hf\n\npython tools/fill_template.py --in_place \\\n    all_models/inflight_batcher_llm/postprocessing/config.pbtxt \\\n    tokenizer_type:llama,tokenizer_dir:meta-llama/Llama-2-7b-chat-hf\n</pre></div>\n\n\n<p>Now, spin up the Docker container and launch the Triton server. Specify the \u201cworld size,\u201d which is the number of GPUs the model was built for, and point to the model_repo just set up.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndocker run -it --rm --gpus all --network host --shm-size=1g \\\n-v $(pwd)/all_models:/all_models \\\n-v $(pwd)/scripts:/opt/scripts \\\nnvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3\n\n# Log in to huggingface-cli to get tokenizer\nhuggingface-cli login --token *****\n\n# Install python dependencies\npip install sentencepiece protobuf\n\n# Launch Server\npython /opt/scripts/launch_triton_server.py --model_repo /all_models/inflight_batcher_llm --world_size 1\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Sending requests</h2>\n\n\n\n<p>To send requests to and interact with the running server, you can use one of the <a href=\"https://github.com/triton-inference-server/client\">Triton Inference Server client libraries</a> or send HTTP requests to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/protocol/extension_generate.html\">generate endpoint</a>. To get started, you can use the more <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/tools/inflight_batcher_llm/end_to_end_streaming_client.py\">fully featured client script</a> or the following curl command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ncurl -X POST localhost:8000/v2/models/ensemble/generate -d \\\n'{\n&quot;text_input&quot;: &quot;How do I count to nine in French?&quot;,\n&quot;parameters&quot;: {\n&quot;max_tokens&quot;: 100,\n&quot;bad_words&quot;:&#91;&quot;&quot;],\n&quot;stop_words&quot;:&#91;&quot;&quot;]\n}\n}'\n</pre></div>\n\n\n<h2 class=\"wp-block-heading\"><a></a>Conclusion</h2>\n\n\n\n<p>Together, TensorRT-LLM and Triton Inference Server provide an indispensable toolkit for optimizing, deploying, and running LLMs efficiently. With the release of TensorRT-LLM as an <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">open-source library on GitHub</a>, it&#8217;s easier than ever for organizations and application developers to harness the potential of these models.</p>\n\n\n\n<p>If you&#8217;re eager to dive into the world of LLMs, now is the time to get started with TensorRT-LLM. Explore its capabilities, experiment with different models and optimizations, and embark on your journey to unlock the incredible power of AI-driven language models. I can\u2019t wait to see the incredible things you all will build.</p>\n\n\n\n<p>For more information about getting started with TensorRT-LLM, see the following resources:</p>\n\n\n\n<ul>\n<li>Access the open-source library on the <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">/NVIDIA/TensorRT-LLM</a> GitHub repo.</li>\n\n\n\n<li>Learn more about <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a>, which provides complete containers (including TensorRT-LLM and NVIDIA Triton) for generative AI deployments.</li>\n\n\n\n<li>Explore <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples\">sample code</a>, <a href=\"https://nvidia.github.io/TensorRT-LLM/performance.html\">benchmarks</a>, and <a href=\"https://nvidia.github.io/TensorRT-LLM/index.html\">TensorRT-LLM documentation</a> on GitHub.</li>\n\n\n\n<li>Purchase <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, an end-to-end AI software platform that includes TensorRT and will soon include TensorRT-LLM, for mission-critical AI inference with enterprise-grade security, stability, manageability, and support. To learn more, <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/contact-sales/\">contact sales</a>.</li>\n\n\n\n<li>Explore <a href=\"https://developer.nvidia.com/tensorrt-getting-started\">resources to get started</a> with TensorRT and TensorRT-LLM.</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Today, NVIDIA announces the public release of TensorRT-LLM to accelerate and optimize inference performance for the latest LLMs on NVIDIA GPUs. This open-source library is now available for free on the /NVIDIA/TensorRT-LLM GitHub repo and as part of the NVIDIA NeMo framework. Large language models (LLMs) have revolutionized the field of artificial intelligence and created &hellip; <a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">Continued</a></p>\n", "protected": false}, "author": 1550, "featured_media": 71654, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1280612", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-inference-on-large-language-models-with-nvidia-tensorrt-llm-now-publicly-available/270006", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1903], "tags": [296, 453, 2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/promo-tensorRT-LLM-tech-blog-2938535-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iDC", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71648"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1550"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71648"}], "version-history": [{"count": 25, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71648/revisions"}], "predecessor-version": [{"id": 73916, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71648/revisions/73916"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71654"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71648"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71648"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71648"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71788, "date": "2023-10-18T12:01:00", "date_gmt": "2023-10-18T19:01:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71788"}, "modified": "2023-11-02T11:14:37", "modified_gmt": "2023-11-02T18:14:37", "slug": "webinar-fast-track-ai-to-the-edge-with-nvidia-tao-and-edge-impulse", "status": "publish", "type": "post", "link": "https://hubs.ly/Q023VJbQ0", "title": {"rendered": "Webinar: Fast Track AI to the Edge with NVIDIA TAO and Edge Impulse"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Discover the power of integrating NVIDIA TAO and Edge Impulse to accelerate AI deployment at the edge.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Discover the power of integrating NVIDIA TAO and Edge Impulse to accelerate AI deployment at the edge.</p>\n", "protected": false}, "author": 1466, "featured_media": 71791, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://hubs.ly/Q023VJbQ0", "_links_to_target": "_blank"}, "categories": [2724, 2758], "tags": [3312, 453, 1953, 1472, 2056, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/tao-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iFS", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71788"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71788"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71788/revisions"}], "predecessor-version": [{"id": 71797, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71788/revisions/71797"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71791"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71788"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71788"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71788"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71620, "date": "2023-10-18T10:52:59", "date_gmt": "2023-10-18T17:52:59", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71620"}, "modified": "2023-11-02T11:14:38", "modified_gmt": "2023-11-02T18:14:38", "slug": "new-self-paced-course-rapids-accelerator-for-apache-spark", "status": "publish", "type": "post", "link": "https://courses.nvidia.com/courses/course-v1:DLI+S-DS-02+V1/", "title": {"rendered": "New Self-Paced Course: RAPIDS Accelerator for Apache Spark"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Dive into the RAPIDS Accelerator for Apache Spark toolset, including the workload qualification tool for estimating speedup on GPU and the profiling tool for tuning jobs.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Dive into the RAPIDS Accelerator for Apache Spark toolset, including the workload qualification tool for estimating speedup on GPU and the profiling tool for tuning jobs.</p>\n", "protected": false}, "author": 1289, "featured_media": 71624, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1279934", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-self-paced-course-rapids-accelerator-for-apache-spark/269882", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://courses.nvidia.com/courses/course-v1:DLI+S-DS-02+V1/", "_links_to_target": "_blank"}, "categories": [696], "tags": [278, 3312, 2964, 1935, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvt-rapids-sp-blog-2961300-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iDa", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71620"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1289"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71620"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71620/revisions"}], "predecessor-version": [{"id": 71625, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71620/revisions/71625"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71624"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71620"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71620"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71620"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71663, "date": "2023-10-18T07:00:00", "date_gmt": "2023-10-18T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71663"}, "modified": "2023-11-02T11:14:39", "modified_gmt": "2023-11-02T18:14:39", "slug": "accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/", "title": {"rendered": "Accelerate AI-Enabled Robotics with Advanced Simulation and Perception Tools on NVIDIA Isaac Platform"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA announced major updates to the NVIDIA Isaac Robotics platform today at <a href=\"https://roscon.ros.org/2023/\">ROSCon 2023</a>. The platform delivers performant perception and high-fidelity simulation to robotics developers worldwide. These updates include the release of <a href=\"https://developer.nvidia.com/isaac-ros\">NVIDIA Isaac ROS 2.0</a> and <a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim 2023.1</a> and perception and simulation upgrades that simplify building and testing performant AI-based robotic applications for ROS developers.&nbsp;</p>\n\n\n\n<p>\u201cROS continues to grow and evolve to provide open-source software for the whole robotics community,\u201d said Geoff Biggs, CTO of the <a href=\"https://robohub.org/author/osrfoundation/\">Open Source Robotics Foundation</a>. \u201cThe new NVIDIA prebuilt ROS 2 packages, launched with this release, will accelerate that growth by making ROS 2 readily available to the vast <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\">NVIDIA Jetson</a> developer community.\u201d</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"893\" height=\"337\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture.png\" alt=\"Four industrial images (Train, Simulate, Build, Deploy + Manage) and graphic representing software architecture.\n\" class=\"wp-image-71717\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture.png 893w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-300x113.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-625x236.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-179x68.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-768x290.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-645x243.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-500x189.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-160x60.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-362x137.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-robotics-platform-software-architecture-291x110.png 291w\" sizes=\"(max-width: 893px) 100vw, 893px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA Isaac Robotics Platform is a collection of technologies for enabling AI in robotics</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Isaac ROS 2.0</h2>\n\n\n\n<p>As robotics evolves toward higher levels of autonomy and the proliferation of diverse sensor technologies continues, the constraints of CPU-bound processing become evident. Accelerated computing has emerged as a natural solution to overcome these bottlenecks.&nbsp;</p>\n\n\n\n<p>Isaac ROS brings much-needed accelerated computing capabilities to the ROS community, enabling the development of next-generation robotics solutions. Beyond offering a collection of accelerated ROS packages, NVIDIA also collaborates closely with <a href=\"https://www.openrobotics.org/\">Open Robotics</a> to enhance the efficiency of the middleware itself. To learn more, see <a href=\"https://developer.nvidia.com/blog/improve-perception-performance-for-ros-2-applications-with-nvidia-isaac-transport-for-ros/\">Improve Perception Performance for ROS 2 Applications with NVIDIA Isaac Transport for ROS</a>.</p>\n\n\n\n<p>The release of Isaac ROS 2.0 achieves production-quality readiness. Significant enhancements include:</p>\n\n\n\n<ul>\n<li><strong>Native ROS 2 Humble support</strong>: NVIDIA will host installation-ready ROS 2 Humble packages for JetPack 5 on Ubuntu 20.04, eliminating the need for source code compilation.</li>\n\n\n\n<li><strong>NITROS ROS bridge</strong>: This optimized ROS bridge delivers a 2x speedup compared to standard ROS bridges, extending Isaac ROS acceleration to ROS Noetic applications.</li>\n\n\n\n<li><strong>CUDA NITROS</strong>: Empowers developers to use their own DNN models with custom encoders and decoders and harness the acceleration capabilities of NITROS.</li>\n\n\n\n<li><strong>Stereolabs ZED camera integration</strong>: Isaac ROS is now integrated with Stereolabs ZED cameras.</li>\n\n\n\n<li><strong>Nova Carter</strong>: Isaac ROS software including Nav 2 navigation stack now supported on <a href=\"https://robotics.segway.com/nova-carter/\">Nova Carter</a>, a reference AMR for robotics R&amp;D, powered by <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">NVIDIA Jetson AGX Orin</a>.</li>\n\n\n\n<li><strong>ESS 3.0 performance</strong>: The new ESS 3.0 depth perception DNN model offers improved accuracy and performance.</li>\n</ul>\n\n\n\n<p>Developers worldwide leverage Isaac ROS for high-performance robotics solutions across diverse domains including agriculture, warehouse automation, last-mile delivery, and service robotics, among others.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1480\" height=\"830\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules.png\" alt=\"Twelve images in 4x3 matrix representing various software modules within NVIDIA Isaac ROS.\n\" class=\"wp-image-71722\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules.png 1480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-300x168.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-768x431.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-645x362.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-500x280.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-software-modules-1024x574.png 1024w\" sizes=\"(max-width: 1480px) 100vw, 1480px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Software modules within NVIDIA Isaac ROS, a key component of the NVIDIA Isaac Robotics platform&nbsp;</em></em></figcaption></figure>\n\n\n\n<p>\u201cIt is the breadth of NVIDIA&#8217;s offering that makes it the perfect fit for <a href=\"https://farmx.co/\">FarmX</a>,\u201d said Dan Hennage, VP of Robotics at FarmX. \u201cFrom the operating environment and libraries in Isaac ROS to the various modules that allow us to deploy on vehicles ranging from drones and large tractors, NVIDIA Jetson and Isaac ROS allow us to focus on developing our application and not worry about the platform.\u201d\u200b \u200b</p>\n\n\n\n<p><a href=\"https://www.dotlumen.com/\">.lumen</a> CEO and Founder Cornel Amariei said, \u201cWhat we\u2019ve built is the most advanced technology for blind assistance, scaling down autonomous driving to what people can comfortably wear on the head. It was only possible because of the NVIDIA Jetson platform, and the optimized packages, nodes, and features in Isaac ROS. From vSLAM to stereo perception, no other platform could have enabled us to do this.\u201d\u200b</p>\n\n\n\n<h2 class=\"wp-block-heading\">Isaac Sim 2023.1&nbsp;</h2>\n\n\n\n<p>Growing adoption of AI in robotics is set to accelerate automation across industries, from manufacturing to logistics to automotive. The challenge for those developing and testing AI models for robotics perception or control is often data scarcity. High-fidelity simulation is the key technology to address this scarcity.</p>\n\n\n\n<p>Built on <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a>, Isaac Sim is a robotics simulator for developing, training, testing, and deploying AI-enabled robots. Powerful built-in capabilities include <a href=\"https://developer.nvidia.com/omniverse/replicator\">NVIDIA Omniverse Replicator</a> for generating synthetic data, and <a href=\"https://developer.nvidia.com/isaac-gym\">Isaac Gym</a> for GPU-accelerated reinforcement learning. With the latest release of Omniverse Replicator 1.10, developers can <a href=\"https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/\">boost synthetic data generation with a low-code, YAML-based configurator</a> and asynchronous rendering.&nbsp;</p>\n\n\n\n<p>Isaac Sim also includes accurate sensor simulations for most of the popular sensors available today. It supports ROS and ROS 2 and can additionally be controlled from a Python script.</p>\n\n\n\n<p>The release of Isaac Sim 2023.1 provides many new features and improvements to advance AI-based robots, including:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>New built-in robot models and sensors</strong>: Quickly get started using built-in robot models and sensors. New robot models include FANUC and Techman. New sensors include <a href=\"https://shop.orbbec3d.com/\">Orbbec</a>, <a href=\"https://www.sensing-world.com/jetson_camera_ecosystem\">SENSING</a>, <a href=\"http://zvision.xyz/en/h-default.html\">Zvision</a>, Ouster, and RealSense.</li>\n\n\n\n<li><strong>Improved ROS and ROS 2 support</strong>: Create custom ROS and ROS 2 messages to support your simulation applications. URDF/MJCF importers are now open source to give you more power when importing your robots into Isaac Sim.</li>\n\n\n\n<li><strong>Enhanced synthetic data generation (SDG) support</strong>: Randomization in simulation added for manipulator and mobile robot applications. Learn more about new SDG capabilities enabled by the latest release of Omniverse Replicator.</li>\n\n\n\n<li><strong>Warehouse builder modular 3D assets</strong>: Use SimReady warehouse scenes and assets to quickly create compelling warehouse environments to test and exercise robot solutions.&nbsp;</li>\n</ul>\n\n\n\n<p>Many companies are developing robots and automation solutions following a simulation-first approach. This requires high-fidelity simulation to validate design and throughput before building the solution.\u00a0</p>\n\n\n\n<p>\u201cAt <a href=\"https://www.co.bot/\">Collaborative Robotics</a>, we have a deep conviction that the future of robotics involves collaborative robots working alongside humans,&#8221; said Jon Battles, VP of Technology Strategy. &#8220;We&#8217;ve adopted a sim-first development approach, using Isaac Sim extensively to accelerate our development and deployment timelines.&#8221;&nbsp;</p>\n\n\n\n<p><a href=\"https://www.rios.com/\">RIOS</a> VP of Engineering Chris Paulson said, &#8220;NVIDIA Isaac Sim is pivotal for how RIOS designs, tests, and implements advanced AI-powered robots-as-a-service (RaaS) work cells for the intelligent factory of the future. Isaac Sim is a critical platform for reducing project risk, derisking new work cell designs, and simplifying the development of complex robotic task execution. Our internal customer-facing tools also leverage Isaac Sim as a platform to quickly and efficiently deliver state-of-the-art robotics to our customers.&#8221;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1125\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse.png\" alt=\"Image of three robot arms in an industrial warehouse setting.\n\" class=\"wp-image-71726\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robots-in-industrial-warehouse-1024x576.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Built-in robot models new to NVIDIA Isaac Sim 2023.1 include the FANUC Robot Arm (left), Fraunhofer EvoBOT (center), and Techman Cobot (right)</em></em></figcaption></figure>\n\n\n\n<p>The NVIDIA Isaac Robotics platform is designed from the ground up to advance AI in robotics. The latest&nbsp; <a href=\"https://developer.nvidia.com/isaac-ros\">Isaac ROS</a> updates will make NVIDIA AI perception easier to leverage for ROS developers. And the latest release of <a href=\"https://developer.nvidia.com/isaac-sim\">Isaac Sim</a> is packed with new features to easily develop, test, and train AI robots in the virtual world before deploying them to the real world.<strong> </strong>Join the thousands of developers working with NVIDIA Isaac ROS and Isaac Sim.&nbsp;</p>\n\n\n\n<p>To learn more, register for the upcoming Isaac ROS webinar <a href=\"https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3998202/isaac-ros-webinar-series\">Need for Noetic Speed: Bringing NITROS to ROS</a>. Join <a href=\"https://www.nvidia.com/en-us/events/roscon/\">NVIDIA at ROSCon 2023</a> for the latest announcements, demos, contests, and partner news. And check out the NVIDIA speakers and panelists at <a href=\"https://www.robobusiness.com/speakers/\">RoboBusiness 2023</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA announced major updates to the NVIDIA Isaac Robotics platform today at ROSCon 2023. The platform delivers performant perception and high-fidelity simulation to robotics developers worldwide. These updates include the release of NVIDIA Isaac ROS 2.0 and NVIDIA Isaac Sim 2023.1 and perception and simulation upgrades that simplify building and testing performant AI-based robotic applications &hellip; <a href=\"https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/\">Continued</a></p>\n", "protected": false}, "author": 1101, "featured_media": 71716, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1279844", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-on-nvidia-isaac-platform/269859", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [453, 3130, 1305, 347, 2571, 1410], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-isaac-ros-platform-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iDR", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71663"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1101"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71663"}], "version-history": [{"count": 24, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71663/revisions"}], "predecessor-version": [{"id": 71802, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71663/revisions/71802"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71716"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71663"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71663"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71663"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71526, "date": "2023-10-18T07:00:00", "date_gmt": "2023-10-18T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71526"}, "modified": "2023-11-02T11:14:39", "modified_gmt": "2023-11-02T18:14:39", "slug": "boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/", "title": {"rendered": "Boost Synthetic Data Generation with Low-Code Workflows in NVIDIA Omniverse Replicator 1.10"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Data is the lifeblood of AI systems, which rely on robust datasets to learn and make predictions or decisions. For perception AI models specifically, it is essential that data reflects real-world environments and incorporates the array of scenarios. This includes edge use cases for which data is often difficult to collect, such as <a href=\"https://developer.nvidia.com/blog/developing-smart-city-traffic-management-systems-with-openusd-and-synthetic-data/\">street traffic</a> and <a href=\"https://developer.nvidia.com/blog/how-to-train-an-object-detection-model-for-visual-inspection-with-synthetic-data/\">manufacturing assembly lines</a>.</p>\n\n\n\n<p>To bootstrap and accelerate training computer vision models, AI and machine learning (ML) engineers can leverage the power of <a href=\"https://www.nvidia.com/en-us/omniverse/synthetic-data/\">synthetic data</a> in conjunction with real-world data. Synthetic data generation (SDG) enables AI and ML engineers to generate large sets of diverse training data to address an infinite diversity of use cases that span visual inspection, robotics, and autonomous driving.&nbsp;</p>\n\n\n\n<p>With the latest update of <a href=\"https://developer.nvidia.com/omniverse/replicator\">NVIDIA Omniverse Replicator</a>, a core extension of the <a href=\"https://developer.nvidia.com/omniverse\">NVIDIA Omniverse</a> platform built on Universal Scene Description (OpenUSD), developers can build more powerful synthetic data generation pipelines than ever before. New feature highlights include:</p>\n\n\n\n<ul>\n<li>Unlocking the power of synthetic data for AI developers with low-code, YAML-based configurator.</li>\n\n\n\n<li>Scaling the overall rendering process through asynchronous rendering that disaggregates the sensor simulation from rendering tasks.</li>\n\n\n\n<li>Achieving greater flexibility during the data generation process with event-based conditional triggers.&nbsp;</li>\n</ul>\n\n\n\n<p>Omniverse Replicator enables developers to build a data factory for training computer vision models. Additionally, Replicator is highly customizable and extensible, making it amenable to fit into many computer vision workflows.&nbsp;</p>\n\n\n\n<p>Replicator is integrated into <a href=\"https://developer.nvidia.com/isaac-sim\">NVIDIA Isaac Sim</a> for robotics and <a href=\"https://developer.nvidia.com/drive/simulation\">NVIDIA DRIVE Sim</a> for autonomous vehicle workflows. At ROSCon 2023, NVIDIA announced major updates to the NVIDIA Isaac Robotics platform that simplify building and testing performant AI-based robotics applications.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Simplified and tailored solutions</h2>\n\n\n\n<p>Previous limitations with the Replicator extension required developers to write extensive lines of code to generate data for model training. AI and ML engineers not familiar with 3D content generation lacked an efficient method for generating data.</p>\n\n\n\n<p>Now, rather than writing extensive lines of code for a pre-existing scene, developers can use the YAML-based descriptive file to simply describe the parameters to change using syntax (lights, environment, location, for example). This approach makes it easier to track SDG parameters as part of the model creation and performance lineage, empowering the true data-centric model development approach.</p>\n\n\n\n<p>In addition, developers can use the YAML file to batch-generate data using Replicator through <a href=\"https://docs.omniverse.nvidia.com/farm/latest/index.html\">Omniverse Farm</a> running on an <a href=\"https://www.nvidia.com/en-us/data-center/products/ovx/\">NVIDIA OVX system</a> with minimal user intervention. Users can easily share and distribute code recipes to create new versions of the same file and create an automated pipeline for data generation.\u00a0</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/IHbbOnN1bAE?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Learn a simple YAML-based workflow for generating training data by randomizing the location of objects commonly found in a warehouse</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Scaling synthetic data generation with asynchronous rendering</h2>\n\n\n\n<p>World simulation, sensor simulation, and rendering tasks for SDG are typically implemented as a tightly integrated synchronous application. This limits the flexibility to simulate sensors operating at different rates without compromising performance.&nbsp;</p>\n\n\n\n<p>Asynchronous rendering runs the simulation and rendering of sensors asynchronously from one another, empowering users with finer control over the entire process. This enables developers to render synthetic data \u200cat scale using multiple GPUs, thereby increasing \u200cthroughput.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">Superior flexibility for SDG with event-based triggers&nbsp;</h2>\n\n\n\n<p>In Replicator, <em>triggers</em> dictate when specific nodes, such as randomizers or writers, are activated. The system supports <em>on-frame triggers</em>, which activate nodes every frame, and <em>on-time triggers,</em> which activate nodes at set time intervals.</p>\n\n\n\n<p>The latest Replicator release also introduces <em>conditional triggers, </em>which enable the activation of nodes based on specific events or conditions. Developers can now establish custom logic through their own functions, offering more refined control over randomizers and writers.</p>\n\n\n\n<p>Roboticists using the newest version of NVIDIA Isaac Sim can use this feature to initiate the movement of an autonomous mobile robot (AMR) in response to a particular event. This offers a robust method for controlling when and how SDG is produced, depending on simulation events.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"480\" height=\"255\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/robot-guided-fallen-objects-1.gif\" alt=\"A gif showing a robot\u2019s movement in a scene guided by a conditional trigger towards fallen objects.\n\" class=\"wp-image-71812\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. A conditional trigger in NVIDIA Isaac Sim randomizes a scene for training an autonomous mobile robot</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Start developing with Omniverse Replicator</h2>\n\n\n\n<p>These are just a few of the new Omniverse Replicator 1.10 features for boosting SDG pipelines. To learn about additional features including material support, postrender augmentations, and 2D and 3D scatter node enhancements, see the <a href=\"https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_replicator.html\">Replicator documentation</a>.</p>\n\n\n\n<p>To <a href=\"https://developer.nvidia.com/omniverse/get-started\">start developing your own SDG applications</a> with Omniverse Replicator, <a href=\"https://www.nvidia.com/en-us/omniverse/download/\">download Omniverse free</a> and follow the instructions for <a href=\"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/getting_started.html\">getting started with Replicator in Omniverse Code</a>.</p>\n\n\n\n<p>To learn more about Replicator, check out the <a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-35d98b97-8abf-4f92-883a-c898801f28b4/\">Replicator tutorials</a>. Join the <a href=\"https://discord.com/invite/XWQNJDNuaC\">NVIDIA Omniverse Discord Server</a> to chat with the community, and check out the <a href=\"https://discord.com/channels/827959428476174346/975837676869656637\">synthetic data generation Discord channel</a>.</p>\n\n\n\n<p>Follow Omniverse on <a href=\"https://www.instagram.com/nvidiaomniverse/\">Instagram</a>, <a href=\"https://twitter.com/nvidiaomniverse\">Twitter</a>, <a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\">YouTube</a>, and <a href=\"https://medium.com/@nvidiaomniverse\">Medium</a> for additional resources and inspiration. You can also check out the <a href=\"https://forums.developer.nvidia.com/c/omniverse/300\">NVIDIA Developer Forums</a> for information from Omniverse experts.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Data is the lifeblood of AI systems, which rely on robust datasets to learn and make predictions or decisions. For perception AI models specifically, it is essential that data reflects real-world environments and incorporates the array of scenarios. This includes edge use cases for which data is often difficult to collect, such as street traffic &hellip; <a href=\"https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/\">Continued</a></p>\n", "protected": false}, "author": 1465, "featured_media": 71584, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1279845", "discourse_permalink": "https://forums.developer.nvidia.com/t/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/269860", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 3110, 63, 503], "tags": [3284, 453, 3327, 1718], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/bounding-box-composite.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iBE", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71526"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1465"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71526"}], "version-history": [{"count": 26, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71526/revisions"}], "predecessor-version": [{"id": 71873, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71526/revisions/71873"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71584"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71526"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71526"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71526"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71669, "date": "2023-10-17T06:00:00", "date_gmt": "2023-10-17T13:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71669"}, "modified": "2023-11-02T11:14:40", "modified_gmt": "2023-11-02T18:14:40", "slug": "unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/", "title": {"rendered": "Unlock Faster Image Generation in Stable Diffusion Web UI with NVIDIA TensorRT"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://github.com/Stability-AI/generative-models\">Stable Diffusion</a> is an open-source <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a> image-based model that enables users to generate images with simple text descriptions. Gaining traction among developers, it has powered popular applications like <a href=\"https://www.wombo.ai/\">Wombo</a> and <a href=\"https://prisma-ai.com/lensa\">Lensa</a>.&nbsp;</p>\n\n\n\n<p>End users typically access the model through distributions that package it together with a user interface and a set of tools. The most popular distribution is the <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">Automatic 1111 Stable Diffusion Web UI</a>. This post explains how leveraging <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> can double the performance of a model. It features an example using the Automatic 1111 Stable Diffusion Web UI.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">Efficient generative AI requires GPUs</h2>\n\n\n\n<p>Stable Diffusion is a deep learning model that uses diffusion processes to generate images based on input text and images. While it can be a useful tool to enhance creator workflows, the model is computationally intensive. Generating a single batch of four images takes minutes on nonspecialized hardware like CPUs, which breaks workflows and can be a barrier for many developers.</p>\n\n\n\n<p>Without dedicated hardware, AI features are slow because CPUs are not inherently designed for the highly parallel operations demanded by neural networks, and are instead optimized for general-purpose tasks. Stable Diffusion exemplifies why GPUs are necessary to run AI efficiently.\u00a0</p>\n\n\n\n<h2 class=\"wp-block-heading\">NVIDIA TensorRT accelerates performance</h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/\">GeForce RTX GPUs</a> excel at parallelized work, required to run generative AI models. They are also equipped with dedicated hardware called <a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\">Tensor Cores</a> that accelerate matrix operations for AI use cases. The best way to enable these optimizations is with NVIDIA TensorRT SDK, a high-performance deep learning inference optimizer.\u00a0</p>\n\n\n\n<p>TensorRT provides layer fusion, precision calibration, kernel auto-tuning, and other capabilities that significantly boost the efficiency and speed of deep learning models. This makes it indispensable for real-time applications and resource-intensive tasks like Stable Diffusion.</p>\n\n\n\n<p>TensorRT substantially accelerates performance. In the case of Stable Diffusion Web UI image generation, it doubled the number of image generations per minute, compared to the most accelerated method previously used (PyTorch xFormers).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"998\" height=\"632\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1.png\" alt=\"Comparison of images generated per minute of Apple M2 Ultra and GeForce RTX 4090 (with both PyTorch xFormers and TensorRT acceleration).\" class=\"wp-image-71701\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1.png 998w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-625x396.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-768x486.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-645x408.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-474x300.png 474w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-image-generation-performance1-174x110.png 174w\" sizes=\"(max-width: 998px) 100vw, 998px\" /><figcaption class=\"wp-element-caption\"><br><em>Figure 1. NVIDIA TensorRT acceleration doubles the number of image generations per minute</em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">Image generation: Stable Diffusion 1.5, 512 x 512, batch size 1, Stable Diffusion Web UI from Automatic 1111 (for NVIDIA) and Mochi (for Apple)<br>Hardware: GeForce RTX 4090 with Intel i9 12900K; Apple M2 Ultra with 76 cores</p>\n\n\n\n<h2 class=\"wp-block-heading\">Implementing TensorRT in a Stable Diffusion pipeline</h2>\n\n\n\n<p>NVIDIA has published a <a href=\"https://github.com/NVIDIA/TensorRT/tree/release/8.6/demo/Diffusion\">TensorRT demo of a Stable Diffusion pipeline</a> that provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point if you\u2019re interested in turbocharging your diffusion pipeline and bringing lightning-fast inference to your applications.</p>\n\n\n\n<p>Building on this foundation, the TensorRT pipeline was then applied to a project commonly used by Stable Diffusion developers. Implementing TensorRT into the Stable Diffusion Web UI further democratizes generative AI and provides broad, easy access.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1194\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images.png\" alt=\"Screenshot of Stable Diffusion Web UI\nwith generated images.\n\" class=\"wp-image-71678\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-300x179.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-625x373.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-768x459.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-1536x917.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-645x385.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-151x90.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-362x216.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-184x110.png 184w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/stable-diffusion-images-1024x612.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Images generated in the Stable Diffusion Web UI&nbsp;</em></figcaption></figure>\n\n\n\n<p>This journey began with the introduction of a TensorRT Python package for Windows, which significantly simplified the installation process. Even those with minimal technical knowledge can easily install and start using TensorRT.</p>\n\n\n\n<p>Once installed, it provides an intuitive user interface that triggers the ahead-of-time compilation required for TensorRT engines. A caching mechanism drastically reduces compile times. These simplifications free users to focus on core tasks. The integration is flexible: dynamic shapes enable users to render different resolutions with minimal impact on performance. This implementation provides a useful tool for developers. Leverage this plug-in to enhance your own Stable Diffusion pipelines.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started with TensorRT</h2>\n\n\n\n<p>To download the Stable Diffusion Web UI TensorRT extension, visit <a href=\"https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT\">NVIDIA/Stable-Diffusion-WebUI-TensorRT</a> on GitHub. And check out <a href=\"https://github.com/NVIDIA/TensorRT/tree/release/8.6/demo/Diffusion\">NVIDIA/TensorRT</a> for a demo showcasing the acceleration of a Stable Diffusion pipeline. For more details about the Automatic 1111 TensorRT extension, see <a href=\"https://nvidia.custhelp.com/app/answers/detail/a_id/5487\">TensorRT Extension for Stable Diffusion Web UI</a>.\u00a0</p>\n\n\n\n<p>For broader guidance on how to integrate TensorRT into your applications, see <a href=\"https://developer.nvidia.com/accelerate-ai-applications/get-started\">Getting Started with NVIDIA AI for Your Applications.</a> Learn how to profile your pipeline to pinpoint where optimization is critical and where minor changes can have a big impact. Accelerate your AI pipeline by choosing a machine learning framework, and discover SDKs for video, graphic design, photography, and audio.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Stable Diffusion is an open-source generative AI image-based model that enables users to generate images with simple text descriptions. Gaining traction among developers, it has powered popular applications like Wombo and Lensa.&nbsp; End users typically access the model through distributions that package it together with a user interface and a set of tools. The most &hellip; <a href=\"https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/\">Continued</a></p>\n", "protected": false}, "author": 1609, "featured_media": 71671, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1279083", "discourse_permalink": "https://forums.developer.nvidia.com/t/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/269738", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1968, 1903], "tags": [453, 3257, 1953, 367, 492], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/spacecraft-in-flight-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-iDX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71669"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1609"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71669"}], "version-history": [{"count": 43, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71669/revisions"}], "predecessor-version": [{"id": 71768, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71669/revisions/71768"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71671"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71669"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71669"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71669"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71544, "date": "2023-10-13T14:18:33", "date_gmt": "2023-10-13T21:18:33", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71544"}, "modified": "2023-11-02T11:14:40", "modified_gmt": "2023-11-02T18:14:40", "slug": "supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance/", "title": {"rendered": "Supercharge Graph Analytics at Scale with GPU-CPU Fusion for 100x Performance"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Graphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. According to one study, by 2025 <a href=\"https://techwireasia.com/2023/01/are-organizations-finally-understanding-the-capabilities-of-graph-technologies/\">graph technologies will be used in 80% of data and analytics innovations</a>, which will help facilitate rapid decision making across organizations.</p>\n\n\n\n<p>When working with graphs containing millions of nodes, the execution duration of algorithms such as Louvain on a CPU can stretch up to several hours. This prolonged processing time not only impacts developer productivity, but also leads to suboptimal overall performance outcomes.&nbsp;</p>\n\n\n\n<p>Harnessing the parallel processing power of GPUs can significantly accelerate graph training times. The benchmark results demonstrate the remarkable potential of GPU acceleration in outpacing CPU-based computations by more than 100x.&nbsp;</p>\n\n\n\n<p>This significant increase in speed showcases the clear advantages of incorporating GPUs into <a href=\"https://www.nvidia.com/en-us/glossary/data-science/graph-analytics/\">graph analytics</a>. This post explains the architecture behind achieving these 100x performance gains.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Understanding the components</h2>\n\n\n\n<p>At the core of this game-changing architecture are the following three key components, each playing a crucial role.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GPU acceleration with cuGraph</h3>\n\n\n\n<p><a href=\"https://github.com/rapidsai/cugraph/blob/branch-23.12/README.md\">cuGraph</a>, the NVIDIA GPU-accelerated graph analytics library, takes the lead in turbocharging your graph computations. Traditional CPU-based graph processing can often be a bottleneck, especially when dealing with large-scale graphs. cuGraph unleashes the raw processing power of <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 GPUs</a>, specifically designed for high-performance computing (HPC), to handle complex graph algorithms with unrivaled speed.<br><br>Graph algorithms such as PageRank, Louvain, and Betweenness Centrality are inherently parallelizable, making them ideal candidates for GPU acceleration. The thousands of cores in the NVIDIA A100 GPUs enable simultaneous processing of data, drastically reducing computation times compared to CPU-based approaches.</p>\n\n\n\n<h3 class=\"wp-block-heading\">TigerGraph graph database capabilities</h3>\n\n\n\n<p>While cuGraph optimizes graph analytics with GPUs, the <a href=\"https://www.tigergraph.com/\">TigerGraph</a> graph database complements the GPU acceleration with its efficiency in storing and querying interconnected data. TigerGraph&#8217;s distributed architecture, along with its Turing-complete GSQL language and native support for graph data, enable it to handle complex relationships and real-time queries and updates with remarkable flexibility.</p>\n\n\n\n<p>TigerGraph&#8217;s data structure, known as the Graph Model, provides a highly scalable representation of graph data. It optimizes data locality and traversal, reducing I/O bottlenecks during graph processing. The seamless integration of TigerGraph with cuGraph ensures that data flows effortlessly between the graph database and the GPU-accelerated analytics, maximizing \u200cperformance gains.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Enhancing GSQL with advanced functionality</h3>\n\n\n\n<p>To complete this fusion, ZettaBolt has designed custom user-defined functions (UDFs) that act as the bridge between TigerGraph and cuGraph. UDFs enable you to write and seamlessly integrate your own C++ code into the TigerGraph ecosystem.&nbsp;</p>\n\n\n\n<p>They enable communication between GSQL and cuGraph&#8217;s Python service over the Thrift RPC layer. This enables the smooth flow of data and computations between the graph database and the GPU-accelerated analytics, unlocking new possibilities for graph algorithm optimization.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Traditional and accelerated PageRank calculation</h2>\n\n\n\n<p>This section explores how to put this powerful GPU-CPU fusion to work with practical examples. It introduces two distinct methods for running PageRank calculations: the traditional CPU-based <code>tg_pagerank</code> and the accelerated <code>accel_pagerank</code> that leverages the GPU-CPU fusion architecture.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Traditional approach: tg_pagerank</h3>\n\n\n\n<p>This traditional approach employs the <code>tg_pagerank</code> query to calculate PageRank scores. It relies on CPU-based processing and is suitable for scenarios where GPU acceleration is not available or required. For more details, see <a href=\"https://github.com/tigergraph/gsql-graph-algorithms/blob/master/algorithms/Centrality/pagerank/global/unweighted/tg_pagerank.gsql\">tigergraph/gsql-graph-algorithms</a> on GitHub.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Query tg_pagerank(\n    v_type,            # Vertex type representing persons in the graph\n    e_type\",           # Edge type representing friendships between persons\n    max_change=0.001,  # Maximum change in PageRank scores for convergence\n    maximum_iteration=25,# Maximum number of iterations for convergence\n    damping=0.85,      # Damping factor for the PageRank calculation\n    top_k=100,         # Number of top results to display\n    print_results=True,# Whether to print the PageRank results\n    result_attribute=\"\", # Optional attribute to store the PageRank results\n    file_path=\"\",      # Optional file path to save the results\n    display_edges=False# Whether to display the edges during computation\n)\n</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Accelerated approach: accel_pagerank</h3>\n\n\n\n<p>Accelerate your PageRank computations with <code>accel_pagerank</code>. This query harnesses the GPU-CPU fusion architecture to significantly boost performance, making it ideal for large-scale graph processing (<a href=\"https://drive.google.com/file/d/1SK51wtXw6hXue7poi6aimDx-YYjnZsfh/view?usp=drive_link\">GSQL</a>).</p>\n\n\n\n<pre class=\"wp-block-code\"><code>QUERY accel_pagerank(\n  INT numServers,          #Number of servers to distribute computation\n  INT seg_size,            #Segment size for processing\n  STRING v_type,           #Vertex type\n  STRING e_type,           #Edge type\n  FLOAT max_change=0.00001,#Maximum change threshold for convergence\n  INT maximum_iteration=50,# Maximum number of iterations\n  FLOAT damping=0.85,      #Damping factor for PageRank calculation\n  INT top_k=100,           #Top-k results to retrieve\n  BOOL print_accum=TRUE,   #Print accumulated results (default:   TRUE)\n  STRING result_attr=\"\",   #Result attribute name\n  STRING file_path,        #File path for storing results\n  BOOL display_edges=FALSE,#Display edges in results (default: FALSE)\n  STRING profile_path,     #Path for profiling data\n  STRING graph_name,       #Name of the graph\n  STRING server_name,      #Server address\n  UINT port,               #Port for communication\n  UINT total_segments,     #Total segments for computation\n  STRING tmp_dir,          #Temporary directory for processing\n  INT streaming_limit=300000, #Streaming data limit\n  BOOL cache_graph=FALSE   #Cache the graph (default: FALSE)\n)\n</code></pre>\n\n\n\n<h3 class=\"wp-block-heading\">Seamless transition</h3>\n\n\n\n<p>Switching between the traditional <code>tg_pagerank</code> and accelerated <code>accel_pagerank</code><strong> </strong>queries is as simple as changing your API call. You don&#8217;t need to worry about the underlying technical details. The transition is designed to be effortless, so you can easily adapt the graph processing to your needs. Whether you require the raw power of GPU acceleration or prefer the familiarity of CPU-based processing, your graph analytics will seamlessly align with your requirements.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Creating the architecture</h2>\n\n\n\n<p>This section walks you through the step-by-step process of the architecture, where the magic of GPU-CPU fusion comes to life.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1064\" height=\"700\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1.png\" alt=\"Diagram showing the three-step process for high-performance graph analytics with TigerGraph, cuGraph, and GSQL. Step 1: Streamlining data flow from TigerGraph nodes to NVIDIA GPUs with Zetta bridge and GSQL. Step 2: Invoking GPU-accelerated algorithms on streamed data with GSQL. Step 3: Parallel data retrieval from GPU and storage on TigerGraph as vertex attributes. \n\" class=\"wp-image-71660\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1.png 1064w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-300x197.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-625x411.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-175x115.png 175w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-768x505.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-645x424.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-456x300.png 456w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-137x90.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-362x238.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-167x110.png 167w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/process-high-performance-graph-analytics-1-1024x674.png 1024w\" sizes=\"(max-width: 1064px) 100vw, 1064px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Process for high-performance graph analytics with TigerGraph, cuGraph, and GSQL</em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Streaming edges from TigerGraph to cuGraph</h3>\n\n\n\n<p>The journey begins with the efficient streaming of edges from TigerGraph to cuGraph for GPU-accelerated processing. As the edges are read in parallel from TigerGraph, they are collected in batches until the edge count reaches a predetermined threshold (1 million, for example). Once the batch is complete, it is flushed to cuGraph through the Thrift RPC layer.</p>\n\n\n\n<p>Streaming edges in batches optimizes data transfer, reducing overhead and ensuring that the GPU-accelerated processing receives a continuous flow of data. The streaming process efficiently prepares the graph data for \u200csubsequent GPU-based computations.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GPU-accelerated computation with PageRank&nbsp;</h3>\n\n\n\n<p>With the entire graph data now residing in cuGraph memory on the NVIDIA A100 GPUs, the architecture is poised to unleash its true potential.&nbsp;</p>\n\n\n\n<p>Consider the classic PageRank algorithm as an example. PageRank calculates the importance of nodes in a graph based on the number and quality of incoming links. The algorithm is an excellent candidate for GPU acceleration because it is iterative and parallelizable. cuGraph&#8217;s GPU-based PageRank algorithm processes the entire graph in parallel, efficiently traversing the network and updating the PageRank scores iteratively.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Obtaining results using Thrift RPC layer</h3>\n\n\n\n<p>Once the GPU-accelerated computation is complete, it&#8217;s time to get the results. The PageRank scores, now residing in cuGraph memory, are obtained using the Thrift RPC layer and brought back to the UDFs.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Achieving the 100x speedup breakthrough</h2>\n\n\n\n<p>The performance comparisons speak for themselves\u2014real-world benchmarks demonstrate the awe-inspiring results achieved with this hybrid architecture. The 100x speedup is a significant leap in graph algorithm performance.</p>\n\n\n\n<p>With this architecture, developers gain a competitive edge in diverse domains including social networks, recommendation systems, graph-based machine learning (ML), and more.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Graph algorithm performance comparison&nbsp;</h2>\n\n\n\n<p>This benchmark demonstrates the performance of two prominent graph algorithms, Louvain and PageRank, using TigerGraph and TigerGraph alongside cuGraph on a high-performance GPU infrastructure powered by the NVIDIA A100 80GB GPUs and the AMD EPYC 7713 64-Core Processor, with a single-node configuration featuring 512GB of RAM.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Benchmark dataset</h3>\n\n\n\n<p>Graphalytics, developed by the Linked Data Benchmark Council (LDBC), is a comprehensive benchmark suite designed for evaluating the performance of graph database management systems (GDBMSs) and graph processing frameworks. It offers real-world datasets, diverse workloads, and a range of graph algorithms to help researchers and organizations assess system efficiency and scalability. For more information, see <a href=\"https://ldbcouncil.org/benchmarks/graphalytics/\">LDBC Graphalytics Benchmark</a>.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Graph</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Louvain</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>PageRank</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Vertices</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Edges</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>CPU </strong><strong><br></strong><strong>(sec)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>CPU + GPU </strong><strong><br></strong><strong>(sec)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Speedup </strong><strong><br></strong><strong>(x)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>CPU </strong><strong><br></strong><strong>(sec)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>CPU + GPU </strong><strong><br></strong><strong>(sec)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Speedup </strong><strong><br></strong><strong>(x)</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">2,396,657</td><td class=\"has-text-align-center\" data-align=\"center\">64,155,735</td><td class=\"has-text-align-center\" data-align=\"center\">1,265</td><td class=\"has-text-align-center\" data-align=\"center\">7</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>172</strong></td><td class=\"has-text-align-center\" data-align=\"center\">1,030</td><td class=\"has-text-align-center\" data-align=\"center\">7</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>147</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">4,610,222</td><td class=\"has-text-align-center\" data-align=\"center\">129,333,677</td><td class=\"has-text-align-center\" data-align=\"center\">2,288</td><td class=\"has-text-align-center\" data-align=\"center\">12</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>188</strong></td><td class=\"has-text-align-center\" data-align=\"center\">2,142</td><td class=\"has-text-align-center\" data-align=\"center\">19</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>113</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">8,870,942</td><td class=\"has-text-align-center\" data-align=\"center\">260,379,520</td><td class=\"has-text-align-center\" data-align=\"center\">4,723</td><td class=\"has-text-align-center\" data-align=\"center\">27</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>174</strong></td><td class=\"has-text-align-center\" data-align=\"center\">4,542</td><td class=\"has-text-align-center\" data-align=\"center\">38</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>120</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">17,062,472</td><td class=\"has-text-align-center\" data-align=\"center\">523,602,831</td><td class=\"has-text-align-center\" data-align=\"center\">9,977</td><td class=\"has-text-align-center\" data-align=\"center\">77</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>130</strong></td><td class=\"has-text-align-center\" data-align=\"center\">8,643</td><td class=\"has-text-align-center\" data-align=\"center\">46</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>188</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. The TigerGraph CPU-based solution compared to its cuGraph-accelerated counterpart</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Best practices and considerations</h2>\n\n\n\n<p>As you embark on your own GPU-CPU fusion journey, consider the following best practices and considerations to optimize your graph analytics.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Algorithm selection for GPU acceleration&nbsp;</h3>\n\n\n\n<p>The synergy between CPU and GPU is crucial in selecting the right algorithms for GPU acceleration. While some graph algorithms are highly parallelizable and benefit significantly from the thousands of cores in NVIDIA A100 GPUs, others may require more sequential processing, making CPU a better choice. By strategically offloading parallelizable algorithms to the GPU, you can achieve remarkable speedup and efficiency in graph computations, while leveraging the strengths of both CPU and GPU for optimal performance.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Efficient data preprocessing</h3>\n\n\n\n<p>The efficient data preprocessing phase is where CPU and GPU synergy plays a vital role. While TigerGraph&#8217;s powerful graph database capabilities handle the initial data retrieval and processing on the CPU, the GPU-accelerated cuGraph library efficiently streams and batches the graph data for further computation. The parallel processing power of the GPU ensures a continuous flow of data, minimizing overhead and bottlenecks during the data transfer phase, leading to a seamless flow of data for accelerated analytics.</p>\n\n\n\n<h3 class=\"wp-block-heading\">GPU memory management&nbsp;</h3>\n\n\n\n<p>Effective GPU memory management is a critical consideration to avoid performance bottlenecks during graph computations. Both CPU and GPU play significant roles in this process. The CPU ensures efficient data handling and allocation before transferring the relevant data to the GPU for processing. On the GPU side, the parallel processing capabilities efficiently utilize the available memory to perform computations on large-scale graphs. The tight collaboration between CPU and GPU in memory management contributes to smooth, optimized GPU-accelerated graph processing.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Future work and possibilities</h2>\n\n\n\n<p>There are exciting possibilities and enhancements to explore in the area of GPU-CPU fusion for graph analytics. Some key areas for future work and potential developments are detailed below.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Reducing GPU memory footprint&nbsp;</h3>\n\n\n\n<p>Optimizing GPU memory use is essential for handling massive graphs and increasing algorithm scalability. Future areas of focus include memory-efficient data structures, graph partitioning techniques, and smart caching mechanisms to reduce GPU memory footprint. Efficiently managing memory enables processing even larger graphs with improved performance.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Expanding the graph algorithm library&nbsp;</h3>\n\n\n\n<p>The current architecture supports graph algorithms like PageRank, Louvain, and Betweenness Centrality. However, there is a vast landscape of graph algorithms waiting to be explored. Expanding the graph algorithm library to include more diverse and complex algorithms will enable developers to address a broader range of graph analytics challenges.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Integrating support for GNN and other RAPIDS libraries&nbsp;</h3>\n\n\n\n<p>Generalized <a href=\"https://blogs.nvidia.com/blog/2022/10/24/what-are-graph-neural-networks/\">graph neural networks (GNNs)</a> have become a popular choice for various graph-related tasks. Integrating support for GNN and other RAPIDS libraries, such as cuML and cuGraphML, will enrich the architecture with cutting-edge deep learning capabilities for graph-based ML tasks. This integration will enable seamless exploration of both traditional graph algorithms and emerging ML models, fostering innovation and versatility in graph analytics.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Performance optimization and tuning&nbsp;</h3>\n\n\n\n<p>Continuous performance optimization and tuning are critical to unlocking the full potential of GPU-CPU fusion. Conducting in-depth profiling and benchmarking, leveraging GPU-specific optimizations, and fine-tuning algorithms for specific graph characteristics will lead to more speedups and efficiency gains.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>This post has explored how the seamless blending of GPUs and CPUs can supercharge graph algorithm performance. The power of TigerGraph&#8217;s database prowess combined with cuGraph GPU acceleration creates an unbeatable partnership. The smooth data flow achieves an astonishing 100x speed boost, propelling graph analytics into new frontiers.</p>\n\n\n\n<p>GPU-CPU fusion is set to reshape how teams explore data and navigate intricate networks and relationships. From social networks to machine learning, GPU-CPU fusion unlocks endless possibilities. Future areas of work include optimizing memory efficiency, broadening algorithm coverage, and fine-tuning performance, ensuring that this architecture remains at the forefront of graph analytics. Embrace the fusion and redefine the boundaries of data exploration.</p>\n\n\n\n<p>Ready to get started with accelerated graph processing? If you have TigerGraph (3.9.X) and NVIDIA GPUs (with RAPIDS support), reach out to <a href=\"https://www.tigergraph.com/contact/\">TigerGraph</a> or <a href=\"mailto:support@zettabolt.com\">Zettabolt</a> to express your interest in exploring accelerated graph processing. They will guide you through the initial steps, provide you with the necessary information, and assist in setting up the infrastructure for accelerated graph processing.</p>\n\n\n\n<p>Once the infrastructure is in place, you&#8217;ll have access to accelerated queries designed to optimize your graph processing tasks. These queries harness the power of NVIDIA GPUs and the TigerGraph platform for enhanced performance. Explore and benchmark the accelerated graph processing capabilities to experience significantly improved performance in graph analytics.</p>\n\n\n\n<p>TigerGraph and Zettabolt will continue to provide assistance and answer questions as you explore accelerated graph processing and new possibilities for handling large-scale graph data efficiently.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Graphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. According to one study, by 2025 graph technologies will be used in 80% of data and analytics innovations, which will help facilitate rapid decision making across organizations. When working with &hellip; <a href=\"https://developer.nvidia.com/blog/supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance/\">Continued</a></p>\n", "protected": false}, "author": 1892, "featured_media": 71547, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1277494", "discourse_permalink": "https://forums.developer.nvidia.com/t/supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance/269391", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696, 1968], "tags": [1101, 453, 145, 3052, 1953, 695], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/neural-network-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iBW", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71544"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1892"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71544"}], "version-history": [{"count": 22, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71544/revisions"}], "predecessor-version": [{"id": 71668, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71544/revisions/71668"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71547"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71544"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71544"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71544"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71308, "date": "2023-10-13T09:00:00", "date_gmt": "2023-10-13T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71308"}, "modified": "2023-11-02T11:14:41", "modified_gmt": "2023-11-02T18:14:41", "slug": "advanced-api-performance-debugging", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-api-performance-debugging/", "title": {"rendered": "Advanced API Performance: Debugging"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA offers a large suite of tools for graphics debugging, including <a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA Nsight System</a> for CPU debugging, and <a href=\"https://developer.nvidia.com/nsight-graphics\">Nsight Graphics</a> for GPU debugging. <a href=\"https://developer.nvidia.com/nsight-aftermath\">Nsight Aftermath</a> is useful for analyzing crash dumps.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Recommended</h2>\n\n\n\n<ul>\n<li>Always check the validation layers and make sure they don&#8217;t output any errors.</li>\n\n\n\n<li>Use Nsight Aftermath for detailed DirectX 12 or Vulkan GPU exception debugging.\n<ul>\n<li>Using the Nsight Aftermath Monitor is an easy way to get started without the need for code integration.&nbsp;</li>\n\n\n\n<li>For even more control over the GPU crash dump functionality, consider using the Aftermath SDK to integrate the crash dump capabilities into your own code.\n<ul>\n<li>Crash dump generation can always be enabled since there is no associated runtime cost.&nbsp; As noted later, debug checkpoints can have measurable CPU overhead and should not be used in shipping applications.</li>\n\n\n\n<li>You can use the callbacks provided in the API to save the crash dumps to the local disk or push them to the cloud.</li>\n</ul>\n</li>\n\n\n\n<li>For more information, see the <a href=\"https://developer.nvidia.com/nsight-aftermath\">Nsight Aftermath SDK</a>&nbsp;product page.</li>\n</ul>\n</li>\n\n\n\n<li>Isolate problems using debug checkpoints.\n\n\n<ul>\n<li>These APIs enable inserting checkpoints in the GPU command stream, making it possible to narrow down crashes to certain subsections of the command stream.</li>\n\n\n\n<li>Use the API supported by Nsight Aftermath. For more information and samples, see the <a href=\"https://github.com/NVIDIA/nsight-aftermath-samples\">/NVIDIA/nsight-aftermath-samples</a> GitHub repo.</li>\n</ul>\n\n\n\n<ul>\n<li>Alternatively, use the DirectX 12 cross-vendor solution:\n<ul>\n<li>Use <code>ID3D12GraphicsCommandList2::WriteBufferImmediate</code> or <a href=\"https://devblogs.microsoft.com/directx/dred/\">DRED</a>.&nbsp;</li>\n\n\n\n<li>It isn&#8217;t supported in conjunction with Nsight Aftermath, so it is better to avoid mixing these.</li>\n</ul>\n</li>\n\n\n\n<li>Add as a runtime flag in the engine to be able to toggle this functionality.\n<ul>\n<li>These markers are far from free and have a runtime cost associated with them (serializing GPU, CPU call overhead, and the time to capture call-stacks).</li>\n\n\n\n<li>Keeping them enabled by default could have a severe performance cost.</li>\n\n\n\n<li>See disadvantages in the <strong>Not recommended</strong> section.</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li>Build shaders with debug info.\n<ul>\n<li>Compile with /Zi to embed debug info into the shader binary.</li>\n\n\n\n<li>This is helpful when using debugging tools like NVIDIA Nsight Graphics.</li>\n\n\n\n<li>Also, Nsight Aftermath could give you source-level GPU crash info using this information.</li>\n</ul>\n</li>\n\n\n\n<li>Use Nsight Aftermath crash dumps to identify the type of error that occurs during a crash.\n<ul>\n<li>Device hung:\n<ul>\n<li>These can occur due to a single command list taking longer than a few seconds to execute.</li>\n\n\n\n<li>Microsoft Windows terminates the driver after a few seconds of no apparent feedback from the driver and GPU (TDR).</li>\n\n\n\n<li>This can also happen in case of extreme workloads (massive pixel overdraw, or degraded ray tracing acceleration structures).</li>\n</ul>\n</li>\n\n\n\n<li>Page faults\n<ul>\n<li>These are caused by invalid memory accesses: either an out-of-bounds read/write or a resource that is not valid anymore.</li>\n</ul>\n</li>\n\n\n\n<li>For more information, see <a href=\"https://docs.nvidia.com/drive/archive/drive_os_5.1.12.0L/nsight-graphics/getting-started/index.html\">How to Set Up and Inspect GPU Crash Dumps</a>.</li>\n</ul>\n</li>\n\n\n\n<li>Generic debugging advice for graphics and compute-related problems:\n<ul>\n<li>Check whether all referenced memory is valid and correct at all times when the GPU accesses the data.</li>\n\n\n\n<li>Check whether descriptors point to the right resources, which are fully allocated and initialized.</li>\n\n\n\n<li>Check whether data reads and writes are not going out of bounds.</li>\n\n\n\n<li>Use debug checkpoints and GPU crash dump functionality from Nsight Aftermath to narrow down the location of the crash.</li>\n\n\n\n<li>DirectX 12: The debug layer could help here, but for these problems, GPU-based validation must be enabled, which generally makes the application run extremely slow with complex scenes. It could still be useful for unit or regression testing.</li>\n</ul>\n</li>\n\n\n\n<li>Generic debugging advice for NVIDIA RTX-related problems:\n\n\n<ul>\n<li>Check whether the input vertex or index data are all valid.\n<ul>\n<li>Invalid indices could crash the GPU builder kernel.&nbsp;</li>\n\n\n\n<li>Invalid vertices could affect the acceleration structures and make performance extremely slow.</li>\n</ul>\n</li>\n\n\n\n<li>Degenerate triangles or tricks to disconnect triangles that work in a rasterizer do not work as intended in an acceleration tree and can cause big problems. Check if such tricks are not being employed, for example, to disconnect or delete geometry. Exclusively use valid geometry instead.</li>\n\n\n\n<li>Check whether memory is all still valid at the moment data is being referenced by builder or ray tracing kernels.</li>\n\n\n\n<li>Check whether all textures and buffers used by ray tracing kernels are all valid.</li>\n\n\n\n<li>Check whether descriptors are correct and the shader binding tables are valid.</li>\n\n\n\n<li>Debug checkpoints are not useful for debugging ray tracing workloads because of the indirections happening inside the RT kernels potentially touching thousands of shader permutations. At most, it can tell you if the crash happened in the builder or the ray-tracing kernel.</li>\n</ul>\n\n\n\n<ul>\n<li>Instead, use Nsight Aftermath crash debugging to get an approximate idea of the crash:\n<ul>\n<li>Page fault: Memory-related issue, usually out-of-bounds read-write or trying to access a resource that was removed or not copied to the GPU yet.</li>\n\n\n\n<li>GPU hang (TDR): Infinite loops, too complex shading, or too many rays.</li>\n</ul>\n</li>\n\n\n\n<li>Having a way to simplify code and binding requirements could be useful, like disabling textures or reducing shader permutations.\n<ul>\n<li>For example, have a debug view showing barycentrics only (no shader binding table requirements).\n<ul>\n<li>Check for broken-looking geometry or spots where performance gets severely degraded due to corrupted triangle data.</li>\n\n\n\n<li>Visually verify output from dynamic sources, such as deformed geometry or skinned meshes in a ray tracing-only view.</li>\n</ul>\n</li>\n\n\n\n<li>Being able to fully disable dynamic geometry can help to isolate these kinds of issues as well.</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li>Simplify debugging by adding flags in the application to the following:\n<ul>\n<li>Serialize the GPU/CPU at the queue level.</li>\n\n\n\n<li>Serialize the GPU/CPU at the command list level.</li>\n\n\n\n<li>Disable async compute.</li>\n\n\n\n<li>Disable async copies.</li>\n\n\n\n<li>Add full barriers between compute, dispatch, and copy calls in the command lists (NULL UAV/memory barrier).</li>\n\n\n\n<li>Do anything else you can to remove parallelism. It\u2019s much harder to debug and pinpoint where a problem comes from when the GPU is running multiple workloads at the same time.</li>\n\n\n\n<li>Don\u2019t keep any of these suggestions enabled by default. They should be strictly debug-only flags. Reducing parallelism significantly degrades performance.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Not recommended</h2>\n\n\n\n<ul>\n<li>The use of excessive debug checkpoints.\n<ul>\n<li>They have a non-negligible CPU and GPU performance cost.</li>\n\n\n\n<li>Use them sparingly. Aim for ~100 per frame, preferably less.</li>\n\n\n\n<li>Best not to use them at all for the end user (developer or QA only), or enable them when a GPU hang has been detected. You could also make them an option to toggle by the end user.</li>\n</ul>\n</li>\n\n\n\n<li>Assuming that a CPU call stack will tell you anything about a GPU problem.\n<ul>\n<li>Crashes with a call stack pointing to the driver usually manifest as a random graphics API call failing due to an internal device lost event.</li>\n\n\n\n<li>Use the Nsight Aftermath crash dump or debug checkpoints to pinpoint where the fault occurs.</li>\n</ul>\n</li>\n\n\n\n<li>Testing on a single machine (excluding the effect of bad hardware).\n<ul>\n<li>A corrupted memory (CPU or GPU), overclocking, and bad cooling can all contribute to random faults. Nsight Aftermath has no way of differentiating these from valid errors.</li>\n\n\n\n<li>A telltale sign could be that crashes happen randomly without any pattern across the GPU on a single machine but not another that has similar specifications.</li>\n\n\n\n<li>Try to validate results on more than one machine with similar hardware, software, and driver versions.</li>\n</ul>\n</li>\n\n\n\n<li>Permitting users to run with extremely outdated NVIDIA drivers\n<ul>\n<li>Outdated drivers can have unexpected behaviors and are harder to get reliable crash dumps from.</li>\n\n\n\n<li>Find a driver version that works reliably. Show a popup that says the driver is out of date when it is earlier than that version. Don\u2019t stop users from running the application or game, but discourage them from doing so as it can cause system instability.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Acknowledgments</h3>\n\n\n\n<p><em>Thanks to Patrick Neill, Jeffrey Kiel, Justin Kim, Andrew Allan, and Louis Bavoil for their help with this post.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA offers a large suite of tools for graphics debugging, including NVIDIA Nsight System for CPU debugging, and Nsight Graphics for GPU debugging. Nsight Aftermath is useful for analyzing crash dumps.&nbsp; Recommended Not recommended Acknowledgments Thanks to Patrick Neill, Jeffrey Kiel, Justin Kim, Andrew Allan, and Louis Bavoil for their help with this post.</p>\n", "protected": false}, "author": 1888, "featured_media": 66457, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1277388", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-api-performance-debugging/269368", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 503], "tags": [2424, 514, 29, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/Advanced-API-series.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iy8", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71308"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1888"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71308"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71308/revisions"}], "predecessor-version": [{"id": 71616, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71308/revisions/71616"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/66457"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71308"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71308"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71308"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 71572, "date": "2023-10-12T10:00:00", "date_gmt": "2023-10-12T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=71572"}, "modified": "2023-11-02T11:14:42", "modified_gmt": "2023-11-02T18:14:42", "slug": "workshop-model-parallelism-building-and-deploying-large-neural-networks", "status": "publish", "type": "post", "link": "https://nvda.ws/3ZOWCvr", "title": {"rendered": "Workshop: Model Parallelism: Building and Deploying Large Neural Networks"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how to train the largest neural networks and deploy them to production.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how to train the largest neural networks and deploy them to production.</p>\n", "protected": false}, "author": 1466, "featured_media": 71573, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/3ZOWCvr", "_links_to_target": "_blank"}, "categories": [696], "tags": [3312, 2964, 1935, 453, 3052, 369, 1177], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/10/nvt-model-parallelism-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-iCo", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71572"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=71572"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71572/revisions"}], "predecessor-version": [{"id": 71582, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/71572/revisions/71582"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/71573"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=71572"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=71572"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=71572"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]