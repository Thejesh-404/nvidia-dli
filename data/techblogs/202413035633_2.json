[{"id": 76535, "date": "2024-01-12T10:58:48", "date_gmt": "2024-01-12T18:58:48", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76535"}, "modified": "2024-01-25T10:17:35", "modified_gmt": "2024-01-25T18:17:35", "slug": "just-released-cublasdx", "status": "publish", "type": "post", "link": "https://nvda.ws/3SdQyKO", "title": {"rendered": "Just Released: cuBLASDx"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>cuBLASDx allows you to perform BLAS calculations inside your CUDA kernel, improving the performance of your application. Available to download in Preview now.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>cuBLASDx allows you to perform BLAS calculations inside your CUDA kernel, improving the performance of your application. Available to download in Preview now.&nbsp;</p>\n", "protected": false}, "author": 1140, "featured_media": 76537, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1331757", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-cublasdx/278806", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3SdQyKO", "_links_to_target": "_blank"}, "categories": [852, 696, 3110], "tags": [453, 3420], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/cublasdx-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jUr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76535"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1140"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76535"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76535/revisions"}], "predecessor-version": [{"id": 76592, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76535/revisions/76592"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76537"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76535"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76535"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76535"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76427, "date": "2024-01-11T11:00:00", "date_gmt": "2024-01-11T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76427"}, "modified": "2024-01-25T10:17:36", "modified_gmt": "2024-01-25T18:17:36", "slug": "free-digital-webinar-series-how-to-get-started-with-ai-inference", "status": "publish", "type": "post", "link": "https://nvda.ws/48NqBXI", "title": {"rendered": "Free Digital Webinar Series: How to Get Started with AI Inference"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how to improve your AI model performance with this series of expert-led talks on the NVIDIA AI inference platform.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how to improve your AI model performance with this series of expert-led talks on the NVIDIA AI inference platform.</p>\n", "protected": false}, "author": 1466, "featured_media": 76434, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/48NqBXI", "_links_to_target": "_blank"}, "categories": [852, 3110], "tags": [296, 1935, 453, 1940, 367, 1177, 1981], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/inference-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jSH", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76427"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76427"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76427/revisions"}], "predecessor-version": [{"id": 76442, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76427/revisions/76442"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76434"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76427"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76427"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76427"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76336, "date": "2024-01-10T11:00:00", "date_gmt": "2024-01-10T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76336"}, "modified": "2024-01-25T10:17:37", "modified_gmt": "2024-01-25T18:17:37", "slug": "experience-real-time-audio-and-video-communication-with-nvidia-maxine", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/experience-real-time-audio-and-video-communication-with-nvidia-maxine/", "title": {"rendered": "Experience Real-Time Audio and Video Communication with NVIDIA Maxine\u00a0"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The <a href=\"https://developer.nvidia.com/maxine\">NVIDIA Maxine</a> developer platform redefines video conferencing and editing by providing developers and businesses with a variety of low-code implementation options. These include GPU-accelerated AI microservices , SDKs, and NVIDIA-hosted API endpoints for AI enhancement of audio and video streams in real time.&nbsp;</p>\n\n\n\n<p>The latest Maxine developer platform release introduces early access to Voice Font, improvements to video-driven Live Portrait, and improvements to the popular Maxine Eye Contact feature.&nbsp;In addition, the Video Live Portrait and Voice Font features are now available in the <a href=\"https://catalog.ngc.nvidia.com/models?filters=platform%7Cpltfm_maxine%7CMaxine&amp;orderBy=weightPopularDESC&amp;query=\">NVIDIA NGC catalog</a>. You can now experience Maxine pretrained generative AI models in action on NVIDIA-accelerated cloud infrastructure.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\">NVIDIA AI Foundation Models</a> such as Maxine demonstrate how enterprises can now connect their applications to read-to-integrate NVIDIA Foundations API endpoints and quickly create and deploy performance-optimized AI models with a reduced TCO.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"563\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-625x563.png\" alt=\"NVIDIA Maxine Live Portrait webpage as seen on NVIDIA AI Foundation Models.\" class=\"wp-image-76346\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-625x563.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-300x270.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-768x692.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-645x581.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-333x300.png 333w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-100x90.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-362x326.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-122x110.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot-1024x923.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-on-nvidia-ai-foundation-models-screenshot.png 1191w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. You can now experience NVIDIA Maxine Live Portrait and Voice Font </em></em></figcaption></figure></div>\n\n\n<p>The Maxine team is also offering select partners the opportunity to give feedback on early versions of Maxine\u2019s new Studio Voice and Speech-driven Live Portrait features. For more information, visit <a href=\"https://developer.nvidia.com/maxine-microservice-early-access\">Maxine Microservices Early Access Program</a> and <a href=\"https://developer.nvidia.com/maxine-early-access\">Maxine SDK Early Access Program</a>.</p>\n\n\n\n<p>\u201cWith the Maxine developer platform, you can now experience state-of-the-art Maxine features on NVIDIA AI Foundations,\u201d said Rochelle Pereira, Director of Engineering, Maxine Developer Platform. \u201cYou can design your deployment plan on a CSP of your choice or <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>, and choose your integration touch point ranging from microservice containers to SDK libraries or even ready-to-integrate <a href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/\">NVIDIA AI Foundation Endpoints</a>. Enhancing real-time audio and video communication in your application workflow just got a lot easier.\u201d&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">New feature highlights</h2>\n\n\n\n<p>NVIDIA Maxine enables clear communications and increased presence for speakers in video conferences, live streams, and offline video. Maxine\u2019s state-of-the-art AI models create high-quality effects that can be achieved with standard microphones and cameras.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Natural eye movement</h3>\n\n\n\n<p>The new production Maxine Eye Contact now has smoother transitions in gaze redirection and fine-grained controls for more natural eye movement. Eye Contact is available for developers to evaluate through the <a href=\"https://developer.nvidia.com/maxine-microservice-early-access\">Maxine Early Access Program</a> and for production use through <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">2D photo animation</h3>\n\n\n\n<p>The newest Maxine release also sees improvements to video-driven Live Portrait including increases in robustness and background stability. Maxine Live Portrait has been a game-changer, enabling 2D photo animation driven by video. This new Maxine release also introduces speech-driven Live Portrait that enables speech as a new driving modality.&nbsp;</p>\n\n\n\n<p>You can now animate 2D photos with speech, providing a sense of presence even when conditions don\u2019t permit real-time video streaming. Combined with <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a> translation service and NVIDIA Maxine Voice Font, speech-driven Live Portrait ushers in new possibilities in the realm of 2D animation.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Speech-driven personas</h3>\n\n\n\n<p>NVIDIA Maxine video and speech Live Portrait animation AI microservices are ideal for anyone who does not want to appear on camera. Create a unique persona for an individual or a company using a stylized or photorealistic portrait photo. Speech-driven Live Portrait is available to select partners for feedback.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/7tHUgPWmhzc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Animate 2D portraits in real time using speech, no meshes or rigging required</em></em></figcaption></figure>\n\n\n\n<h3 class=\"wp-block-heading\">Voice customization</h3>\n\n\n\n<p>With the new Maxine Voice Font feature, a generative AI model now available in the <a href=\"https://developer.nvidia.com/maxine-microservice-early-access\">Maxine Early Access Program</a>, you can customize your voice to a desired timbre. Generate a unique voice for a brand or replicate your voice for use with other translation microservices. This enables you to speak in different languages in your own voice, for example. The feature can convert audio samples into a digital voice with just 30 seconds of reference audio.</p>\n\n\n\n<p>Check out the samples below to experience Voice Font.</p>\n\n\n\n<p>Origin audio sample: </p>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/pete2_aimic1_24k_original_norm.wav\"></audio></figure>\n\n\n\n<p>Reference audio sample: </p>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Sheri_24k_font_norm.wav\"></audio></figure>\n\n\n\n<p>Voice Font output, applying reference audio voice to origin audio:</p>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/pete2_aimic1_24k_final.wav\"></audio></figure>\n\n\n\n<p>Also available to select development partners for feedback is the latest NVIDIA Maxine AI enhancement capability. With Studio Voice, you can enhance a recording from an inexpensive microphone with the characteristics of a high-end studio microphone. Studio Voice removes speech frequency degradations caused by low-quality microphones. Additionally, characteristics like dynamic range and bandwidth extension are added using a pretrained neural net, giving the resulting audio a rich and vibrant sound.&nbsp;</p>\n\n\n\n<p>Get a sneak preview of Studio Voice with the samples below:</p>\n\n\n\n<p>Input voice on basic microphone:</p>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Slide2-original.wav\"></audio></figure>\n\n\n\n<p>Studio Voice output:</p>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Slide2-output.wav\"></audio></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>With NVIDIA Maxine, you can use AI to enhance your audio and video communication in real time. The latest Maxine release is available exclusively through <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>. This enables users to access enterprise support, production-ready tools including <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, and more. <a href=\"https://catalog.ngc.nvidia.com/models?filters=platform%7Cpltfm_maxine%7CMaxine&amp;orderBy=weightPopularDESC&amp;query=\">Try the newest NVIDIA Maxine features</a>.</p>\n\n\n\n<p>For early access to the latest NVIDIA Maxine features and to provide feedback, apply for the <a href=\"https://developer.nvidia.com/maxine-microservice-early-access\">Maxine Microservices Early Access Program</a> or <a href=\"https://developer.nvidia.com/maxine-early-access\">Maxine SDK Early Access Program</a>. To provide feedback on speech-driven Live Portrait or Studio Voice (not yet released), contact Greg Jones, Maxine Product Management, at gjones@nvidia.com.&nbsp;</p>\n\n\n\n<p>You can also provide feedback through the <a href=\"https://crowdsource.nvidia.com/en-us/maxine/\">NVIDIA Maxine and NVIDIA Broadcast App Survey</a> to help improve Maxine features in upcoming releases.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Maxine Live Portrait webpage as seen on NVIDIA AI Foundation Models.</p>\n", "protected": false}, "author": 1023, "featured_media": 76417, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1330196", "discourse_permalink": "https://forums.developer.nvidia.com/t/experience-real-time-audio-and-video-communication-with-nvidia-maxine/278521", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [453, 1953, 2057, 2587], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-input-video-animating-2d-image-1-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jRe", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76336"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1023"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76336"}], "version-history": [{"count": 50, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76336/revisions"}], "predecessor-version": [{"id": 77066, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76336/revisions/77066"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76417"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76336"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76336"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76336"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75584, "date": "2024-01-09T09:00:00", "date_gmt": "2024-01-09T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75584"}, "modified": "2024-01-25T10:17:37", "modified_gmt": "2024-01-25T18:17:37", "slug": "enhancing-phone-customer-service-with-asr-customization", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/", "title": {"rendered": "Enhancing Phone Customer Service with ASR Customization"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>At the core of understanding people correctly and having natural conversations is automatic speech recognition (ASR). To make customer-led voice assistants and automate customer service interactions over the phone, companies must solve the unique challenge of gaining a caller\u2019s trust through qualities such as understanding, empathy, and clarity.</p>\n\n\n\n<p>Telephony-bound voice is inherently challenging from a speech recognition perspective. Background noise, poor call quality, and various dialects and accents make understanding a caller\u2019s words difficult. Traditional language understanding systems have limited support for voice in general, and how a person speaks differs fundamentally from how they type or text.</p>\n\n\n\n<p>In this post, we discuss <a href=\"https://poly.ai/\">PolyAI</a>\u2019s exploration journey with third-party, out-of-the-box, and in-house customized <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva </a>ASR solutions. The goal is to deliver voice experiences that let callers speak however they like, providing helpful and natural responses at every turn of the conversation.&nbsp; The in-house fine-tuned Riva ASR models resulted in notable accuracy improvement on a variety of different validation real-world customer call datasets.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Out-of-the-box ASR challenges for effective customer interactions</h2>\n\n\n\n<p>Out-of-the-box ASR tools are typically prepared for non-noisy environments and speakers who clearly enunciate and have expected accents. These systems can\u2019t predict what a caller will say, how they might say it, or their speaking tempo. While out-of-the-box solutions can be useful, they can\u2019t be tailored to specific business needs and objectives.&nbsp;</p>\n\n\n\n<p>To achieve accurate voice assistants that handle customer interactions efficiently, organizations require an ASR system that can be fine-tuned to significantly improve word error rate (WER).</p>\n\n\n\n<h2 class=\"wp-block-heading\">Advantages and challenges of building an in-house ASR solution</h2>\n\n\n\n<p>To truly understand people from different places, with different accents, and in noisy environments, conversational systems can use multiple ASR systems, phoneme matching, biasing keywords, and post-processing tools.&nbsp;</p>\n\n\n\n<p>The machine learning team at PolyAI rigorously tested numerous ASR systems, often on multiple models, and applied spoken language understanding (SLU) principles to improve transcription accuracy (Figure 1). This work significantly improved the accuracy of speech recognition in real customer phone calls.&nbsp;</p>\n\n\n\n<p>Optimizing the caller experience further required the development of an in-house solution.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1527\" height=\"662\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack.png\" alt=\"Diagram shows stack components: ASR systems, phoneme matching, biasing keywords, and post-processing tools.\" class=\"wp-image-75797\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack.png 1527w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-625x271.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-768x333.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-645x280.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-500x217.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-254x110.png 254w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-tech-stack-1024x444.png 1024w\" sizes=\"(max-width: 1527px) 100vw, 1527px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. PolyAI tech stack</em></figcaption></figure></div>\n\n\n<p>The PolyAI tech stack enables voice assistants to accurately understand alphanumeric inputs and people from different places, with different accents, and in noisy environments.</p>\n\n\n\n<p>Developing an in-house solution approach offers the following advantages:</p>\n\n\n\n<ul>\n<li><strong>Better accuracy and performance</strong> with flexible fine-tuning of model parameters on extensive data and voice activity detector (VAD) adaptation for the specific ways in which people talk with the system.</li>\n\n\n\n<li><strong>Full compliance </strong>with a bring-your-own-cloud (BYOC) approach that delivers the model and the whole conversational system to clients with zero data transfers to third-party providers.</li>\n</ul>\n\n\n\n<p>With great benefits comes a unique set of challenges. Building an in-house solution requires heavy investment in the following areas:</p>\n\n\n\n<ul>\n<li><strong>Expensive pretraining data</strong>: Most models require large quantities of good quality, annotated, pretraining data.&nbsp;</li>\n\n\n\n<li><strong>Latency optimization</strong>: This area is often overlooked in the research process. Contrary to chat conversation, voice conversation operates on milliseconds. Every millisecond counts. Adding latency at the start of the conversation gives even less time when calling the large language models (LLM) or text-to-speech (TTS) models.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Choosing and finetuning ASR models for an in-house solution</h2>\n\n\n\n<p>After a substantial search for an ASR solution that addresses building in-house solution challenges, PolyAI decided to use <a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html\">NVIDIA Riva</a> for the following reasons:&nbsp;</p>\n\n\n\n<ul>\n<li><strong>Cutting-edge accuracy of pretrained models</strong> trained on a substantial volume of conversational speech data.&nbsp;</li>\n\n\n\n<li><strong>Enhanced accuracy with full model customization</strong>, including <a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#fine-tuning-existing-models\">acoustic model customization </a>for different accents, noisy environments, or poor audio quality.</li>\n\n\n\n<li><strong>High inference performance </strong>based on tight coupling with <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a> and battle-tested to handle machine learning servicing.</li>\n</ul>\n\n\n\n<p>Initial trials with an in-house ASR model provided valuable insights into the fine-tuning process. This led to the development of a robust and flexible fine-tuning methodology, incorporating diverse validation sets to ensure optimal performance.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Conversational system for testing out-of-the-box and in-house ASR solutions&nbsp;</h2>\n\n\n\n<p>Typical conversational systems use public switched telephone networks (PSTN) or session initiation protocol (SIP) connections to transfer calls into the tech stack.&nbsp;</p>\n\n\n\n<p>Call information from these systems is then sent to third-party ASR cloud service providers or in-house ASR solutions. For PolyAI\u2019s testing of ASR solutions (Figure 2), after a call is transcribed, it is sent to a PolyAI voice assistant, where natural language models generate a response. The response is then transferred back into the audio wave through in-house TTS or third-party providers.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1227\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b.png\" alt=\"Diagram includes a telephony gateway, audio gateway, natural language models, and text-to-speech.\" class=\"wp-image-76377\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-300x184.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-625x384.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-768x471.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-1536x943.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-645x396.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-489x300.png 489w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-147x90.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-362x222.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/polyai-architecture-b-1024x629.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. PolyAI architecture for testing ASR solutions</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Creating a real-world ASR testing dataset</h2>\n\n\n\n<p>PolyAI identified 20 hours of the most challenging conversations split equally between UK and US region calls to test the accuracy of third-party, out-of-the-box, and in-house ASR solutions. These were the calls with noisy environments and ones where other ASR models\u2014in-house or third-party providers\u2014had previously failed.&nbsp;&nbsp;</p>\n\n\n\n<p>These failure calls varied from single-word utterances, such as \u2018yes\u2019 or \u2018no\u2019 answers, to much longer responses. PolyAI manually annotated them and established a word error rate (WER) below 1%, essential when dealing with fine-tuning ASR models.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Notable accuracy improvement of an in-house customized ASR solution&nbsp;</h2>\n\n\n\n<p>Fine-tuning two in-house ASR models using only 20 hours of data already resulted in a notable mean WER improvement for the US English model, reducing it by ~8.4% compared to the best model from CSP (Table 1). The importance of choosing the right model should be noted since different CSP out-of-the-box ASR models resulted in 44.51% mean WER.&nbsp;</p>\n\n\n\n<p>Even more remarkable is that the WER median of in-house US English ASR solution reached 0%. This achievement was validated across various data sets, ensuring the fine-tuning was not overfitting a specific use case. This versatility allows the model to perform well across different projects where people use specific keywords, enabling the accurate understanding of particular phrases and enhancing overall median performance.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>US English</strong></td><td><strong>Provider</strong></td><td><strong>Model</strong></td><td><strong>Language</strong></td><td><strong>WER Mean [%]</strong></td><td><strong>WER MEdian [%]</strong></td></tr><tr><td>0</td><td>Poly AI</td><td>Fine-Tuned</td><td>En-US</td><td><strong>20.32</strong></td><td><strong>0.00</strong></td></tr><tr><td>1</td><td>Poly AI</td><td>Fine-TUned</td><td>En-All</td><td>22.19</td><td>7.14</td></tr><tr><td>2</td><td>CSP</td><td>Best</td><td>En-US</td><td>22.22</td><td>7.69</td></tr><tr><td>9</td><td>CSP</td><td>Worst</td><td>En-US</td><td><strong>44.51</strong></td><td><strong>33.33</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. PolyAI in-house US English ASR solution achieved better accuracy with acoustic model fine-tunings than third-party out-of-the-box ASR</em></figcaption></figure>\n\n\n\n<p>A similar pattern is observed with the UK English ASR solution (Table 2).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>UK English</strong></td><td><strong>Provider</strong></td><td><strong>Model</strong></td><td><strong>Language</strong></td><td><strong>WER Mean [%]</strong></td><td><strong>WER MEdian [%]</strong></td></tr><tr><td>0</td><td>Poly AI</td><td>Fine-Tuned</td><td>En-UK</td><td><strong>20.99</strong></td><td><strong>8.33</strong></td></tr><tr><td>1</td><td>Poly AI</td><td>Fine-TUned</td><td>En-All</td><td>22.77</td><td>10.00</td></tr><tr><td>2</td><td>CSP</td><td>Best</td><td>En-UK</td><td>25.15</td><td>14.29</td></tr><tr><td>9</td><td>CSP</td><td>Worst</td><td>En-UK</td><td><strong>33.46</strong></td><td><strong>25.00</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. PolyAI in-house UK English ASR solution achieved better accuracy with acoustic model fine-tunings than third-party out-of-the-box ASR</em></figcaption></figure>\n\n\n\n<p>Only 20 hours of fine-tuning data demonstrates the potential for further fine-tuning. More importantly, the in-house fine-tuned ASR model kept the same score when evaluated on a variety of different validation datasets as when it was in its original pretrained state.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>For effectively automating customer interactions over the phone, fully customized ASR models play a pivotal role in solving the challenges of the voice channel, including background noise, poor call quality, and various dialects and accents. Dive deeper into PolyAI\u2019s ASR transformative journey and explore the possibilities of speech AI and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a> by checking out <a href=\"https://www.nvidia.com/en-us/events/speech-ai-day/\">Speech AI Day</a> sessions.</p>\n\n\n\n<p>PolyAI, part of <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception</a>, provides a customer-led conversational platform for enterprises. To reimagine customer service with a best-in-class voice experience, see <a href=\"https://poly.ai/call-recordings/\">PolyAI&#8217;s product</a> and <a href=\"https://poly.ai/request-a-demo/\">sign up for a free trial</a>. Join the conversation on Speech AI in the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/deep-learning/riva/475\">NVIDIA Riva forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>At the core of understanding people correctly and having natural conversations is automatic speech recognition (ASR). To make customer-led voice assistants and automate customer service interactions over the phone, companies must solve the unique challenge of gaining a caller\u2019s trust through qualities such as understanding, empathy, and clarity. Telephony-bound voice is inherently challenging from a &hellip; <a href=\"https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/\">Continued</a></p>\n", "protected": false}, "author": 1974, "featured_media": 75790, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1329413", "discourse_permalink": "https://forums.developer.nvidia.com/t/enhancing-phone-customer-service-with-asr-customization/278365", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [1078, 453, 3545, 3166, 106], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jF6", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75584"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1974"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75584"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75584/revisions"}], "predecessor-version": [{"id": 76379, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75584/revisions/76379"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75790"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75584"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75584"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75584"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75967, "date": "2024-01-08T11:00:00", "date_gmt": "2024-01-08T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75967"}, "modified": "2024-01-25T10:17:38", "modified_gmt": "2024-01-25T18:17:38", "slug": "new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/", "title": {"rendered": "New Models MolMIM and DiffDock Power Molecule Generation and Molecular Docking in NVIDIA BioNeMo"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>The search for viable drugs is one of the most formidable challenges at the intersection of science, technology, and medicine.&nbsp; Mathematically, the odds of randomly stumbling across a good therapeutic candidate are staggeringly small. This is owed primarily to the astronomically large number of ways that just a handful of atoms can be connected together to make what appear at first glance to be drug-like compounds.&nbsp;&nbsp;</p>\n\n\n\n<p>Upon deeper inspection of these molecules, the vast majority would make for unsuitable therapeutics. A clinically viable drug must possess a multitude of characteristics or properties, any one of which\u2014if missing or out of range\u2014could render a drug ineffective or even toxic.&nbsp; Examples of properties that drug hunters seek include those that characterize a drug\u2019s binding affinity, solubility, membrane permeability, molecular weight, and stability, just to name a few.&nbsp;</p>\n\n\n\n<p>In essence, the pursuit of drug candidates is a multiple-objective optimization problem.&nbsp;&nbsp;</p>\n\n\n\n<p>Generative AI models like <a href=\"https://www.nvidia.com/en-us/clara/bionemo/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/clara/bionemo/\">NVIDIA BioNeMo</a>\u2019s <a href=\"https://arxiv.org/pdf/2208.09016\" data-type=\"link\" data-id=\"https://arxiv.org/pdf/2208.09016\">MolMIM</a> are designed to directly address the challenge of finding molecules with the right properties. Using MolMIM, researchers can generate molecules that maximize a user-specified scoring function, or oracle function for short. MolMIM performs controlled generation, navigating the learned internal representation of chemical space guided by the oracle function that the user provided.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1346\" height=\"687\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow.png\" alt=\"Diagram shows the workflow for molecule generation from seed embeddings to a molecule optimized for the Oracle function.\" class=\"wp-image-75970\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow.png 1346w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-300x153.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-625x319.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-768x392.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-645x329.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-500x255.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-216x110.png 216w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/molecule-generation-workflow-1024x523.png 1024w\" sizes=\"(max-width: 1346px) 100vw, 1346px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Molecule generation workflow with MolMIM</em></figcaption></figure></div>\n\n\n<p>Researchers can define any objective function they wish, even functions that compute multiple objectives. Under the hood, MolMIM uses a gradient-free numerical optimization algorithm called CMA-ES to navigate the latent space of the model. The generative process is iterative:&nbsp;</p>\n\n\n\n<ul>\n<li>At each iteration, MolMIM generates a batch of molecules.</li>\n\n\n\n<li>Armed with the user-provided oracle function, the properties of the molecules are calculated.</li>\n\n\n\n<li>The latent vectors are then updated using the covariance matrix adaptation evolution strategy (CMA-ES) to generate the next batch of molecules.&nbsp;</li>\n\n\n\n<li>The algorithm continues until convergence.</li>\n</ul>\n\n\n\n<p>The optimized molecular generation workflow is incredibly simple using the BioNeMo Cloud API with BioNeMo\u2019s Python client library. To define an oracle function, all that you have to do is write a Python function that follows a simple call signature. The function should take a list of SMILES and return a NumPy array of scores of the same length. With the oracle function defined, an optimizer can be instantiated from the BioNeMo Python client and molecule generation can begin!&nbsp;&nbsp;</p>\n\n\n\n<p>MolMIM is available in early access through BioNeMo\u2019s Cloud API, which launches January 19, 2024.&nbsp;</p>\n\n\n\n<p>Also coming to BioNeMo\u2019s Cloud API is an update accelerating DiffDock, an AI model that predicts the three-dimensional structure of a protein-ligand complex, a crucial step in the drug discovery process. With BioNeMo\u2019s updated version of DiffDock, researchers can predict the three-dimensional pose of a protein-ligand complex more than 2.5x faster than a baseline implementation on identical hardware.&nbsp;For more information, see <a href=\"https://developer.nvidia.com/blog/build-generative-ai-pipelines-for-drug-discovery-with-bionemo-service/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/build-generative-ai-pipelines-for-drug-discovery-with-bionemo-service/\">Build Generative AI Pipelines for Drug Discovery with NVIDIA BioNeMo Service</a>.</p>\n\n\n\n<p>Sign up for <a href=\"https://www.nvidia.com/en-us/clara/bionemo/\">early access to the NVIDIA BioNeMo Cloud API</a> or get started immediately with <a href=\"https://www.nvidia.com/en-us/clara/bionemo/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/clara/bionemo/\">BioNeMo Framework</a> for model training.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The search for viable drugs is one of the most formidable challenges at the intersection of science, technology, and medicine.&nbsp; Mathematically, the odds of randomly stumbling across a good therapeutic candidate are staggeringly small. This is owed primarily to the astronomically large number of ways that just a handful of atoms can be connected together &hellip; <a href=\"https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/\">Continued</a></p>\n", "protected": false}, "author": 1463, "featured_media": 75968, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1328755", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-nvidia-bionemo/278236", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [2385, 453, 1948], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hc-tech-blog-jpm24-bionemo-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jLh", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75967"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1463"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75967"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75967/revisions"}], "predecessor-version": [{"id": 76319, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75967/revisions/76319"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75968"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75967"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75967"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75967"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76275, "date": "2024-01-08T08:31:00", "date_gmt": "2024-01-08T16:31:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76275"}, "modified": "2024-01-25T10:17:39", "modified_gmt": "2024-01-25T18:17:39", "slug": "new-stable-diffusion-models-accelerated-with-nvidia-tensorrt", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/", "title": {"rendered": "New Stable Diffusion Models Accelerated with NVIDIA TensorRT"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>At CES, NVIDIA shared that SDXL Turbo, LCM-LoRA, and Stable Video Diffusion are all being accelerated by NVIDIA TensorRT. These enhancements allow GeForce RTX GPU owners to generate images in real-time and save minutes generating videos, vastly improving workflows.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/fUAEBoJCJW8?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Accelerate Stable Diffusion with NVIDIA RTX GPUs</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">SDXL Turbo</h2>\n\n\n\n<p><a href=\"https://stability.ai/news/stability-ai-sdxl-turbo\">SDXL Turbo</a> achieves state-of-the-art performance with a new distillation technology, enabling single-step image generation. NVIDIA hardware, accelerated by Tensor Cores and TensorRT, can produce up to four images per second, giving you access to real-time SDXL image generation for the first time ever. For more information about non-commercial and commercial use, see the <a href=\"https://stability.ai/membership#select_membership\">Stability AI Membership</a> page.&nbsp;</p>\n\n\n\n<p><a href=\"https://huggingface.co/stabilityai/sdxl-turbo-tensorrt\">Download the SDXL Turbo model</a> on Hugging Face.</p>\n\n\n\n<h2 class=\"wp-block-heading\">LCM-LoRA</h2>\n\n\n\n<p>Low-Rank Adaptation (LoRA) is a training technique for fine-tuning Stable Diffusion models. Combined with the latent consistency model (LCM), a LoRA checkpoint enables you to drastically reduce the number of sampling steps needed to produce a Stable Diffusion image. This improves speed dramatically at the cost of an image quality hit. <a href=\"https://huggingface.co/docs/diffusers/main/en/using-diffusers/inference_with_lcm_lora\">LCM-LoRA</a> can run ~9x faster because it uses only four steps (compared to 50 steps traditionally) and is accelerated by TensorRT optimizations.&nbsp;</p>\n\n\n\n<p><a href=\"https://huggingface.co/stabilityai/stable-diffusion-xl-1.0-tensorrt\">Download the LCM-LoRA model</a> on Hugging Face.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Stable Video Diffusion</h2>\n\n\n\n<p><a href=\"https://stability.ai/stable-video\">Stable Video Diffusion</a> by Stability AI is their first foundation model for generative video based on the image model Stable Diffusion. Stable Video Diffusion runs up to 40% faster with TensorRT, potentially saving up to minutes per generation. For more information about non-commercial and commercial use, see the <a href=\"https://stability.ai/membership#select_membership\">Stability AI Membership</a> page.&nbsp;</p>\n\n\n\n<p>The Stable Video Diffusion model will be available for download soon.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started with Stable Diffusion</h2>\n\n\n\n<p>To download the Stable Diffusion Web UI TensorRT extension, see the <a href=\"https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT\">NVIDIA/Stable-Diffusion-WebUI-TensorRT</a> GitHub repo. The newly released update to this extension includes TensorRT acceleration for SDXL, SDXL Turbo, and LCM-LoRA.&nbsp;</p>\n\n\n\n<p>For a demo showcasing the acceleration of a Stable Diffusion pipeline, see <a href=\"https://github.com/NVIDIA/TensorRT/tree/release/8.6/demo/Diffusion\">NVIDIA/TensorRT</a>. For more information about the Automatic 1111 TensorRT extension, see <a href=\"https://nvidia.custhelp.com/app/answers/detail/a_id/5487\">TensorRT Extension for Stable Diffusion Web UI</a>.</p>\n\n\n\n<p>Have an idea for a generative AI-powered Windows app or plugin? Enter the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/rtx-developer-contest\">NVIDIA Generative AI on RTX PCs Developer Contest</a> and you could win a GeForce RTX 4090 GPU, a full GTC in-person conference pass, and more.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>At CES, NVIDIA shared that SDXL Turbo, LCM-LoRA, and Stable Video Diffusion are all being accelerated by NVIDIA TensorRT. These enhancements allow GeForce RTX GPU owners to generate images in real-time and save minutes generating videos, vastly improving workflows. SDXL Turbo SDXL Turbo achieves state-of-the-art performance with a new distillation technology, enabling single-step image generation. &hellip; <a href=\"https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/\">Continued</a></p>\n", "protected": false}, "author": 1726, "featured_media": 76289, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1328622", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/278161", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 3110, 1903], "tags": [453, 3257, 1958], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ces-stable-diffusion-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jQf", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76275"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1726"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76275"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76275/revisions"}], "predecessor-version": [{"id": 76332, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76275/revisions/76332"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76289"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76275"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76275"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76275"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76227, "date": "2024-01-08T08:30:00", "date_gmt": "2024-01-08T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76227"}, "modified": "2024-01-25T10:17:39", "modified_gmt": "2024-01-25T18:17:39", "slug": "get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/", "title": {"rendered": "Get Started with Generative AI Development for Windows PCs with NVIDIA RTX"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Generative AI and <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">large language models</a> (LLMs) are changing human-computer interaction as we know it. Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences. This post discusses several NVIDIA end-to-end developer tools for creating and deploying both text-based and visual LLM applications on NVIDIA RTX AI-ready PCs.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Developer tools for building text-based generative AI projects</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/tensorrt#inference\">NVIDIA TensorRT-LLM</a> is an open-source large language model (LLM) inference library. It provides an easy-to-use Python API to define LLMs and build <a href=\"https://developer.nvidia.com/tensorrt\">TensorRT</a> engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. NVIDIA TensorRT-LLM also contains components to create Python and C++ runtimes to run inference with the generated TensorRT engines.</p>\n\n\n\n<p>To get started with TensorRT-LLM, visit the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main\">NVIDIA/TensorRT-LLM</a> GitHub repo. Check out the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/windows\">TensorRT-LLM for Windows</a> developer environment setup details.&nbsp;</p>\n\n\n\n<p>In desktop applications, model quantization is crucial for compatibility with PC GPUs, which often have limited VRAM. TensorRT-LLM facilitates this process through its support for model quantization, enabling models to occupy a smaller memory footprint with the help of the TensorRT-LLM Quantization Toolkit.&nbsp;</p>\n\n\n\n<p>To start exploring post-training quantization using TensorRT-LLM Quantization Toolkit, see the <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/quantization\">TensorRT-LLM Quantization Toolkit Installation Guide</a> on GitHub.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Model compatibility and pre-optimized models</h2>\n\n\n\n<p>TensorRT-LLM provides the capability to define models through its Python API and is pre-equipped to support a diverse range of LLMs. Quantized model weights are available, specifically optimized for NVIDIA RTX PCs on NVIDIA GPU Cloud (NGC), enabling rapid deployment of these models.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Model Name</strong></td><td><strong>Model Location</strong></td></tr><tr><td>Llama 2 7B &#8211; Int4-AWQ</td><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/models/llama2-7b\">Download</a></td></tr><tr><td>Llama 2 13B &#8211; Int4-AWQ</td><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/models/llama2-13b\">Download</a></td></tr><tr><td>Code Llama 13B &#8211; Int4-AWQ</td><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/models/code_llama/files?version=1.1\">Download</a></td></tr><tr><td>Mistral 7B &#8211; Int4-AWQ</td><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/models/mistral-7b-int4-chat\">Download</a></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Pre-optimized text-based LLMs that run on Windows PC for NVIDIA RTX with the NVIDIA TensorRT-LLM backend</em></figcaption></figure>\n\n\n\n<p>You can also build TensorRT engines for a wide variety of models supported by TensorRT-LLM. Visit <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples\">TensorRT-LLM/examples</a> on GitHub to see all supported models.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Developer resources and reference applications</h3>\n\n\n\n<p>Check out these reference projects for more information:</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/NVIDIA/trt-llm-rag-windows/tree/release/1.0\">TRT-LLM RAG on Windows</a>:<strong> </strong>This repository demonstrates a <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG) pipeline, using <code>llama_index</code> on Windows with Llama 2 13B &#8211; int4, TensorRT-LLM, and FAISS.&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/trt-llm-as-openai-windows/tree/release/1.0\">OpenAI API Spec Web Server</a>: Drop-in replacement REST API compatible with OpenAI API spec using TensorRT-LLM as the inference backend.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Minimum system requirements</h3>\n\n\n\n<p>Supported GPU architectures for TensorRT-LLM include<strong> </strong>NVIDIA Ampere and above, with a minimum of 8GB RAM<strong>.</strong> It is suggested to use Windows 11 and above, for an optimal experience<strong>.&nbsp;</strong></p>\n\n\n\n<h2 class=\"wp-block-heading\">Developer tools for building visual generative AI projects</h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> SDK is a high-performance deep learning inference optimizer. It provides layer fusion, precision calibration, kernel auto-tuning, and other capabilities that significantly boost the efficiency and speed of deep learning models. This makes it indispensable for real-time applications and resource-intensive models like Stable Diffusion, substantially accelerating performance. <a href=\"https://developer.nvidia.com/tensorrt-getting-started\">Get started with NVIDIA TensorRT</a>.</p>\n\n\n\n<p>For broader guidance on how to integrate TensorRT into your applications, see <a href=\"https://developer.nvidia.com/accelerate-ai-applications/get-started#tensorrt-resources\">Getting Started with NVIDIA AI for Your Applications.</a> Learn how to profile your pipeline to pinpoint where optimization is critical and where minor changes can have a significant impact. Accelerate your AI pipeline by choosing a machine learning framework, and discover SDKs for video, graphic design, photography, and audio.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Developer demos and reference applications</h3>\n\n\n\n<p>Check out these resources for more information:</p>\n\n\n\n<ul>\n<li><a href=\"https://github.com/NVIDIA/TensorRT/tree/release/8.6/demo/experimental/HuggingFace-Diffusers\">How to Optimize Models like Stable Diffusion with TensorRT</a>: This demo notebook showcases the acceleration of Stable Diffusion inference pipeline using TensorRT through Hugging Face.</li>\n\n\n\n<li><a href=\"https://github.com/huggingface/diffusers/tree/main/examples/community#tensorrt-text2image-stable-diffusion-pipeline\">Example TRT Pipeline for Stable Diffusion</a>: An example of how TensorRT can be used to accelerate the text-to-image Stable Diffusion inference pipeline.</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT\">TensorRT Extension for Stable Diffusion Web UI</a>: A working example of TensorRT accelerating the most popular Stable Diffusion web UI.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Use the resources in this post to easily add generative AI capabilities to applications powered by the existing installed base of 100 million NVIDIA RTX PCs. </p>\n\n\n\n<p>Share what you develop with the NVIDIA developer community by entering the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/rtx-developer-contest\">NVIDIA Generative AI on NVIDIA RTX Developer Contest</a> for a chance to win a <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/\">GeForce RTX 4090</a> GPU, a full in-person <a href=\"https://www.nvidia.com/gtc/\">NVIDIA GTC</a> conference pass, and more.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI and large language models (LLMs) are changing human-computer interaction as we know it. Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences. This post discusses several NVIDIA end-to-end developer tools for creating and deploying both text-based and visual LLM applications on NVIDIA RTX &hellip; <a href=\"https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/\">Continued</a></p>\n", "protected": false}, "author": 1980, "featured_media": 76395, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1328618", "discourse_permalink": "https://forums.developer.nvidia.com/t/get-started-with-generative-ai-development-for-windows-pcs-with-nvidia-rtx/278159", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [296, 453, 570, 367, 3673], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-generative-ai.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jPt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76227"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1980"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76227"}], "version-history": [{"count": 52, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76227/revisions"}], "predecessor-version": [{"id": 76391, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76227/revisions/76391"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76395"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76227"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76227"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76227"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76184, "date": "2024-01-08T08:30:00", "date_gmt": "2024-01-08T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76184"}, "modified": "2024-01-25T10:17:40", "modified_gmt": "2024-01-25T18:17:40", "slug": "spotlight-convai-reinvents-non-playable-character-interactions", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/", "title": {"rendered": "Spotlight: Convai Reinvents Non-Playable Character Interactions"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.convai.com/\">Convai</a> is a versatile developer platform for designing characters with advanced multimodal perception abilities. These characters are designed to integrate seamlessly into both the virtual and real worlds.&nbsp;</p>\n\n\n\n<p>Whether you&#8217;re a creator, game designer, or developer, Convai enables you to quickly modify a non-playable character (NPC), from backstory and knowledge to voice and personality. You can do this through the <a href=\"https://convai.com/\">playground user interface</a> or programmatically through the API. Within minutes, creators can witness their characters responding in their own unique style while embodying a spatially-aware NPC capable of performing a wide range of actions.</p>\n\n\n\n<p>Convai believes in pushing the state of the art and enabling its users to experience the best in gaming. Some of the fundamental blocks for realistic AI NPCs are facial animations, lip synchronization, and emotion control for the character&#8217;s voices, all of which needs to be generated in real time during gameplay.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"751\" height=\"425\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1.png\" alt=\"Graphic of examples of features supported by Convai technologies, including Ask Anything, Digital Humans, and Intelligent NPC.\" class=\"wp-image-76206\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1.png 751w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-300x170.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-625x354.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-645x365.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-500x283.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-362x205.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-features-supported-convai-1-194x110.png 194w\" sizes=\"(max-width: 751px) 100vw, 751px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Examples of features supported by Convai technologies&nbsp;</em></figcaption></figure></div>\n\n\n<p>Convai tapped into <a href=\"https://developer.nvidia.com/ace\">NVIDIA Avatar Cloud Engine</a> (ACE), which delivers AI models and microservices that developers can integrate into their pipelines. They used NVIDIA <a href=\"https://www.nvidia.com/en-us/omniverse/apps/audio2face/\">Audio2Face</a> to power the facial animations of their characters, and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a> for speech-to-text and text-to-speech.&nbsp;</p>\n\n\n\n<p>The team is currently planning to leverage <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a> and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a>, which is part of the<a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\"> NVIDIA AI Enterprise</a> software platform, to develop and deploy its custom LLMs with the lowest possible latency.&nbsp;&nbsp;</p>\n\n\n\n<p>\u201cGenerative AI-powered characters in virtual worlds unlock various use cases and experiences that were previously impossible,\u201d said Convai Founder and CEO Purnendu Mukherjee.&nbsp; \u201cConvai is leveraging NVIDIA ACE technologies such as Riva automatic speech recognition and Audio2Face to enable lifelike non-playable characters with low latency response times and high fidelity natural animation.\u201d</p>\n\n\n\n<p>Convai also developed an extension in <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> that enables users to connect their 3D character assets with intelligent conversation agents. <a href=\"https://developer.nvidia.com/omniverse\">Developers can use this extension</a> to take the backstory and voice they have created in Convai and add it to a customizable 3D character in Omniverse. Once in Omniverse, users can have a conversation with the animated character using their computer\u2019s microphone input.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/EvwCu3m6CQc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Learn how to use the Convai extension for NVIDIA Omniverse</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Heighten immersive interactions with AI-driven Convai characters</h2>\n\n\n\n<p>Traditional NPCs in gaming suffer from many limitations that directly impact the overall player experience. Examples include rigid conversational patterns and a lack of contextual awareness. To address these limitations, Convai has developed a proprietary AI-driven suite of technologies.&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Context-aware responsiveness</h3>\n\n\n\n<p>This innovative approach consists of enabling adaptive and context-aware NPC responsiveness. Characters are equipped with spatial cognition and scene understanding, enabling dynamic comprehension of their environment. Consequently, NPCs perceive situational elements, objects, characters, and their corresponding states in real time.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"762\" height=\"405\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1.png\" alt=\"Visual from the NVIDIA Kairos demo of labeled items in the ramen shop that AI NPCs are aware of.\" class=\"wp-image-76380\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1.png 762w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-300x159.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-625x332.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-645x343.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-500x266.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-362x192.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ramen-shop-with-items-labeled-nvidia-kairos-demo-1-207x110.png 207w\" sizes=\"(max-width: 762px) 100vw, 762px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Environment-aware NPCs </em></figcaption></figure></div>\n\n\n<p>This heightened awareness, alongside understanding human commands through NVIDIA Riva ASR, empowers NPCs to identify and interact with objects, engage in conversations about their surroundings, and perform actions based on their motivation or as prompted by the player&#8217;s conversations.</p>\n\n\n\n<p>Other known issues with traditional NPC interactions often come from rigid scripts and predictable behavior. Convai champions more organic interactions by integrating LLM-driven conversation systems with behavior trees that define the default behaviors of the characters. This leads to more dynamic engagements and intelligent responses, while staying true to the original story and gameplay the designer intended.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/UamcBgWz0i8?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. NVIDIA ACE and Convai are helping developers bring digital avatars to life with generative AI</em></figcaption></figure>\n\n\n\n<p>Characters feel more lifelike if conversations have consequences involving not only responses but also actions. Examples include interacting with the environment or other characters. Convai NPCs can navigate complex verbal instructions and convert them to natural actions in the game engine. Need to grab a snack from the vending machine? No problem\u2014Convai\u2019s complex action sequences enable NPCs to easily handle multistep tasks.&nbsp;</p>\n\n\n\n<p>To get started creating characters that respond dynamically to various actions, check out the <a href=\"https://www.youtube.com/@convai\">Convai tutorials</a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Emotional awareness</h3>\n\n\n\n<p>To make the gaming experience even more immersive, Convai characters can perceive emotions and emote naturally. NPCs are emotionally aware, reflecting their feelings through pertinent facial expressions, voices, and gestures. This adds a layer of realism and immersion, making your NPCs feel genuinely alive as they respond with emotional awareness, which adapts throughout the conversation.&nbsp;</p>\n\n\n\n<p>To learn more, check out the Convai <a href=\"https://www.youtube.com/@convai\">tutorial on facial expressions</a>. You can also apply to <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd7cy7EVlfF-Pb68-sz6Dwix-u4aHV3zbTzLlfLbyevIzyTmQ/viewform\">join the Convai early action program for emotional voices.</a></p>\n\n\n\n<p>NPCs are not limited to interacting with the player\u2014they can also interact with fellow NPCs, making the world come alive with real-time generated dialogue content. Creators can decide the topic and the flow of the conversation between the NPCs, making the conversation pertinent to the world they are crafting, while leaving some flexibility in the storyline for conversations personalized to the player\u2019s experience. For more details, check out the Convai <a href=\"https://www.youtube.com/@convai\">tutorial video</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Convai\u2019s forward-thinking features and approaches transform NPC interactions and elevate gaming with nuanced storytelling, diverse personalities, emotional responsiveness, and contextual accuracy. Currently, more than 15K people have signed up to use Convai. The latest Convai technology also opens the door for multiple emerging use cases, such as building brand representatives and tutors. NVIDIA showcased Convai and the latest <a href=\"https://developer.nvidia.com/ace\">NVIDIA ACE</a> technologies in the <a href=\"https://www.youtube.com/watch?v=UamcBgWz0i8\">NVIDIA Kairos demo</a> that debuted at <a href=\"https://www.ces.tech/\">CES 2024</a>.&nbsp;</p>\n\n\n\n<p>Learn more about <a href=\"https://www.convai.com/\">Convai</a> technologies.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Convai is a versatile developer platform for designing characters with advanced multimodal perception abilities. These characters are designed to integrate seamlessly into both the virtual and real worlds.&nbsp; Whether you&#8217;re a creator, game designer, or developer, Convai enables you to quickly modify a non-playable character (NPC), from backstory and knowledge to voice and personality. You &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/\">Continued</a></p>\n", "protected": false}, "author": 1930, "featured_media": 76199, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1328616", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-convai-reinvents-non-playable-character-interactions/278157", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 3110], "tags": [3063, 453, 1409, 2379, 3166], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/two-characters-in-front-of-buildings-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jOM", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76184"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1930"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76184"}], "version-history": [{"count": 45, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76184/revisions"}], "predecessor-version": [{"id": 77064, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76184/revisions/77064"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76199"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76184"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76184"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76184"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76174, "date": "2024-01-08T08:30:00", "date_gmt": "2024-01-08T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76174"}, "modified": "2024-01-25T10:17:40", "modified_gmt": "2024-01-25T18:17:40", "slug": "supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/", "title": {"rendered": "Supercharging LLM Applications on Windows PCs with NVIDIA RTX Systems"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Large language models (LLMs) are fundamentally changing the way we interact with computers. These models are being incorporated into a wide range of applications, from internet search to office productivity tools. They are advancing real-time content generation, text summarization, customer service chatbots, and question-answering use cases.&nbsp;</p>\n\n\n\n<p>Today, LLM-powered applications are running predominantly in the cloud. However, many use cases that would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.&nbsp;</p>\n\n\n\n<p>AT CES 2024, NVIDIA announced several developer tools to accelerate LLM inference and development on NVIDIA RTX Systems for Windows PCs. You can now use NVIDIA end-to-end developer tools to create and deploy LLM applications on NVIDIA RTX AI-ready PCs.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Support for community models and native connectors</h2>\n\n\n\n<p>NVIDIA just announced optimized support for popular community models, including <a href=\"https://huggingface.co/microsoft/phi-2\">Phi-2</a>, in addition to existing support for Llama2, Mistral-7B, and Code Llama on NVIDIA RTX systems. These models provide extensive developer choice, along with best-in-class performance using the <a href=\"https://blogs.nvidia.com/blog/tensorrt-llm-windows-stable-diffusion-rtx/\">NVIDIA TensorRT-LLM inference backend</a>.\u00a0</p>\n\n\n\n<p>NVIDIA collaborated with the open-source community to develop native connectors for TensorRT-LLM to popular application frameworks such as <a rel=\"noreferrer noopener\" href=\"https://www.llamaindex.ai/\" target=\"_blank\">LlamaIndex</a>. These connectors offer seamless integration on Windows PCs to commonly used application development tools. View the example for the LlamaIndex\u00a0 <a rel=\"noreferrer noopener\" href=\"https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_tensorrt.html\" target=\"_blank\">implementation of the connector here</a>.</p>\n\n\n\n<p>We\u2019ve also developed an OpenAI Chat API wrapper for TensorRT-LLM so that you can easily switch between running LLM applications on the cloud or on local Windows PCs by just changing one line of code. Now, you can use a similar workflow with the same popular community frameworks, whether they are designing applications in the cloud or on a local PC with NVIDIA RTX.&nbsp;</p>\n\n\n\n<p>These latest advancements can now all be accessed through two recently launched open-source developer reference applications:</p>\n\n\n\n<ul>\n<li>A <a href=\"https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/\">retrieval augmented generation</a> (RAG) project running entirely on Windows PC with an NVIDIA RTX GPU and using TensorRT-LLM and LlamaIndex.</li>\n\n\n\n<li>A reference project that runs the popular <a href=\"https://continue.dev/\">continue.dev</a> plugin entirely on a local Windows PC, with a web server for OpenAI Chat API compatibility.&nbsp;</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">RAG on Windows using TensorRT-LLM and LlamaIndex</h3>\n\n\n\n<p>The RAG pipeline consists of the Llama-2 13B model, TensorRT-LLM, LlamaIndex, and the FAISS vector search library. You can now easily talk to your data with this reference application. Figure 1 shows a dataset that consists of NVIDIA GeForce News.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/trt-llm-rag-windows/tree/release/1.0/media\">Get started with this application now.</a></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"3316\" height=\"1718\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/rag-demo-2.gif\" alt=\"video showing the retrieval augmented generation project dashboard\" class=\"wp-image-76464\"/><figcaption class=\"wp-element-caption\"><em>Video 1. A retrieval augmented generation reference application running entirely on Windows PC with an NVIDIA RTX System</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Continue.dev Visual Studio Code extension on PC with CodeLlama-13B</h3>\n\n\n\n<p>Originally, the continue.dev plugin was designed to provide LLM-powered code assistance using ChatGPT in the cloud. It works natively with the Visual Studio Code integrated development environment. Using the OpenAI Chat API wrapper for TensorRT-LLM, with just one line of code change, this plugin now uses a Code Llama-13B model running locally on an NVIDIA RTX-enabled PC. This offers an easy path for fast, local LLM inferencing.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/trt-llm-as-openai-windows\">Try this reference project on GitHub now</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Benefits of running LLMs locally</h2>\n\n\n\n<p>Running LLMs locally on PCs offers several advantages:</p>\n\n\n\n<ul>\n<li><strong>Cost:</strong> No cloud-hosted API or infrastructure costs for LLM inference. Directly access your compute resources.</li>\n\n\n\n<li><strong>Always-on:</strong> Availability of LLM capabilities everywhere you go, without relying on high-bandwidth network connectivity.</li>\n\n\n\n<li><strong>Performance:</strong> Latency is independent of network quality, offering lower latency as the entire model is running locally. This can be important for real-time use cases such as gaming or video conferencing. NVIDIA RTX offers the fastest PC accelerator with up to 1300 TOPS.</li>\n\n\n\n<li><strong>Data privacy:</strong> Private and proprietary data can always stay on the device.</li>\n</ul>\n\n\n\n<p>With over 100M systems shipped, NVIDIA RTX offers a large installed base of users for new LLM-powered applications.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Developer workflows for LLMs on NVIDIA RTX</h2>\n\n\n\n<p>You can now seamlessly run LLMs on NVIDIA RTX AI-ready PCs with the following options:</p>\n\n\n\n<ul>\n<li>Access pre-optimized models on HuggingFace, <a href=\"https://ngc.nvidia.com/\">NGC</a>, and<a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\"> NVIDIA AI Foundations</a>.</li>\n\n\n\n<li>Train or customize models on custom data in <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/?ncid=pa-srch-goog-406223&amp;gad_source=1&amp;gclid=EAIaIQobChMI-oXtmty1gwMV7jutBh0b4QJWEAAYASAAEgKLZ_D_BwE#cid=dl34_pa-srch-goog_en-us&amp;_bt=663061981833&amp;_bk=dgx%20cloud&amp;_bm=b&amp;_bn=g&amp;_bg=150037786306\">NVIDIA DGX Cloud</a> with <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo Framework</a>.</li>\n\n\n\n<li>Quantize and optimize the models for best performance on NVIDIA RTX with TensorRT-LLM.</li>\n</ul>\n\n\n\n<p>This workflow is powered by the NVIDIA AI platform, alongside popular development tools such as <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">NVIDIA AI Workbench</a> to seamlessly migrate between cloud and PC.&nbsp;</p>\n\n\n\n<p>AI Workbench provides you with the flexibility to collaborate on and migrate generative AI projects between GPU-enabled environments in just a few clicks. Projects can start locally on a PC or workstation and can then be scaled out anywhere for training: data center, public cloud, or NVIDIA DGX Cloud. You can then bring models back to a local NVIDIA RTX system for inference and lightweight customization with TensorRT-LLM.&nbsp;</p>\n\n\n\n<p>AI Workbench will be released as a beta later this month.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started</h2>\n\n\n\n<p>With the latest updates, you can now use popular community models and frameworks in the same workflow to build applications that run either in the cloud or locally on Windows PC with NVIDIA RTX. Easily add LLM capabilities to applications powered by the existing 100M installed base of NVIDIA RTX PCs.</p>\n\n\n\n<p>For more information about developing LLM-based applications and projects now, see <a href=\"https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems\">Get Started with Generative AI Development on Windows PC with NVIDIA RTX Systems</a>.&nbsp;</p>\n\n\n\n<p>Have an idea for a generative AI-powered Windows app or plugin? Enter the <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/rtx-developer-contest\">NVIDIA Generative AI on NVIDIA RTX developer contest</a> and you could win a GeForce RTX 4090 GPU, a full GTC in-person conference pass, and more.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) are fundamentally changing the way we interact with computers. These models are being incorporated into a wide range of applications, from internet search to office productivity tools. They are advancing real-time content generation, text summarization, customer service chatbots, and question-answering use cases.&nbsp; Today, LLM-powered applications are running predominantly in the cloud. &hellip; <a href=\"https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/\">Continued</a></p>\n", "protected": false}, "author": 1475, "featured_media": 76216, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1328617", "discourse_permalink": "https://forums.developer.nvidia.com/t/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/278158", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [296, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/genai-blog-2936009-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jOC", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76174"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1475"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76174"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76174/revisions"}], "predecessor-version": [{"id": 76467, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76174/revisions/76467"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76216"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76174"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76174"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76174"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76147, "date": "2024-01-08T08:30:00", "date_gmt": "2024-01-08T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76147"}, "modified": "2024-01-25T10:17:41", "modified_gmt": "2024-01-25T18:17:41", "slug": "building-lifelike-digital-avatars-with-nvidia-ace-microservices", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/", "title": {"rendered": "Building Lifelike Digital Avatars with NVIDIA ACE Microservices"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">Generative AI</a> technologies are revolutionizing how games are produced and played. Game developers are exploring how these technologies can accelerate their content pipelines and provide new gameplay experiences previously thought impossible. One area of focus, digital avatars, will have a transformative impact on how gamers will interact with non-playable characters (NPCs).</p>\n\n\n\n<p>Historically, NPCs have predetermined responses and facial animations, where players can only communicate within a limited set of options. These player interactions tend to be transactional, short-lived, and oftentimes skipped.&nbsp;</p>\n\n\n\n<p>However, with <a href=\"https://developer.nvidia.com/ace\">NVIDIA Avatar Cloud Engine (ACE)</a>, middleware, tool, and game developers can take four state-of-the-art AI models and implement them into an end-to-end digital avatar solution. The ACE models use a flexible combination of local and cloud resources that transform gamer input into a dynamic character response. The models include the following:</p>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva Automatic Speech Recognition</a><strong> </strong>(Riva ASR)<strong> </strong>for transcribing human speech.</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html\">NVIDIA Riva Text-to-Speech</a> (Riva TTS) to generate audible speech.</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/omniverse/apps/audio2face/\">NVIDIA Audio2Face (A2F)</a><strong> </strong>to generate facial expressions and lip movements.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/nemo-llm-service-early-access\">NVIDIA NeMo Large Language Model (NeMo LLM)</a><strong> </strong>to understand player text and transcribed voice and generate a response.</li>\n</ul>\n\n\n\n<p>NVIDIA announced that the A2F and Riva ASR microservices are now available for middleware, tool, and game developers looking to enhance game studio NPCs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1000\" height=\"800\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr.jpg\" alt=\"Announcement slide for Audio2Face and Riva ASR with simplified diagrams showing how audio input is transformed into a visual character speaking and transcribed text, respectively.\" class=\"wp-image-76160\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr.jpg 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-300x240.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-625x500.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-144x115.jpg 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-768x614.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-645x516.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-375x300.jpg 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-113x90.jpg 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-362x290.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ace-announcement-audio2face-riva-asr-138x110.jpg 138w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. New NVIDIA ACE microservices</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Explore microservices through NVIDIA AI Foundry</h2>\n\n\n\n<p>If you have an NVIDIA AI Enterprise license, you can access the microservices now and then deploy them on DGX Cloud or any CSP or private cloud.&nbsp;</p>\n\n\n\n<p>The A2F microservice now has emotional support and quality improvements including lip sync. NVIDIA Riva ASR supports more languages than ever\u2014Italian, EU Spanish, German, and Mandarin\u2014with the overall accuracy being much improved.</p>\n\n\n\n<p>Later this month, you can go to <a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA AI Foundation Models</a> to explore, experience, and evaluate these available AI models directly from a browser or through API endpoints running a fully accelerated stack. You can deploy anywhere with NVIDIA AI Enterprise.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Kairos demo evolves with new technologies from Convai</h2>\n\n\n\n<p>In collaboration with <a href=\"https://www.convai.com/\">Convai</a>, NVIDIA showed the latest version of the Kairos demo to showcase how next-generation AI NPCs will revolutionize gaming. Convai is an NPC developer platform that makes it easy for you to enable characters in 3D worlds to have human-like conversation, perception, and action abilities.</p>\n\n\n\n<p>\u201cGenerative-AI-powered characters in virtual worlds unlock various use cases and experiences that were previously impossible. Convai is leveraging <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/#:~:text=Try%20NVIDIA%20Riva%20Automatic%20Speech%20Recognition\">Riva ASR</a> and A2F to enable lifelike non-playable characters (NPC) with low latency response times and high fidelity natural animation,\u201d said Purnendu Mukherjee, founder and CEO at Convai.</p>\n\n\n\n<p>Open-ended conversations with NPCs open up a world of possibilities for interactivity in games. However, conversations should have consequences that could lead to potential actions. To carry out actions from NPCs, they must be aware of the world around them and be able to interact dynamically.&nbsp;</p>\n\n\n\n<p>With our partner Convai and their latest releases, we take our collaborative demo to the next level, enabling these AI NPCs with the following new features:</p>\n\n\n\n<ul>\n<li><strong>Spatial awareness:</strong> Enables game characters to interact and describe the world throughout conversations.&nbsp;</li>\n\n\n\n<li><strong>Actions:</strong> Enables game characters to interact with items in the game world based on the conversation, for example, delivering a bottle of sake when requested.</li>\n\n\n\n<li><strong>NPC-to-NPC Interaction:</strong> Enables game characters to have generated conversations without the player\u2019s interaction.&nbsp;</li>\n</ul>\n\n\n\n<p>Convai has integrated the new NVIDIA ACE microservices, Audio2Face and Riva ASR. Game characters now get improved lip sync, better expression, and accurate speech detection when listening to the player.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1645\" height=\"564\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai.png\" alt=\"Diagram shows Convai character API workflow with NVIDIA microservices and the Convai Universal Engine plugin.\" class=\"wp-image-76161\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai.png 1645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-300x103.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-625x214.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-179x61.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-768x263.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-1536x527.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-645x221.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-500x171.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-362x124.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-321x110.png 321w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-pipeline-with-convai-1024x351.png 1024w\" sizes=\"(max-width: 1645px) 100vw, 1645px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA ACE in the Convai pipeline</em></figcaption></figure></div>\n\n\n<p>Here\u2019s a look at the new Kairos demo.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-rich is-provider-embed-handler wp-block-embed-embed-handler wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/UamcBgWz0i8?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Kairos Demo</em></figcaption></figure>\n\n\n\n<p>Convai is working closely with NVIDIA to deliver the next generation of AI-powered digital characters. For more information about getting started with their platform, see&nbsp;<a href=\"https://docs.convai.com/api-docs/convai-playground/playground-walkthrough\">Playground Walkthrough</a>.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Top digital avatar developers embrace NVIDIA ACE</h2>\n\n\n\n<p>NVIDIA is working with top developers in the gaming ecosystem to create digital avatars that use ACE technologies, including <a href=\"https://charisma.ai/\">Charisma.AI</a>, <a href=\"https://inworld.ai/\">Inworld</a>, <a href=\"https://www.mihoyo.com/en/\">miHoYo</a>, <a href=\"https://www.neteasegames.com/\">NetEase Games</a>, <a href=\"https://youtu.be/PSg-4HmLvfI\">OurPalm</a>, <a href=\"https://www.tencent.com/\">Tencent</a>, <a href=\"https://www.ubisoft.com/en-us/\">Ubisoft</a>, and <a href=\"https://www.digitalhumans.com/\">UneeQ</a>.</p>\n\n\n\n<p>\u201cThis is a milestone moment for AI in games,&#8221; said Tencent Games.&nbsp;\u201cNVIDIA ACE and Tencent Games will help lay the foundation that will bring digital avatars with individual, lifelike personalities and interactions to video games.\u201d</p>\n\n\n\n<p>\u201cFor years NVIDIA has been the pied piper of gaming technologies, delivering new and innovative ways to create games. NVIDIA is making games more intelligent and playable through the adoption of gaming AI technologies, which ultimately creates a more immersive experience,\u201d said Zhipeng Hu, senior vice president of NetEase and head of the LeiHuo business group.</p>\n\n\n\n<p>Learn more about the <a href=\"https://www.nvidia.com/en-us/omniverse/apps/audio2face/\">NVIDIA Audio2Face</a> and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva ASR</a> microservices, explore the technologies on <a href=\"https://catalog.ngc.nvidia.com/ai-foundation-models\">NVIDIA AI Foundation Models</a>, and deploy anywhere with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> to begin integrating intelligent NPCs into your solutions today.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI technologies are revolutionizing how games are produced and played. Game developers are exploring how these technologies can accelerate their content pipelines and provide new gameplay experiences previously thought impossible. One area of focus, digital avatars, will have a transformative impact on how gamers will interact with non-playable characters (NPCs). Historically, NPCs have predetermined &hellip; <a href=\"https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/\">Continued</a></p>\n", "protected": false}, "author": 1402, "featured_media": 76164, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1328619", "discourse_permalink": "https://forums.developer.nvidia.com/t/building-lifelike-digital-avatars-with-nvidia-ace-microservices/278160", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 1050, 3110, 1903], "tags": [3544, 1078, 3065, 453, 1958, 106], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ces-ace-microservices-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jOb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76147"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1402"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76147"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76147/revisions"}], "predecessor-version": [{"id": 76580, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76147/revisions/76580"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76164"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76147"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76147"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76147"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 76141, "date": "2024-01-08T08:30:00", "date_gmt": "2024-01-08T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=76141"}, "modified": "2024-01-11T11:49:32", "modified_gmt": "2024-01-11T19:49:32", "slug": "contest-build-generative-ai-on-nvidia-rtx-pcs", "status": "publish", "type": "post", "link": "https://nvda.ws/3ROt4KC", "title": {"rendered": "Contest: Build Generative AI on NVIDIA RTX PCs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>NVIDIA is announcing the Generative AI on RTX PCs Developer Contest &#8211; designed to inspire innovation within the developer community. Build and submit your next innovative generative AI projects on Windows PC with RTX Systems, and you could win an RTX 4090 GPU, a full GTC in-person conference pass, and more in great prizes.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA is announcing the Generative AI on RTX PCs Developer Contest &#8211; designed to inspire innovation within the developer community. Build and submit your next innovative generative AI projects on Windows PC with RTX Systems, and you could win an RTX 4090 GPU, a full GTC in-person conference pass, and more in great prizes.</p>\n", "protected": false}, "author": 1475, "featured_media": 76144, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3ROt4KC", "_links_to_target": "_blank"}, "categories": [1235, 1050, 3110, 1903], "tags": [296, 453, 2932, 1958], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-on-rtx-owned-web-module-bb580_440-l-copy.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jO5", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76141"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1475"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76141"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76141/revisions"}], "predecessor-version": [{"id": 76146, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76141/revisions/76146"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/76144"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76141"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76141"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76141"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75534, "date": "2024-01-05T14:14:41", "date_gmt": "2024-01-05T22:14:41", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75534"}, "modified": "2024-01-11T11:49:33", "modified_gmt": "2024-01-11T19:49:33", "slug": "improving-cuda-initialization-times-using-cgroups-in-certain-scenarios", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/", "title": {"rendered": "Improving CUDA Initialization Times Using cgroups in Certain Scenarios"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.&nbsp;</p>\n\n\n\n<p>This post discusses the various methods to accomplish this and their performance benefits.</p>\n\n\n\n<h2 class=\"wp-block-heading\">GPU isolation</h2>\n\n\n\n<p>GPU isolation can be achieved on Linux systems by using Linux tools like <code>cgroups</code>. In this section, we first discuss a lower-level approach and then a higher-level possible approach.</p>\n\n\n\n<p>Another method exposed by CUDA to isolate devices is the use of <code>CUDA_VISIBLE_DEVICES</code>. Although functionally similar, this approach has limited initialization performance gains compared to the <code>cgroups</code> approach.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Isolating GPUs using cgroups V1</h3>\n\n\n\n<p>Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use <code>cgroups</code> to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it.</p>\n\n\n\n<p>The following code provides a low-level example of how to employ <code>cgroups</code> and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n# Create a mountpoint for the cgroup hierarchy as root\n$&gt; cd /mnt\n$&gt; mkdir cgroupV1Device\n\n# Use mount command to mount the hierarchy and attach the device subsystem to it\n$&gt; mount -t cgroup -o devices devices cgroupV1Device\n$&gt; cd cgroupV1Device\n# Now create a gpu subgroup directory to restrict/allow GPU access\n$&gt; mkdir gpugroup\n$&gt; cd gpugroup\n# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\n$&gt; ls gpugroup\ntasks      devices.deny     devices.allow\n\n# Launch a shell from where the CUDA process will be executed. Gets the shells PID\n$&gt; echo $$\n\n# Write this PID into the tasks files in the gpugroups folder\n$&gt; echo &lt;PID&gt; tasks\n\n# List the device numbers of nvidia devices with the ls command\n$&gt; ls -l /dev/nvidia*\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\n\n# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\n$&gt; echo 'c 195:1 rmw' &gt; devices.deny\n\n# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\n# To provide the CUDA process access to GPU1, we should write the following to devices.allow\n\n$&gt; echo 'c 195:1 rmw' &gt; devices.allow\n</pre></div>\n\n\n<p>When you are done with the tasks, unmount the <code>/cgroupV1Device</code> folder with the umount command.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\numount /mnt/cgroupV1Device\n</pre></div>\n\n\n<p>To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here\u2019s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.</p>\n\n\n\n<p>In the <code>/gpugroup</code> folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the <code>tasks</code> file:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n$&gt; echo &lt;PID&gt; tasks\n</pre></div>\n\n\n<p>Now add GPU5 and GPU6 to the denied list:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n$&gt; echo 'c 195:5 rmw' &gt; devices.deny\n$&gt; echo 'c 195:6 rmw' &gt; devices.deny\n</pre></div>\n\n\n<p>At this point, the CUDA process can\u2019t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the <code>devices.allow</code> file and the rest of the GPUs should be added to the <code>devices.deny</code> file.&nbsp;</p>\n\n\n\n<p>The access controls apply per process. Multiple processes can be added to the <code>tasks</code> file to propagate the same controls to more than one process.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Isolating GPUs using the bubblewrap utility</h3>\n\n\n\n<p>The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n# install bubblewrap utility on Debian-like systems\n$&gt;sudo apt-get install -y bubblewrap\n\n# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\n\n#!/bin/sh\n# bwrap.sh\nGPU=$1;shift   # 0, 1, 2, 3, ..\nif &#91; &quot;$GPU&quot; = &quot;&quot; ]; then echo &quot;missing arg: gpu id&quot;; exit 1; fi\nbwrap \\\n        --bind / / \\\n        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\n        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\n        &quot;$@&quot;\n\n\n# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\n$&gt; ./bwrap.sh 0 ./test_cuda_app &lt;args&gt;\n</pre></div>\n\n\n<p>More than one GPU can be exposed to a CUDA process by extending the <code>dev-bind</code> option in the code example.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Performance benefits of GPU isolation&nbsp;</h2>\n\n\n\n<p>In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance.png\" alt=\"Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\" class=\"wp-image-75547\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. CUDA initialisation performance comparison between a </em>cgroup<em>-constrained process and the default scenario on a four-GPU test system</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>GPU isolation using <code>cgroups</code> offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul>\n<li><a href=\"https://docs.kernel.org/admin-guide/cgroup-v1/index.html\">Control Groups version 1 \u2014 The Linux Kernel documentation</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html\">cuInit</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such &hellip; <a href=\"https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/\">Continued</a></p>\n", "protected": false}, "author": 1968, "featured_media": 75549, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1327681", "discourse_permalink": "https://forums.developer.nvidia.com/t/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/277990", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 503], "tags": [453, 126, 1914, 796], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jEi", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75534"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1968"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75534"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75534/revisions"}], "predecessor-version": [{"id": 76259, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75534/revisions/76259"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75549"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75534"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75534"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75534"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75817, "date": "2024-01-05T11:23:39", "date_gmt": "2024-01-05T19:23:39", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75817"}, "modified": "2024-01-11T11:49:34", "modified_gmt": "2024-01-11T19:49:34", "slug": "develop-ml-ai-with-metaflow-deploy-with-triton-inference-server", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/", "title": {"rendered": "Develop ML and AI with Metaflow and Deploy with NVIDIA Triton Inference Server"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>There are many ways to deploy ML models to production. Sometimes, a model is run once per day to refresh forecasts in a database. Sometimes, it powers a small-scale but critical decision-making dashboard or speech-to-text on a mobile device. These days, the model can also be a custom large language model (LLM) backing a novel AI-driven product experience.</p>\n\n\n\n<p>Often, the model is exposed to its environment through an API endpoint with microservices, enabling the model to be queried in real time. While this may sound straightforward, as there are plenty of frameworks for building and deploying microservices in general, serving models in a serious production setting is surprisingly non-trivial.</p>\n\n\n\n<p>Consider the following typical challenges (Table 1).</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Model training</strong></td><td>Can you train and deploy models continuously without human intervention?&nbsp;</td><td>Can you develop new models easily, test deployments locally, and experiment confidently?</td><td>Can you manage features during development and deployment consistently?</td></tr><tr><td><strong>Model deployment</strong></td><td>Can the deployment handle various types of models you want to use?</td><td>Can the model produce responses quickly enough to support the desired product experience?</td><td>What happens if the responses are bad? Can you trace the lineage and the model that produced them?</td></tr><tr><td rowspan=\"3\"><strong>Infrastructure</strong></td><td>Is the deployment highly available enough to support the SLA target?</td><td>Does the deployment use hardware resources efficiently?</td><td>Can you monitor the requests and responses easily?</td></tr><tr><td>Does the deployment integrate with your existing infrastructure and policies?</td><td>Can you deploy models in a cost-efficient manner?</td><td>Can the deployment scale to enough requests per second?</td></tr><tr><td>Does the infrastructure provide secure access to sensitive datasets and models?</td><td>Can infrastructure scale to meet compute needs? Does cost go to zero when it isn\u2019t being used?&nbsp;</td><td>Can users tune performance knobs, such as GPU card types and amounts, to reduce TCO?&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Challenges of model serving in a production environment</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">From prototype to production</h2>\n\n\n\n<p>To address these challenges holistically, consider the full lifecycle of an ML system from the early stages of development to the deployment (and back).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"2400\" height=\"1260\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1.png\" alt=\"Diagram lists steps starting with develop locally, explore with notebooks, create a workflow, version everything, scale vertically, scale horizontally, access data quickly, schedule execution, package model and libraries, monitor model, and deploy endpoint. The debug step circles back to the prototype starting point.\" class=\"wp-image-75938\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1.png 2400w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-625x328.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-768x403.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-1536x806.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-2048x1075.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-645x339.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-210x110.png 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/feedback-loop-prototype-production-1-1024x538.png 1024w\" sizes=\"(max-width: 2400px) 100vw, 2400px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The end-to-end feedback loop between prototype and production</em></figcaption></figure></div>\n\n\n<p>While you could cover the journey by adopting a separate tool for each step, a smoother developer experience and a faster time to deployment can be achieved by providing a consistent API that connects the dots.&nbsp;</p>\n\n\n\n<p>With this vision in mind, Netflix started developing a Python library called Metaflow in 2017, which was open-sourced in 2019. Since then, it has been adopted by thousands of leading ML and AI organizations across industries from real estate and drones to gaming and healthcare.</p>\n\n\n\n<p>Metaflow covers all the concerns in the first part of the journey: how to develop production-grade reactive ML workflows, access data and train models easily at scale, and keep track of all the work comprehensively.</p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/EgNpvB7SHVw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Triggering a Metaflow flow based on an external event</em></figcaption></figure>\n\n\n\n<p>Today, you can adopt Metaflow as open source, or have it deployed in your cloud account with <a href=\"https://outerbounds.com\">Outerbounds</a>, a fully managed ML and AI platform, which layers additional security, scalability, and developer productivity features on top of the open-source package.</p>\n\n\n\n<p>With Metaflow, you can address the first three challenges related to developing and producing models. To deploy the models for real-time inference, you need a model serving stack. That\u2019s where <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> comes into play.</p>\n\n\n\n<p>NVIDIA Triton Inference Server is an open-source model serving framework developed by NVIDIA. It supports a wide variety of models, which it can handle efficiently both on CPUs and GPUs.&nbsp;</p>\n\n\n\n<p>Outerbounds and NVIDIA are collaborating to make the NVIDIA inference stack more easily accessible for a wide variety of ML and AI use cases. The combination of the two open-source frameworks enables you to develop machine learning and AI-powered models and systems quickly and deploy them as high-performance, production-grade services.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Deploy on NVIDIA Triton Inference Server</h2>\n\n\n\n<p>To support production AI at an enterprise level, NVIDIA Triton Inference Server is included in the NVIDIA AI Enterprise software platform, which offers enterprise-grade security, support and stability.</p>\n\n\n\n<p>While a number of model serving frameworks are available, both as open-source and as managed services, NVIDIA Triton Inference Server stands out for several reasons:</p>\n\n\n\n<ul>\n<li>It is highly performant, thanks to it being implemented in C++ and capable of using GPUs efficiently. This makes it a great choice for latency\u2013 and throughput-sensitive applications.</li>\n\n\n\n<li>It is versatile, capable of handling many different model families thanks to its pluggable backends.</li>\n\n\n\n<li>It is tested and tempered, thanks to years of development and large-scale usage driven by NVIDIA.</li>\n</ul>\n\n\n\n<p>These features make NVIDIA Triton Inference Server a particularly capable model serving stack to which workflows can push trained models.</p>\n\n\n\n<p>Metaflow helps you prototype models and the workflows around them and test them at scale, while keeping track of all the work performed. When the workflow shows enough promise, it is straightforward to integrate it to surrounding software systems and orchestrate it reliably in production.</p>\n\n\n\n<p>While Metaflow exposes all the necessary functionality through a simple, developer-focused Python API, this is powered by a thick stack of infrastructure. The stack integrates with data stores like Amazon S3, facilitates large-scale compute on Kubernetes, and uses production-grade workflow orchestrators like Argo Workflows.</p>\n\n\n\n<p>When models have been trained successfully, the responsibility shifts to NVIDIA Triton Inference Server. Similar to the training infrastructure, the inference side requires a surprisingly non-trivial stack of infrastructure.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Infrastructure for model serving</h2>\n\n\n\n<p>It is not too hard to implement a simple service that exposes a simple model, such as a logistic regression model, over HTTPS. A basic version of something like this is doable in a few hundred lines of Python using a framework like FastAPI.</p>\n\n\n\n<p>However, an improvised model serving solution like this is not particularly performant. Python is an expressive language but it does not excel at processing requests quickly. It is not scalable without extra infrastructure: A single FastAPI process can handle only so many requests per second. Also, the solution is not versatile out of the box, if you want to replace the logistic regression model with a more sophisticated deep regression model, for example.</p>\n\n\n\n<p>You could try to address these shortcomings incrementally. But as the solution grows more complex, so does the surface area of bugs, security issues, and other glitches, which is motivation for a more tempered solution.</p>\n\n\n\n<p>NVIDIA Triton Inference Server addresses these challenges by deconstructing the stack into the following key components.</p>\n\n\n\n<ul>\n<li>A <strong>frontend</strong> that is responsible for receiving requests over HTTP or gRPC and routing them to the backend.&nbsp;</li>\n\n\n\n<li>One or more <strong>backend</strong> layers that are responsible for interacting with a particular model family.&nbsp;</li>\n</ul>\n\n\n\n<p>NVIDIA Triton Inference Server supports pluggable backends, implementations of which exist for ONNX, Python-native models, tree-based models, LLMs, and a number of other model types. This makes it possible to handle versatile models with one stack.</p>\n\n\n\n<p>The combination of a high-performance frontend, capable of handling tens of thousands of requests per second, and a backend optimized for a certain model type, provides a low-latency path from request to response. With NVIDIA Triton Inference Server, the whole request-handling path can stay in native code, reducing request latency and increasing throughput relative to a Python-based model serving solution.&nbsp;</p>\n\n\n\n<p>Low-level server optimizations are important for applications that leverage custom LLMs, which require serving large deep learning models with low-latency inferences. To further optimize LLM inference, NVIDIA has introduced <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">TensorRT-LLM</a>. TensorRT-LLM is an SDK that makes it significantly easier for Python developers to build production-quality LLM servers. TensorRT-LLM works out-of-the-box as a backend with NVIDIA Triton Inference Server.</p>\n\n\n\n<p>No matter the scale or server backend, an individual NVIDIA Triton Inference Server instance is typically deployed with a container orchestrator like Kubernetes. A separate layer, a deployment orchestrator, is needed to manage the instances, autoscale the cluster on demand, manage model lifecycle, request routing, and other infrastructure concerns, listed in the last two rows of Table 1.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Integrating the training and the serving stacks</h2>\n\n\n\n<p>While both training and serving require infrastructure stacks of their own, you want to align them tightly for a few reasons.&nbsp;</p>\n\n\n\n<p>First, deploying a model should be a routine operation, which shouldn\u2019t require many extra lines of code or worse, manual effort.</p>\n\n\n\n<p>Second, you want to maintain a full lineage about deployed models, so you can understand the whole chain from raw data and preprocessing to trained models and finally deployed models producing real-time inferences. This feature comes in handy when dealing with A/B experiments, deploying hundreds of parallel models, or debugging responses from an endpoint.</p>\n\n\n\n<p>In the workflow guiding this post, you deploy models to NVIDIA Triton Inference Server in a manner that carries the version information across the stacks. This way, you can backtrack inferences all the way to the raw data.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"2400\" height=\"1260\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage.png\" alt=\"Workflow diagram shows that each stage has a separate ID for tracking.\" class=\"wp-image-75936\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage.png 2400w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-625x328.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-768x403.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-1536x806.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-2048x1075.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-645x339.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-210x110.png 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/end-to-end-lineage-1024x538.png 1024w\" sizes=\"(max-width: 2400px) 100vw, 2400px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. End-to-end lineage from API response to data transformations and modeling</em></figcaption></figure></div>\n\n\n<p>End-to-end lineage and debuggability means that when an endpoint hosting a model responds to a request, you can trace the prediction back to the workflow that produced the model and data it was trained on.</p>\n\n\n\n<p>In practice, every response contains an NVIDIA Triton Inference Server deployment ID, which maps to a Metaflow run ID, which in turn enables you to inspect the data used to produce the model.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Example: Train and serve a tree-based model</h2>\n\n\n\n<p>To show the end-to-end workflow in action, here\u2019s a practical example. You can follow along using the <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack\">/triton-metaflow-starter-pack</a> GitHub repo.</p>\n\n\n\n<p>The <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack/blob/main/trees/train/flow.py\">example workflow</a> addresses a fraud detection problem, a classification task for predicting loan defaults. It trains many Scikit-learn models in parallel, selects the best one, and pushes the model to a cloud-based model registry used by NVIDIA Triton Inference Server. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"950\" height=\"736\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2.png\" alt=\"Screenshot from the Metaflow UI shows the workflow steps: start, preprocess, train, eval, deploy, and end.\" class=\"wp-image-76229\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2.png 950w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-300x232.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-625x484.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-148x115.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-768x595.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-645x500.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-387x300.png 387w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-116x90.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-362x280.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/fraud-classifier-dag-2-142x110.png 142w\" sizes=\"(max-width: 950px) 100vw, 950px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The Metaflow UI, an interface for monitoring ML workflows</em></figcaption></figure></div>\n\n\n<p>The Metaflow UI enables the monitoring and visualizations of your workflow runs, organizing workflows by a run ID and seamlessly tracking all artifacts that the workflow runs produce.</p>\n\n\n\n<p>To give you an idea what the workflow code looks like, the following code example defines the first three steps: start, preprocess, and train.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nclass FraudClassifierTreeSelection(FlowSpec):\n\n     @step\n     def start(self):\n     self.next(self.preprocess)\n\n     @batch(cpu=1, memory=8000)\n     @card\n     @step\n     def preprocess(self):\n     self.compute_features()\n     self.setup_model_grid(model_list=&#91;&quot;Random Forest&quot;])\n     self.next(self.train, foreach=&quot;model_grid&quot;)\n\n     @batch(cpu=4, memory=16000)\n     @card\n     @step\n     def train(self):\n     self.model_name, self.model_grid = self.input\n     self.best_model = self.smote_pipe(\n          self.model_grid, self.X_train_full, self.y_train_full\n          )\n     self.next(self.eval)\n\t...\n</pre></div>\n\n\n<p>The train step executes a number of parallel training tasks using <a href=\"https://docs.metaflow.org/metaflow/basics#foreach\">Metaflow\u2019s foreach construct</a>. In this case, you execute the tasks in the cloud using <a href=\"https://docs.metaflow.org/scaling/remote-tasks/aws-batch\">AWS Batch</a>, by specifying a <code>@batch</code> decorator. Run the workflow manually with the following command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\npython train/flow.py run \\\n--model-repo s3://outerbounds-datasets/triton/tree-models/\n</pre></div>\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Tl74_4_6nA4?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Deploy Triton Artifacts from Metaflow Demo</em></figcaption></figure>\n\n\n\n<p>When Metaflow is deployed in your environment, you don\u2019t have to write any other configuration or Dockerfiles besides writing and executing this workflow. The workflows can be deployed using a single command to run on a <a href=\"https://docs.metaflow.org/production/scheduling-metaflow-flows/introduction\">schedule</a> or be <a href=\"https://docs.metaflow.org/production/event-triggering\">triggered by events</a> in the broader system.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Preparing models for NVIDIA Triton Inference Server</h3>\n\n\n\n<p>The deploy step of the workflow takes care of preparing a model for deployment with NVIDIA Triton Inference Server. This happens through the following steps:</p>\n\n\n\n<ol>\n<li>Set the backend and other properties in NVIDIA Triton Inference Server\u2019s model configuration. Some of these properties depend on what happens during training, so you <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack/blob/main/trees/train/ops.py#L9\">dynamically create the file</a> and include it in the artifact package sent to cloud storage during model training runs.</li>\n\n\n\n<li>NVIDIA Triton Inference Server also needs the model representation itself. Because you\u2019re concerned about efficient serving, serialize the model using <a href=\"https://treelite.readthedocs.io/en/latest/\">Treelite</a>, a pattern that works with Scikit-learn tree models, XGBoost, and LightGBM. Place the resulting checkpoint.tl file in the model repository, and NVIDIA Triton Inference Server knows what to do from there.&nbsp;</li>\n\n\n\n<li>Serialized model files are pushed to Amazon S3 using <a href=\"https://docs.metaflow.org/scaling/data#data-in-s3-metaflows3\">Metaflow\u2019s built-in optimized Amazon S3 client</a>, naming them based on a unique workflow run ID that can be used to access all information used to train the model. This way, you maintain a full lineage from inference responses back to data processing and training workflows.</li>\n</ol>\n\n\n\n<h3 class=\"wp-block-heading\">NVIDIA Triton Inference Server performance</h3>\n\n\n\n<p>For serving tree models, NVIDIA Triton Inference Server works best with the <a href=\"https://github.com/triton-inference-server/fil_backend\">FIL backend</a>. We ran a simple benchmark to see how the inference latency compares to a baseline Python-based API server using <a href=\"https://fastapi.tiangolo.com/\">FastAPI</a> as a frontend and <a href=\"https://www.uvicorn.org/\">Uvicorn</a> as a backend. You can reproduce the results using the <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack/tree/main\">triton-metaflow-starter-pack</a> GitHub repo.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1000\" height=\"800\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent.png\" alt=\"The results demonstrate that hosting a tree-based model using the Triton server with a FIL backend outperforms a FastAPI server with a Uvicorn backend by over an order of magnitude.\" class=\"wp-image-75935\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-768x614.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-113x90.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/whisker-plot-transparent-138x110.png 138w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Response latencies (boxes show the quartiles of response latencies; whiskers show the rest of the distribution; circles are specific request times determined to be outliers)</em></figcaption></figure></div>\n\n\n<p>The response time that we observed (not counting the network overhead) was 0.44 ms \u00b1 0.64 ms for NVIDIA Triton Inference Server compared to 5.15 ms \u00b1 0.9 ms for FastAPI. This is more than an order of magnitude difference.&nbsp;</p>\n\n\n\n<p>The benchmark was run on a <a href=\"https://coreweave.com/\">CoreWeave</a> server with eight Xeon cores, demonstrating that NVIDIA Triton Inference Server can provide big speedups in any environment, and not only on NVIDIA GPUs.&nbsp;</p>\n\n\n\n<p>We plan to compare more interesting server combinations, explore NVIDIA Triton Inference Server optimizations such as dynamic request batching, and extend to the real-world complexity of network overhead.</p>\n\n\n\n<h3 class=\"wp-block-heading\">NVIDIA Triton Inference Server for production inference</h3>\n\n\n\n<p>Security, reliability, and enterprise-grade support are critical for production AI.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> is a production-ready inference platform that includes NVIDIA Triton Inference Server. It is designed to accelerate time-to-value with enterprise-grade security, support, and API stability to ensure performance and high availability.&nbsp;</p>\n\n\n\n<p>You can use this inference platform to relieve the burden of maintaining and securing the complex software platform of AI.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Fine-tuning and serving an LLM</h2>\n\n\n\n<p>In 2023, I\u2019d be remiss to discuss model serving without mentioning LLMs. NVIDIA Triton Inference Server has extensive support for deep learning models and <a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">increasingly sophisticated support for serving LLMs</a>. Yet, serving LLMs efficiently is a deep and quickly evolving topic.</p>\n\n\n\n<p>To give you a small sample of serving LLMs with NVIDIA Triton Inference Server, I used <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack/tree/main/llm\">this workflow</a> to fine-tune a Llama2 model from HuggingFace to produce QLoRAs, and serve the resulting model with NVIDIA Triton Inference Server. For more information, see <a href=\"https://outerbounds.com/blog/llm-tuning-metaflow/\">Fine-tuning a Large Language Model using Metaflow, featuring LLaMA and LoRA</a> and <a href=\"https://outerbounds.com/blog/better-faster-stronger-fine-tuning/\">Better, Faster, Stronger LLM Fine-tuning</a>.</p>\n\n\n\n<p>As I did previously, I constructed the <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack/blob/main/llm/finetune/ops.py#L10\">NVIDIA Triton Inference Server</a> <a href=\"https://github.com/outerbounds/triton-metaflow-starter-pack/blob/main/llm/finetune/ops.py#L10\">configuration</a> on the fly after the workflow finishes model training. This example is meant as a proof-of-concept.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Next steps</h2>\n\n\n\n<p>Start experimenting with the end-to-end AI stack highlighted in this post in your own environment. For more information, see the following resources:</p>\n\n\n\n<ul>\n<li><a href=\"https://docs.metaflow.org/getting-started/install\">Installing Metaflow</a></li>\n\n\n\n<li><a href=\"https://outerbounds.com/platform/\">Outerbounds</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\">NVIDIA Triton Inference Server</a> is available with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> under free 90-day <a href=\"https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;ProductFamily=NVAIEnterprise\">software evaluation licenses</a></li>\n</ul>\n\n\n\n<p>If you have any questions or feedback, join thousands of ML/AI developers and engineers in the <a href=\"http://slack.outerbounds.co\">Metaflow community</a> Slack channel.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>There are many ways to deploy ML models to production. Sometimes, a model is run once per day to refresh forecasts in a database. Sometimes, it powers a small-scale but critical decision-making dashboard or speech-to-text on a mobile device. These days, the model can also be a custom large language model (LLM) backing a novel &hellip; <a href=\"https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/\">Continued</a></p>\n", "protected": false}, "author": 1976, "featured_media": 75821, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1327629", "discourse_permalink": "https://forums.developer.nvidia.com/t/develop-ml-and-ai-with-metaflow-and-deploy-with-nvidia-triton-inference-server/277979", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [2584, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/metaflow-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jIR", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75817"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1976"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75817"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75817/revisions"}], "predecessor-version": [{"id": 76256, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75817/revisions/76256"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75821"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75817"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75817"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75817"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75226, "date": "2024-01-05T11:00:00", "date_gmt": "2024-01-05T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75226"}, "modified": "2024-01-22T14:08:04", "modified_gmt": "2024-01-22T22:08:04", "slug": "video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/", "title": {"rendered": "Video Encoding at 8K60 with Split-Frame Encoding and NVIDIA Ada Lovelace Architecture"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Capturing video footage and playing games at 8K resolution with 60 frames per second (FPS) is now possible, thanks to advances in camera and display technologies. Major leading multimedia companies including RED Digital Cinema, Nikon, and Canon have already introduced 8K60 cameras for both the consumer and professional markets.&nbsp;</p>\n\n\n\n<p>On the display side, with the newest HDMI 2.1 standard, 8K60 is now widely available, supporting both gaming monitors and smart TVs. While 8K60 provides \u200cstunning image quality and sharpness, it comes with the significant cost of consuming more data for both transfer and storage.&nbsp;</p>\n\n\n\n<p>Fast codecs are therefore paramount in bridging the gap between the sensors and the display. To make 8K60 widely available, <a href=\"https://www.nvidia.com/en-us/geforce/ada-lovelace-architecture/\">NVIDIA Ada Lovelace</a> GPU architecture provides multiple <a href=\"https://docs.nvidia.com/video-technologies/video-codec-sdk/12.1/nvenc-application-note/index.html#nvenc-capabilities\">NVENC</a> engines to accelerate video encoding performance while maintaining high image quality. (Two NVENCs are provided with NVIDIA RTX 4090 and 4080 and three NVENCs with NVIDIA RTX 6000 Ada Lovelace or L40.)&nbsp;</p>\n\n\n\n<p>In practice, this can double or triple the encoding performance with a single GPU when compared to previous generations, enabling 8K60 video encoding and beyond.</p>\n\n\n\n<p>This post showcases how the multiple available NVENCs in NVIDIA Ada Lovelace architecture are leveraged using a split-frame encoding (SFE) technique to achieve 8K60 video encoding performance. We explore how this SFE technique works at 4K and 8K resolutions and how to enable it through the NVENCODE API. Finally, we present several benchmarks that show, in practice, the massive performance benefits of this technique.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Split-frame encoding</h2>\n\n\n\n<p>SFE is a technique that enables exploiting multiple NVENCs present in NVIDIA Ada Lovelace GPUs when encoding a single video sequence by splitting the frames and encoding each partial frame with different NVENC engines. It was introduced in <a href=\"https://developer.nvidia.com/video-codec-sdk\">NVIDIA Video Codec SDK</a> 12.0.&nbsp;SFE can effectively split the encoding work across the available NVENCs (Figure 1). However, until now, SFE was implicitly enabled based on the encoding preset, tuning information, and resolution to support 8K live encoding in HEVC or AV1. (Note that 8K is not supported on H.264.) To learn more, see <a href=\"https://developer.nvidia.com/blog/improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture/\">Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"480\" height=\"612\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1.png\" alt=\"Diagram of two-way split-frame encoding with NVENCODE API, NVENC 0, and NVENC 1 (top to bottom).\" class=\"wp-image-75233\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1-235x300.png 235w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1-90x115.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1-71x90.png 71w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1-362x462.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/two-way-split-frame-encoding-1-86x110.png 86w\" sizes=\"(max-width: 480px) 100vw, 480px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Two-way split-frame encoding</em></em></figcaption></figure></div>\n\n\n<p>With NVIDIA Video Codec SDK 12.1, you can enable or disable the SFE feature. This means that SFE can now be used to take advantage of two or even three NVENCs present within the <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/\">NVIDIA RTX 4090</a> and the <a href=\"https://www.nvidia.com/en-us/design-visualization/rtx-6000/\">NVIDIA RTX 6000 Ada Generation</a>, respectively, without resolution, preset, and tuning information restrictions. This enables the application to double or even triple the encoding performance when encoding a single video sequence by using a two-way or three-way SFE. Such performance is especially important when encoding 8K, which is a particularly demanding use case.</p>\n\n\n\n<h2 class=\"wp-block-heading\">SFE at 4K and 8K resolution</h2>\n\n\n\n<p>How SFE is applied can vary depending on the resolution and selected video codec. When using HEVC with SFE turned off, expect only a single slice to be used. When two-way or three-way SFE is used, two or three slices, respectively, are used. These horizontally separate each frame. It applies to both 4K and 8K resolutions (Figure 2). Additionally, the same applies to AV1 when encoding video up to 4K resolution. However, AV1 uses tiles instead of slices to create these independent frame partitions.&nbsp;</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"776\" height=\"147\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding.png\" alt=\"ree images showing generated frame partitions when encoding HEVC at 4K and 8K and AV1 at 4K for the several SFE configurations. HEVC 4K/8K and AV1 4K with no SFE (left); HEVC 4K/8K and AV1 4K with two-way SFE (center); HEVC 4K/8K and AV1 4K with three-way SFE (right). \" class=\"wp-image-76556\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding.png 776w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-300x57.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-625x118.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-179x34.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-768x145.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-645x122.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-500x95.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-160x30.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-362x69.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-hevc-encoding-581x110.png 581w\" sizes=\"(max-width: 776px) 100vw, 776px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Generated frame partitions when encoding HEVC at 4K and 8K and AV1 at 4K for the several SFE configurations: HEVC 4K/8K and AV1 4K with no SFE (left); HEVC 4K/8K and AV1 4K with two-way SFE (center); HEVC 4K/8K and AV1 4K with three-way SFE (right)</em></em></figcaption></figure></div>\n\n\n\n<p>When encoding 8K video with AV1, consider that the maximum tile resolution defined by the standard is 4096 x 2304 pixels. This means that when encoding 8K video, each frame will be split into four tiles, each with a quarter of the resolution (3840 x 2160 pixels). When SFE is used, to achieve the same performance benefits as for HEVC, each tile will be further split horizontally, for eight or 12 tiles, for two-way and three-way SFE, respectively (Figure 3).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"776\" height=\"144\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding.png\" alt=\"Three images showing generated frame partitions when encoding AV1 at 8K for the several SFE configurations: AV1 8K with no SFE (left); AV1 8K with two-way SFE (center); AV1 8K with three-way SFE (right).\n\" class=\"wp-image-76559\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding.png 776w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-300x56.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-625x116.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-179x33.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-768x143.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-645x120.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-500x93.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-160x30.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-362x67.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/generated-frame-partitions-av1-encoding-593x110.png 593w\" sizes=\"(max-width: 776px) 100vw, 776px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Generated frame partitions when encoding AV1 at 8K for the several SFE configurations: AV1 8K with no SFE (left); AV1 8K with two-way SFE (center); AV1 8K with three-way SFE (right)</em></figcaption></figure>\n\n\n\n<p>Table 1 summarizes the number of partial frames and resolution to expect per codec and input video resolution.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>Codec</strong></td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>Video resolution</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"6\"><strong>Number of partial frames and resolution</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>No SFE</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Two-way SFE</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Three-way SFE</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>HEVC</strong></td><td class=\"has-text-align-center\" data-align=\"center\">4K</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">1x&nbsp;3840 x 2160</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">2x&nbsp;3840 x 1080</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">3x&nbsp;3840 x 720</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">8K</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">1x 7680 x 4320</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">2x 7680 x 2160</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">3x 7680 x 1440</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>AV1</strong></td><td class=\"has-text-align-center\" data-align=\"center\">4K</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">1x&nbsp;3840 x 2160</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">2x 3840 x 1080</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">3x 3840 x 720</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">8K</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">4x&nbsp;3840 x 2160</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">8x 3840 x 1080</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">12x 3840 x 720</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 1. Summary of number of partial frames and respective resolution per codec when encoding 4K and 8K videos</em></em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Enabling split-frame encoding</h2>\n\n\n\n<p>With the API update of <a href=\"https://developer.nvidia.com/nvidia-video-codec-sdk/download\">Video Codec SDK 12.1</a>, in the latest NVENCODER API header, you can find <code>NV_ENC_SPLIT_ENCODE_MODE</code>. This enables control over SFE, as shown in Table 2. It is now quite easy to configure SFE using either implicit or explicit modes. <code>NV_ENC_SPLIT_AUTO_MODE</code> and <code>NV_ENC_SPLIT_AUTO_FORCED_MODE</code> provide a way to use the SFE implicit mode. To learn more, see <a href=\"https://developer.nvidia.com/blog/improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture/\">Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture</a>.</p>\n\n\n\n<p>The remaining options refer to explicit SFE configuration. These include forcing SFE to be disabled, two-way, or three-way. To force two-way or three-way SFE requires an NVIDIA GPU with the appropriate number of NVENC engines.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>NV_ENC_SPLIT_ENCODE_MODE</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>SFE type</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Description</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NV_ENC_SPLIT_AUTO_MODE (0)</td><td class=\"has-text-align-center\" data-align=\"center\">Auto Mode (default)</td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\">Two-way SFE will be implicitly triggered based on input video resolution and encoding parameters</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NV_ENC_SPLIT_AUTO_FORCED_MODE (1)</td><td class=\"has-text-align-center\" data-align=\"center\">Force Auto Mode</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NV_ENC_SPLIT_TWO_FORCED_MODE (2)</td><td class=\"has-text-align-center\" data-align=\"center\">Force two-way SFE</td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"3\">The respective SFE configuration will be used regardless of the input video and encoding parameters</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NV_ENC_SPLIT_THREE_FORCED_MODE (3)</td><td class=\"has-text-align-center\" data-align=\"center\">Force three-way SFE</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NV_ENC_SPLIT_DISABLE_MODE (15)</td><td class=\"has-text-align-center\" data-align=\"center\">Force no SFE</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Selected SFE type and description per <code>NV_ENC_SPLIT_ENCODE_MODE</code> option</em></figcaption></figure>\n\n\n\n<p>The latest Video Codec SDK encoding sample <code>AppEncMultiInstance</code> also highlights how to add explicit SFE control to an application.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Performance and compression efficiency benchmarking</h2>\n\n\n\n<p>Several configurations and input 8K videos were tested, which are listed in Table 3.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Benchmarking configuration</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">GPU</td><td class=\"has-text-align-center\" data-align=\"center\">GPU RTX 6000 Ada Generation (3 NVENCs)</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Input videos</td><td class=\"has-text-align-center\" data-align=\"center\">7 videos (4 gaming and 3 natural)</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Encoders</td><td class=\"has-text-align-center\" data-align=\"center\">HEVC and AV1</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Presets</td><td class=\"has-text-align-center\" data-align=\"center\">P1 (fastest), P4 (medium) and P7 (slowest)</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Tuning Information</td><td class=\"has-text-align-center\" data-align=\"center\">Low latency (LL) and high quality (HQ)</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Bitrates</td><td class=\"has-text-align-center\" data-align=\"center\">15, 20, 60, 150, and 250 Mbps</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. Benchmarking configuration summary</em></figcaption></figure>\n\n\n\n<p>Two types of benchmarks were performed:</p>\n\n\n\n<p><strong>Transcoding Performance</strong>: Transcoding was used to minimize the influence of system bottlenecks (file I/O and memory copies between CPU and GPU). To test transcoding, the original 8K videos were pre-encoded with very high bitrates. During transcoding, NVDEC decodes the video. It is encoded by one to three NVENCs, when no split, two-way SFE, and three-way SFE are used, respectively. The performance results are shown in Figures 4 and 5 for HEVC and AV1, respectively.</p>\n\n\n\n<p><strong>Compression Efficiency Penalty</strong>: By splitting encoding work across several NVENCs, a compression efficiency penalty is expected. To measure this penalty, BD-RATE was used across several benchmark configurations to compare the compression efficiency between no-split, two-way SFE, and three-way SFE. This metric indicates the average compression efficiency penalty for the same objective quality. The objective quality metric used in these benchmarks was PSNR. The compression efficiency penalty results are shown in Figures 6 and 7 for HEVC and AV1, respectively.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1408\" height=\"691\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc.png\" alt=\"Bar chart showing average performance benchmarking results for 8K transcoding using HEVC.\n\" class=\"wp-image-75264\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc.png 1408w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-768x377.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-645x317.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-500x245.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-224x110.png 224w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-hevc-1024x503.png 1024w\" sizes=\"(max-width: 1408px) 100vw, 1408px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Average performance benchmarking results for 8K transcoding using HEVC</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1420\" height=\"692\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1.png\" alt=\"Bar chart showing average performance benchmarking results for 8K transcoding using AV1.\n\" class=\"wp-image-75266\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1.png 1420w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-625x305.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-768x374.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-645x314.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-362x176.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-226x110.png 226w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/benchmarking-results-8k-transcoding-av1-1024x499.png 1024w\" sizes=\"(max-width: 1420px) 100vw, 1420px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. Average performance benchmarking results for 8K transcoding using AV1</em></em></figcaption></figure>\n\n\n\n<p>When using two-way SFE, expect an average performance scaling of about 1.8x for both HEVC and AV1. Three-way SFE can achieve a performance scaling of up to 2.95x for HEVC and 2.31x for AV1. In practice, this enables 8K60 video encoding with NVIDIA RTX 6000 Ada Generation, using both HEVC and AV1, with LL and HQ tuning information at a medium preset (P4).</p>\n\n\n\n<p>Given that one to three NVENCs and a single NVDEC are used, NVDEC may become the bottleneck when transcoding 8K. For this reason, the fastest preset (P1) can result in the FPS reaching a maximum of about 120 FPS on average. This is the average maximum performance achieved by a single NVDEC at 8K.&nbsp;</p>\n\n\n\n<p>You can observe better scaling as long as NVDEC isn&#8217;t the bottleneck. This is the case for slower presets, such as P4 and P7, where the performance scaling is much better in comparison to P1.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1407\" height=\"690\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc.png\" alt=\"Chart showing average compression efficiency penalty results for 8K encoding using HEVC.\n\" class=\"wp-image-75272\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc.png 1407w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-768x377.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-645x316.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-500x245.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-224x110.png 224w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-hevc-1024x502.png 1024w\" sizes=\"(max-width: 1407px) 100vw, 1407px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. Average compression efficiency penalty results for 8K encoding using HEVC</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1409\" height=\"693\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1.png\" alt=\"Chart showing average compression efficiency penalty results for 8K encoding using AV1.\n\" class=\"wp-image-75275\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1.png 1409w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-300x148.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-768x378.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-645x317.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-500x246.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-224x110.png 224w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/average-compression-penalty-results-8k-encoding-av1-1024x504.png 1024w\" sizes=\"(max-width: 1409px) 100vw, 1409px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 7. Average compression efficiency penalty results for 8K encoding using AV1</em></em></figcaption></figure>\n\n\n\n<p>In general, the compression efficiency penalty isn&#8217;t expected to exceed 2% for two-way SFE and 4% for three-way SFE when using BD-RATE (PSNR) to measure quality. This penalty is more noticeable for HQ tuning information than for LL. Additionally, according to the benchmarks performed, this penalty is slightly more prominent when using HEVC compared to AV1.&nbsp;</p>\n\n\n\n<p>Although this compression efficiency penalty is still relatively low compared to the performance tradeoff, it&#8217;s up to the user to determine if the required use case benefits from more performance or compression efficiency. Regardless, the NVENCODE API provides full control over SFE not only for 8K but also for lower resolutions.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>Split-frame encoding (SFE) is a breakthrough feature that unlocks video encoding capabilities at 8K60 and beyond. It empowers users to harness the power of multiple NVENCs within NVIDIA Ada Lovelace architecture GPUs for encoding a single video sequence. This post has explained the performance advantages of two-way SFE (using two NVENCs) and three-way SFE (using three NVENCs). The latest <a href=\"https://developer.nvidia.com/nvidia-video-codec-sdk/download\">NVIDIA Video Codec SDK</a> provides explicit control over SFE for optimal customization.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Capturing video footage and playing games at 8K resolution with 60 frames per second (FPS) is now possible, thanks to advances in camera and display technologies. Major leading multimedia companies including RED Digital Cinema, Nikon, and Canon have already introduced 8K60 cameras for both the consumer and professional markets.&nbsp; On the display side, with the &hellip; <a href=\"https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/\">Continued</a></p>\n", "protected": false}, "author": 1964, "featured_media": 75227, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1327616", "discourse_permalink": "https://forums.developer.nvidia.com/t/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/277977", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1235], "tags": [3139, 453, 1281, 796], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvenc-comparison.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jzk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75226"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1964"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75226"}], "version-history": [{"count": 53, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75226/revisions"}], "predecessor-version": [{"id": 77065, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75226/revisions/77065"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75227"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75226"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75226"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75226"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75946, "date": "2024-01-04T06:00:00", "date_gmt": "2024-01-04T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75946"}, "modified": "2024-01-11T11:49:35", "modified_gmt": "2024-01-11T19:49:35", "slug": "accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/", "title": {"rendered": "Accelerating Inference on End-to-End Workflows with H2O.ai and NVIDIA"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Data scientists are combining <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a> and predictive analytics to build the next generation of AI applications. In financial services, AI modeling and inference can be used for solutions such as alternative data for investment analysis, AI intelligent document automation, and fraud detection in trading, banking, and payments.</p>\n\n\n\n<p><a href=\"https://h2o.ai/\">H2O.ai</a> and NVIDIA are working together to provide an end-to-end workflow for generative AI and data science, using the NVIDIA AI Enterprise platform and H2O.ai&#8217;s LLM Studio and Driverless AI AutoML. H2O.ai also uses NVIDIA AI Enterprise to deploy next-generation AI inference, including large language models (LLMs) for safe and trusted enterprise-grade FinanceGPTs and custom applications at scale.</p>\n\n\n\n<p>This integration aims to help organizations develop and deploy their own LLMs and customized models for various applications, beyond natural language processing (NLP), including image generation. These models enable the use of multiple modalities of content\u2014such as text, audio, video, images, and code\u2014to generate new content for broader applications.</p>\n\n\n\n<p>There&#8217;s a pressing need for FSI firms to use generative AI and accelerate the innovation that leads to new product opportunities and reduced costs in operational areas. Both regulated (banks, broker-dealers, asset managers, and insurers) and unregulated (hedge funds and proprietary traders) financial institutions are working on big-picture solutions with the convergence of generative AI and data science applications.</p>\n\n\n\n<p>This post provides an overview of different data science use cases and the tools to build integrated applications for the following progression areas as financial institutions adopt the latest AI models:\u00a0</p>\n\n\n\n<ul>\n<li>Newer generative AI and LLMs use cases\n<ul>\n<li>Accelerated inference for trading and risk</li>\n\n\n\n<li>Intelligent automation and chatbot experiences</li>\n</ul>\n</li>\n\n\n\n<li>Data science and accelerated machine learning analytics on NVIDIA Triton Inference Server</li>\n\n\n\n<li>Converged generative AI and predictive analytics</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\">Generative AI and LLMs on accelerated inferencing for trading and risk</h2>\n\n\n\n<p>The holy grail of investing is to stay ahead of markets with real-time calculations and tap into alternative data sources, which are typically unstructured. 70% or more of organizational information is unstructured and is currently not tapped by organizations, as detailed in an <a href=\"https://blog.box.com/90-your-data-unstructured-and-its-full-untapped-value\">IDC report on unstructured data</a>.&nbsp;</p>\n\n\n\n<p>Today, capital markets typically rely on traditional sources of tabular data called <em>market data</em>, which is stored in columns and rows. Newer, alternative data sources must be tapped to gain an information edge.&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Alternative data sources&nbsp;</h3>\n\n\n\n<p>Alternative data is a category of unstructured information obtained from non-traditional sources with context unavailable through traditional market data. It can provide deeper insights, extend the fundamental analysis that is the qualitative process, and capture the behavioral finance element. Sources of alternative data can include any of the following:</p>\n\n\n\n<ul>\n<li>Earnings call transcripts</li>\n\n\n\n<li>Federal Reserve meeting notes</li>\n\n\n\n<li>Social media</li>\n\n\n\n<li>News</li>\n\n\n\n<li>Satellite imagery</li>\n\n\n\n<li>Financial filings</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1659\" height=\"850\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-transcript-sentiment.gif\" alt=\"GIF shows an earnings transcript sentiment generated from AI.\" class=\"wp-image-75961\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Generative AI for transcript sentiment</em></figcaption></figure></div>\n\n\n<p>Alternative data uses this information to provide a deeper understanding of a company&#8217;s financial health or counterparty (Figure 2) for analyzing financial filings, which can be used in trading and risk management decisions. Investment leaders must contend with analyzing this unstructured alternative to make timely, informed decisions and stay ahead of the market.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1834\" height=\"796\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings.png\" alt=\"Two screenshots of a public filing and an AI-generated summary.\" class=\"wp-image-75960\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings.png 1834w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-625x271.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-768x333.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-1536x667.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-645x280.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-500x217.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-253x110.png 253w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-analysis-SEC-10-K-filings-1024x444.png 1024w\" sizes=\"(max-width: 1834px) 100vw, 1834px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Generative AI analyzing SEC 10-K filings</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Impact on trading and risk</h3>\n\n\n\n<p>With generative AI and predictive analytics converging, financial firms are using NVIDIA and H2O.ai integrations to build their own generative AI NLP models and LLMs for alternative data to serve internal needs and external customers. As a result, they are helping their customers outperform markets and work on their fiduciary duty to achieve excess returns and alpha while minimizing risk for decision-making.\u00a0</p>\n\n\n\n<p>One key use case for NLP LLMs with generative AI has been to isolate signals from unstructured data which is usually noisy and then index to structured data to drive insights through sentiment analysis, question and answer, and summarization (for example, your FirmGPT) for a universe of ticker symbols. This process can provide structured signals similar to fundamental analysis in financial markets that can be used for downstream models to meet risk and return goals.</p>\n\n\n\n<p>H2O.ai\u2019s h2oGPT LLM integrated with <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a>, part of\u00a0the NVIDIA AI Enterprise platform, can provide quick, generative AI <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/#:~:text=a%20foundation%20model.-,LLMOps,-is%20a%20distinct\">LLMOps</a> ability to data scientists to train and productionalize applications at a lower cost of operation since customers can train and deploy multiple models within their enterprises.\u00a0</p>\n\n\n\n<p>Organizations can use foundational LLMs on unstructured sources of information, such as financial news, along with <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">customization</a> techniques for understanding the financial domain better.</p>\n\n\n\n<p>End users can interface with customized models to source and validate ground truth responses by using <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG).\u00a0</p>\n\n\n\n<p>H2O.ai\u2019s LLM solutions running on NVIDIA Triton Inference Server showed accelerated inference with more tokens and lower latency.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"591\" height=\"443\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens.png\" alt=\"Bar chart showing a comparison of output tokens, with Triton Inference Server having ~600 output tokens/second and vLLMSystem having ~550 output tokens/second.\" class=\"wp-image-75959\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens.png 591w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens-362x271.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/higher-output-tokens-147x110.png 147w\" sizes=\"(max-width: 591px) 100vw, 591px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Higher output tokens with NVIDIA Triton Inference Server in H20.ai\u2019s environment</em></figcaption></figure></div>\n\n\n<p>Figure 3 shows the reduced total cost of ownership (TCO) with increased return on investment (ROI) for deploying and operating multiple LLMs. For more information, see <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/\">NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200</a>.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Generative AI for intelligent automation and chatbot experiences</h2>\n\n\n\n<p>Generative AI LLMs are also used for chatbots for internal employees and external customer experiences. Chatbots help with organizational productivity and operations, generating cost savings, increasing profit margins, and driving enhanced customer experience with customer churn.\u00a0\u00a0</p>\n\n\n\n<p>In customer experiences through generative AI, the goal is to enable hyper-personalization and satisfaction (Figure 4), increasing the net promoter score (NPS) and customer satisfaction score (CSAT) for your organization.&nbsp;</p>\n\n\n\n<p>For internal employee experiences, AI tools enhance productivity by helping to relay information to management for decision-making insights. Externally, AI tools can be used to cater to customers through agent assistants.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"639\" height=\"409\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation.png\" alt=\"Screenshot of the H20.ai chatbot.\" class=\"wp-image-75958\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation.png 639w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-300x192.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-625x400.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-469x300.png 469w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-141x90.png 141w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-362x232.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gen-ai-next-best-conversation-172x110.png 172w\" sizes=\"(max-width: 639px) 100vw, 639px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Generative AI next best conversation</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Customer Spotlight: Commonwealth Bank of Australia</h3>\n\n\n\n<p>You can use H20.ai guardrails and NVIDIA NeMo Guardrails with tools such as langchain to develop more powerful custom GPT models with the maximum flexible options. H20.ai also has model governance, guardrails, and explainability tools that help with model explanations, all for use by customers in regulated environments.\u00a0</p>\n\n\n\n<p>H2O.ai has democratized generative AI so that enterprises can safely build their own state-of-the-art LLMs. Commonwealth Bank of Australia has used this to build its own GPT, tailored to what its customers need.</p>\n\n\n\n<p>Both NVIDIA and H20.ai believe every organization needs to own its LLM and GPT just as they need to own their brand, data, algorithms, and models. Open-source generative AI is bringing transformation to democratizing value from AI while preserving data, code, and content ownership.</p>\n\n\n\n<p>\u201cWe have a shared vision with <a href=\"https://www.linkedin.com/company/h2oai/\">H2O.ai</a> for democratizing AI responsibly and, through our partnership, we are now able to create ground-up generative AI solutions that allow us to have true control of the way we use data, techniques, and training. Responsible AI is not only a question of bias and explainability. It is also about the accountability you take: for the way you develop it and the outcomes you deliver,\u201d said <a href=\"https://www.linkedin.com/in/danjermyn/overlay/about-this-profile/\">Dan Jermyn</a>, chief decision scientist at the Commonwealth Bank of Australia.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Data science and accelerated machine learning analytics on NVIDIA Triton Inference Server</h2>\n\n\n\n<p>AI is revolutionizing risk assessment for credit, fraud modeling, and risk management using predictive techniques and algorithms such as <a href=\"https://www.nvidia.com/en-us/glossary/data-science/xgboost/\">XGBoost</a>. Accelerated machine learning is applied in financial services for use cases such as <a href=\"https://developer.nvidia.com/blog/limit-order-book-dataset-generation-for-accelerated-short-term-price-prediction-with-rapids/\">predicting limit order book prices</a>, underwriting credit products, and detecting fraud in financial transactions.\u00a0</p>\n\n\n\n<p>A fraud prediction pipeline contains not just one model but a series tied together, enabling more dynamic abilities to provide insights into prospective customers. Many fraud models are based on behavioral attributes for various segments (Figure 5):</p>\n\n\n\n<ul>\n<li>Geographic location vs. where the transaction happened</li>\n\n\n\n<li>Transaction type (card present vs. card not present)</li>\n\n\n\n<li>Average transaction size&nbsp;</li>\n\n\n\n<li>Average transaction frequency over varied time windows and time aggregations\n<ul>\n<li>For example, 30/60/90 days vs. weekly/monthly/quarterly/annually</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>These are based on various business rules or predictive feature characteristics. For example, if the same merchant is running increasingly large transactions over a short period, then the model algorithm can flag the transaction for potential fraud.</p>\n\n\n\n<p>In anti-money laundering (AML) and transaction fraud, the key is to know your customer (KYC) and know your customer&#8217;s customer (Customer). You establish their identity and understand the behavioral attributes of customers and other agents, including fraud perpetrators.</p>\n\n\n\n<p>In each set of models, you understand more about the customers and actors using AI techniques, such as intelligent automation. This lets you understand whether the customer\u2019s identity was breached to assign a better score to the transaction. Better scores help with the reliability of the transaction and provide a safe and secure experience to customers. Transactions are stopped based on the identity verification key for KYC and behavioral attributes, and whether fraudsters or real customers are performing the transaction.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1404\" height=\"955\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai.png\" alt=\"Screenshot of the H20.ai interface for fraud detection.\" class=\"wp-image-75957\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai.png 1404w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-300x204.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-625x425.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-169x115.png 169w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-768x522.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-645x439.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-441x300.png 441w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-132x90.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-362x246.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-162x110.png 162w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/transaction-fraud-detection-ai-1024x697.png 1024w\" sizes=\"(max-width: 1404px) 100vw, 1404px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Transaction fraud detection AI</em></figcaption></figure></div>\n\n\n<p>AML and fraud detection empower financial institutions to stay ahead of emerging threats. Multiple factors play a key role in preventing fraud in financial applications:</p>\n\n\n\n<ul>\n<li>Intelligent document automation based on generative AI models where more diverse information is processed for identity</li>\n\n\n\n<li>Behavioral attributes&nbsp;</li>\n\n\n\n<li>Traditional data science</li>\n</ul>\n\n\n\n<p>H2O.ai Driverless AI has integrated NVIDIA Triton Inference Server, RAPIDS cuML, and RAPIDS cuDF (Figure 6), enabling financial institutions to accelerate their fraud detection applications.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1229\" height=\"691\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture.png\" alt=\"Diagram shows how data scientists would use the H20.ai and NVIDIA AI architecture.\" class=\"wp-image-75956\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture.png 1229w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-driverless-ai-architecture-1024x576.png 1024w\" sizes=\"(max-width: 1229px) 100vw, 1229px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. H2O.ai Driverless AI architecture with RAPIDS and NVIDIA Triton Inference Server (Source: </em><a href=\"https://docs.h2o.ai/driverless-ai/1-10-lts/docs/userguide/deployment.html#deploy-via-triton\"><em>H20.ai</em></a><em>)</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Converged generative AI and predictive analytics</h2>\n\n\n\n<p>We are increasingly seeing a convergence of data science and AI requirements from end customers. \u00a0AI applications based on LLMs, even if they are based on unstructured, multi-modal data like NLP, image, and audio (Figure 7), have to be tied to tabular data science solutions. End financial solutions use a combination of data science techniques and accelerated predictive analytics such as XGBoost with upcoming generative AI solutions.\u00a0</p>\n\n\n\n<p>There is a convergence of new generative AI greenfield solutions with legacy brownfield data science solutions to make end-user applications more productive and AI-savvy.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1204\" height=\"854\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization.png\" alt=\"Screenshot of the interface for viewing transcript, AI-generated summary, and sentiment.\" class=\"wp-image-75955\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization.png 1204w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-300x213.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-625x443.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-162x115.png 162w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-768x545.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-645x458.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-423x300.png 423w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-127x90.png 127w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-362x257.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-155x110.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/audio-summarization-1024x726.png 1024w\" sizes=\"(max-width: 1204px) 100vw, 1204px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Converged AI audio summarization and sentiment</em></figcaption></figure></div>\n\n\n<h3 class=\"wp-block-heading\">Customer spotlight: North American Bancard</h3>\n\n\n\n<p>This is reflected at North American Bancard (NAB), which is exploring opportunities to seamlessly integrate more of H2O.ai&#8217;s next-gen LLM-powered applications into its operations driven by a shared alignment on goals and vision. This partnership promises to yield significant benefits in optimizing NAB&#8217;s operations, streamlining its business processes, and automating internal workflows. Ultimately, this enables greater fulfillment and productivity.</p>\n\n\n\n<p>&#8220;H2O.ai consistently demonstrates a vested interest in our continued success and is ready to explore new ways to leverage LLM-powered solutions to enable greater fulfillment and productivity,&#8221; said Jeffrey Vagg, chief data and analytics officer at NAB.</p>\n\n\n\n<p>Vagg envisions that generative AI has the potential to elevate NAB&#8217;s performance, enhance merchant support, and bolster fraud detection capabilities, effectively identifying malicious actors attempting to exploit vulnerabilities within the payments industry.</p>\n\n\n\n<p>\u201cWe believe every organization can safely create and own their own LLMs to bring transformation to their customers,\u201d Vagg added. The depth of trust that NAB instills in H2O.ai is proving instrumental in navigating the intricacies of generative AI.&nbsp;&nbsp;</p>\n\n\n\n<p>Working with H20.ai, NVIDIA provides those key elements needed for customer success and aligned goals for creating their own LLMs and customization.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Blocking issues and solutions</h2>\n\n\n\n<p>Financial firms tend to fall into two camps: regulated and unregulated. Regulated institutions are typically large financial institutions like banks that deploy AI at scale across multiple business units. On the other end, unregulated institutions include hedge funds and proprietary trading firms, which are more advanced in adopting technologies. Though institutions initially lagged in technology adoption, the gap is narrowing.</p>\n\n\n\n<p>Another blocker for AI adoption in financial services has been the ability to customize AI models to the financial domain and financial applications to deliver better accuracy. Traditionally, legacy data science applications are based on tabular data and is required for banks to productionalize these models in a compliant manner.&nbsp;</p>\n\n\n\n<p>Both regulated and unregulated institutions want to be able to harness their data science resources, which are scarce and widely sought after. They want to build financial AI applications with maximum accuracy for the financial domain and make the best use of their time and productivity.\u00a0</p>\n\n\n\n<p>As the customer stories show, both customers have adopted AI LLM Ops and MLOPs solutions and need productivity tools to get to enterprise production use cases faster in financial services. This needs development productivity accelerators that can bring together such diverse solutions, integrating generative AI with data science and analytic models into a single application workflow.&nbsp;</p>\n\n\n\n<p>H20.ai and NVIDIA provide the ability to develop integrated financial domain-specific applications in a shorter period, harnessing the ability of NVIDIA AI Enterprise with H20.ai LLM Studio and H20.ai Driverless. <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> supports accelerated, high-performance inference with </p>\n\n\n\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/\">NVIDIA Triton Inference Server</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/tensorrt#inference\">NVIDIA TensorRT-LLM</a></li>\n\n\n\n<li>Other <a href=\"https://www.nvidia.com/en-us/ai-data-science/\">NVIDIA AI</a> software</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"1512\" height=\"846\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system.png\" alt=\"Diagram of AI tools on H20.ai and NVIDIA platforms.\" class=\"wp-image-75954\" style=\"aspect-ratio:1.7857142857142858;width:614px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system.png 1512w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-300x168.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-625x350.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-768x430.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-645x361.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-500x280.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-197x110.png 197w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/converged-ai-system-1024x573.png 1024w\" sizes=\"(max-width: 1512px) 100vw, 1512px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. Converged AI ecosystem powered by H2O.ai and NVIDIA</em></figcaption></figure></div>\n\n\n<p>H2O.ai\u2019s LLM Studio is an open-source, no-code solution that can empower organizations to fine-tune and evaluate LLMs. Financial institutions can customize models to their domain with NVIDIA NeMo, a framework that provides the ability to fine-tune and supervise models with various customizations such as prompt learning, instruction tuning, and RLHF (Figure 8).</p>\n\n\n\n<p>h2oGPT APIs are sustainably built on open source\u2013the most responsible way forward for regulated industries like financial services.&nbsp;</p>\n\n\n\n<p>Developers have total control and customization over their AI models. This level of customization and control is unmatched by anything in the market today.</p>\n\n\n\n<p>Models and a customization framework can efficiently accomplish tasks for your domain with your own intellectual property ready for distribution and monetization at a fraction of the cost to customize, run, and operate against other proprietary LLM models.</p>\n\n\n\n<p>Customers can use h2oGPT built-in guardrails along with NeMo Guardrails, to empower enterprise AI use cases.</p>\n\n\n\n<h3 class=\"wp-block-heading\">NVIDIA NeMo Retriever</h3>\n\n\n\n<p>With H20.ai\u2019s built-in RAG, you can seamlessly integrate generative AI models into your existing data store. <a href=\"https://h2o.ai/\">H20.ai</a>, along with NVIDIA <a href=\"https://nvidianews.nvidia.com/news/nemo-retriever-generative-ai-microservice\">NeMo Retriever</a>, can use NVIDIA-optimized RAG capabilities. </p>\n\n\n\n<p>NeMo Retriever is a generative AI microservice that lets enterprises connect custom LLMs to enterprise data to deliver highly accurate responses for their AI applications and provide more accurate responses. It is part of the <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> software platform.</p>\n\n\n\n<p>Developers using the microservice can connect their AI applications to business data wherever it resides, bringing the compute and software tooling applications to your data. This can help companies develop and run NVIDIA-accelerated inference applications on virtually any data center or cloud.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Summary</h2>\n\n\n\n<p>NVIDIA and H20.ai together offers the maximum flexibility and tools to develop generative AI applications to end customers.\u00a0 This offers a quick pathway to develop and productionalize converged AI and data science applications at a fast pace, solving a significant bottleneck to enterprises.\u00a0</p>\n\n\n\n<p>As a result, you can develop your own LLMs where the intellectual property and the ability to monetize and redistribute resides with you. You can turn existing investments, data, and resources into revenue centers, realizing returns out of AI investments and subsequently investing in further AI projects.&nbsp;</p>\n\n\n\n<p>Together, this maximizes ROI, lowers TCO, and offers enhanced productivity with the flexibility and choice in tooling and framework to build converged applications, offering unprecedented value to our end customers.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Data scientists are combining generative AI and predictive analytics to build the next generation of AI applications. In financial services, AI modeling and inference can be used for solutions such as alternative data for investment analysis, AI intelligent document automation, and fraud detection in trading, banking, and payments. H2O.ai and NVIDIA are working together to &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/\">Continued</a></p>\n", "protected": false}, "author": 981, "featured_media": 75977, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1327003", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/277855", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [296, 453], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jKW", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75946"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/981"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75946"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75946/revisions"}], "predecessor-version": [{"id": 76155, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75946/revisions/76155"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75977"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75946"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75946"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75946"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75551, "date": "2024-01-02T09:00:00", "date_gmt": "2024-01-02T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75551"}, "modified": "2024-01-11T11:49:36", "modified_gmt": "2024-01-11T19:49:36", "slug": "webinar-accelerating-large-scale-genomics-research", "status": "publish", "type": "post", "link": "https://info.nvidia.com/parabricks-academic-webinar-emea/?nvid=nv-int-tblg-465186-vt16", "title": {"rendered": "Webinar: Accelerating Large-Scale Genomics Research"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Learn how the Francis Crick Institute is using NVIDIA Clara Parabricks to enable key parts of TRACERx EVO, a new program that builds on the discoveries made in the world\u2019s largest long-term lung study.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how the Francis Crick Institute is using NVIDIA Clara Parabricks to enable key parts of TRACERx EVO, a new program that builds on the discoveries made in the world\u2019s largest long-term lung study.</p>\n", "protected": false}, "author": 1115, "featured_media": 75785, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://info.nvidia.com/parabricks-academic-webinar-emea/?nvid=nv-int-tblg-465186-vt16", "_links_to_target": "_blank"}, "categories": [696, 503], "tags": [1910, 2385, 453, 1163], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hc-social-launchpad-parabricks-2048x1024-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jEz", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75551"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75551"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75551/revisions"}], "predecessor-version": [{"id": 75561, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75551/revisions/75561"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75785"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75551"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75551"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75551"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75811, "date": "2023-12-20T14:15:39", "date_gmt": "2023-12-20T22:15:39", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75811"}, "modified": "2023-12-21T11:01:14", "modified_gmt": "2023-12-21T19:01:14", "slug": "q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update/", "title": {"rendered": "Q&A: Looking Back to When 1997\u2019s Quake II Got a Path Tracing Update"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>In 2019, if you wanted to check out the cutting edge in video game graphics, you needed an NVIDIA GeForce RTX 20 Series GPU and a copy of a game that was released in 1997, <em>Quake II</em>. With those pieces in hand, you could be among the first players in the world to see path tracing running in real-time on a consumer GPU. \u201cSomehow a game from 1997 convinced me it was time to upgrade,\u201d enthused <a href=\"https://www.pcgamer.com/i-bought-a-dollar600-graphics-card-to-play-quake-2-with-ray-tracing/\">PC Gamer</a> critic Wes Fenlon.&nbsp;</p>\n\n\n\n<p>As I look back at the history of NVIDIA RTX, now with over <a href=\"https://www.nvidia.com/en-us/geforce/news/rtx500-celebration-dlss-ray-tracing-new-games-win-prizes/\">500 RTX games and applications</a> powered by DLSS, ray tracing, and AI-enhanced technologies, I thought it would be a good time to sit down and talk with <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/author/alexeypanteleev\" target=\"_blank\">Alexey Panteleev</a>, an NVIDIA engineer who worked closely on the <em>Quake II RTX</em> project.</p>\n\n\n\n<p><strong>Why did you choose <em>Quake II</em> as the subject for NVIDIA RTX\u2019s first demonstration of path tracing?</strong></p>\n\n\n\n<p>We didn\u2019t choose <em>Quake II</em>, it chose us. The project started from Q2VKPT, a tech demo created by Christoph Schied, who integrated a simple real-time path tracer into <em>Quake II</em>. We saw it as something new and cool and pushed the project to the finish line.</p>\n\n\n\n<p><strong>When people download and play <em>Quake II RTX</em>, what details would you like them to look closely at?</strong></p>\n\n\n\n<p>My personal favorite detail in <em>Quake II</em> <em>RTX</em> is the reflection and refraction system. It uses a novel algorithm for denoising combined reflections and refractions without making them blurry or ghosty.&nbsp;</p>\n\n\n\n<p>Other than that, it\u2019s all about the lighting. Look at how accurate all the lighting and shadowing effects are in the game world! Every lamp on every level is casting physically correct soft shadows.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/unGtBbhaPeU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video. Official trailer for Quake II RTX</em></figcaption></figure>\n\n\n\n<p><strong>Why do you think <em>Quake II</em> has such enduring interest from players? It just got 28 new campaign levels from MachineGames this year!</strong></p>\n\n\n\n<p><em>Quake II</em> is one of the well-known classic games, and it\u2019s no wonder that it\u2019s got a huge fan base. After the remastered version of the original <em>Quake</em> was released in 2021 and warmly welcomed by the public, it made total sense for MachineGames to continue working on its sequel.</p>\n\n\n\n<p><strong>Have you seen the work that Reddit user </strong><a href=\"https://wccftech.com/quake-ii-remaster-looks-great-when-coupled-with-rtx-renderer/\"><strong>mSteward207</strong></a><strong> has done to make the <em>Quake II</em> remaster compatible with the NVIDIA RTX renderer? Any thoughts or suggestions to share?</strong></p>\n\n\n\n<p>Yes, I saw those screenshots. More importantly, I received and merged some pull requests on GitHub that improved the compatibility of <em>Quake II</em> <em>RTX</em> with the maps from the remaster. I\u2019m not ready to say what exactly is left to make all the remastered maps and models work really well with the NVIDIA RTX renderer at this time, but I guess there\u2019s some content work to be done, such as authoring PBR materials.</p>\n\n\n\n<p><strong>Do you believe that path tracing will ultimately be the standard for lighting and rendering in games?</strong></p>\n\n\n\n<p>It\u2019s already being used in some of this year\u2019s biggest games, including <em>Cyberpunk 2077: Phantom Liberty</em> and <em>Alan Wake II</em>. The most recent product from Lightspeed Studios, RTX Remix, is designed to bring path-traced lighting to many retro games, too. Perhaps most games will have a realistic rendering that features path-traced lighting, and some stylized games just won\u2019t need it. We\u2019ll see.</p>\n\n\n\n<p><strong>How can modders of classic games get similar results?</strong></p>\n\n\n\n<p>Use RTX Remix! It\u2019s still going through active development, but we\u2019ve seen some impressive results from the modding community already.</p>\n\n\n\n<p><strong>Was the process of bringing path tracing to <em>Minecraft</em> <em>with RTX</em> the same, or were there different approaches to the two games?</strong></p>\n\n\n\n<p>The principles of real-time path tracing used in <em>Quake II</em> <em>RTX</em> and <em>Minecraft</em> <em>with RTX</em> are largely the same, with some interesting differences that the cubic world of <em>Minecraft</em> enables, such as using an irradiance cache. <em>Minecraft</em> is also more challenging to render because its worlds are by design dynamic and user-modifiable, so you cannot bake anything at all or make map-specific tweaks.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Next steps</h2>\n\n\n\n<p>To play <em>Quake II</em> <em>RTX</em> for yourself, go to <a href=\"https://store.steampowered.com/agecheck/app/1089130/\">Steam</a>. For more information about NVIDIA RTX, see the <a href=\"https://developer.nvidia.com/rtx/path-tracing\">NVIDIA RTX Path Tracing SDK</a>. To get started with the ultimate modding platform, see <a href=\"https://www.nvidia.com/en-us/geforce/rtx-remix/\">RTX Remix</a> and our array of tools for <a href=\"https://developer.nvidia.com/industries/game-development\">game developers</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In 2019, if you wanted to check out the cutting edge in video game graphics, you needed an NVIDIA GeForce RTX 20 Series GPU and a copy of a game that was released in 1997, Quake II. With those pieces in hand, you could be among the first players in the world to see path &hellip; <a href=\"https://developer.nvidia.com/blog/q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update/\">Continued</a></p>\n", "protected": false}, "author": 508, "featured_media": 75812, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1320942", "discourse_permalink": "https://forums.developer.nvidia.com/t/q-a-looking-back-to-when-1997-s-quake-ii-got-a-path-tracing-update/276784", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [637, 1958, 483], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/qa-quake-rtx-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jIL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75811"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/508"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75811"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75811/revisions"}], "predecessor-version": [{"id": 75934, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75811/revisions/75934"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75812"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75811"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75811"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75811"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75874, "date": "2023-12-20T13:10:44", "date_gmt": "2023-12-20T21:10:44", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75874"}, "modified": "2023-12-20T15:52:41", "modified_gmt": "2023-12-20T23:52:41", "slug": "qa-real-time-ray-tracing-in-a-cinematic-scene", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/qa-real-time-ray-tracing-in-a-cinematic-scene/", "title": {"rendered": "Q&A: Real-Time Ray Tracing in a Cinematic Scene"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Six years ago, real-time ray tracing was seen as a pipe dream. Back then, cinematic-quality rendering required computer farms to slowly bake every frame overnight\u2014a painstaking process.&nbsp;</p>\n\n\n\n<p>By 2018, this level of performance was achievable in real-time, at 45 frames per second, enabling applications like video games to take a massive leap in graphical quality.&nbsp;</p>\n\n\n\n<p>As part of our <a rel=\"noreferrer noopener\" href=\"https://www.nvidia.com/en-us/geforce/news/rtx500-celebration-dlss-ray-tracing-new-games-win-prizes/\" target=\"_blank\">RTX 500 celebration</a>, we wanted to take a look back at NVIDIA\u2019s Project Sol. This real-time cinematic series showed off visuals at a level of fidelity that audiences had only ever seen in high-end animated films. Check them out below:</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/u8tDgvvGWSE?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video. Project Sol: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-center\"><strong>Additional videos in the series</strong><br><a rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=pNmhJx8yPLk\" target=\"_blank\">Video &#8211; Project Sol Part 2: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX</a><br><a rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=b2WOjo0C-xE\" target=\"_blank\">Video &#8211; Project Sol Part 3: A Real-Time Ray-Tracing Cinematic Scene Powered by NVIDIA RTX</a></p>\n\n\n\n<div style=\"height:14px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<p>We recently caught up with <a href=\"https://developer.nvidia.com/blog/author/gleone/\" target=\"_blank\" rel=\"noreferrer noopener\">Gabriele Leone</a>, a senior design director at NVIDIA and the creative director behind the Project Sol series.</p>\n\n\n\n<p><strong>When you first started working on the Project Sol real-time cinematic, what were your expectations for the final project, and how close did you get to your initial vision?</strong></p>\n\n\n\n<p>Working on Project Sol was an exhilarating experience, especially because it was our first dive into developing real-time content with consumer GPUs supporting real-time ray tracing. Our prior project, Reflections, was a collaboration with Epic that relied on the power of an NVIDIA DGX Station. But with Project Sol, we shifted to using just a single GeForce RTX 2080 GPU, which was a remarkable leap forward. We did come quite close to realizing our vision, although, as with any project, we faced our share of deadlines and constraints. But overall, it was a fantastic journey.</p>\n\n\n\n<p><strong>What is the difference between path tracing and ray tracing?</strong></p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/rtx/path-tracing/\" target=\"_blank\" rel=\"noreferrer noopener\">Path tracing</a> and <a href=\"https://developer.nvidia.com/rtx/ray-tracing\" target=\"_blank\" rel=\"noreferrer noopener\">ray tracing</a> are both rendering techniques, but they have key differences. Ray tracing is a technique for generating an image by tracing the path of light as pixels in an image plane, simulating effects like reflections and shadows. Path tracing, on the other hand, is a more comprehensive method that extends ray tracing by simulating the paths of many rays per pixel to produce more realistic lighting and shading. While ray tracing is great for producing visually accurate images, path tracing takes it further by accounting for the complex interactions of light in a scene.</p>\n\n\n\n<p><strong>Why was it so common for people to say that real-time ray tracing was at least 10 years away? How did NVIDIA speed up that timeline?</strong></p>\n\n\n\n<p>The notion that real-time ray tracing was over a decade away was widespread at the time, with some estimates even stretching to 15 years. NVIDIA approached this challenge from all fronts, both in hardware and software. The introduction of RT and Tensor Cores and the development of <a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/rtx/dlss\" target=\"_blank\">DLSS</a> played pivotal roles. It&#8217;s amazing to reflect on the tremendous strides made in just five years, accelerating what many thought was a distant future. Now we have 500 RTX games and apps using RT or DLSS.&nbsp;</p>\n\n\n\n<p><strong>In the Project Sol series, the hero\u2019s face is never fully lit. Is that due to challenges with realistic skin lighting?&nbsp;</strong></p>\n\n\n\n<p>The choice not to fully reveal the hero&#8217;s face was more a result of our project timeline constraints than technical challenges with skin lighting. Character development, particularly with realistic facial features, requires considerable time and resources, which we didn&#8217;t have at our disposal for this project. So, it wasn&#8217;t as much about the difficulties in rendering skin, but more about prioritizing project elements within the given time frame.</p>\n\n\n\n<p><strong>What artists or studios have influenced you?</strong></p>\n\n\n\n<p>My artistic influences are more rooted in the games I grew up playing than specific studios. Titles like <em>Warcraft 2 and 3</em>, <em>Starcraft: Brood War</em>, <em>Counter-Strike 1.6</em>, <em>Monkey Island 2 and 3</em>, <em>Broken Sword</em>, <em>Silent Hill 1 and 2</em>, Super <em>Smash Bros. Melee</em>, and <em>The Legend of Zelda: Ocarina of Time </em>and <em>Wind Waker</em> deeply influenced me. These games shaped my love for art and real-time graphics. In terms of artists, Dal\u00ed, Craig Mullins, and Sparth have been significant influences.</p>\n\n\n\n<p><strong>What\u2019s the next major challenge for graphics technology?</strong></p>\n\n\n\n<p>The next big frontier in graphics technology is achieving flawless real-time path tracing. This involves perfecting all possible effects under every conceivable condition, representing a significant leap in the realism and visual fidelity of real-time graphics.<br>Thanks to \u200ccontinuous advancements in RTX GPUs, which are becoming increasingly powerful, coupled with software innovations like DLSS 3.5, this path-traced reality is becoming more tangible. Games such as <em>Alan Wake 2</em> and <em>Cyberpunk 2077: Ultimate Edition</em> serve as prime examples of the direction in which we&#8217;re headed, indicating an even more exciting future for graphics technology.</p>\n\n\n\n<p><strong>What are your tips for developers who are just starting to add real-time ray tracing to their development pipeline?</strong></p>\n\n\n\n<p>For developers starting with real-time ray tracing, I&#8217;d advise focusing on how it can enhance the overall gaming experience. Ray tracing opens new possibilities that weren&#8217;t possible before. However, simply turning on RTX won&#8217;t unlock its full potential. Consider how ray-traced shadows, reflections, and global illumination can be strategically used. Thoughtful art direction, aligned with these capabilities, can significantly elevate your game&#8217;s visual impact.<br><br>Learn more about the <a href=\"https://developer.nvidia.com/rtx/ray-tracing\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA RTX platform</a> and our tools for <a href=\"https://developer.nvidia.com/industries/game-development\">game developers</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Six years ago, real-time ray tracing was seen as a pipe dream. Back then, cinematic-quality rendering required computer farms to slowly bake every frame overnight\u2014a painstaking process.&nbsp; By 2018, this level of performance was achievable in real-time, at 45 frames per second, enabling applications like video games to take a massive leap in graphical quality.&nbsp; &hellip; <a href=\"https://developer.nvidia.com/blog/qa-real-time-ray-tracing-in-a-cinematic-scene/\">Continued</a></p>\n", "protected": false}, "author": 508, "featured_media": 75886, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1320925", "discourse_permalink": "https://forums.developer.nvidia.com/t/q-amp-a-real-time-ray-tracing-in-a-cinematic-scene/276777", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 503], "tags": [1958, 483, 1480], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/game-dev-press-rtx-500-project-sol-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jJM", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75874"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/508"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75874"}], "version-history": [{"count": 27, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75874/revisions"}], "predecessor-version": [{"id": 75927, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75874/revisions/75927"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75886"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75874"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75874"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75874"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75832, "date": "2023-12-20T11:03:54", "date_gmt": "2023-12-20T19:03:54", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75832"}, "modified": "2023-12-20T13:39:11", "modified_gmt": "2023-12-20T21:39:11", "slug": "new-release-nvidia-tao-5-2", "status": "publish", "type": "post", "link": "https://nvda.ws/47brM22", "title": {"rendered": "New Release: NVIDIA TAO 5.2"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>With the latest NVIDIA TAO 5.2, you can now run zero-shot inference for panoptic segmentation with ODISE, create custom 3D object pose models, and boost inference throughput for vision transformers using FasterViT. Download now.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>With the latest NVIDIA TAO 5.2, you can now run zero-shot inference for panoptic segmentation with ODISE, create custom 3D object pose models, and boost inference throughput for vision transformers using FasterViT. Download now.</p>\n", "protected": false}, "author": 1115, "featured_media": 75833, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1320888", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-release-nvidia-tao-5-2/276767", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/47brM22", "_links_to_target": "_blank"}, "categories": [2724, 2758, 63], "tags": [3312, 1958], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/gtc-s23-promo-tao-blender-kv-2716100-blog-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jJ6", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75832"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75832"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75832/revisions"}], "predecessor-version": [{"id": 75837, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75832/revisions/75837"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75833"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75832"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75832"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75832"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75170, "date": "2023-12-20T10:00:00", "date_gmt": "2023-12-20T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75170"}, "modified": "2023-12-20T10:06:09", "modified_gmt": "2023-12-20T18:06:09", "slug": "just-released-nvidia-cublasmp", "status": "publish", "type": "post", "link": "https://nvda.ws/3v8CJ7h", "title": {"rendered": "Just Released: cuBLASMp"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>cuBLASMp is a high-performance, multi-process, GPU-accelerated library for distributed basic dense linear algebra. It is available to download in Preview now.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>cuBLASMp is a high-performance, multi-process, GPU-accelerated library for distributed basic dense linear algebra. It is available to download in Preview now.</p>\n", "protected": false}, "author": 1140, "featured_media": 75171, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "", "discourse_permalink": "", "wpdc_publishing_response": "success", "wpdc_publishing_error": "Embed url has already been taken", "footnotes": "", "_links_to": "https://nvda.ws/3v8CJ7h", "_links_to_target": "_blank"}, "categories": [852, 696, 3110], "tags": [1958], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuBLASMp-featured-image.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jyq", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75170"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1140"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75170"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75170/revisions"}], "predecessor-version": [{"id": 75843, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75170/revisions/75843"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75171"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75170"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75170"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75170"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75395, "date": "2023-12-19T11:00:00", "date_gmt": "2023-12-19T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75395"}, "modified": "2023-12-18T11:51:41", "modified_gmt": "2023-12-18T19:51:41", "slug": "breakthrough-in-functional-annotation-with-hifi-nn", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/", "title": {"rendered": "Breakthrough in Functional Annotation with HiFi-NN"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Enzymes are vital biological catalysts for a multitude of processes, from cellular metabolism to industrial manufacturing. The applications of artificial intelligence for enzyme generation is an exciting field of research with direct applications in the life sciences. Advances in these scientific challenges are a critical necessity to further advance drug discovery, environmental science, and bioengineering.&nbsp;</p>\n\n\n\n<p>Currently, only a tiny fraction of Earth&#8217;s vast array of life forms has been sequenced, hindering the broader application and generalization of machine learning algorithms within the complex realm of sequence design. Improved methods for functional annotation are a vital component in enzyme research, enabling the identification and characterization of the functions of newly discovered enzymes. This is key to understanding complex biological processes and enhancing the data used for generative workflows.</p>\n\n\n\n<p><a href=\"https://www.basecamp-research.com/\">Basecamp Research</a>, a London-based Bio-AI company and <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception</a> member, recently used NVIDIA GPUs to train a Hierarchically Fine-tuned Nearest Neighbor method (HiFi-NN). This approach has shown significant improvements over existing models in recall, precision, and F1 scores, surpassing the state-of-the-art (SoTA) in enzyme annotation by over 15%.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Unique data collected in global expeditions</h2>\n\n\n\n<p>Basecamp Research tackles the most complex biological design challenges across the biotech industry. Existing datasets have significant shortcomings:</p>\n\n\n\n<ul>\n<li>Small representativity, covering only <a href=\"https://www.pnas.org/doi/10.1073/pnas.1521291113#:~:text=When%20extended%20to,12%20microbial%20species.\">0.001% of life on earth</a></li>\n\n\n\n<li>No consistent metadata</li>\n\n\n\n<li>Lack of stakeholder consent and engagement before data collection</li>\n</ul>\n\n\n\n<p>Basecamp opted to develop its proprietary biological data resource through biodiversity partnerships with nature parks across five continents and 23 countries. They sent their scientists on worldwide expeditions to discover new genomes, enzymes, and biological relationships from the most extreme and extraordinary biomes.&nbsp;</p>\n\n\n\n<p>In under two years, they created BaseGraph, the largest knowledge graph of natural biodiversity, containing over 5.5B relationships with a genomic context exceeding 70 kilobases per protein. Their extensive long-read sequencing is complemented by comprehensive metadata collection, enabling them to link proteins of interest to specific reactions and desired process conditions.&nbsp;</p>\n\n\n\n<p>Basecamp Research\u2019s AI strategy is data-centric for two reasons: their proprietary data enhances model performance in a rapidly commoditizing algorithmic landscape and significantly compensates for the lack of diversity in publicly available data.&nbsp;</p>\n\n\n\n<p>Their knowledge graph, built from the ground up, captures and re-creates the complexities of four billion years of protein evolution in nature. This data advantage enables their AI and product teams to outperform SoTA annotation and design models, addressing complex design challenges in biotech industries, from gene-writing therapeutics to plastic degradation.</p>\n\n\n\n<h2 class=\"wp-block-heading\">In silico functional annotation</h2>\n\n\n\n<p>To tackle the challenge of in silico functional annotation of proteins and enzymes, Basecamp Research&#8217;s deep learning (DL) team developed HiFi-NN search. HiFi-NN annotates protein sequences with enzyme commission (EC) numbers beating the current bioinformatics tool of choice (<a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&amp;PAGE_TYPE=BlastSearch&amp;BLAST_SPEC=&amp;LINK_LOC=blasttab&amp;LAST_PAGE=blastp\">blastp</a>), as well as other SoTA DL models, including <a href=\"https://www.science.org/doi/abs/10.1126/science.adf2465\">CLEAN</a>, in precision and recall (Table 1).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Method</strong></td><td><strong>Recall</strong></td><td><strong>Precision</strong></td><td><strong>F1-score</strong></td></tr><tr><td>ECPred</td><td>0.0197</td><td>0.0197</td><td>0.0197</td></tr><tr><td>DEEPpre</td><td>0.0403</td><td>0.0415</td><td>0.0386</td></tr><tr><td>DeepEC</td><td>0.0724</td><td>0.1184</td><td>0.0846</td></tr><tr><td>ProteInfer</td><td>0.1382</td><td>0.2434</td><td>0.1662</td></tr><tr><td>ProteinVec</td><td>0.2961</td><td>0.4901</td><td>0.3378</td></tr><tr><td>BLASTp</td><td>0.3750</td><td>0.5083</td><td>0.3852</td></tr><tr><td>DeepECtransformer</td><td>0.3026</td><td>0.5263</td><td>0.3511</td></tr><tr><td>CLEAN</td><td>0.4671</td><td>0.5844</td><td>0.4947</td></tr><tr><td>HiFi-NN (Swissprot)</td><td>0.5724</td><td>0.5505</td><td>0.5304</td></tr><tr><td><strong>HiFi-NN (Swissprot + 3M curated sequences)</strong></td><td><strong>0.5921</strong></td><td><strong>0.6657</strong></td><td><strong>0.6015</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. HiFi-NN results compared to existing tools and other SoTA DL models</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\">BCR 3M refers to the version of the model retrained with 3M selected, environmentally diverse sequences from Basecamp Research\u2019s BaseGraph.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"761\" height=\"443\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results.png\" alt=\"Bar chart comparing the Recall, Precision, and F1 scores of seven bioinformatics models, with HiFi-NN models outperforming others.\" class=\"wp-image-75425\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results.png 761w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-300x175.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-625x364.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-645x375.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-500x291.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-155x90.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-362x211.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-results-189x110.png 189w\" sizes=\"(max-width: 761px) 100vw, 761px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. HiFi-NN results compared to SoTA models in Enzyme Functional Annotation on the price-149 enzyme benchmarking dataset</em></figcaption></figure></div>\n\n\n<p>Developed in collaboration with advisors Noelia Ferruz and Kevin Yang, HiFi-NN was accepted by NeurIPS, a prestigious AI conference, for presentation at their Machine Learning for Structural Biology workshop in December 2023.&nbsp;</p>\n\n\n\n<p>The model uses contrastive learning of EC numbers and the inherent hierarchy of the enzyme commission annotation system for natural augmentation. Trained on eight NVIDIA A100 GPUs on a Lambda Labs instance with CUDA version 11.8 and NCCL version 2.14.3, it employs PyTorch Lightning for distributed-data parallel training. Experiment management and tracking are done through the Hydra framework and Weights and Biases. The model boasts over 3M parameters.</p>\n\n\n\n<p>HiFi-NN&#8217;s superior performance in SoTA annotation methods is attributed to using the hierarchical nature of enzyme function represented in the EC numbering system and supplementing the training set with proprietary sequences from Basecamp\u2019s knowledge graph.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Proprietary sequences</h2>\n\n\n\n<p>The proprietary Basecamp sequences that HiFi-NN is supplemented with originate from environments spanning five continents, and a 110<sup>o</sup> C temperature range to ensure as much sequence and environmental diversity in the training set as possible. As a result, HiFi-NN outperforms all SoTA models on benchmarking datasets and performs particularly well on protein sequences from functional dark matter, that is, those sequences with low similarity to any known enzymes.&nbsp;</p>\n\n\n\n<p>In fact, the Basecamp team used HiFi-NN to annotate a large representative portion of the <a href=\"https://www.ebi.ac.uk/metagenomics\"><em>MGnify</em></a> microbial protein database that was previously not annotated.</p>\n\n\n\n<p>In addition to outperforming all previous annotation models, HiFi-NN is also particularly easy to use and generates annotation labels at great speed. For example, it annotates the entire human proteome within 24 minutes on a single NVIDIA A100 GPU.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Fast enzyme identification with Johnson Matthey&nbsp;</h2>\n\n\n\n<p>Breakthroughs like HiFi-NN, which enhance our capacity to predict the physical properties of biological entities, are set to reduce the need for extensive screening of candidates using resource-heavy lab methods.</p>\n\n\n\n<p>Basecamp Research&#8217;s partnership with an FTSE100 chemicals company, <a href=\"https://matthey.com/media/2023/basecamp-research-partnership\" target=\"_blank\" rel=\"noreferrer noopener\">Johnson Matthey</a><em>,</em> underscores the importance of computational advancements in addressing industrial challenges. A notable project with this partner involved their researchers spending over a year and a half testing thousands of enzyme variants in their labs without success.</p>\n\n\n\n<p>Johnson Matthey&#8217;s objective was to find an enzyme with broad specificity capable of processing multiple bulky substrates, a more complex task compared to working with smaller substrates. Within a week, Basecamp Research employed its entirely in silico techniques to identify an enzyme (Figure 2) meeting these criteria, positioning it for potential commercialization.</p>\n\n\n\n<p>The research group&#8217;s leader expressed admiration for Basecamp Research&#8217;s capability to rapidly discover and develop an enzyme, a task that had eluded them for years. This success laid the groundwork for an expanded collaborative effort on various enzyme development initiatives.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-medium\"><img decoding=\"async\" loading=\"lazy\" width=\"260\" height=\"300\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-260x300.png\" alt=\"Visual representation of a three-dimensional biomolecular structure.\" class=\"wp-image-75423\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-260x300.png 260w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-625x722.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-100x115.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-78x90.png 78w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-362x418.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein-95x110.png 95w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/basecamp-protein.png 634w\" sizes=\"(max-width: 260px) 100vw, 260px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Basecamp Research&#8217;s proprietary protein</em></figcaption></figure></div>\n\n\n<h2 class=\"wp-block-heading\">Bolstering AI development in life sciences&nbsp;</h2>\n\n\n\n<p>Functional annotation plays a pivotal role for researchers, especially in practical scenarios:&nbsp;</p>\n\n\n\n<ul>\n<li>In drug discovery, it aids in the creation of targeted treatments by elucidating enzyme interactions within the body.&nbsp;</li>\n\n\n\n<li>In the realm of industrial biotechnology, it enables the bespoke design of enzymes, tailored for specific industrial applications, promoting more environmentally friendly production methods.&nbsp;</li>\n\n\n\n<li>It also offers crucial insights into evolutionary biology by revealing the developmental trajectory of enzymes across different species.&nbsp;</li>\n</ul>\n\n\n\n<p>In essence, functional annotation, driven by machine learning, transcends its role as a mere scientific instrument. it acts as a catalyst for innovation and exploration across diverse sectors, including healthcare and environmental science.</p>\n\n\n\n<p>Basecamp Research, along with other life sciences entities, is integrating its workflow with NVIDIA BioNeMo, a generative AI platform geared towards drug discovery. This platform streamlines and expedites the training of models. With BioNeMo, organizations can tailor and deploy AI models for various purposes, including 3D protein structure prediction, <em>de novo</em> protein and small molecule generation, property prediction, and molecular docking.</p>\n\n\n\n<p>If you&#8217;re interested in exploring BioNeMo, opportunities are available through the BioNeMo Framework beta release or the API early access program., For more information, see the <a href=\"https://www.nvidia.com/en-gb/clara/bionemo/\">NVIDIA BioNeMo</a> product page.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Enzymes are vital biological catalysts for a multitude of processes, from cellular metabolism to industrial manufacturing. The applications of artificial intelligence for enzyme generation is an exciting field of research with direct applications in the life sciences. Advances in these scientific challenges are a critical necessity to further advance drug discovery, environmental science, and bioengineering.&nbsp; &hellip; <a href=\"https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/\">Continued</a></p>\n", "protected": false}, "author": 1575, "featured_media": 75420, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1319917", "discourse_permalink": "https://forums.developer.nvidia.com/t/breakthrough-in-functional-annotation-with-hifi-nn/276637", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [2385], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hifi-nn-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-jC3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75395"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1575"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75395"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75395/revisions"}], "predecessor-version": [{"id": 75810, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75395/revisions/75810"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75420"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75395"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75395"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75395"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 74885, "date": "2023-12-19T09:50:21", "date_gmt": "2023-12-19T17:50:21", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=74885"}, "modified": "2024-01-11T11:49:37", "modified_gmt": "2024-01-11T19:49:37", "slug": "year-in-review-trending-posts-of-2023", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2023/", "title": {"rendered": "Most Popular NVIDIA Technical Blog Posts of 2023: Generative AI, LLMs, Robotics, and Virtual Worlds Breakthroughs"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>As we approach the end of another exciting year at NVIDIA, it&#8217;s time to look back at the most popular stories from the NVIDIA Technical Blog in 2023. </p>\n\n\n\n<p>Groundbreaking research and developments in fields such as generative AI, large language models (LLMs), high-performance computing (HPC), and robotics are leading the way in transformative AI solutions and capturing the interest of our readers. Other top posts explore advancements in video technology and video conferencing, enhancing the user experience, alongside breakthroughs in AI security.</p>\n\n\n\n<p>The following are some of the highlights from 2023.</p>\n\n\n\n<div style=\"height:40px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-625x352.jpg\" alt=\"A group of different animals standing together.\" class=\"wp-image-59217 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-625x352.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-300x169.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-179x101.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-768x432.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-1536x864.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-500x281.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-160x90.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-362x204.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-196x110.jpg 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1-1024x576.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/Gen-AI-Blog-Feature-Image-1600x9001-1.jpg 1600w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai/\" target=\"_blank\">Rapidly Generate 3D Assets for Virtual Worlds with Generative AI</a></p>\n\n\n\n<p style=\"font-size:14px\">New generative AI technologies on NVIDIA Omniverse enhance 3D asset creation in virtual environments. These advancements aim to make the creation of virtual worlds on the metaverse faster and easier. </p>\n</div></div>\n\n\n\n<div style=\"height:40px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><a href=\"https://developer.nvidia.com/blog/updating-the-cuda-linux-gpg-repository-key/\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"351\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-625x351.png\" alt=\"Person in a video conference using Eye Contact feature with eye contact directly at the camera.\" class=\"wp-image-59544 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/Video-Conferencing-Maxine-Eye-Contact-2.png 1019w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></a></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact/\" target=\"_blank\">Improve Human Connection in Video Conferences with NVIDIA Maxine Eye Contact</a></p>\n\n\n\n<p style=\"font-size:14px\">NVIDIA Maxine Eye Contact revolutionizes video conferencing by using AI to adjust your gaze toward the camera in real time. It also maintains natural eye color and adapts to different head positions and gaze directions, creating a more authentic and connected virtual interaction.</p>\n</div></div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-625x352.png\" alt=\"TensorRTLLM illustration.\" class=\"wp-image-70556 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-2048x1152.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/09/inference-visual-tensor-rt-llm-1024x576.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\" target=\"_blank\">NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs</a></p>\n\n\n\n<p style=\"font-size:14px\">NVIDIA TensorRT-LLM, a component of the NVIDIA NeMo framework, is tailored to boost LLM inference on NVIDIA H100 GPUs. This open-source library offers optimized processing and supports multi-GPU and multi-node setups, enabling efficient and scalable deployment of LLMs in generative AI applications.</p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--625x352.png\" alt=\"NVIDIA Jetson Orin Nano Developer Kit\" class=\"wp-image-62248 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render--1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/03/jetson-orin-nano-developer-kit-3d-render-.png 1829w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/tag/diffusion-models/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/develop-ai-powered-robots-smart-vision-systems-and-more-with-nvidia-jetson-orin-nano-developer-kit/\" target=\"_blank\">Develop AI-Powered Robots, Smart Vision Systems, and More with NVIDIA Jetson Orin Nano Developer Kit</a></p>\n\n\n\n<p style=\"font-size:14px\">The latest NVIDIA Jetson Orin Nano Developer Kit is a powerful tool for developing AI-powered robots and smart vision systems. Offering a huge boost in AI performance over the prior generation,&nbsp;it is compatible with all NVIDIA Jetson Orin Nano and NX modules for prototyping edge AI products. </p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-625x352.png\" alt=\"NeMo Guardrails illustration.\" class=\"wp-image-63747 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/04/nemo-guardrails-social-devnews-1920x1080-1.png 1920w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/tag/diffusion-models/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems/\" target=\"_blank\">NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems</a></p>\n\n\n\n<p style=\"font-size:14px\">A toolkit for developing safe and trustworthy LLM conversational systems, NeMo Guardrails enables developers to implement rules that maintain safe and relevant conversations. It integrates with LLMs like ChatGPT, is built on the NVIDIA Colang language, and is available through NVIDIA AI Foundations.</p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-625x352.png\" alt=\"LLM workflow demo.\" class=\"wp-image-66857 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/llm-social-working-with-llms-2694250-1920x1080-1-1536x864-1.png 1536w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/\" target=\"_blank\">An Introduction to Large Language Models: Prompt Engineering and P-Tuning</a></p>\n\n\n\n<p style=\"font-size:14px\">This introduction to LLMs covers key techniques like prompt engineering and tuning. It discusses how LLMs function, their role in AI applications like text generation, and the significance of creating effective prompts and optimizing performance in various scenarios.</p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-625x352.jpg\" alt=\"Two men working at a desktop computer in an office.\" class=\"wp-image-66877 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-625x352.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-300x169.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-179x101.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-768x432.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-1536x864.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-645x363.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-960x540.jpg 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-500x281.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-160x90.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-362x204.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-196x110.jpg 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team-1024x576.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/NVIDIA-AI-Red-Team.jpg 1600w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/tag/diffusion-models/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/\" target=\"_blank\">NVIDIA AI Red Team: An Introduction</a></p>\n\n\n\n<p style=\"font-size:14px\">The NVIDIA AI Red Team details its approach to assessing and mitigating risks in AI and machine learning systems from an information security standpoint. A group of security professionals and data scientists, they aim to identify and address risks related to technical vulnerabilities, harm and abuse scenarios, and other security challenges in ML systems.</p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-625x352.png\" alt=\"Grace CPU Superchip illustration.\" class=\"wp-image-59834 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/grace-cpu-superchip-1.png 1308w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-cpu-superchip-architecture-in-depth/\" target=\"_blank\">NVIDIA Grace CPU Superchip Architecture In-Depth</a></p>\n\n\n\n<p style=\"font-size:14px\">Take an in-depth look at the architecture and features of the NVIDIA Grace CPU Superchip. Offering major advancements in compute density and power efficiency, the Grace CPU excels in memory bandwidth and data movement efficiency, making it a powerhouse for HPC and AI workloads.</p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-625x352.png\" alt=\"A side-by-side comparison of two versions of a graphic. \" class=\"wp-image-59495 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/01/NVENC-video-performance-comparison-1.png 1920w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/tag/diffusion-models/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture/\" target=\"_blank\">Improving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture</a></p>\n\n\n\n<p style=\"font-size:14px\">Improve video quality and performance using AV1 codec and the NVIDIA Ada Lovelace architecture. This integration enhances video encoding and decoding, improving compression efficiency, quality, and increased throughput, making it ideal for various video applications.</p>\n</div></div>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-center\" style=\"grid-template-columns:40% auto\"><figure class=\"wp-block-media-text__media\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-625x352.png\" alt=\"TensorRT-LLM improves ease of use and extensibility through an open-source modular Python API for defining, optimizing, and executing new architectures and enhancements as LLMs evolve, and can be customized easily.\" class=\"wp-image-75907 size-full\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/inference-visual-tensor-rt-llm-2048x1152-1.png 2048w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure><div class=\"wp-block-media-text__content\">\n<p class=\"has-medium-font-size\"><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/\" target=\"_blank\"></a><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/blog/tag/diffusion-models/\" target=\"_blank\"></a><a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs</a></p>\n\n\n\n<p style=\"font-size:14px\">TensorRT-LLM consists of the TensorRT deep learning compiler and includes optimized kernels, pre\u2013 and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs.</p>\n</div></div>\n\n\n\n<div style=\"height:5px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div style=\"height:5px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<p><a rel=\"noreferrer noopener\" href=\"https://developer.nvidia.com/email-signup\" target=\"_blank\">Subscribe</a> to the Developer Newsletter and stay in the loop on 2024 content tailored to your interests.  Follow us on <a rel=\"noreferrer noopener\" href=\"https://www.instagram.com/nvidiadeveloper/\" target=\"_blank\">Instagram</a>, <a rel=\"noreferrer noopener\" href=\"https://twitter.com/NVIDIADeveloper\" target=\"_blank\">Twitter</a>, <a rel=\"noreferrer noopener\" href=\"https://www.youtube.com/user/NVIDIADeveloper\" target=\"_blank\">YouTube</a>, and <a rel=\"noreferrer noopener\" href=\"https://discord.com/invite/nvidiadeveloper?mkt_tok=MTU2LU9GTi03NDIAAAGIbmeGj6XTWyk3hajR2JsQlvNH8norVxFBM2Wu_4jx48J-t_yUHNLOOQk7xJ8IE4Fd9z5ENIh5sMOpiSePtDYPSOLMHyb7s7tfiQxXMzBR0v7Xv7HW8Q\" target=\"_blank\">Discord</a> for the latest developer news.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As we approach the end of another exciting year at NVIDIA, it&#8217;s time to look back at the most popular stories from the NVIDIA Technical Blog in 2023. Groundbreaking research and developments in fields such as generative AI, large language models (LLMs), high-performance computing (HPC), and robotics are leading the way in transformative AI solutions &hellip; <a href=\"https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2023/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 75827, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1107161", "discourse_permalink": "https://forums.developer.nvidia.com/t/year-in-review-trending-posts-of-2022/238561", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1235, 852, 3110, 63, 503, 1903], "tags": [3139, 453, 1944, 608, 2932, 1953, 1958, 1962, 3673], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/ov-siggraph-22-1600x900-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jtP", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74885"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=74885"}], "version-history": [{"count": 46, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74885/revisions"}], "predecessor-version": [{"id": 75912, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/74885/revisions/75912"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75827"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=74885"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=74885"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=74885"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75563, "date": "2023-12-18T13:44:27", "date_gmt": "2023-12-18T21:44:27", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75563"}, "modified": "2024-01-04T13:33:19", "modified_gmt": "2024-01-04T21:33:19", "slug": "accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/", "title": {"rendered": "Accelerate Quantum Circuit Simulation with NVIDIA cuQuantum 23.10"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p><a href=\"https://developer.nvidia.com/cuquantum-sdk\">NVIDIA cuQuantum</a> is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use it to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.&nbsp;</p>\n\n\n\n<p>cuQuantum aims to deliver at the speed of light on NVIDIA GPUs and CPUs for quantum circuit simulations. Users of quantum computing frameworks can leverage cuQuantum-backed simulators to realize GPU acceleration for their workloads.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">What\u2019s new in cuQuantum 23.10?&nbsp;</h2>\n\n\n\n<p>cuQuantum 23.10 includes updates to both <a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/cutensornet/index.html\">NVIDIA cuTensorNet</a> and <a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/custatevec/index.html\">NVIDIA cuStateVec</a>. New features include support for NVIDIA Grace Hopper systems. For more information, see the<a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/cuquantum_sdk_release_notes.html#cuquantum-sdk-v23-10\"> cuQuantum 23.10 release notes.</a>&nbsp;&nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\">Tensor network high-level APIs and gradients&nbsp;</h3>\n\n\n\n<p>cuTensorNet provides high-level APIs to facilitate quantum simulator developers to program in an intuitive way to make the most of their capabilities. This technology enables developers to abstract away specific tensor network knowledge when creating simulators. This makes building a tensor-network-based quantum simulator simpler, as it covers expectations, measurements, samples, and other elements.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"825\" height=\"440\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1.png\" alt=\"The chart shows that at a range of scales, cuTensorNet outperforms TensorCircuit with cotengra for pathfinding and PyTorch and JAX for contractions by as much as 4-5.9x on the same NVIDIA H100 GPUs.\u00a0\" class=\"wp-image-75728\" style=\"aspect-ratio:1.875;object-fit:cover;width:833px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1.png 825w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-625x333.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-768x410.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-645x344.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-500x267.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-362x193.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/QAOA-TN-Benchmarks-on-NVIDIA-H100-GPUs-1-206x110.png 206w\" sizes=\"(max-width: 825px) 100vw, 825px\" /><figcaption class=\"wp-element-caption\">Fig 1. Running the QAOA algorithm (p=1) on cuTensorNet high-level APIs compared with TensorCircuit&#8217;s cotengra pathfinder and both PyTorch and Jax contractions. We show significant speedup that scales better and is up 4-5.9x faster on the same H100 Hardware</figcaption></figure></div>\n\n\n<p>We&#8217;ve introduced <a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/cutensornet/api/functions.html#_CPPv435cutensornetComputeGradientsBackwardK19cutensornetHandle_t28cutensornetContractionPlan_tA_PCKvPKvA_PCv7int32_t32cutensornetWorkspaceDescriptor_t12cudaStream_t\">experimental support for gradient calculations</a> from a given tensor network aimed at accelerating quantum machine learning with tensor networks. This enables drastic speedups for QML and adjoint differentiation-based workflows with cuTensorNet.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Scale up state vector simulations with fewer devices&nbsp;</h3>\n\n\n\n<p>cuStateVec introduces new APIs for<a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/custatevec/host_state_vector_migration.html\"> host-to-device state vector swap</a>, which enables the use of CPU memory with GPUs to further scale simulations. Now, 40 qubit state vector simulations only require 16 NVIDIA Grace Hopper systems instead of 128 NVIDIA H100 80GB GPUs. As you can see by the speedup offered by these systems, NVIDIA Grace Hopper drastically outperforms the NVIDIA Hopper GPU architecture when used with other CPUs, and CPU-only implementations. This leads to substantial cost and energy savings for each workload.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img decoding=\"async\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/36-Qubit-Runtimes-for-GPU-CPU-Swap-Simulations-5.png\" alt=\"A graph showing the runtime for 36 qubit algorithms performed with DGX H100 80 GB + Intel Xeon 8480CL, GH200 (one GPU, one CPU), and Intel Xeon 8480CL. \" class=\"wp-image-75564\" style=\"aspect-ratio:1.6172506738544474;width:734px;height:auto\"/><figcaption class=\"wp-element-caption\">Figure 2. <em>cuQuantum on one NVIDIA Grace Hopper Superchip efficiently simulates a 36-qubit system that otherwise requires eight NVIDIA H100 GPUs and is faster than other CPU + GPU combinations. GH200 is 5.1-8.8x faster than Intel Xeon Platinum 8480CL </em></figcaption></figure>\n\n\n\n<p>Additional API-level and kernel-level optimizations have been made to improve performance further. Grace Hopper systems provide better runtimes than other CPU and Hopper systems. The chip-to-chip interconnect and better CPU afford faster runtimes.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" width=\"683\" height=\"423\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup.png\" alt=\"A graph showing the speedup for 33-qubit Quantum Fourier Transform performed with DGX H100 80 GB, GH200 (one GPU), and Intel Xeon 8480CL.\u00a0\" class=\"wp-image-75580\" style=\"aspect-ratio:1.6146572104018913;width:747px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup.png 683w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-484x300.png 484w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/cuQuantum-speedup-178x110.png 178w\" sizes=\"(max-width: 683px) 100vw, 683px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. cuQuantum on NVIDIA GH200 is 94x faster at running 33-qubit Quantum Fourier Transform simulations than Intel Xeon 8480CL dual-socket. NVIDIA H100, which launches with the same Intel CPU is 61x faster</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Getting started with cuQuantum&nbsp;</h2>\n\n\n\n<p>cuQuantum offers documentation to help with <a href=\"https://docs.nvidia.com/cuda/cuquantum/latest/getting_started.html\">getting starte</a>d. If you are running \u200ca CSP, we encourage users to check out marketplace listings for each major CSP.&nbsp;</p>\n\n\n\n<p>After you have set up your environment, we recommend checking out our <a href=\"https://github.com/NVIDIA/cuQuantum/tree/main/benchmarks\">benchmark suite on GitHub</a> and validating that you are engaging GPUs in your benchmarks.&nbsp;&nbsp;</p>\n\n\n\n<p>Please reach out with questions, requests, or issues on <a href=\"https://github.com/NVIDIA/cuQuantum/tree/main/benchmarks\">GitHub</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows. With NVIDIA Tensor Core GPUs, developers can use it to speed up quantum circuit simulations based on state vector and tensor network methods by orders of magnitude.&nbsp; cuQuantum aims to deliver at the speed of light on NVIDIA GPUs and &hellip; <a href=\"https://developer.nvidia.com/blog/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/\">Continued</a></p>\n", "protected": false}, "author": 1548, "featured_media": 75597, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1318786", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/276507", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [2735], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc06-hopper-family-gtc22-spring-1600x900-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-jEL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75563"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1548"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75563"}], "version-history": [{"count": 25, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75563/revisions"}], "predecessor-version": [{"id": 76192, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75563/revisions/76192"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75597"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75563"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75563"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75563"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75435, "date": "2023-12-18T11:49:31", "date_gmt": "2023-12-18T19:49:31", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75435"}, "modified": "2024-01-10T10:03:24", "modified_gmt": "2024-01-10T18:03:24", "slug": "teaching-avs-the-language-of-human-driving-behavior-with-trajeglish", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/", "title": {"rendered": "Teaching AVs the Language of Human Driving Behavior with Trajeglish"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Much of the communication between drivers goes beyond turn signals and brake lights. Motioning another car to proceed, looking over to see if another driver is paying attention\u2014even the friendly Jeep wave\u2014all rely on human-based communication rather than vehicle technology.</p>\n\n\n\n<p>As autonomous vehicles (AV) must coexist with human drivers for the foreseeable future, they must be able to interpret this behavior to make safe decisions that don\u2019t interrupt the flow of traffic.</p>\n\n\n\n<p>To address this challenge in training, developers must be able to predict how the future motion of other vehicles is affected by an AV\u2019s actions. In a <a href=\"https://research.nvidia.com/labs/toronto-ai/trajeglish/\">recently published paper</a>, the NVIDIA Research team outlines Trajeglish, an approach to traffic modeling that tokenizes motion in the same way language models tokenize words and phrases to simulate realistic multi-vehicle driving scenarios.</p>\n\n\n\n<p>When compared with 16 other traffic models in the first iteration (V0) of the <a href=\"https://waymo.com/open/challenges/2023/sim-agents-v0\">Waymo Sim Agents Challenge</a>, this tokenization approach resulted in the most realistic traffic trajectories, showing a 3.3% improvement over the previous state-of-the-art model.&nbsp;</p>\n\n\n\n<p>Trajeglish models multi-agent traffic scenarios by breaking each scenario down into tokens, in the same way a language model breaks down a paragraph into words and phrases. By doing so, it can consider each agent and trajectory in relation to each other, predicting motions that cover the full range of possible interactions given their initial locations.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"748\" height=\"319\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios.png\" alt=\"Eight different traffic scenarios showing various predicted trajectories based on the initial positions of the vehicles.\" class=\"wp-image-75452\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios.png 748w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-300x128.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-625x267.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-645x275.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-500x213.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/traffic-scenarios-258x110.png 258w\" sizes=\"(max-width: 748px) 100vw, 748px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Scenarios modeled by Trajeglish given only the initial timestep of the driving log. The initial state used to prompt the model is shown in black.</em></figcaption></figure></div>\n\n\n<p>When given only the initial timestep of a real-world scenario, Trajeglish closely models the log data to realistically simulate how other vehicles react to the actions of the AV.</p>\n\n\n\n<h2 class=\"wp-block-heading\">Modeling human behavior</h2>\n\n\n\n<p>Simulating human driving behavior is relatively straightforward in single-lane highway scenarios, where there are few intersections, objects, or pedestrians.</p>\n\n\n\n<p>Modeling multiple vehicles in urban settings, however, is significantly more difficult given the increase in traffic and road variety. To build traffic models that generalize to a wider range of scenarios, recent approaches aim to imitate driving behavior observed in driving logs.</p>\n\n\n\n<p>Doing so in simulation requires sampling realistic actions for an agent at each timestep that are compatible with actions chosen by all other agents at the current timestep\u2014a relationship known as intra-timestep dependence.</p>\n\n\n\n<p>While actors in the real world behave independently, intra-timestep dependence in traffic modeling is necessary as driving logs are recorded at discrete timesteps, thus any interaction between timesteps appears as coordinated behavior. Communication that is not generally recorded in log data, such as eye contact or turn signals, also contributes to the appearance of coordination among actors in a recorded scenario.&nbsp;</p>\n\n\n\n<p>Trajeglish explicitly models this intra-timestep dependence. It achieves this by tokenizing a given scenario\u2014in the same manner as a language model\u2014enabling the model to predict only the likely trajectories, or tokens, based on the context of the scene. Trajeglish then models the next actions in the timestep by analyzing the distribution of all the tokenized scenarios.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"738\" height=\"156\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories.png\" alt=\"Three images showing three different timesteps, where in each one, a future position is chosen based on its proximity to the current location.\" class=\"wp-image-75456\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories.png 738w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-300x63.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-625x132.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-179x38.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-645x136.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-500x106.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-160x34.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-362x77.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/tokenized-trajectories-520x110.png 520w\" sizes=\"(max-width: 738px) 100vw, 738px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Trajeglish tokenizes trajectories by iteratively finding the token with minimum corner distance to the next state.</em></figcaption></figure></div>\n\n\n<p>This process of predicting the next token continuously builds on itself. After a chosen number of tokens are sampled, Trajeglish has enough context to predict scenarios of various lengths with an arbitrary number of agents.</p>\n\n\n\n<h2 class=\"wp-block-heading\">A leading approach</h2>\n\n\n\n<p>Trajeglish was compared with 16 other models in the V0 leaderboard of the <a href=\"https://waymo.com/open/challenges/2023/sim-agents-v0\">Waymo Sim Agents Challenge</a>. Each model was tasked with simulating 32 scene-consistent trajectories for up to 128 agents at a time, given 1 second of initial driving information.</p>\n\n\n\n<p>The challenge evaluated the realism of each simulation based on <em>distribution matching</em>. Several statistics were computed over the simulated scenarios and compared to the same statistics computed on the recorded scenarios. The closer these statistics matched each other, the higher the score.</p>\n\n\n\n<p>The only model to use tokenization, Trajeglish produced the most realistic outcomes, according to Waymo\u2019s parameters. Qualitatively, Trajeglish dramatically improved performance in scenarios with dense interaction between agents, such as traffic jams, merging scenarios, and four-way stop intersections.</p>\n\n\n\n<p>The Waymo leaderboard evaluated three categories in each simulation: kinematics (such as speed), interactions, or distance to the nearest vehicle, and whether the trajectory remained in the drivable area. Overall realism was a weighted average across these categories.&nbsp;</p>\n\n\n\n<p>According to these parameters, Trajeglish improved over the previous state-of-the-art model in overall realism of scenarios by 3.3% and topped the interaction component by 9.9%.</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1959\" height=\"1120\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim.png\" alt=\"Bar charts comparing overall realism and interaction realism, with Trajeglish displaying the highest results of the top 10 models.\" class=\"wp-image-75460\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim.png 1959w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-300x172.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-625x357.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-768x439.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-1536x878.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-645x369.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-500x286.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-157x90.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-362x207.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-192x110.png 192w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-results-Waymo-Sim-1024x585.png 1024w\" sizes=\"(max-width: 1959px) 100vw, 1959px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Trajeglish results compared with other entrants in the Waymo Sim Agents Challenge. Submissions using ensembling are marked with asterisks.</em></figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\">Conclusion</h2>\n\n\n\n<p>Human driving behavior can be incredibly nuanced, posing a significant challenge to recreating it in simulation. However, by taking a page from language modeling, which deals with similar complexities in human language, the task becomes more manageable.</p>\n\n\n\n<p>As a result, AV developers can leverage higher fidelity traffic models in simulation to accelerate training, testing, and validation.</p>\n\n\n\n<p>To learn more, read the full paper and <a href=\"https://research.nvidia.com/labs/toronto-ai/trajeglish/\">Trajeglish: Learning the Language of Driving Scenarios</a> project page.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Much of the communication between drivers goes beyond turn signals and brake lights. Motioning another car to proceed, looking over to see if another driver is paying attention\u2014even the friendly Jeep wave\u2014all rely on human-based communication rather than vehicle technology. As autonomous vehicles (AV) must coexist with human drivers for the foreseeable future, they must &hellip; <a href=\"https://developer.nvidia.com/blog/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/\">Continued</a></p>\n", "protected": false}, "author": 819, "featured_media": 75739, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1318763", "discourse_permalink": "https://forums.developer.nvidia.com/t/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/276503", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [2892, 3366, 1958, 1962], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Trajeglish-featured-b.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jCH", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75435"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/819"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75435"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75435/revisions"}], "predecessor-version": [{"id": 75809, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75435/revisions/75809"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75739"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75435"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75435"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75435"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 75743, "date": "2023-12-18T11:44:42", "date_gmt": "2023-12-18T19:44:42", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=75743"}, "modified": "2023-12-29T16:29:51", "modified_gmt": "2023-12-30T00:29:51", "slug": "rag-101-retrieval-augmented-generation-questions-answered", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/", "title": {"rendered": "RAG 101: Retrieval-Augmented Generation Questions Answered"}, "content": {"rendered": "<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\n<p>Data scientists, AI engineers, MLOps engineers, and IT infrastructure professionals must consider a variety of factors when designing and deploying a RAG pipeline: from core components like LLM to evaluation approaches.&nbsp;</p>\n\n\n\n<p>The key point is that RAG is a system, not just a model or set of models. This system consists of several stages, which were discussed at a high level in <a href=\"https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/\">RAG 101: Demystifying Retrieval-Augmented Generation Pipelines</a>. All these stages provide opportunities to make design decisions according to your needs.&nbsp;</p>\n\n\n\n<p>Here\u2019s a list of top questions and answers.</p>\n\n\n\n<h2 class=\"wp-block-heading\">When should you fine-tune the LLM vs. using RAG?&nbsp;</h2>\n\n\n\n<p>In the world of LLMs, choosing between fine-tuning, Parameter-Efficient Fine-Tuning (PEFT), prompt engineering, and retrieval-augmented generation (RAG) depends on the specific needs and constraints of your application.</p>\n\n\n\n<ul>\n<li><em>Fine-tuning</em> customizes a pretrained LLM for a specific domain by updating most or all of its parameters with a domain-specific dataset. This approach is resource-intensive but yields high accuracy for specialized use cases.</li>\n\n\n\n<li><em>PEFT</em><strong> </strong>modifies a pretrained LLM with fewer parameter updates, focusing on a subset of the model. It strikes a balance between accuracy and resource usage, offering improvements over prompt engineering with manageable data and computational demands.</li>\n\n\n\n<li><em>Prompt engineering</em> manipulates the input to an LLM to steer its output, without altering the model&#8217;s parameters. It&#8217;s the least resource-intensive method, suitable for applications with limited data and computational resources.</li>\n\n\n\n<li>RAG<strong> </strong>enhances LLM prompts with information from external databases, effectively a sophisticated form of prompt engineering.</li>\n</ul>\n\n\n\n<p>It\u2019s not about using one technique or another. In fact, these techniques can be used in tandem. For example, PEFT might be integrated into a RAG system for further refinement of the LLM or embedding model. The best approach depends on the application&#8217;s specific requirements, balancing accuracy, resource availability, and computational constraints.</p>\n\n\n\n<p>For more information about customization techniques that you can use to improve domain-specific accuracy, see <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">Selecting Large Language Model Customization Techniques</a>.\u00a0</p>\n\n\n\n<p>When building an application with LLMs, start by implementing RAG to enhance the model&#8217;s responses with external information. This approach quickly improves relevance and depth. </p>\n\n\n\n<p>Later, model customization techniques as outlined earlier, can be employed if you need more domain-specific accuracy. This two-step process balances quick deployment with RAG and targeted improvements through model customization with efficient development and continuous enhancement strategies.</p>\n\n\n\n<h2 class=\"wp-block-heading\">How to increase RAG accuracy without fine-tuning?</h2>\n\n\n\n<p>This question deserves not just its own post but several posts. In short, obtaining accuracy in enterprise solutions that leverage RAG is crucial, and fine-tuning is just one technique that may (or may not) improve accuracy in a RAG system.</p>\n\n\n\n<p>First and foremost, find a way to measure your RAG accuracy. If you don\u2019t know where you\u2019re beginning, you won\u2019t know how to improve. There are several frameworks for evaluating RAG systems, such as <a href=\"https://github.com/explodinggradients/ragas\">Ragas</a>, <a href=\"https://github.com/stanford-futuredata/ares\">ARES</a>, and <a href=\"https://github.com/arthur-ai/bench\">Bench</a>.</p>\n\n\n\n<p>After you have done some evaluation for accuracy, there are numerous places to look to improve the accuracy that does not require fine-tuning.&nbsp;</p>\n\n\n\n<p>Although it may sound trivial, first check to make sure that your data is being parsed and loaded correctly in the first place. For example, if documents contain tables or even images, certain data loaders may miss information in documents.&nbsp;</p>\n\n\n\n<p>After data is ingested, it is <em>chunked</em>. This is the process of splitting text into segments. A chunk can be a fixed character length, but there are various chunking methods, such as sentence splitting and recursive chunking. How text is chunked determines how it is stored in an embedding vector for retrieval.&nbsp;</p>\n\n\n\n<p>On top of this, there are many indexing and associated retrieval patterns. For example, several indexes can be constructed for various kinds of user questions and a user query can be routed according to an LLM to the appropriate index.&nbsp;</p>\n\n\n\n<p>There are also a variety of retrieval strategies. The most rudimentary strategy is using cosine similarity with an index, but BM25, custom retrievers, or knowledge graphs can also improve the retrieval.&nbsp;</p>\n\n\n\n<p>Reranking of results from the retriever can also provide additional flexibility and accuracy improvements according to unique requirements. Query transformations can work well to break down more complex questions. Even just changing the LLM\u2019s system prompt can drastically change accuracy.&nbsp;</p>\n\n\n\n<p>At the end of the day, it\u2019s important to take time to experiment and measure the changes in accuracy that various approaches provide.&nbsp;</p>\n\n\n\n<p>Remember, models like the LLM or embedding model are merely a part<em> </em>of a RAG system. There are many ways to improve RAG systems to achieve high accuracy without doing any fine-tuning.&nbsp;&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">How to connect LLMs to data sources?</h2>\n\n\n\n<p>There are a variety of frameworks for connecting LLMs to your data sources, such as LangChain and LlamaIndex. These frameworks provide a variety of features, like evaluation libraries, document loaders, and query methods. New solutions are also coming out all the time. We recommend reading about various frameworks and picking the software and components of the software that make the most sense for your application.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Can RAG cite references for the data that it retrieves?</h2>\n\n\n\n<p>Yes. In fact, it improves the user experience if you can cite references for retrieved data. In the AI chatbot RAG workflow example found in the <a href=\"https://nvda.ws/41gNtfJ\">/NVIDIA/GenerativeAIExamples</a> GitHub repo, we show how to link back to source documents.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What type of data is needed for RAG? How to secure data?</h2>\n\n\n\n<p>Right now, textual data is well supported for RAG. Support in RAG systems for other forms of data like images and tables is improving as more research into multi-modal use cases progresses. You may have to write additional tools for data preprocessing depending on your data and where it\u2019s located. There are a variety of data loaders available from <a href=\"https://llamahub.ai/\">LlamaHub</a> and <a href=\"https://python.langchain.com/docs/integrations/document_loaders/\">LangChain</a>. For more information about building enriched pipelines with chains, see <a href=\"https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/\">Security LLM Systems Against Prompt Injection</a>.</p>\n\n\n\n<p>Securing data, particularly for an enterprise, is paramount. For example, some indexed data may be intended for only a particular set of users. Role-based access control (RBAC), which restricts access to a system depending on roles, can provide data access control. For example, a user session token can be used in the request to the vector database so that information that\u2019s out of scope for that user\u2019s permissions is not returned.&nbsp;&nbsp;</p>\n\n\n\n<p>A lot of the terms for securing a model in the environment are the same as you might use for securing a database or other critical asset. Think about how your system will log activities\u2014the prompt inputs, outputs, and error results\u2014that are the results of production pipelines. These may provide a rich set of data for product training and improvement, but also a source of data leaks like PII that must be carefully managed just as you are managing the model pipelines themselves.&nbsp;&nbsp;</p>\n\n\n\n<p>AI models have many common patterns to cloud deployments. You should take every advantage of tools like RBAC, rate limiting, and other controls common in those environments to make your AI deployments more robust. Models are just one element of these powerful pipelines. For more information, see <a href=\"https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/\">Best Practices for Securing LLM Enabled Applications</a></p>\n\n\n\n<p>One aspect important in any LLM deployment is the nature of interaction with your end users. So much of RAG pipelines are centered on the natural language inputs and outputs. Consider ways to ensure that the experience meets consistent expectations through input/output moderation.&nbsp;</p>\n\n\n\n<p>People can ask questions in many different ways. You can give your LLM a helping hand through tools like <a href=\"https://github.com/NVIDIA/NeMo-Guardrails\">NeMo Guardrails</a>, which can provide secondary checks on inputs and outputs to ensure that your system runs in tip-top shape, addresses questions it was built for, and helpfully guides users elsewhere for questions that the LLM application isn\u2019t built to handle.</p>\n\n\n\n<h2 class=\"wp-block-heading\">How to accelerate a RAG pipeline?</h2>\n\n\n\n<p>RAG systems consist of many components, so there are ample opportunities to accelerate a RAG pipeline:</p>\n\n\n\n<ul>\n<li>Data preprocessing</li>\n\n\n\n<li>Indexing and retrieval</li>\n\n\n\n<li>LLM inference&nbsp;</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\">Data preprocessing</h3>\n\n\n\n<p>Deduplication is the process of identifying and removing duplicate data. In the context of RAG data preprocessing, deduplication can be used to reduce the number of identical documents that must be indexed for retrieval.&nbsp;</p>\n\n\n\n<p><a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/modelguide/pretrainingdatasets/index.html\">NVIDIA NeMo Data Curator</a> uses NVIDIA GPUs to accelerate deduplication by performing min hashing, Jaccard similarity computing, and connected component analysis in parallel. This can significantly reduce the amount of time it takes to deduplicate a large dataset.&nbsp;</p>\n\n\n\n<p>Another opportunity is chunking. Dividing a large text corpus into smaller, more manageable chunks must be done because the downstream embedding model can only encode sentences below the maximum length. Popular embedding models such as OpenAI can encode up to 1536 tokens. If the text has more tokens, it is simply truncated.&nbsp;</p>\n\n\n\n<p><a href=\"https://docs.rapids.ai/api/cudf/stable/\" data-type=\"link\" data-id=\"https://docs.rapids.ai/api/cudf/stable/\">NVIDIA cuDF</a> can be used to accelerate chunking by performing parallel data frame operations on the GPU. This can significantly reduce the amount of time required to chunk a large corpus.</p>\n\n\n\n<p>Lastly, you can accelerate a tokenizer on the GPU. Tokenizers are responsible for converting text into integers as tokens, which are then used by the embedding model. The process of tokenizing text can be computationally expensive, especially for large datasets.</p>\n\n\n\n<h3 class=\"wp-block-heading\">Indexing and retrieval</h3>\n\n\n\n<p>The generation of embeddings is frequently a recurring process since RAG is well-suited for knowledge bases that are frequently updated. Retrieval is done at inference time, so low latency is a requirement. These processes can be accelerated by <a href=\"https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/\" data-type=\"link\" data-id=\"https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/\">NVIDIA NeMo Retriever</a>. NeMo Retriever aims to provide state-of-the-art, commercially ready models and microservices, optimized for the lowest latency and highest throughput.</p>\n\n\n\n<h3 class=\"wp-block-heading\">LLM inference</h3>\n\n\n\n<p>At a minimum, an LLM is used for the generation of a fully formed response. LLMs can also be used for tasks such as query decomposition and routing.&nbsp;</p>\n\n\n\n<p>With several calls to an LLM, low latency for the LLM is crucial. NVIDIA NeMo includes <a href=\"https://developer.nvidia.com/tensorrt\" data-type=\"link\" data-id=\"https://developer.nvidia.com/tensorrt\">TensorRT-LLM</a> for model deployment, which optimizes the LLM to achieve both ground-breaking inference acceleration and GPU efficiency.&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/triton-inference-server\" data-type=\"link\" data-id=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> also enables the optimized LLM to be deployed for high-performance, cost-effective, and low-latency inference.</p>\n\n\n\n<h2 class=\"wp-block-heading\">What are solutions to improve latency for a chatbot?</h2>\n\n\n\n<p>Beyond using the suggestions to accelerate your RAG pipeline like NeMo Retriever and NeMo inference container with Triton Inference Server and TensorRT-LLM, it is important to consider using streaming to improve the perceived latency of the chatbot. As responses can be long, a streaming UI displaying parts of the response as they become available can mitigate \u200cperceived latency.&nbsp;</p>\n\n\n\n<p>It may be worthwhile to consider using a smaller LLM that is fine-tuned for your use case. In general, smaller LLMs have much lower latency than larger LLMs.&nbsp;</p>\n\n\n\n<p>Some fine-tuned 7B models <a href=\"https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications\">have demonstrated</a> out-performing the accuracy of GPT-4 on specific tasks, for example, SQL generation. For example, <a href=\"https://blogs.nvidia.com/blog/llm-semiconductors-chip-nemo/\">ChipNeMo</a>, a custom LLM built for internal use at NVIDIA to help engineers generate and optimize software for chip design, uses a 13B fine-tuned model instead of a 70B-parameter model. TensorRT-LLM delivers model optimizations such as flashing, FlashAttention, PagedAttention, Distillation, and Quantization for running smaller fine-tuned models locally, which can be used to decrease the memory used by the LLM.&nbsp;</p>\n\n\n\n<p>Latency for LLM responses is a function of the time to first token (TTFT) and the time per output token (TPOT).&nbsp;</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=latency+%3D+TTFT+%2B+TPOT%28number%5C_of%5C_tokens%5C_to%5C_generate%29&#038;bg=transparent&#038;fg=000&#038;s=1&#038;c=20201002\" alt=\"latency = TTFT + TPOT(number&#92;_of&#92;_tokens&#92;_to&#92;_generate)\" class=\"latex\" /></p>\n\n\n\n<p>Both TTFT and TPOT will be lower for a smaller LLM.&nbsp;</p>\n\n\n\n<h2 class=\"wp-block-heading\">Get started building RAG in your enterprise</h2>\n\n\n\n<p>By using RAG, you can provide up-to-date and proprietary information with ease to LLMs and build a system that increases user trust, improves user experiences, and reduces hallucinations.</p>\n\n\n\n<p>Explore the NVIDIA <a href=\"https://nvda.ws/47OvlMU\">AI chatbot RAG workflow</a> to get started building a chatbot that can accurately answer domain-specific questions in natural language using up-to-date information.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Data scientists, AI engineers, MLOps engineers, and IT infrastructure professionals must consider a variety of factors when designing and deploying a RAG pipeline: from core components like LLM to evaluation approaches.&nbsp; The key point is that RAG is a system, not just a model or set of models. This system consists of several stages, which &hellip; <a href=\"https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/\">Continued</a></p>\n", "protected": false}, "author": 1973, "featured_media": 75788, "comment_status": "open", "ping_status": "open", "sticky": false, "template": "", "format": "standard", "meta": {"publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1318760", "discourse_permalink": "https://forums.developer.nvidia.com/t/rag-101-retrieval-augmented-generation-questions-answered/276501", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [2932], "acf": [], "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/retrieval-augmented-generation-questions-answered.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-jHF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75743"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1973"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75743"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75743/revisions"}], "predecessor-version": [{"id": 76096, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75743/revisions/76096"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75788"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75743"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75743"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75743"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]