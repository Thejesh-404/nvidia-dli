{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 03: Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Lesson 03! Now that we have set up our retriever and can get search results through API calls, we could present the results in many ways. We could hook into an existing tool that consumes APIs or build something custom using a framework designed for easy web apps like Streamlit. Even just dumping results into a spreadsheet and visualizing it can help evaluate search results. \n",
    "\n",
    "To validate our system, however, we need to record feedback! While we might start with qualitative feedback from manually observing the results of a few searches, we'll need to start recording the feedback data in order to make quantitative assessments of our RAG system's effectiveness and iterate on the design.\n",
    "\n",
    "Certainly, you'll often be able to tell with just a few starter searches if something is drastically wrong, so getting that kind of qualitative feedback first is valuable. Building effective RAG systems requires rapid iterative prototyping, so it's good to use a small test dataset for these trials--like our relatively tiny dataset of just a few hundred blog articles for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will focus on search, the LLM, and the judge UI and database.**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/03_overview.png\" width=\"850\" alt=\"architecture diagram showing the search, judge UI/database, and LLM highlighted\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're staring this lesson with all your services in the correct state, please restart them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FastAPI as a backend and Vue.js as a frontend, we've custom built a Judge UI for searching different asset types via semantic or keyword search. This is essentially just a bit of wiring that is making very simple REST calls to the router. Via SQLAlchemy, we've integrated FastAPI with a little backend SQLite database to store the feedback we generate by voting on results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already launched the `judge` service in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose logs judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `judge` service is available on port 5007. Execute the following cell to generate a link to open it in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var host = window.location.host;\n",
    "var url = 'http://'+host+':5007';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open the judge UI.</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how it lets you choose what type of search, switch some of the basic parameters, and select which asset type you are searching."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Judge UI, compare the results from searching through the summarization chunks vs. the standard chunking. Standard chunks are much more granular and better suited for very specific Q&A tasks. Summarized chunks are better for discovering content you may want to then dive in deeper and read. \n",
    "\n",
    "With standard chunking, the chunks are ordered by score, so sometimes you get multiple sections from the same document; in an app UI, you might organize those search results together so users still see multiple relevant documents in their results.\n",
    "\n",
    "**For the remainder of this notebook we're going to focus on our summarization method of chunking, so we can leave that asset type checked and uncheck the other one in the UI.**\n",
    "\n",
    "We're doing so because the summarization method produces fewer overall chunks, which makes evaluation faster for the purposes of this introductory notebook. However, the same principles would apply for standard chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and keyword search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how some searches are more suited to semantic search and some to keyword search? Imagine the use case where we want to search for a product name, and in particular a relatively new product that may not be in the training data of our embedding model.\n",
    "\n",
    "Search for `H200` using semantic search and note how some documents mismatch on H20.ai, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting with the Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's vote. First let's evaluate how semantic search does on this query.\n",
    "\n",
    "1. Make sure we're in Semantic Search mode.\n",
    "2. Type `H200` in the search box.\n",
    "3. Make sure only \"TechBlog Posts Summaries\" is checked in Asset Types.\n",
    "4. Vote thumbs up or down on the 10 results. This populates our SQLite database.\n",
    "\n",
    "Now let's evaluate how keyword search does on this query.\n",
    "1. Switch to Keyword Search mode.\n",
    "2. Type `*H200` in the search box. **Include the asterisk so that we get wildcard matches on the closely-related GH200.**\n",
    "3. Make sure only \"TechBlog Posts Summaries\" is checked in Asset Types.\n",
    "4. Vote thumbs up or down on the 10 results. This populates our SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Votes into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite DBs are each a single file, which we can easily query with Python and load into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_db_filepath = os.path.abspath(os.path.join(os.getcwd(), \"db\", \"sql_app.db\"))\n",
    "sql_db_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def select_all_feedback() -> pd.DataFrame: \n",
    "    # Read sqlite query results into a pandas DataFrame\n",
    "    con = sqlite3.connect(sql_db_filepath)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM feedback\", con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "df = select_all_feedback()\n",
    "\n",
    "# Verify that result of SQL query is stored in the dataframe\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in most data science evaluation tasks, we are interested in precision and recall. Remembering the definitions of each:\n",
    "\n",
    "- **Precision:** Total number of *relevant* documents retrieved / Total number of documents retrieved.\n",
    "- **Recall:** Total number of relevant documents *retrieved* / Total number of relevant documents *in the database*.\n",
    "\n",
    "In retrievers, those scores are typically calculated with the system set for some arbitrary number of results K (indicated as Precision@K, Recall@K).\n",
    "\n",
    "You may see variations on these like Mean Average Precision, F1 score (F1@K), or other rank-based metrics like Mean Reciprocal Rank (MRR) or Normalized Cumulative Discounted Gain (NCDG). We'll just focus on Precision and Recall in this course.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/precision-recall.png\" width=\"600\" alt=\"Precision Recall\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We voted on the top 5 results for these two different methods, so we can calculate Precision@5 just by doing a bit of `groupby` magic with pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first filter to all the feedback we put in manually (a.k.a. human feedback)\n",
    "hf = df[df['username'] != 'llmjudge']\n",
    "\n",
    "# Next transform query column so that it strips wildcards and lowercases everything\n",
    "hf[\"query\"] = hf[\"query\"].str.replace(\"*\", \"\").str.lower()\n",
    "\n",
    "print(hf.shape)\n",
    "print(hf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hf.groupby([\"query\", \"search_type\", \"asset_type\"]).aggregate(precision=(\"vote_value\", \"mean\")).reset_index(drop=False)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our single \"H200\" query, keyword search of the summarization chunks had a precision of 1.0, and semantic search of the summarization chunks had a precision of 0.6.\n",
    "\n",
    "We would then repeat this for not just a single query, but multiple queries across our domain of interest, averaging our precision values across those queries. We would then repeat this process with other chunking strategies for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is a trickier metric to evaluate than precision because our denominator is the total number of relevant documents in the entire database--and finding relevant documents automatically is the point of the system we are trying to build!\n",
    "\n",
    "If we were to brute force this with human feedback through the UI, it would take a long time, especially as the database grows in size--a good reminder is to do initial evaluation on a subset of the dataset.\n",
    "\n",
    "We can make recall easier to calculate with workarounds. For example, we could try a number of different search types (both semantic and keyword) and chunking strategies with the same query, then assume (or test!) that those multiple strategies found most relevant documents.\n",
    "\n",
    "We've actually done that in our case on a small scale; ideally, we'd want to be more thorough, with additional strategies.\n",
    "\n",
    "First, we need to calculate the total number of unique document ids that are positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalpos = hf[hf[\"vote_value\"] == 1].groupby(\"query\").aggregate(totalpos=(\"chunk_id\", \"nunique\")).reset_index(drop=False)\n",
    "totalpos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we join this to our `hf` dataframe so we can calculate recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = pd.merge(hf, totalpos, on=[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_grouped = hf.groupby([\"query\", \"search_type\", \"asset_type\"]).aggregate(precision=(\"vote_value\", \"mean\"), recall=(\"vote_value\", \"sum\"), totalpos=(\"totalpos\", \"first\")).reset_index(drop=False)\n",
    "hf_grouped[\"recall\"] = hf_grouped[\"recall\"] / hf_grouped[\"totalpos\"]\n",
    "hf_grouped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have our recall! Or at least a plausible proxy for it, much faster to calculate than going through every document in the database manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM As A Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting human feedback is pretty important for evaluation. However, unless you have access to an automated way of continually collecting user preferences - as search engines track whether or not a user clicked on a search result - it will require a lot of dedicated person-hours to build up your database of feedback, especially as your database of document chunks scales.\n",
    "\n",
    "This leads to a natural follow-up question: can we ask a machine to evaluate whether a document is relevant or not? If so, we could solve the problem by just throwing compute at it.\n",
    "\n",
    "We are actually getting close to another important topic in information retrieval systems: **rerankers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-encoders vs Cross-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been using a \"bi-encoder\" to embed our documents. It's called a bi-encoder because we are calculating the output score from two embeddings--in our case, the user's query and the document text they are searching through. We are calculating the output score as a similarity between the embedding vectors, like cosine similarity.\n",
    "\n",
    "Typical rerankers use cross-encoders instead of bi-encoders. They are lightweight language models (compared to something like GPT). They take in two pieces of text at once and encode them jointly, spitting out a similarity score directly instead of producing two vectors and then calculating an output score. \n",
    "\n",
    "These models are usually more accurate than bi-encoders+cosine similarity because they can take into account interactions between the pieces of text. However, they don't scale as well as bi-encoders, because cross-encoders need to directly score each query-result pair.\n",
    "\n",
    "If we had a really good cross-encoder model, we could ask that model to grade each query against each document and calculate precision and recall through that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives for Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, training a good cross-encoder is challenging, in part because the datasets we would most like to use (broad search engine results, which are essentially human evaluations of query-passage matches!) are often proprietary. The best cross-encoders on HuggingFace, for instance often use the [MS MARCO](https://microsoft.github.io/msmarco/) dataset--an excellent search engine dataset with a license that restricts commercial use. \n",
    "\n",
    "In the absence of a good, readily-available cross-encoder, we can turn to a general purpose LLM like GPT, Llama 2, Mistral, etc. These are orders of magnitude larger and slower than a simple cross-encoder, but should be more powerful than the cross-encoder at determining relevance and therefore able to perform evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tech Blog Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need access to all the document summaries that we saved in Lesson 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the summaries from the json file\n",
    "with open(\"data/techblogs_summaries/saved.json\", \"r\") as f:\n",
    "    saved_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(saved_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = saved_summaries['https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/'][0]['text']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLM to Check Summary Relevance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send a request to Mixtral to determine if this text is relevant to the query \"H200\". Here we import a `ChatOpenAI` instance of our local NIM Mixtral 8x7B model configured and ready for use with LangChain from an [`llms` helper file](llms.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.nim_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Remote LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, instead of using our local model, you can also use either NVIDIA AI Foundation's Mixtral 8x7B model or OpenAI's gpt-3.5-turbo.\n",
    "\n",
    "For either of these 2 options you'll need an API key. For more details about NVIDIA AI Foundation and obtaining a free API key, see [the notebook *NVIDIA AI Foundation.ipynb*](./NVIDIA%20AI%20Foundation.ipynb).\n",
    "\n",
    "After obtaining an appropriate API key, uncomment the appropriate cell below, add your API key, and run the cell to set `llm` to the remote LLM you chose to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA AI Foundation Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('NVIDIA_API_KEY', '<your_nvidia_api_key>')\n",
    "# llm = llms.nvai_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('OPENAI_API_KEY', '<your_openai_api_key>')\n",
    "# llm = llms.openai_gpt3_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Initialize a semaphore object with a limit of 3.\n",
    "limit = asyncio.Semaphore(3)\n",
    "\n",
    "async def async_generate(llm, msg):\n",
    "    resp = await llm.agenerate([msg])\n",
    "    return resp.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI bot being used by NVIDIA to determine if a passage of text is relevant to a search query. \"\n",
    "            + \"All of the passages of text are in some way related to NVIDIA, so in order to be relevant it needs to be a strict match between the topic of the passage and the topic of the query. \"\n",
    "            + 'Format your output as a JSON object with a single boolean field \"relevant\". ',\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Is the following passage strictly relevant to a search query for \"{query}\"?\\nPassage: {passage}',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = []\n",
    "\n",
    "# truncating the list to limit to a subset of urls \n",
    "# this example is just to illustrate the point\n",
    "# the first 5 urls should be irrelevant to H200, the next 2 should be relevant \n",
    "urls = list(saved_summaries.keys())[0:5] + ['https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/', 'https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/']\n",
    "\n",
    "for url in urls:\n",
    "    title = saved_summaries[url][0]['document_title']\n",
    "    summary = saved_summaries[url][0]['text']\n",
    "    print(title)\n",
    "    print(summary)\n",
    "    print(\"-----\")\n",
    "    passage = title + \"\\n\" + summary\n",
    "    messages = template.format_messages(query=\"H200\", passage=passage)\n",
    "    batch_messages.append(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.generate(batch_messages)\n",
    "for gen in response.generations:\n",
    "    print(gen[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results can now be saved in a data structure and used as a proxy for ground truth when evaluating the retrieval results and calculating precision and recall.\n",
    "\n",
    "The code above can be easily modified to perform the full evaluation with an LLM as a judge. In that case, you may want to evaluate which LLM can most cost-effectively perform that task. This kind of high-intensity, sustained generation could be well-suited for a smaller model run in a batch job--and the smaller the model, the more easily you could also tune it to more closely mimic human evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "The final product of this course and accompanying notebook for Lesson 04 uses only semantic search, but hopefully you can see the potential effectiveness of combining the two methods together. \n",
    "\n",
    "How could you create a hybrid search that merged/re-ranked the results of both search types? We'll leave that for you to research and work on yourself after this course is over."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
