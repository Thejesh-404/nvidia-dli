{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 02: Loading the Vector/Document Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Lesson 02! In this section, we will focus on how to launch a database and search it with both semantic and keyword search.\n",
    "\n",
    "Like we mentioned in Lesson 00, our RAG system is comprised of modular and independently scalable services, each running in its own container--an architecture well-suited to deploying in a cloud environment. \n",
    "\n",
    "In addition to using `docker-compose`, which is particularly well suited to single node deployments like the environment you are working in today, public cloud providers include managed container orchestration services that help run this kind of architecture; popular examples include [Amazon Elastic Container Service (ECS)](https://aws.amazon.com/ecs/), [Azure Container Apps](https://azure.microsoft.com/en-us/products/container-apps/), and [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine).\n",
    "\n",
    "NVIDIA provides a related service, with a serverless API to deploy and manage AI workloads on GPUs, called [NVIDIA Cloud Functions (NVCF)](https://docs.nvidia.com/cloud-functions/user-guide/latest/cloud-function/overview.html).\n",
    "\n",
    "The NVCF API supports HTTP polling, HTTP streaming & gRPC. Deep learning models (including embedding functions and LLMs) are especially easy to prepare and serve on NVCF through [Triton Inference Server](https://developer.nvidia.com/triton-inference-server), but NVCF supports containers using other backends as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will focus on the embedder and hybrid search.**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/02_overview.png\" width=\"850\" alt=\"architecture diagram with the embedder and search highlighted\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're staring this lesson with all your services in the correct state, please restart them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving our Embedding Model with Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You deployed a NVIDIA Triton Inference Server in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker-compose logs triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA Triton Inference Server is open-source software for fast and scalable AI inference on both GPU and CPU, and it's the standard way we run inference at NVIDIA. For more information, see the [Triton Inference Server readme on GitHub](https://github.com/triton-inference-server/server#documentation).\n",
    "\n",
    "Triton:\n",
    "- supports models and code in Python, C++, TensorFlow 1.x and 2.x, PyTorch, ONNX, TensorRT, RAPIDS FIL (for XGBoost, Scikit-learn Random Forest, and LightGBM), and OpenVINO.\n",
    "- optimizes inference for multiple query types (real-time, batch, streaming) and also supports model ensembles.\n",
    "- works with NVIDIA GPUs and x86 & ARM CPUs, including models . \n",
    "- runs on scale-out cloud or data center, enterprise edge, and even on embedded devices like the NVIDIA Jetson, in both bare metal and virtualized environments (e.g. VMware vSphere), with dedicated NVIDIA Triton builds for running on Windows, Jetson, and ARM SBSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Embedding Model to TensorRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed our chunks in a vector database, we'll need an embedding model. As we mentioned in the previous lesson, we're using the `SentenceTransformers` framework with the `e5-large-unsupervised` embedding model. In order to further increase inference speed, we can convert the PyTorch model into a TensorRT engine file and then serve the TensorRT engine with Triton.\n",
    "\n",
    "We have already performed this conversion for you, and the model is already available in your running `triton` service, but to do it yourself, please refer to `triton/README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a code snippet to check that Triton is up and running (check that we get a 200 status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v http://triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed With Triton Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our `router` container's source code, there is a function that takes in text as strings, sends them to Triton Inference Server using the Triton Python client, and receives vectors of floating-point numbers in response. Here we look at that function and its supporting source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make the necessary imports for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "import tritonclient.http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define variables that will be used within our function.\n",
    "\n",
    "Note: `triton_host` in this environment is `triton`, but in your own environment, depending on how you run the `triton` container, this hostname may very well be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_host = \"triton\"\n",
    "triton_port = \"8000\"\n",
    "triton_model_name = \"transformer_tensorrt_inference\"\n",
    "triton_model_version = \"1\"\n",
    "\n",
    "triton_url = f\"{triton_host}:{triton_port}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_with_triton(query: List[str]) -> List[List[float]]:\n",
    "    triton_client = tritonclient.http.InferenceServerClient(\n",
    "        url=triton_url, verbose=False\n",
    "    )\n",
    "\n",
    "    triton_batch_size = len(query)\n",
    "    triton_inputs = []\n",
    "    triton_outputs = []\n",
    "    triton_text_input = tritonclient.http.InferInput(\n",
    "        name=\"TEXT\", shape=(triton_batch_size,), datatype=\"BYTES\"\n",
    "    )\n",
    "    triton_text_input.set_data_from_numpy(np.asarray(query, dtype=object))\n",
    "    triton_inputs.append(triton_text_input)\n",
    "    triton_outputs.append(\n",
    "        tritonclient.http.InferRequestedOutput(\"output\", binary_data=False)\n",
    "    )\n",
    "\n",
    "    inference_results = triton_client.infer(\n",
    "        model_name=triton_model_name,\n",
    "        model_version=triton_model_version,\n",
    "        inputs=triton_inputs,\n",
    "        outputs=triton_outputs,\n",
    "    )\n",
    "\n",
    "    embedded_query = inference_results.as_numpy(\"output\").tolist()\n",
    "    return embedded_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Embed With Triton Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when used in information retrieval, this embedding model works best when text queries start with the prefix \"query: \", and text documents start with the prefix \"passage: \", according to [the model card on HuggingFace](https://huggingface.co/intfloat/e5-large-unsupervised#faq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedded_query = embed_with_triton([\"query: deep learning\"])\n",
    "print(embedded_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list containing one string (in this case, a query) produces a list containing one 1024-dimensional vector of floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedded_query))\n",
    "print(len(embedded_query[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our embedding model set up, let's look at [Redis](https://redis.io/), an open-source database (among other things).\n",
    "\n",
    "We chose Redis as our database for a number of reasons.\n",
    "1. Redis is extremely fast, and we need to minimize latency for the operations it will be performing.\n",
    "2. Redis is well-supported and easy to deploy through a ready-to-go container.\n",
    "3. Redis supports both vector and keyword search: vector search through the relatively recent [RedisVL](https://github.com/RedisVentures/redisvl) project, and a fairly robust suite of [search and query features](https://redis.io/docs/interact/search-and-query/) for more traditional keyword search. Notably, Redis supports BM25, the default algorithm behind the popular Elasticsearch system--making it easy to transition smoothly between the two systems.\n",
    "4. Redis unifies our vector database with our document (and metadata) database, so we don't have to worry about maintaining keys in a separate index like [FAISS](https://faiss.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Redis Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You launched the Redis in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker-compose logs redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to use a `router` service whose job is to serve as an API entry point and route calls between the other three components: `chunking`, `triton`, and `redis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Router Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already launched the `router` service in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose logs router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Router Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the `chunking` service, `router` is also a FastAPI web application, with automatic documentation generation. Inside the `router` application, we use LangChain, which makes it easy to experiment by swapping out components like LLMs and prompts.\n",
    "\n",
    "The `router` service is available on port 5006. Execute the following cell to generate a link to open it in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var host = window.location.host;\n",
    "var url = 'http://'+host+':5006';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open router service API docs.</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we have endpoints for searching as well as data insert, delete and dump endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Redis Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can search, we need to fill our database with data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we'll use the asynchronous `httpx` library to load our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the asset types the router expects\n",
    "import httpx \n",
    "import json\n",
    "\n",
    "response = httpx.get(\"http://router:5006/asset-types\")\n",
    "asset_types_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(asset_types_json, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first upload the chunks we got by breaking each article into sentence groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data', 'techblogs')\n",
    "file_list = [x for x in sorted(os.listdir(data_dir)) if \".json\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    with open(os.path.join(data_dir, filename), \"r\") as in_file:\n",
    "        data = json.load(in_file)\n",
    "    for item in data:\n",
    "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
    "        if not item[\"link\"].startswith(\n",
    "            \"https://developer.nvidia.com/blog\"\n",
    "        ):  # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
    "            # print(f\"Skipping URL {item['link']}\")\n",
    "            continue\n",
    "        document_title = item[\"title\"][\"rendered\"]\n",
    "        document_url = item[\"link\"]\n",
    "        document_html = item[\"content\"][\"rendered\"]\n",
    "        document_date = item[\"date_gmt\"]\n",
    "        document_date_modified = item[\"modified_gmt\"]\n",
    "        payloads.append(\n",
    "            {\n",
    "                \"strategy\": \"heading_section_sentence\",\n",
    "                \"code_behavior\": \"remove_code_sections\",\n",
    "                \"chunk_min_words\": 250,\n",
    "                \"chunk_overlap_words\": 50,\n",
    "                \"input_type\": \"html\",\n",
    "                \"input_str\": document_html,\n",
    "                \"additional_metadata\": {\n",
    "                    \"document_title\": document_title,\n",
    "                    \"document_url\": document_url,\n",
    "                    \"document_date\": document_date,\n",
    "                    \"document_date_modified\": document_date_modified,\n",
    "                },\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total num payloads: {len(payloads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payloads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_url = \"http://chunking:5005/api/chunking\"\n",
    "existing_items_url = \"http://router:5006/search/keyword\"\n",
    "delete_url = \"http://router:5006/data/delete\"\n",
    "insert_url = \"http://router:5006/data/insert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a semaphore object with a limit of 3.\n",
    "limit = asyncio.Semaphore(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk up an article\n",
    "async def chunking_request(client: httpx.AsyncClient, payload: dict):\n",
    "    chunking_resp = await client.post(chunking_url, json=payload, timeout=15)\n",
    "    return chunking_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if any chunks already exist in the db that match this document url\n",
    "async def get_existing_items_request(client: httpx.AsyncClient, payload: dict, asset_type: str):\n",
    "    existing_items_resp = await client.post(\n",
    "        existing_items_url,\n",
    "        json={\n",
    "            \"field\": \"document_url\",\n",
    "            \"value\": payload[\"additional_metadata\"][\"document_url\"],\n",
    "            \"asset_types\": [asset_type],\n",
    "            \"search_type\": \"exact\",\n",
    "            \"k\": 1000,  # some large number to ensure we don't hit default limit of 10\n",
    "        },\n",
    "        timeout=15,\n",
    "    )\n",
    "    return existing_items_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete items with certain ids\n",
    "async def delete_request(client: httpx.AsyncClient, results: list, asset_type: str):\n",
    "    delete_resp = await client.post(\n",
    "        delete_url,\n",
    "        json={\n",
    "            \"asset_type\": asset_type,\n",
    "            \"ids\": [x[\"id\"] for x in results],\n",
    "        },\n",
    "        timeout=15,\n",
    "    )\n",
    "    print(delete_resp.status_code)\n",
    "    return delete_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def upload_techblogs_chunks(client: httpx.AsyncClient, payload: dict):\n",
    "    async with limit:\n",
    "        try:\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        except:  # retry once\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        print(\n",
    "            f\"{payload['additional_metadata']['document_url']} | num chunks: {len(chunks)}\"\n",
    "        )\n",
    "\n",
    "        # gets ids of existing items with this url\n",
    "        try:\n",
    "            existing_items = await get_existing_items_request(client, payload, \"techblogs\")\n",
    "        except:  # retry once\n",
    "            existing_items = await get_existing_items_request(client, payload, \"techblogs\")\n",
    "\n",
    "        if len(existing_items) > 0:\n",
    "            results = existing_items[0][\"results\"]\n",
    "            if len(results) > 0:\n",
    "                # delete items that are associated with this url\n",
    "                try:\n",
    "                    deleted_items = await delete_request(client, results, \"techblogs\")\n",
    "                except:  # retry once\n",
    "                    deleted_items = await delete_request(client, results, \"techblogs\")\n",
    "                print(f\"Deleted ids reponse: {deleted_items}\")\n",
    "\n",
    "        # insert: send chunks to redis\n",
    "        resp = await client.post(\n",
    "            insert_url,\n",
    "            json={\n",
    "                \"asset_type\": \"techblogs\",\n",
    "                \"chunks\": chunks,\n",
    "            },\n",
    "            timeout=15,\n",
    "        )\n",
    "        print(f\"Inserted {len(resp.json())} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for payload in payloads:\n",
    "            tasks.append(upload_techblogs_chunks(client, payload))\n",
    "\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# If this were not in Jupyter we would run this\n",
    "# asyncio.run(main())\n",
    "\n",
    "# Since we are in a notebook, Jupyter is already running its own event loop\n",
    "# so we can just simply await main()\n",
    "await main()\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"Took {end - start} seconds\")\n",
    "\n",
    "# This should take around 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techblogs_assettype = None\n",
    "\n",
    "for assettype in asset_types_json:\n",
    "    if assettype[\"name\"] ==\"techblogs\":\n",
    "        techblogs_assettype = assettype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(techblogs_assettype, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to update the `assettypes` redis index with some metadata about how the `techblogs` index was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techblogs_assettype[\"chunking_params\"] = json.dumps(\n",
    "    {\n",
    "        \"strategy\": \"heading_section_sentence\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "        \"chunk_min_words\": 250,\n",
    "        \"chunk_overlap_words\": 50,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(techblogs_assettype, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_asset_types_url = \"http://router:5006/asset-types/update\"\n",
    "response = httpx.post(update_asset_types_url, json={\"data\": techblogs_assettype})\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our database by hitting the `/data/dump` router endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_response = httpx.post(\"http://router:5006/data/dump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(dump_response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the Redis database was indeed saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis \n",
    "\n",
    "r = redis.Redis(host='redis', port=6379)\n",
    "r.lastsave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we wanted to drop an index\n",
    "\n",
    "# from langchain.vectorstores.redis import Redis\n",
    "\n",
    "# Redis.drop_index(\n",
    "#     index_name=\"assettypes\", delete_documents=True, redis_url=\"redis://localhost:6379\"\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can head back to our API docs and confirm that our data are available. Try the semantic search examples. Use the dropdown to select the second example: semantic search. Set k and asset_types to non-default values.\n",
    "\n",
    "Next, try the keyword search example, but modify the query to `cell phone`. The literal phrase `cell phone` doesn't return any articles, but the phrase `mobile phone` does. In cases where the concept is more important than the exact words, semantic search can help.\n",
    "\n",
    "If we go back to the semantic search and modify the third example to try `cell phone`, we do get relevant results. The second result should be good. The first result talks about pixels and cameras--both associated with cell phones, by themselves, and also through the Google Pixel cell phone. If you see many cases like this where words' multiple meanings throw off your domain-specific search interests, you can finetune the embedding model to prefer domain-related matches to general matches.\n",
    "\n",
    "Conversely, let's try a search for a specific product name that has no general-language meaning, like `H200`. Here is where a keyword search makes more sense, because we want the exact product name and not the meaning of \"H\" and \"200\" (try in particular wildcard search: `*H200`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Search\n",
    "\n",
    "Now instead of using the docs, let's use Python and httpx to hit our `/search/semantic` endpoint. In this environment, the `router` service is available at the hostname `router`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_endpoint = \"http://router:5006/search/semantic\"\n",
    "\n",
    "response = httpx.post(\n",
    "    search_endpoint, json={\"query\": \"cgroups\", \"k\": 3, \"asset_types\": [\"techblogs\"]}\n",
    ")\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have some valuable metadata from our chunking service.\n",
    "\n",
    "- `text_components` contains a list of the sentences that were found in this section.\n",
    "- `contains_code` is a boolean that indicates whether code was in that particular sentence.\n",
    "- `only_code` is another boolean that indicates whether the entire sentence is comprised of code.\n",
    "\n",
    "Because we specified that `code_behavior` was `remove_code_sections` the `text` attribute of the items in the `results` in the response is essentially a concatenated string formed by the sentences that were not \n",
    "entirely made up of code (i.e., `only_code == False`). You will still see some small amount of code in the text (i.e., `contains_code == True and only_code == False`), but these are usually single words in \n",
    "a sentence of natural language.\n",
    "\n",
    "The advantage of this is, we can still go and extract the `only_code` sections because they are available through the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = response.json()[0]['results'][1]\n",
    "result1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, because of some constraints from how data structures can be stored in redis, we'll need to convert the JSON strings into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_section_index = json.loads(result1[\"heading_section_index\"])\n",
    "heading_section_title = json.loads(result1[\"heading_section_title\"])\n",
    "paragraph_index = json.loads(result1[\"paragraph_index\"])\n",
    "contains_code = json.loads(result1[\"contains_code\"])\n",
    "only_code = json.loads(result1[\"only_code\"])\n",
    "text_components = json.loads(result1[\"text_components\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(heading_section_index) == len(heading_section_title) == len(paragraph_index) == len(contains_code) == len(only_code) == len(text_components)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get all the text, including both natural language and code sections, and join it together as it appeared in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "last_hsi = None\n",
    "\n",
    "for i in range(len(text_components)):\n",
    "    if last_hsi is None or last_hsi != heading_section_index[i]:\n",
    "        text += heading_section_title[i] + \"\\n\"\n",
    "    text += text_components[i]\n",
    "    if only_code[i]:\n",
    "        text += \"\\n\"\n",
    "    else:\n",
    "        text += \" \"\n",
    "    # look ahead\n",
    "    if i < len(text_components) - 1:\n",
    "        if paragraph_index[i] != paragraph_index[i+1]:\n",
    "            text += \"\\n\"\n",
    "    \n",
    "    last_hsi = heading_section_index[i]\n",
    "\n",
    "print(text.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can extract exclusively the code and ignore the natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "for i in range(len(text_components)):    \n",
    "    if only_code[i]:\n",
    "        text += text_components[i]\n",
    "        text += \"\\n\"\n",
    "\n",
    "print(text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing TechBlog Summaries in Redis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our code from the previous lesson and use it to generate summaries for each article in our batch asynchronously.\n",
    "\n",
    "We will use an LLM to generate the summaries, so let's begin by instatiating an LLM instance to work with. Here we import a `ChatOpenAI` instance of our local NIM Mixtral 8x7B model configured and ready for use with LangChain from an [`llms` helper file](llms.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.nim_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Remote LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, instead of using our local model, you can also use either NVIDIA AI Foundation's Mixtral 8x7B model or OpenAI's gpt-3.5-turbo.\n",
    "\n",
    "For either of these 2 options you'll need an API key. For more details about NVIDIA AI Foundation and obtaining a free API key, see [the notebook *NVIDIA AI Foundation.ipynb*](./NVIDIA%20AI%20Foundation.ipynb).\n",
    "\n",
    "After obtaining an appropriate API key, uncomment the appropriate cell below, add your API key, and run the cell to set `llm` to the remote LLM you chose to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA AI Foundation Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('NVIDIA_API_KEY', '<your_nvidia_api_key>')\n",
    "# llm = llms.nvai_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('OPENAI_API_KEY', '<your_openai_api_key>')\n",
    "# llm = llms.openai_gpt3_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Tech Blog Payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll construct a payload for each techblog that contains the blog's HTML, along with various metadata fields, and chunking guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "\n",
    "file_list = [x for x in sorted(os.listdir(data_dir)) if '.json' in x]\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    with open(os.path.join(data_dir, filename), 'r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "        \n",
    "    for item in data:\n",
    "        \n",
    "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
    "        if not item['link'].startswith(\"https://developer.nvidia.com/blog\"): # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
    "            # print(f\"Skipping URL {item['link']}\")\n",
    "            continue\n",
    "            \n",
    "        document_title = item['title']['rendered']\n",
    "        document_url = item['link']\n",
    "        document_html = item['content']['rendered']\n",
    "        document_date = item['date_gmt']\n",
    "        document_date_modified = item['modified_gmt']\n",
    "        \n",
    "        payload = {\n",
    "            \"strategy\": \"heading_section\",\n",
    "            \"code_behavior\": \"remove_code_sections\",\n",
    "            \"input_type\": \"html\",\n",
    "            \"input_str\": document_html,\n",
    "            \"additional_metadata\": {\n",
    "                \"document_title\": document_title,\n",
    "                \"document_url\": document_url,\n",
    "                \"document_date\": document_date,\n",
    "                \"document_date_modified\": document_date_modified,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        payloads.append(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total num payloads: {len(payloads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payloads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [None] * len(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the summaries from the json file\n",
    "with open(\"data/techblogs_summaries/saved.json\", \"r\") as f:\n",
    "    saved_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a semaphore object with a limit of 3.\n",
    "limit = asyncio.Semaphore(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_generate(llm, msg):\n",
    "    resp = await llm.agenerate([msg])\n",
    "    return resp.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if we want to save new summaries.\n",
    "# saved_summaries = {}\n",
    "\n",
    "async def upload_techblogs_summaries(llm, client: httpx.AsyncClient, payload: dict):\n",
    "    async with limit:\n",
    "\n",
    "        try:\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        except:  # retry once\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        print(\n",
    "            f\"{payload['additional_metadata']['document_url']} | num chunks: {len(chunks)}\"\n",
    "        )\n",
    "\n",
    "        clean_text_no_code = \"\\n\".join([x[\"text\"] for x in chunks])\n",
    "        clean_text_with_code = \"\\n\".join([ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks])\n",
    "        \n",
    "        # Ask LLM for summaries\n",
    "\n",
    "        # uncomment if we want to save new summaries\n",
    "        # template = ChatPromptTemplate.from_messages(\n",
    "        #     [(\"user\", \"Summarize the following article in 200 words or less:\\n{user_input}\")]\n",
    "        # )\n",
    "\n",
    "        # msg = template.format_messages(\n",
    "        #     user_input=clean_text_no_code\n",
    "        # )\n",
    "\n",
    "        # summary = await async_generate(llm, msg)\n",
    "        # summary_with_metadata = [\n",
    "        #     {\n",
    "        #         \"text\": payload[\"additional_metadata\"][\"document_title\"] + \"\\n\" + summary,\n",
    "        #         \"text_components\": [ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks],\n",
    "        #         \"document_title\": payload[\"additional_metadata\"][\"document_title\"],\n",
    "        #         \"document_url\": payload[\"additional_metadata\"][\"document_url\"],\n",
    "        #         \"document_date\": payload[\"additional_metadata\"][\"document_date\"],\n",
    "        #         \"document_date_modified\": payload[\"additional_metadata\"][\"document_date_modified\"],\n",
    "        #         \"document_full_text\": clean_text_with_code\n",
    "        #     }\n",
    "        # ]\n",
    "        # saved_summaries[payload[\"additional_metadata\"][\"document_url\"]] = summary_with_metadata\n",
    "\n",
    "        # load summary we've already generated\n",
    "        # comment the following line if we want to save new summaries\n",
    "        summary_with_metadata = saved_summaries[payload[\"additional_metadata\"][\"document_url\"]]\n",
    "\n",
    "        # gets ids of existing items with this url\n",
    "        try:\n",
    "            existing_items = await get_existing_items_request(client, payload, \"summarize_techblogs\")\n",
    "        except:  # retry once\n",
    "            existing_items = await get_existing_items_request(client, payload, \"summarize_techblogs\")\n",
    "\n",
    "        if len(existing_items) > 0:\n",
    "            results = existing_items[0][\"results\"]\n",
    "            if len(results) > 0:\n",
    "                # delete items that are associated with this url\n",
    "                try:\n",
    "                    deleted_items = await delete_request(client, results, \"summarize_techblogs\")\n",
    "                except:  # retry once\n",
    "                    deleted_items = await delete_request(client, results, \"summarize_techblogs\")\n",
    "                print(f\"Deleted ids reponse: {deleted_items}\")\n",
    "\n",
    "        # insert: send chunks to redis\n",
    "        resp = await client.post(\n",
    "            insert_url,\n",
    "            json={\n",
    "                \"asset_type\": \"summarize_techblogs\",\n",
    "                \"chunks\": summary_with_metadata,\n",
    "            },\n",
    "            timeout=15,\n",
    "        )\n",
    "        print(f\"Inserted {len(resp.json())} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        # for i in range(0, 7):\n",
    "        for i in range(0, len(payloads)):\n",
    "            tasks.append(upload_techblogs_summaries(llm, client, payloads[i]))\n",
    "\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# If this were not in Jupyter we would run this\n",
    "# asyncio.run(main())\n",
    "\n",
    "# Since we are in a notebook, Jupyter is already running its own event loop\n",
    "# so we can just simply await main()\n",
    "await main()\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"Took {end - start} seconds\")\n",
    "\n",
    "# This should take around 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the summaries as a json file\n",
    "with open(\"data/techblogs_summaries/saved.json\", \"w\") as f:\n",
    "    json.dump(saved_summaries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(saved_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_summaries[payloads[0]['additional_metadata']['document_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techblogs_summaries_assettype = None\n",
    "\n",
    "for assettype in asset_types_json:\n",
    "    if assettype[\"name\"] ==\"summarize_techblogs\":\n",
    "        techblogs_summaries_assettype = assettype\n",
    "\n",
    "print(json.dumps(techblogs_summaries_assettype, indent=2))\n",
    "\n",
    "techblogs_summaries_assettype[\"chunking_params\"] = json.dumps(\n",
    "    {\n",
    "        \"strategy\": \"summarization\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(json.dumps(techblogs_summaries_assettype, indent=2))\n",
    "\n",
    "\n",
    "update_asset_types_url = \"http://router:5006/asset-types/update\"\n",
    "response = httpx.post(update_asset_types_url, json={\"data\": techblogs_summaries_assettype})\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "dump_response = httpx.post(\"http://router:5006/data/dump\")\n",
    "print(json.dumps(dump_response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "We now two indexes in Redis: `techblogs`, which contains chunks of roughly 250 words, and `summarize_techblogs` which contains the summaries written by ChatGPT.\n",
    "\n",
    "In the next notebook, we'll look at how we can evaluate the search results from these indexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
